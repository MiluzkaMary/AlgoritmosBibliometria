@article{CHANG201323,
title = {Discovering Taiwanese design college students’ learning performance and imaginative capacity},
journal = {Thinking Skills and Creativity},
volume = {10},
pages = {23-39},
year = {2013},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2013.05.005},
url = {https://www.sciencedirect.com/science/article/pii/S1871187113000266},
author = {Hsiang-Tang Chang and Tung-I. Lin},
keywords = {Imaginative capacity, Imagination, Learning performance, Design college, RASCH measurement},
abstract = {Imagination affects not only the structure of design ideas at the initial stage but also influences the manifestation of final products. The purpose of this study was to investigate the association between Taiwanese design college students’ imaginative capacity and their learning performance in class. On the basis of recent scholarship, the authors proposed several reasonably related factors, which were classified into three aspects: personality traits, learning atmosphere, and imaginative thinking. They then verified and discussed four research questions through a teaching experiment with 63 junior college students in YunTech, Taiwan. To proceed smoothly without significantly changing the current teaching process, the authors developed a set of supplementary teaching material and two sets of questionnaires which they then used in the teaching experiment. The results of the teaching experiment proved and suggested the following points corresponding to the research questions: (1) students’ senior high school backgrounds have an effect on their imaginative capacities; (2) judges from other schools should be invited to join the judgement to ensure fairness and with a broader scope; (3) students’ imaginative capacity indeed has an effect on the grade of their final products in the judgement; (4) teachers can identify students with higher imaginative capacity through the responses to the proposed supplementary teaching materials and questionnaires used in the study's curricula. Furthermore, the supplementary teaching material is conjectured to be able to inspire students’ imaginative capacity.}
}
@article{YIN2024133163,
title = {Mobileception-ResNet for transient stability prediction of novel power systems},
journal = {Energy},
volume = {309},
pages = {133163},
year = {2024},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2024.133163},
url = {https://www.sciencedirect.com/science/article/pii/S0360544224029384},
author = {Linfei Yin and Wei Ge},
keywords = {Transient stability, Deep learning, MobileNet-v2, Convolutional neural network, Inception-ResNet-v2},
abstract = {Power system transient stability prediction (TSP) is particularly important as power systems change and evolve, including the rapid growth of renewable energy, the proliferation of electric vehicles, and the construction of smart grids. Traditional time-domain simulation methods are time-consuming and cannot achieve online prediction. Direct methods are poorly adapted and cannot be applied to complex power systems. Existing machine learning algorithms only classify the transient stability without providing the degree of transient stability of the system. Therefore, a fast and accurate power system TSP method is needed to assist operators in implementing timely measures to improve the stability of the power system running. This study proposes a Mobileception-ResNet network, Mobileception-ResNet is formed by Inception-ResNet-v2, MobileNet-v2, and a fully connected layer. In this study, Mobileception-ResNet and nine comparison models are experimented on two node systems, i.e., the IEEE 10–39 and 69–300 systems. In the IEEE 10–39 system, the root mean square error, mean absolute error, and mean absolute percentage error of Mobileception-ResNet are 44.13 %, 36.74 %, and 39.96 % lower, and the coefficient of determination is 0.04 % higher, respectively, when compared to the comparative model with the best evaluation indicator; in the IEEE 69–300 system, the corresponding values are 2.6 %, 12.83 %, 12.55 %, and 0.01 %, respectively.}
}
@incollection{WANG2022238,
title = {1.10 - CyberGIS and Geospatial Data Science for Advancing Geomorphology},
editor = {John (Jack) F. Shroder},
booktitle = {Treatise on Geomorphology (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {Oxford},
pages = {238-259},
year = {2022},
isbn = {978-0-12-818235-2},
doi = {https://doi.org/10.1016/B978-0-12-818234-5.00122-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012818234500122X},
author = {Shaowen Wang and Michael P. Bishop and Zhe Zhang and Brennan W. Young and Zewei Xu},
keywords = {Artificial intelligence, CyberGIS, Deep learning, Geomorphology, Geospatial data science, Land cover science, LiDAR, Uncertainty},
abstract = {Theoretical and practical issues in geomorphology have not been adequately addressed due to a lack of formalization and digital representation of spatial and temporal concepts, given the limitations associated with modern-day geographic information systems (GIS). Rapid advancements in geospatial technologies have resulted in new sensors and large volumes of geospatial data that have yet to be fully exploited given a variety of computational issues. Computational limitations involving storage, preprocessing, analysis, and modeling pose significant problems for Earth scientists. Consequently, advanced cyberinfrastructure is required to address geospatial data-science issues involving communication, representation, computation, information production, decision-making, and geovisualization. We identify and discuss important aspects of exploiting advances in cyberinfrastructure that involve computational scalability, artificial intelligence, and uncertainty characterization and analysis for addressing issues in the Earth sciences. Such developments can be termed cyber geographic information science and systems (cyberGIS). We discuss this important topic by addressing the significant overlap of concepts in GIS and geomorphology that can be formalized, digitally represented, implemented, and evaluated with cyberGIS. We then introduce the fundamentals of cyberinfrastructure and cyberGIS, including a discussion of the utilization of artificial intelligence and deep learning. We finally provide one case study demonstrating operational cyberGIS capabilities.}
}
@article{KUO2013510,
title = {Cultural Evolution Algorithm for Global Optimizations and its Applications},
journal = {Journal of Applied Research and Technology},
volume = {11},
number = {4},
pages = {510-522},
year = {2013},
issn = {1665-6423},
doi = {https://doi.org/10.1016/S1665-6423(13)71558-X},
url = {https://www.sciencedirect.com/science/article/pii/S166564231371558X},
author = {H.C. Kuo and C.H. Lin},
keywords = {Cultural Algorithm, Genetic Algorithm, Nelder-Mead’s simplex method, Global optimization},
abstract = {The course of socio-cultural transition can neither be aimless nor arbitrary, instead it requires a clear direction. A common goal of social species’ evolution is to move towards an advanced spiritual and conscious state. This study aims to develop a population-based algorithm on the basis of cultural transition goal. In this paper, the socio-cultural model based on a system thought framework could be used to develop a cultural evolution algorithm (CEA). CEA leverage four strategies, each consists of several search methods with similar thinking. Seven benchmark functions are utilized to validate the search performance of the proposed algorithm. The results show that all of the four strategies of cultural evolution algorithm have better performance when compared with relevant literatures. Finally, the CEA was then applied to optimize two different reliability engineering problems, a Serial-Parallel System design and a Bridge System design. For the Serial-Parallel System design, the CEA achieved the exact solution with ease, and for the Bridge System design, the solution obtained by the CEA is superior to those from other literatures.}
}
@incollection{FREUND20151,
title = {Chapter 1 - Introduction},
editor = {Jack Freund and Jack Jones},
booktitle = {Measuring and Managing Information Risk},
publisher = {Butterworth-Heinemann},
address = {Boston},
pages = {1-11},
year = {2015},
isbn = {978-0-12-420231-3},
doi = {https://doi.org/10.1016/B978-0-12-420231-3.00001-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780124202313000014},
author = {Jack Freund and Jack Jones},
keywords = {Analysis, Assessment, Assumptions, Bald tire, Risk, Threat, Vulnerability},
abstract = {This chapter makes the case for the need for quantitative risk management. It begins with the Bald Tire thought experiment to help make the case for a need to articulate assumptions, discuss terminology, and makes plain the factors of risk that we care about modeling and how to communicate them effectively to management. This section also discusses the difference between risk assessment and risk analysis, and details the deficiencies in current approaches that treat the two the same. Lastly, the chapter spells out the progression of topics for the remainder of the book and offers some words of advice on how thinking about risk will impact your ability to make better decisions in all aspects of your life.}
}
@article{RUAN2023100872,
title = {Public perception of electric vehicles on Reddit and Twitter: A cross-platform analysis},
journal = {Transportation Research Interdisciplinary Perspectives},
volume = {21},
pages = {100872},
year = {2023},
issn = {2590-1982},
doi = {https://doi.org/10.1016/j.trip.2023.100872},
url = {https://www.sciencedirect.com/science/article/pii/S2590198223001197},
author = {Tao Ruan and Qin Lv},
keywords = {Cross-platform, Twitter, Reddit, Electric vehicles, Public perception, Topic modeling, Computational social science},
abstract = {Electrified mobility such as electric vehicles (EVs) is a promising solution to reduce carbon emissions in transportation and mitigate global warming. Understanding public perception of EVs can help better support their adoption. A previous study shows that online social networks (OSNs) such as Reddit can be a valuable source for studying public perceptions of EVs and provide different perspectives from traditional methods that leverage surveys, questionnaires, or interviews (Ruan and Lv, 2022). Our work aims to investigate this direction further through the following research question: Given the distinct mechanisms of various OSNs, can we obtain a more comprehensive picture of public perception of EVs by integrating the analysis from different platforms? Specifically, our study is based on EV-related discussions on two popular OSN platforms: Twitter and Reddit. We have collected 3,437,917 Reddit posts (including 274,979 submissions and 3,162,938 comments) and 7,383,327 Tweets between January 2011 and December 2020 and analyzed them from several perspectives. Our analysis shows that users have had different topic and sentiment patterns in EV-related discussions on the two platforms over the past decade. We also leverage the verified account information on Twitter to reveal that the most influential users are politicians and news media; however, the general public has very different conversation patterns with the two types of accounts — politicians seem to be increasingly (over) optimistic about EVs while the public may think differently.}
}
@article{SATO2019293,
title = {Statistical analysis of word usage in biological publications since 1965: Historical delineation highlighting an emergence of function-oriented discourses in contemporary molecular and cellular biology},
journal = {Journal of Theoretical Biology},
volume = {462},
pages = {293-303},
year = {2019},
issn = {0022-5193},
doi = {https://doi.org/10.1016/j.jtbi.2018.11.017},
url = {https://www.sciencedirect.com/science/article/pii/S0022519318305708},
author = {Naoki Sato and Kaoru Sato},
keywords = {Contemporary biology, Function, History of biology, Statistical text analysis, Role, Social responsibility of research},
abstract = {Typical studies on the history of science, or particularly of biology, have been focused on a particular scientist or book, but this selection has a risk of being arbitrary. To find a more objective way of studying history of biology, we applied a statistical method. First, we downloaded from the PubMed database all available titles and abstracts of 934,807 articles in 32 selected journals from 1965 to 2014, and extracted most frequently used 322 terms by text mining. Clustering of these terms according to the annual frequency of usage resulted in three main clusters: Cluster 1 represented terms that were no longer used frequently, Cluster 3 included terms that became abundantly used recently, and Cluster 2 contained terms constantly used. Three phases were delineated in the history of biology over the past 50 years, with transitions in 1987 and 1997. In contrast with our tacit understanding that “function” is a key notion in biological thinking, the results suggest that function-oriented discourses are a new habit of biologists in the genomic era after 1997, in which biological researches focus on identifying a link between a molecule or a structure with its function. We hypothesize that, in spite of repeated warnings, function-related discourses have a teleological connotation, which is easily misunderstood by general audience and, with emphatic expressions such as “important” and “essential”, fit to the need for justification of researches as part of researcher's responsibility for public funding.}
}
@article{LEACH2024,
title = {The engineering legacy of the FIFA World Cup Qatar 2022: Al Janoub stadium},
journal = {Proceedings of the Institution of Civil Engineers - Structures and Buildings},
year = {2024},
issn = {0965-0911},
doi = {https://doi.org/10.1680/jstbu.22.00127},
url = {https://www.sciencedirect.com/science/article/pii/S0965091124000155},
author = {Jon Leach and Craig Sparrow and Federico Iori and Hamad N. Al-Nuaimi and Mohammed Z. E. B. Elshafie and Nasser A. Al-Nuaimi},
keywords = {buildings, structures & design, thermal effects, wind loading & aerodynamics},
abstract = {Al Janoub was the first new-build stadium designed for the FIFA World Cup Qatar 2022. This paper describes the journey of the engineering design of the 40 000 seat stadium, from the concept and detailed design development stages led by AECOM and Zaha Hadid Architects, through to the design and build contract on site. An architectural jewel located in Al Wakrah, just south of the city of Doha, the stadium was a world-first, using state-of-the-art computational analysis and physical modelling to create a safe, cooled environment that satisfies FIFA's requirements for both player and spectator comfort in the extreme temperatures of the region. A sustainable post-tournament legacy was also a key factor of the design, allowing it to be reduced to a 20 000-capacity stadium for the local football club and community. The task of integrating the stadium's stringent performance requirements on this path-finder project, including extensive scientific research and development, was a challenge that was overcome through close collaboration between the design team and the Supreme Committee's subject matter experts. The project's success as a test-bed helped it to set the standard for other stadia as part of the overall FIFA World Cup Qatar 2022 programme.}
}
@article{NGUYEN2022108381,
title = {Knowledge mapping of digital twin and physical internet in Supply Chain Management: A systematic literature review},
journal = {International Journal of Production Economics},
volume = {244},
pages = {108381},
year = {2022},
issn = {0925-5273},
doi = {https://doi.org/10.1016/j.ijpe.2021.108381},
url = {https://www.sciencedirect.com/science/article/pii/S0925527321003571},
author = {Tiep Nguyen and Quang Huy Duong and Truong Van Nguyen and You Zhu and Li Zhou},
keywords = {, , , },
abstract = {Physical Internet (PI) is an open global logistics system of which components are hyperconnected for increased efficiency and sustainability. Digital twin (DT), referring to the virtual representation of a physical object, is well-perceived as a key driver in the development of PI-based Supply Chain Management (SCM). Due to the capabilities of real-time monitoring and evaluation of large-scale complex systems, significant research efforts have been made to exploit values of PI/DT in SCM. Despite this, the current literature remained largely unstructured and scattered due to a lack of systematic literature reviews to synergise research findings, analyse the evolution of research fronts and extract emerging trends in the field. To address this issue, the paper deploys a bibliometric knowledge mapping approach to provide a bird's eye view of the current research status in the PI/DT-SCM area. Using CiteSpace's keyword co-occurrence network, 518 journal articles are clustered into 10 key research streams on PI/DT applications in: job shop scheduling, smart manufacturing design, PI-based SCM, manufacturing virtualisation, information management, sustainability development, data analytics, manufacturing operations management, simulation and optimisation, and assembly process planning. Based on citation burst rate, keywords representing research frontiers of the PI/DT are detected and their temporal evolutions are discussed. Likewise, some identified emerging research trends are production process and system, robotics, computer architecture, and cost. Finally, seven future research directions are suggested, which emphasise on several PI/DT-related issues, including business ecosystem, sustainability development, SC downstream management, cognitive thinking in Industry 5.0, citizen twin in digital society, and SC resilience.}
}
@article{ZHANG2022786,
title = {Research on Graph Neural Network in Stock Market},
journal = {Procedia Computer Science},
volume = {214},
pages = {786-792},
year = {2022},
note = {9th International Conference on Information Technology and Quantitative Management},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.11.242},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922019524},
author = {Wenjun Zhang and Zhensong Chen and Jianyu Miao and Xueyong Liu},
keywords = {Graph neural networks, stocks, forecasts},
abstract = {The stock market is a very important part of the financial field, and the prediction of the stock market has a great relationship with the returns and risk safety of the entire financial field. With the continuous mature application of machine learning and deep learning in other fields, such as image processing and text analysis, people begin to focus on the use of different models so as to predict stock volatility. However, in view of the unique multi-source and heterogeneous characteristics of stock information, the artificial neural network relying on deep learning cannot make a good prediction on it. At this time, the graph neural network that can well analyze the graph structure data is gradually favored by scholars at home and abroad, and the research thinking is also expanding. This dissertation examines the purpose of deeply analyzing the methods of different graph neural network models on stock prediction through an inductive study of amount of relevant literature. In this paper, we not only classify the literature by various graph neural network models, but also describe objectively the models and ideas presented in each paper. By referring to literature, this paper summarizes the previous research results, analyzes the applicability and results of different methods, and lays a foundation for better stock prediction in the future.}
}
@article{ERDMANN202242,
title = {A generative framework for the study of delusions},
journal = {Schizophrenia Research},
volume = {245},
pages = {42-49},
year = {2022},
note = {Computational Approaches to Understanding Psychosis},
issn = {0920-9964},
doi = {https://doi.org/10.1016/j.schres.2020.11.048},
url = {https://www.sciencedirect.com/science/article/pii/S0920996420306277},
author = {Tore Erdmann and Christoph Mathys},
keywords = {Delusion, Dirichlet process, Hierarchical predictive coding, Auxiliary hypothesis, Epistemic trust},
abstract = {Despite the ubiquity of delusional information processing in psychopathology and everyday life, formal characterizations of such inferences are lacking. In this article, we propose a generative framework that entails a computational mechanism which, when implemented in a virtual agent and given new information, generates belief updates (i.e., inferences about the hidden causes of the information) that resemble those seen in individuals with delusions. We introduce a particular form of Dirichlet process mixture model with a sampling-based Bayesian inference algorithm. This procedure, depending on the setting of a single parameter, preferentially generates highly precise (i.e. over-fitting) explanations, which are compartmentalized and thus can co-exist despite being inconsistent with each other. Especially in ambiguous situations, this can provide the seed for delusional ideation. Further, we show by simulation how the excessive generation of such over-precise explanations leads to new information being integrated in a way that does not lead to a revision of established beliefs. In all configurations, whether delusional or not, the inference generated by our algorithm corresponds to Bayesian inference. Furthermore, the algorithm is fully compatible with hierarchical predictive coding. By virtue of these properties, the proposed model provides a basis for the empirical study and a step toward the characterization of the aberrant inferential processes underlying delusions.}
}
@article{VAKILI2021112687,
title = {The development of a transdisciplinary policy framework for shipping companies to mitigate underwater noise pollution from commercial vessels},
journal = {Marine Pollution Bulletin},
volume = {171},
pages = {112687},
year = {2021},
issn = {0025-326X},
doi = {https://doi.org/10.1016/j.marpolbul.2021.112687},
url = {https://www.sciencedirect.com/science/article/pii/S0025326X21007219},
author = {Seyedvahid Vakili and Aykut I. Ölçer and Fabio Ballini},
keywords = {Energy Efficiency Design Index, Energy Efficiency Existing Ship Index, Enhanced Ship Energy Efficiency Management Plan, Policy, Transdisciplinary, Underwater noise pollution},
abstract = {One of the newly emerging environmental issues is underwater noise pollution. It has both negative environmental and socio-economic impacts and threatens sustainable shipping. While other types of shipping pollutants have been regulated and societal awareness has been raised, due to the intangible characteristics of underwater noise pollution, there is neither societal awareness nor an international legally binding instrument to mitigate underwater noise pollution. This paper aims to raise awareness of ship owners regarding UWN pollution by introducing the sources of UWN pollution, as well as proposing a transdisciplinary policy for shipping companies to mitigate UWN pollution from their ships. The proposed policy is aligned with IMO's initial GHG strategy, especially the Energy Efficiency Design Index, Energy Efficiency Existing Ship Index, and Enhanced Ship Energy Efficiency Management Plan. This multi-dimensional approach will make stakeholders more enthusiastic to tackle underwater noise pollution while enhancing the efficient use of capacities and resources.}
}
@article{KRUIJVER2022102748,
title = {The number of alleles in DNA mixtures with related contributors},
journal = {Forensic Science International: Genetics},
volume = {61},
pages = {102748},
year = {2022},
issn = {1872-4973},
doi = {https://doi.org/10.1016/j.fsigen.2022.102748},
url = {https://www.sciencedirect.com/science/article/pii/S1872497322000898},
author = {Maarten Kruijver and James M. Curran},
keywords = {DNA mixtures, Probabilistic genotyping},
abstract = {The maximum allele count (MAC) across loci and the total allele count (TAC) are often used to gauge the number of contributors to a DNA mixture. Computational strategies that predict the total number of alleles in a mixture arising from a certain number of contributors of a given population have been developed. Previous work considered the restricted case where all of the contributors to a mixture are unrelated. We relax this assumption and allow mixture contributors to be related according to a pedigree. We introduce an efficient computational strategy. This strategy based on first determining a probability distribution on the number of independent alleles per locus, and then conditioning on this distribution to compute a distribution of the number of distinct alleles per locus. The distribution of the number of independent alleles per locus is obtained by leveraging the Identical by Descent (IBD) pattern distribution which can be computed from the pedigree. We explain how allelic dropout and a subpopulation correction can be accounted for in the calculations.}
}
@article{JHA2025103700,
title = {Transitive reasoning: A high-performance computing model for significant pattern discovery in cognitive IoT sensor network},
journal = {Ad Hoc Networks},
volume = {167},
pages = {103700},
year = {2025},
issn = {1570-8705},
doi = {https://doi.org/10.1016/j.adhoc.2024.103700},
url = {https://www.sciencedirect.com/science/article/pii/S1570870524003111},
author = {Vidyapati Jha and Priyanka Tripathi},
keywords = {Transitive reasoning, Knowledge discovery, CIoT, Probabilistic clustering, Total variation regularization},
abstract = {Current research on the Internet of Things (IoT) has given rise to a new field of study called cognitive IoT (CIoT), which aims to incorporate cognition into the designs of IoT systems. Consequently, the CIoT inherits specific attributes and challenges from IoT. The CIoT applications generate vast, diverse, constantly changing, and time-dependent data due to the billions of devices involved. The efficient operation of these CIoT systems requires the extraction of valuable insights from vast data sources in a computationally efficient manner. Therefore, this study proposes transitive reasoning to glean significant concepts and patterns from a 21.25-year environmental dataset. To reduce the effects of missing entries, the proposed methodology includes a grouping of data using probabilistic clustering and applying total variance regularization in the alternate direction method of multipliers (ADMM) to regularize the sensory data. As a result, noisy entries will be less conspicuous. Afterward, it calculates the transitional plausibility value for each cluster using the transited value and then turns it into binary data to create concept lattices. In addition, each concept that is formed is assigned a weight, and the concept with the largest transitive strength value is chosen, followed by calculating the mean value. Therefore, this pattern is seen as significant. Experimental results on 21.25-year environmental data show an accuracy of over 99.5%, outperforming competing methods, as shown by cross-validation using multiple metrics.}
}
@article{KOCHEN1958267,
title = {The acquisition and utilization of information in problem solving and thinking},
journal = {Information and Control},
volume = {1},
number = {3},
pages = {267-288},
year = {1958},
issn = {0019-9958},
doi = {https://doi.org/10.1016/S0019-9958(58)80005-4},
url = {https://www.sciencedirect.com/science/article/pii/S0019995858800054},
author = {Manfred Kochen and Eugene H. Galanter},
abstract = {Some of the logical consequences of drawing a distinction between the following two aspects of problem-solving behavior are explored: (a) actions directed toward the acquisition of information to guide future actions toward valuable goals; (b) actions directed toward the utilization of accumulated information to attain a valuable goal. An experimental paradigm accomplishing this separation is described for the case of an environment of periodic sequences of binary events. A general way of describing behavioral strategies is developed in terms of: (a) a plan for when to acquire information, to guess an outcome, or to guess at the solution; and (b) a program for how to compute guesses from the information accumulated. The structure of the binary environmental sequences, the structure of these behavioral strategies, and the relations between them are analyzed, and certain strategies which maximize value are suggested. Computing machine interpretations of certain specific strategies for a restricted kind of experiment are displayed, and predictions from these are compared with experimental data from pilot studies performed with human subjects.}
}
@incollection{SHAH2017251,
title = {Chapter Seven - What Makes Everyday Scientific Reasoning So Challenging?},
editor = {Brian H. Ross},
series = {Psychology of Learning and Motivation},
publisher = {Academic Press},
volume = {66},
pages = {251-299},
year = {2017},
issn = {0079-7421},
doi = {https://doi.org/10.1016/bs.plm.2016.11.006},
url = {https://www.sciencedirect.com/science/article/pii/S0079742116300214},
author = {Priti Shah and Audrey Michal and Amira Ibrahim and Rebecca Rhodes and Fernando Rodriguez},
keywords = {ANCOVA, Anecdotes, Causality bias, Decision making, Heuristic vs. analytic thinking, Science education, Scientific reasoning, Selection bias, Statistical validity},
abstract = {Informed citizens are expected to use science-based evidence to make decisions about health, behavior and public policy. To do so, they must judge whether the evidence is consistent with the claims presented (theory-evidence coordination). Unfortunately, most individuals make numerous errors in theory-evidence coordination. In this chapter, we provide an overview of research on science evidence evaluation, drawing from research in cognitive and developmental psychology, science and statistics education, decision sciences, political science and science communication. Given the breadth of this research area, we highlight some influential studies and reviews across these different topics. This body of research provides several clues about: (1) why science evidence evaluation is challenging, (2) the influence of the content and context of the evidence and (3) how the characteristics of the individual examining the evidence impact the quality of the evaluations. Finally, we suggest some possible directions for empirical research on improving evidence evaluation and point to the responsibility of scientists, especially social and behavioral scientists, in communicating their findings to the public. Overall, our goal is to give readers an interdisciplinary view of science evidence evaluation research and to integrate research from different scientific communities that address similar questions.}
}
@article{BISWAL2024110150,
title = {Unlocking the potential of signature-based drug repurposing for anticancer drug discovery},
journal = {Archives of Biochemistry and Biophysics},
volume = {761},
pages = {110150},
year = {2024},
issn = {0003-9861},
doi = {https://doi.org/10.1016/j.abb.2024.110150},
url = {https://www.sciencedirect.com/science/article/pii/S0003986124002728},
author = {Sruti Biswal and Bibekanand Mallick},
keywords = {Cancer, Anticancer drug, Drug repurposing, Gene signature},
abstract = {Cancer is the leading cause of death worldwide and is often associated with tumor relapse even after chemotherapeutics. This reveals malignancy is a complex process, and high-throughput omics strategies in recent years have contributed significantly in decoding the molecular mechanisms of these complex events in cancer. Further, the omics studies yield a large volume of cancer-specific molecular signatures that promote the discovery of cancer therapy drugs by a method termed signature-based drug repurposing. The drug repurposing method identifies new uses for approved drugs beyond their intended initial therapeutic use, and there are several approaches to it. In this review, we discuss signature-based drug repurposing in cancer, how cancer omics have revolutionized this method of drug discovery, and how one can use the cancer signature data for repurposed drug identification by providing a step-by-step procedural handout. This modern approach maximizes the use of existing therapeutic agents for cancer therapy or combination therapy to overcome chemotherapeutics resistance, making it a pragmatic and efficient alternative to traditional resource-intensive and time-consuming methods.}
}
@article{IBEZIM2024e02226,
title = {Potential dual inhibitors of Hexokinases and mitochondrial complex I discovered through machine learning approach},
journal = {Scientific African},
volume = {24},
pages = {e02226},
year = {2024},
issn = {2468-2276},
doi = {https://doi.org/10.1016/j.sciaf.2024.e02226},
url = {https://www.sciencedirect.com/science/article/pii/S2468227624001728},
author = {Akachukwu Ibezim and Emmanuel Onah and Sochi Chinaemerem Osigwe and Peter Ukwu Okoroafor and Onyeoziri Pius Ukoha and Jair Lage {de Siqueira-Neto} and Fidele Ntie-Kang and Karuppasamy Ramanathan},
keywords = {Hexokinases, Mitochondrial complex I, Cancer, MACCS fingerprints, Boruta algorithms, Machine learning, Metabolic plasticity},
abstract = {Hexokinases (Hks) and mitochondrial complex I (MCI) are involved in the energy metabolism of cells; glycolysis/fermentation and oxidative phosphorylation. Both Hks and MCI are known to play critical roles in either division of metabolic plasticity which enables tumor progression and proliferation in the presence of chemotherapies. Therefore, targeting these enzymes are important in cancer drug resistance. Here, computational models for the prediction of inhibition of Hks were developed based on experimental data and an optimal feature subset that was selected by the Boruta algorithm (a wrapper feature selection algorithm coupled with random forest). Out of the seven models that were explored, a random forest classifier gave the best prediction (GA = 0.84, FNR = 0.12 and AUC = 0.96 for the external dataset). Fragmentation analysis led to the identification of the unique structural scaffolds that characterize hexokinase inhibitors and non-inhibitors. The best Hks inhibition model predicted that 23 molecules out of the 191 dataset of MCI actives (IC50 ≤ 10 µM) that were screened, have more than 60 % probability of exhibiting Hk inhibitory activity. Hence, they are possible dual inhibitors of both targets. Furthermore, the 23 molecules’ core structures are members of the scaffolds that are unique to Hk inhibitors earlier predicted by fragment analysis. The need for dual targeting agents in cancer therapy, particularly in combating cancer drug resistance, highlights the relevance of these findings.}
}
@article{DASILVA2024105785,
title = {Optimization of open web steel beams using the finite element method and genetic algorithms},
journal = {Structures},
volume = {60},
pages = {105785},
year = {2024},
issn = {2352-0124},
doi = {https://doi.org/10.1016/j.istruc.2023.105785},
url = {https://www.sciencedirect.com/science/article/pii/S2352012423018738},
author = {Amilton Rodrigues {da Silva} and Gabriela Pereira Lubke},
keywords = {Open-web beams, Optimization, Genetic algorithm, Finite element method},
abstract = {Studies on structural optimization have gained prominence recently, and the search to consume resources in a more conscious and effective way encourages the use of such techniques in all fields. In this respect, this study aims to use computational optimization techniques to determine the maximum load-bearing capacity of hollow-core steel beams for two groups of different shear lines, one generating beams with opening in the shape of hexagons and the other having the shape of ellipses. The second group includes beams with circular openings as a particular case. A three-node triangular finite element for the analysis of structures in plane stress is used for the structural analysis of the beams. The design variables define the shape and number of opening in the beam, and a computational formulation using a genetic algorithm is presented to find the cut line that maximizes the load capacity of the beam considering different ultimate and service limit states. Numerical and experimental models in the literature are used to validate the implementations presented in this article, and the results of optimized hollow core beams are presented, demonstrating the efficiency of the formulations used.}
}
@article{PRADNYANA2025100307,
title = {An explainable ensemble model for revealing the level of depression in social media by considering personality traits and sentiment polarity pattern},
journal = {Online Social Networks and Media},
volume = {46},
pages = {100307},
year = {2025},
issn = {2468-6964},
doi = {https://doi.org/10.1016/j.osnem.2025.100307},
url = {https://www.sciencedirect.com/science/article/pii/S2468696425000084},
author = {Gede Aditra Pradnyana and Wiwik Anggraeni and Eko Mulyanto Yuniarno and Mauridhi Hery Purnomo},
keywords = {Explainable ensemble model, Personality trait, Sentiment polarity pattern, RoBERTa, Hybrid RF-BiLSTM},
abstract = {Early detection of depression in mental health is crucial for better intervention. Social media has been extensively used to examine users’ behavior, motivating researchers to develop an automatic depression detection model. However, the accuracy and clarity of the reasons behind the detection results still need to be improved. Current research focuses primarily on syntactic and semantic information in user-posted texts, while other aspects of users’ psychological characteristics are often overlooked. Therefore, this study addresses the gap by proposing a novel model integrating personality traits and sentiment polarity patterns into an explainable ensemble model. Specifically, we developed two base learners for the averaged and meta-ensemble learning strategy. The first learner employed the Robustly Optimized BERT Pre-training Approach (RoBERTa). For the second learner, we combined the Random Forest and Bidirectional Long Short-Term Memory (RF-BiLSTM) methods to effectively handle the combination of personality traits and sequential information in sentiment polarity patterns. These additional features are obtained by performing domain adaptation for personality prediction and sentiment analysis using a lexicon-based model. Based on the experimental results, our ensemble model improved depression detection results by leveraging the strengths of each base learner. Our model advanced the state-of-the-art, outperforming existing models with an increase in accuracy and F1-score of 4.14% and 2.99%, respectively. The model successfully enhanced the interpretability of detection results, providing a more comprehensive understanding of the factors underlying depressive symptoms. This research highlights the potential of considering alternative additional features as a promising avenue for enhancing depression detection in social media.}
}
@article{ZHANG2025115558,
title = {Opportunities of applying Large Language Models in building energy sector},
journal = {Renewable and Sustainable Energy Reviews},
volume = {214},
pages = {115558},
year = {2025},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2025.115558},
url = {https://www.sciencedirect.com/science/article/pii/S136403212500231X},
author = {Liang Zhang and Zhelun Chen},
keywords = {Large language models, Building energy efficiency, Building decarbonization, Knowledge extraction, Intelligent control systems, Data infrastructure, Education and training},
abstract = {In recent years, the rapid advancement and impressive capabilities of Large Language Models have been evident across various engineering domains. This paper explores the application, implications, and potential of Large Language Models in building energy sectors, especially energy efficiency and decarbonization studies, based on an extensive literature review and a survey from building engineers and scientists. The paper explores how LLMs can enhance intelligent control systems, automate code generation for software and modeling tools, optimize data infrastructure, and refine analysis of technical reports and papers. Additionally, the paper discusses the role of LLMs in improving regulatory compliance, supporting building lifecycle management, and revolutionizing education and training practices within the sector. Despite the promising potential of Large Language Models, challenges including complex and expensive computation, data privacy, security and copyright, complexity in fine-tuned Large Language Models, and self-consistency are discussed. The paper concludes with a call for future research focused on the enhancement of LLMs for domain-specific tasks, multi-modal LLMs, and collaborative research between AI and energy experts.}
}
@article{ESHAGHI2024107342,
title = {Methods for enabling real-time analysis in digital twins: A literature review},
journal = {Computers & Structures},
volume = {297},
pages = {107342},
year = {2024},
issn = {0045-7949},
doi = {https://doi.org/10.1016/j.compstruc.2024.107342},
url = {https://www.sciencedirect.com/science/article/pii/S0045794924000713},
author = {Mohammad Sadegh Es-haghi and Cosmin Anitescu and Timon Rabczuk},
abstract = {This paper presents a literature review on methods for enabling real-time analysis in digital twins, which are virtual models of physical systems. The advantages of digital twins are numerous, including cost reduction, risk mitigation, efficiency enhancement, and decision-making support. However, their implementation faces challenges such as the need for real-time data analysis, resource limitations, and data uncertainty. The paper focuses on methods for reducing computational demands, which have not been systematically discussed in the literature. The paper reviews and categorizes methods and tools for accelerating the modeling of physical phenomena and reducing the computational needs of digital twins.}
}
@article{WELLMAN1991205,
title = {The ecology of computation: B.A. Huberman, ed.},
journal = {Artificial Intelligence},
volume = {52},
number = {2},
pages = {205-218},
year = {1991},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(91)90044-K},
url = {https://www.sciencedirect.com/science/article/pii/000437029190044K},
author = {Michael P. Wellman}
}
@article{QAMMAR2023e16230,
title = {Statistical analysis of the university sustainability in the higher education institution a case study from the Khyber Pakhtunkhwa province in Pakistan},
journal = {Heliyon},
volume = {9},
number = {5},
pages = {e16230},
year = {2023},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2023.e16230},
url = {https://www.sciencedirect.com/science/article/pii/S2405844023034370},
author = {Naseha Wafa Qammar and Zohaib Ur {Rehman Afridi} and Shamaima Wafa Qammar},
keywords = {Sustainability, Higher education institute, Students and faculty, Pakistan},
abstract = {Educational institutions can incorporate the idea of sustainability at the grass root level for any society. This study is part of an effort to get insight into the campus sustainability in one of the Higher Education Institution (HEI) in the Khyber Pakhtunkhwa region of Pakistan. Aim is to investigation university students' and faculty members insight regarding sustainability. Thus, questionnaire-based survey followed by statistical inference was conducted for the potential outcomes. The questionnaire is comprised of 24 questions, 05 of which are on demographics and the remaining 19 are about sustainability. The sustainability related questions focused mostly on the respondents' knowledge, understanding, and interest in sustainability. A handful of the other questions in the questionnaire were tailored to the university input to achieve sustainability. The dataset is manipulated with basic statistical and computational approaches, and the results are analyzed using mean values. The mean values are further classified into flag values of 0 and 1. Flag value 1 indicates a good marker of the received response, while flag value 0 indicates the least amount of information included in responses. The results show that respondents' knowledge, awareness, interest, and engagement in sustainability are significantly sufficient, as we obtained a flag value of 1 for all questions about sustainability. The study's findings, on the other hand, indicated that the institution is lagging in terms of supporting, disseminating, and implementing campus-wide sustainability-related activities. This study is one of the first initiatives to provide a baseline dataset and substantial information to go a step further in achieving the bottom-line target of being and acting sustainable in the HEI.}
}
@article{COIERA2022100860,
title = {Evidence synthesis, digital scribes, and translational challenges for artificial intelligence in healthcare},
journal = {Cell Reports Medicine},
volume = {3},
number = {12},
pages = {100860},
year = {2022},
issn = {2666-3791},
doi = {https://doi.org/10.1016/j.xcrm.2022.100860},
url = {https://www.sciencedirect.com/science/article/pii/S2666379122004244},
author = {Enrico Coiera and Sidong Liu},
keywords = {evidence-based medicine, evidence synthesis, patient safety, research replication, machine learning, algorithmic transportability, deep learning, clinical trial registries},
abstract = {Summary
Healthcare has well-known challenges with safety, quality, and effectiveness, and many see artificial intelligence (AI) as essential to any solution. Emerging applications include the automated synthesis of best-practice research evidence including systematic reviews, which would ultimately see all clinical trial data published in a computational form for immediate synthesis. Digital scribes embed themselves in the process of care to detect, record, and summarize events and conversations for the electronic record. However, three persistent translational challenges must be addressed before AI is widely deployed. First, little effort is spent replicating AI trials, exposing patients to risks of methodological error and biases. Next, there is little reporting of patient harms from trials. Finally, AI built using machine learning may perform less effectively in different clinical settings.}
}
@article{LUPION2025112832,
title = {A holistic approach for resource-constrained neural network architecture search},
journal = {Applied Soft Computing},
volume = {172},
pages = {112832},
year = {2025},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2025.112832},
url = {https://www.sciencedirect.com/science/article/pii/S1568494625001437},
author = {M. Lupión and N.C. Cruz and E.M. Ortigosa and P.M. Ortigosa},
keywords = {Artificial neural networks, Neural architecture search, Meta-heuristic, TLBO, Neural network encoding, Performance predictor},
abstract = {The design of Artificial Neural Networks (ANN) is critical for their performance. The research field called Neural Network Search (NAS) investigates automated design strategies. This work proposes a novel NAS stack that stands out in three facets. First, the representation scheme encodes problem-specific ANN as plain vectors of numbers without needing auxiliary conversion models. Second, it is a pioneer in relying on the TLBO meta-heuristic. This optimizer supports large-scale problems and only expects two parameters, contrasting with other meta-heuristics used for NAS. Third, the stack includes a new evaluation predictor that avoids evaluating non-promising architectures. It combines several machine learning methods that train as the optimizer evaluates solutions, which avoids preliminary preparing this component and makes it self-adaptive. The proposal has been tested by using it to build a CIFAR-10 classifier while forcing the architecture to have fewer than 150,000 parameters, assuming that the resulting network must be deployed in a resource-constrained IoT device. The designs found with and without the predictor achieve validation accuracies of 78.68% and 80.65%, respectively. Both outperform a larger model from the recent literature. The predictor slightly constraints the evolution of solutions, but it approximately halves the computational effort. After extending the test to the CIFAR-100 dataset, the proposal achieves a validation accuracy of 65.43% with 478,006 parameters in its fastest configuration, competing with current results in the literature.}
}
@article{ROBSON2022101018,
title = {Searching for the principles of a less artificial A.I.},
journal = {Informatics in Medicine Unlocked},
volume = {32},
pages = {101018},
year = {2022},
issn = {2352-9148},
doi = {https://doi.org/10.1016/j.imu.2022.101018},
url = {https://www.sciencedirect.com/science/article/pii/S2352914822001617},
author = {B. Robson and G. Ochoa-Vargas},
keywords = {AI, Algorithms, X factor, Emergent properties, Consciousness, Quantum effects},
abstract = {What would it take to build a computer physician that can take its place amongst human peers? Currently, Neural Nets, especially as so-called “Deep Learning” nets, dominate what is popularly called “Artificial Intelligence”, but to many critics they seem to be little more than powerful data-analytic tools inspired by some of the more basic functions and regions of the human brain such as those involved in early processes in biological vision, classification, and categorization. The deeper nature of human intelligence as the term is normally meant, including relating to consciousness, has been the domain of philosophers, psychologists, and some neuroscientists. Now, attention is turning to neuronal mechanisms in humans and simpler organisms as a basis of a truer AI with far greater potential. Arguably, the approach required should be rooted in information theory and algorithmic science. But as discussed in this paper, caution is required: “just any old information” might not do. The information might need to be of a particular dynamical and actioning nature, and that might significantly impact the kind of computation and computer hardware required. Overall, however, the authors do not favor emergent properties such as those based on complexity and quantum effects. Despite the possible difficulties, such studies could, in return, have substantial benefits for biology and medicine beyond the computational tools that they produce to serve those disciplines.}
}
@article{ADHIKARI2025,
title = {The use of Monte Carlo simulation techniques in brachytherapy: A comprehensive literature review},
journal = {Brachytherapy},
year = {2025},
issn = {1538-4721},
doi = {https://doi.org/10.1016/j.brachy.2025.02.006},
url = {https://www.sciencedirect.com/science/article/pii/S1538472125000364},
author = {Tirthraj Adhikari and Tomas Montenegro and Jae Won Jung and Courtney Oare and Gabriel Fonseca and Luc Beaulieu and Abdullah Alshreef and Clara Ferreira},
keywords = {Monte Carlo simulation, Dosimetry, Heterogeneity, Brachytherapy},
abstract = {ABSTRACT
Monte Carlo techniques have become crucial in brachytherapy since their introduction in the early 1980s, offering significant improvements in source parameter characterizations, and dose calculations. It provides precise dose distributions by modeling complex radiation interactions and can be determine doses in nonhomogeneous detailed cases. They are not affected by experimental artifacts, unlike traditional detectors, and can distinguish between primary and scatter dose components. However, MC techniques have limitations. They are susceptible to systematic errors and require thorough validation against experimental data, despite generally showing smaller standard deviations. Additionally, MC simulations can be computationally intensive and depend heavily on accurate input data and models. Recent research, including 1433 publications identified up to October 2024, highlights the ongoing development and application of MC techniques in brachytherapy. Of these, 426 articles met the inclusion criteria for relevance. This comprehensive review aims to help brachytherapy researchers to identify the appropriate MC code depending on the application in BT research. Of the forty-five MC codes used in BT, MCNP is noted as the most widely used MC code due to its robust modeling capabilities in various materials and geometries. AAPM TG-186 and TG-372 reports have recommended the use of model base dose calculation algorithms, since it can offer more accurate dose calculations over TG-43 formalism, particularly in heterogeneous tissues. Despite these recommendations, further research is needed to refine dosimetry for various isotopes, geometry and media. In essence, MC techniques have greatly enhanced the accuracy, precision and flexibility of brachytherapy techniques, though challenges such as systematic errors, heterogeneities corrections, and high computational demands remain. Continued research and development of MC codes and algorithms are essential for advancing the field and improving clinical outcomes.}
}
@article{WANG2022799,
title = {BMW-TOPSIS: A generalized TOPSIS model based on three-way decision},
journal = {Information Sciences},
volume = {607},
pages = {799-818},
year = {2022},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2022.06.018},
url = {https://www.sciencedirect.com/science/article/pii/S0020025522006004},
author = {Yumei Wang and Peide Liu and Yiyu Yao},
keywords = {Three-way decision, TOPSIS, Reference point, Multiple attribute decision making},
abstract = {TOPSIS (Technique for Order Preference by Similarity to Ideal Solution) uses a pair of a positive ideal solution and a negative ideal solution as two reference points to rank a set of decision alternatives. In some situations, a trade-off of the distances to the two extreme reference points may not necessarily be meaningful. Inspired by the theory of three-way decision as thinking in threes (e.g., two opposite poles and a third middle), in this paper we generalize the classical TOPSIS by adding a third middle reference point. We use a common setting for investigating systematically reference-point-based TOPSIS-style multi-criteria decision-making methods. In particular, we examine three classes of approaches: a) a best reference point based model (i.e., B-TOPSIS) and a worst reference point based model (i.e., W-TOPSIS), b) the classical best and worst reference points based model (i.e., BW-TOPSIS), and c) a new best, mean, and worst reference points based model (i.e., BMW-TOPSIS). The three classes are one-way TOPSIS, two-way TOPSIS, and three-way TOPSIS, respectively. Based on one-way and two-way TOPSISs, we give two specific methods of three-way TOPSIS. The experimental results, compared with the existing TOPSIS methods, show that the BMW-TOPSIS model is feasible and effective.}
}
@incollection{MISHRA202633,
title = {CHAPTER 3 - Introduction to systems approach to materials},
editor = {Rajiv S. Mishra and Indrajit Charit and Ravi Sankar Haridas},
booktitle = {Mechanical Behavior of Materials},
publisher = {Butterworth-Heinemann},
pages = {33-46},
year = {2026},
isbn = {978-0-12-804554-1},
doi = {https://doi.org/10.1016/B978-0-12-804554-1.00012-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128045541000125},
author = {Rajiv S. Mishra and Indrajit Charit and Ravi Sankar Haridas},
keywords = {processing, microstructure, properties, performance, systems approach, unintended microstructural features, failure, materials by microstructural design},
abstract = {Abstract:
This short chapter introduces the systems approach to materials. The classic materials chain is path for preparing, examining, and documenting materials and their properties. It is expressed as: processing → microstructure → properties → performance. This sequence has been recognized from as early as the 1950s (Zener (1948) and Smith (1981)). Yet, a formal nomenclature of “systems approach” became popular after the work of Olson (1989, 1997). An understanding of this systems approach will enhance appreciation of the broader context as we examine the various mechanical behavior concepts. Note that the mechanical behavior of materials forms the structural properties group. The materials link chain implies that mechanical behavior originates from the microstructure, which in turn evolves during the processing of material. While a materials engineer or scientist builds understanding from the atomic level up, the designer starts from the required performance of the component or system. A new key phrase is introduced, “unintended microstructural feature.” An unintended microstructural feature can be defined as anything that is not a part of the original design of the material. This is a very important distinction, as the unexpected failure of material is often linked to such features.}
}
@article{AICHA2022107933,
title = {A mathematical formulation for processing time computing in disassembly lines and its optimization},
journal = {Computers & Industrial Engineering},
volume = {165},
pages = {107933},
year = {2022},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2022.107933},
url = {https://www.sciencedirect.com/science/article/pii/S0360835222000031},
author = {Mahdi Aicha and Imen Belhadj and Moncef Hammadi and Nizar Aifaoui},
keywords = {DP Evaluation, Index of Quality, Operating time, Optimization, Lean thinking},
abstract = {Disassembly is the first practice in maintenance and recycling of industrial products. For productivity and efficiency, it is necessary to optimize its operative manners by reducing: change of tools and directions, process variation, wastes, etc. The simulation of Disassembly Plan (DP) allows the detection and identification of difficulties from the design stage in order to avoid them. This paper proposes a mathematical formulation which combines two principal indicators: the index of Quality (Qi) and the index of processing Time (Ti) in order to choose the best and feasible disassembly plan. The Failure Mode, Effects and Criticality Analysis method is implemented to compute Qi. Ti is obtained according to real manufacturing constraints (workspace, layout, tools, machines, etc.). Based on 5S method, the workspace can be optimized which directly impacts the timing index and contribute to the selection of the best DP. A gear box is used to show up the efficiency of the proposed approach.}
}
@article{LI2025108,
title = {Enhancing energy materials with data-driven methods: A roadmap to long-term hydrogen energy sustainability using machine learning},
journal = {International Journal of Hydrogen Energy},
volume = {119},
pages = {108-125},
year = {2025},
issn = {0360-3199},
doi = {https://doi.org/10.1016/j.ijhydene.2025.03.201},
url = {https://www.sciencedirect.com/science/article/pii/S0360319925013151},
author = {Cheng Li and Jianjun Ma and Des Gibson and Yijun Yan and Muhammad Haroon and Mehak {Bi Bi}},
keywords = {Hydrogen energy, Machine learning, Data driven, Artificial intelligence, Energy materials},
abstract = {In the past few years, there has been a lot of interest in studying new substances and figuring out how their structure affects their activity. This is seen as an alternative to the problems that come with traditional methods of making energy materials, like the high cost of computation, the time consumption, and the low success rate. Improving the study and production of energy materials requires new research, ideas, and methodologies. Some believe that data-driven materials science, enabled by recent advances in artificial intelligence (AI) and machine learning (ML), might modify current scientific knowledge and radically alter the production of energy materials. This includes essential advancements in hydrogen energy, like creating catalysts for producing hydrogen, finding materials for storing hydrogen, and improving fuel cell components. New findings in data-driven materials science suggest that ML technologies enable the development, identification and deployment of improved energy materials while simultaneously making their creation and improvement less of a hassle. This paper argues that funding research into alternative energy materials is an important first step towards achieving global carbon neutrality. Also included is a comprehensive ML concept overview covering topics like open-source databases, feature development, ML algorithms, and ML model assessment, among others. We discuss the modern developments in data-driven material sciences (DDMS) and technology, which cover topics such as materials for alkaline ion batteries, solar energy catalysis, and carbon dioxide recovery. This section concludes with some important ideas for making effective use of ML and some additional difficulties in creating new energy materials.}
}
@article{RICAURTE2020102,
title = {Project-based learning as a strategy for multi-level training applied to undergraduate engineering students},
journal = {Education for Chemical Engineers},
volume = {33},
pages = {102-111},
year = {2020},
issn = {1749-7728},
doi = {https://doi.org/10.1016/j.ece.2020.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S1749772820300464},
author = {Marvin Ricaurte and Alfredo Viloria},
keywords = {Project-based learning, Multi-level, Undergraduate students, Process engineering},
abstract = {This study presents a project-based learning methodology whose particularity is the inclusion of training at different levels of undergraduate engineering programs, which allows for the interaction among students from different semesters who work together on a common project. To show the applicability of the proposed methodology, a project for the industrial production of ethanol from sugar cane was considered. Students enrolled in Process Design (9th semester) and Computer-Assisted Technical Design (5th semester), courses included in the engineering programs offered by the Department of Chemical Engineering at Yachay Tech University (Ecuador), jointly developed it. The details of the project were presented to the students of the Introduction to Engineering course (3rd semester) to boost their interest in the engineering as applied science. The activities carried out in each of the courses are described in detail together with a description of how the learning outcomes were achieved thanks to the implementation of a multi-level training strategy. Teamwork and collaborative-integrated learning are the elements highlighted by the students who participated in the project. Some of the innovative aspects of the proposed methodology include professional training and multi-level learning, the development of logical thinking typical of engineers, the knowledge handover associated with the professional activities of process engineers engaged with real-world projects. Additionally, this methodology prizes the industrial experience that professors at the undergraduate level may have by allowing them to contribute with an engineering vision to the training of young people in engineering projects. This study was inspired by the principle of Constructive Alignment and by goal # 4 (quality education) of the 2030 Agenda for Sustainable Development.}
}
@article{FROWNFELTERLOHRKE201768,
title = {Teaching good Excel design and skills: A three spreadsheet assignment project},
journal = {Journal of Accounting Education},
volume = {39},
pages = {68-83},
year = {2017},
issn = {0748-5751},
doi = {https://doi.org/10.1016/j.jaccedu.2016.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S0748575115300403},
author = {Cynthia {Frownfelter- Lohrke}},
keywords = {Excel, Spreadsheets, Spreadsheet design, Active learning, Project-based learning},
abstract = {Over sixty percent of AIS courses cover Excel because it is an important tool for accounting students to learn and master. Although spreadsheet programs like Excel provide powerful analytical tools for business, in practice, they are often created and used by people with minimal programming experience. Consequently, users can often develop spreadsheets containing critical errors, which, in turn, can cause significant losses for their businesses. Errors can be reduced, however, by learning and employing good spreadsheet design techniques. Good spreadsheet design also makes it easier to update and continue to use a spreadsheet over time. This paper describes a method for teaching spreadsheet design where students complete three spreadsheet assignments in an iterative and repetitive process. By the time students have completed these assignments, they will have acquired good spreadsheet design skills and improved their basic Excel skills.}
}
@article{ZHAO2013278,
title = {An intelligent chiller fault detection and diagnosis methodology using Bayesian belief network},
journal = {Energy and Buildings},
volume = {57},
pages = {278-288},
year = {2013},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2012.11.007},
url = {https://www.sciencedirect.com/science/article/pii/S0378778812005968},
author = {Yang Zhao and Fu Xiao and Shengwei Wang},
keywords = {Fault detection, Fault diagnosis, Centrifugal chiller, Bayesian network},
abstract = {A generic intelligent fault detection and diagnosis (FDD) strategy is proposed in this study to simulate the actual diagnostic thinking of chiller experts. A three-layer Diagnostic Bayesian Network (DBN) is developed to diagnose chiller faults based on the Bayesian Belief Network (BBN) theory. The structure of the DBN is a graphical and qualitative illustration of the intrinsic causal relationships among causal factors in Layer 1, faults in Layer 2 and fault symptoms in Layer 3. The parameters of the DBN represent the quantitative probabilistic relationships among the three layers. To diagnose chiller faults, posterior probabilities of the faults under observed evidences are calculated based on the probability analysis and the graph theory. Compared with other FDD strategies, the proposed strategy can make use of more useful information of the chiller concerned and expert knowledge. It is effective and efficient in diagnosing faults based on uncertain, incomplete and conflicting information. Evaluation of the strategy was made on a 90-ton water-cooled centrifugal chiller reported in ASHRAE RP-1043.}
}
@article{LEVESQUE201427,
title = {On our best behaviour},
journal = {Artificial Intelligence},
volume = {212},
pages = {27-35},
year = {2014},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2014.03.007},
url = {https://www.sciencedirect.com/science/article/pii/S0004370214000356},
author = {Hector J. Levesque},
keywords = {IJCAI Research Excellence},
abstract = {The science of AI is concerned with the study of intelligent forms of behaviour in computational terms. But what does it tell us when a good semblance of a behaviour can be achieved using cheap tricks that seem to have little to do with what we intuitively imagine intelligence to be? Are these intuitions wrong, and is intelligence really just a bag of tricks? Or are the philosophers right, and is a behavioural understanding of intelligence simply too weak? I think both of these are wrong. I suggest in the context of question-answering that what matters when it comes to the science of AI is not a good semblance of intelligent behaviour at all, but the behaviour itself, what it depends on, and how it can be achieved. I go on to discuss two major hurdles that I believe will need to be cleared.}
}
@article{CLEMENTZ2020808,
title = {Testing Psychosis Phenotypes From Bipolar–Schizophrenia Network for Intermediate Phenotypes for Clinical Application: Biotype Characteristics and Targets},
journal = {Biological Psychiatry: Cognitive Neuroscience and Neuroimaging},
volume = {5},
number = {8},
pages = {808-818},
year = {2020},
note = {Understanding the Nature and Treatment of Psychopathology: Letting the Data Guide the Way},
issn = {2451-9022},
doi = {https://doi.org/10.1016/j.bpsc.2020.03.011},
url = {https://www.sciencedirect.com/science/article/pii/S2451902220301002},
author = {Brett A. Clementz and Rebekah L. Trotti and Godfrey D. Pearlson and Matcheri S. Keshavan and Elliot S. Gershon and Sarah K. Keedy and Elena I. Ivleva and Jennifer E. McDowell and Carol A. Tamminga},
keywords = {Biomarkers, Computational neuroscience, Neurobiological, Precision medicine, Psychopathology, Transdiagnostic},
abstract = {Background
Psychiatry aspires to the molecular understanding of its disorders and, with that knowledge, to precision medicine. Research supporting such goals in the dimension of psychosis has been compromised, in part, by using phenomenology alone to estimate disease entities. To this end, we are proponents of a deep phenotyping approach in psychosis, using computational strategies to discover the most informative phenotypic fingerprint as a promising strategy to uncover mechanisms in psychosis.
Methods
Doing this, the Bipolar–Schizophrenia Network for Intermediate Phenotypes (B-SNIP) has used biomarkers to identify distinct subtypes of psychosis with replicable biomarker characteristics. While we have presented these entities as relevant, their potential utility in clinical practice has not yet been demonstrated.
Results
Here we carried out an analysis of clinical features that characterize biotypes. We found that biotypes have unique and defining clinical characteristics that could be used as initial screens in the clinical and research settings. Differences in these clinical features appear to be consistent with biotype biomarker profiles, indicating a link between biological features and clinical presentation. Clinical features associated with biotypes differ from those associated with DSM diagnoses, indicating that biotypes and DSM syndromes are not redundant and are likely to yield different treatment predictions. We highlight 3 predictions based on biotype that are derived from individual biomarker features and cannot be obtained from DSM psychosis syndromes.
Conclusions
In the future, biotypes may prove to be useful for targeting distinct molecular, circuit, cognitive, and psychosocial therapies for improved functional outcomes.}
}
@article{SALEMDEEB2022200069,
title = {Beyond recycling: An LCA-based decision-support tool to accelerate Scotland's transition to a circular economy},
journal = {Resources, Conservation & Recycling Advances},
volume = {13},
pages = {200069},
year = {2022},
issn = {2667-3789},
doi = {https://doi.org/10.1016/j.rcradv.2022.200069},
url = {https://www.sciencedirect.com/science/article/pii/S2667378922000074},
author = {Ramy Salemdeeb and Ruth Saint and Francesco Pomponi and Kimberley Pratt and Michael Lenaghan},
keywords = {Life cycle assessment, Policy development, Resource and waste management, Circular economy, Zero waste},
abstract = {Resources and waste strategies have recently seen a shift in focus from weight-based recycling targets to impact-driven policies. To support this transition, numerous decision-support tools were developed to help identify waste streams with the highest impacts. However, the majority of these tools focus solely on greenhouse gas emissions and show a narrow picture of the overall environmental impacts. Furthermore, they cover burdens associated with direct waste management activities and hence fall short when it comes to highlighting the substantial benefits that can be achieved by preventing waste in the first place. This paper quantitatively demonstrates the necessity to adopt impact-based targets that go beyond estimating the greenhouse gas emissions of waste and highlights the substantial benefits of waste reduction and prevention. Using a state-of-the-art waste environmental footprint tool, the paper quantifies the overall environmental impacts of Scotland's household waste and shows how targeting ‘heavy’ materials does not necessarily have the highest overall environmental benefit. Results show that embodied environmental impacts of household waste dominate the total environmental burdens, contributing more than 90% to the whole life cycle impacts, and hence policymakers should prioritise interventions that aim at waste reduction and prevention. Moreover, our analysis shows that food and textile wastes are high-priority materials in Scotland, with the largest contribution to overall environmental burdens; up to 42% and 30%, respectively. Considering the overall environmental impacts of specific waste materials will enable policymakers to develop more granular and targeted interventions to accelerate our transition to a sustainable circular economy.}
}
@article{OKEREKE2014637,
title = {Virtual testing of advanced composites, cellular materials and biomaterials: A review},
journal = {Composites Part B: Engineering},
volume = {60},
pages = {637-662},
year = {2014},
issn = {1359-8368},
doi = {https://doi.org/10.1016/j.compositesb.2014.01.007},
url = {https://www.sciencedirect.com/science/article/pii/S1359836814000109},
author = {M.I. Okereke and A.I. Akpoyomare and M.S. Bingley},
keywords = {A. Polymer–matrix composites (PMCs), C. Computational modelling, C. Numerical analysis, E. Weaving, Virtual testing},
abstract = {This paper documents the emergence of virtual testing frameworks for prediction of the constitutive responses of engineering materials. A detailed study is presented, of the philosophy underpinning virtual testing schemes: highlighting the structure, challenges and opportunities posed by a virtual testing strategy compared with traditional laboratory experiments. The virtual testing process has been discussed from atomistic to macrostructural length scales of analyses. Several implementations of virtual testing frameworks for diverse categories of materials are also presented, with particular emphasis on composites, cellular materials and biomaterials (collectively described as heterogeneous systems, in this context). The robustness of virtual frameworks for prediction of the constitutive behaviour of these materials is discussed. The paper also considers the current thinking on developing virtual laboratories in relation to availability of computational resources as well as the development of multi-scale material model algorithms. In conclusion, the paper highlights the challenges facing developments of future virtual testing frameworks. This review represents a comprehensive documentation of the state of knowledge on virtual testing from microscale to macroscale length scales for heterogeneous materials across constitutive responses from elastic to damage regimes.}
}
@article{CAUDEK2021317,
title = {Susceptibility to eating disorders is associated with cognitive inflexibility in female university students},
journal = {Journal of Behavioral and Cognitive Therapy},
volume = {31},
number = {4},
pages = {317-328},
year = {2021},
issn = {2589-9791},
doi = {https://doi.org/10.1016/j.jbct.2021.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S2589979121000159},
author = {Corrado Caudek and Claudio Sica and Silvia Cerea and Ilaria Colpizzi and Debora Stendardi},
keywords = {Eating disorders, Cognitive inflexibility, Individual differences, Computational modeling, Reversal learning},
abstract = {Summary
The inability to learn from and adapt to changing feedback in our environment may be etiologically linked to eating disorders (EDs). However, previous investigations on this issue have shown conflicting results. In the current study with a non-clinical sample of female students, we investigated the relation between cognitive inflexibility (CI) and vulnerability to EDs by using a modified version of the probabilistic reversal learning (PRL) task, which requires participants to adapt their response strategy according to changes in stimulus-reward contingencies. We found that females vulnerable to EDs in the general population showed an impaired PRL performance, also after controlling for comorbidity. However, our results also show that the ED construct comprises separate dimensions, which affect contingency learning in opposite manners: some individuals vulnerable to EDs showed impaired contingency learning; others used unimpaired contingency learning skills to pursue self-harming goals. Such results point to the necessity of an appropriate assessment of CI in order to better apply individualized treatment.}
}
@article{CHAUDHARI2025107968,
title = {Alzheimer’s disease prediction using CAdam optimized reinforcement learning-based deep convolutional neural network model},
journal = {Biomedical Signal Processing and Control},
volume = {108},
pages = {107968},
year = {2025},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2025.107968},
url = {https://www.sciencedirect.com/science/article/pii/S1746809425004793},
author = {Puja A. Chaudhari and Suhas S. Khot},
keywords = {Alzheimer prediction, CAdam optimizer, Magnetic Resonance Imaging, Reinforcement learning, Deep Convolutional neural network},
abstract = {Background
Alzheimer’s Disease (AD), a neurological disorder, gradually declines cognitive ability, but detecting it at an early stage can effectively mitigate symptoms. Due to the shortage of expertise medical staff, automatic diagnosis becomes highly important, however, a detailed analysis of brain disorder tissues is required for accurate diagnosis using magnetic resonance imaging (MRI). Various detection methods are introduced to detect AD through MRI, but extracting the optimal brain regions and informative features is still a complicated and time-consuming factor. Moreover, the class imbalance issue of the OASIS and ADNI datasets needs to be addressed.
Method
Here, a Coyote Adam optimized Reinforcement Learning-Deep Convolutional Neural Network (CAdam-RL-DCNN) is proposed to address the aforementioned issues on AD detection using MRI. The effectiveness of the proposed method relies on effectively detecting the features automatically and SMOTE handles the class imbalance issues of the dataset through the minority samples. The computational complexity of the model is reduced through the appropriate model training using the proposed CAdam optimizer, which incorporates adaptive parameters of Adam using social behaviors and invasive hunting of coyote optimizer. In addition, the hybrid features combining the ResNet features, statistical features and modified textural pattern reduces the data complexity and promotes the model training towards an improved performance in AD prediction.
Result
The proposed model attains 96.31% accuracy, 97.50% sensitivity, 94.06% specificity, 93.87% precision, 97.50% recall, and 95.65% F1-score using ADNI dataset. Furthermore, the proposed model attains the superior performance achieving 95.09% accuracy, 94.52% sensitivity, 95.57% specificity, 93.14% precision, 94.52% recall, and 93.83% F1-score using OASIS dataset respectively.}
}
@article{BALKHI20051223,
title = {Proteomics of Acute Myeloid Leukemia: Cytogenetic Risk Groups Differ Specifically in Their Proteome, Interactome and Posttranslational Protein Modifications.},
journal = {Blood},
volume = {106},
number = {11},
pages = {1223},
year = {2005},
issn = {0006-4971},
doi = {https://doi.org/10.1182/blood.V106.11.1223.1223},
url = {https://www.sciencedirect.com/science/article/pii/S0006497119761138},
author = {Mumtaz Y. Balkhi and Mulu Geletu and Maximilian Christopeit and Hermann M. Behre and Gerhard Behre},
abstract = {Acute Myeloid Leukemia (AML) is characterized by specific cytogenetic aberrations which are strong determinants of prognostic outcome and therapeutic response. Because the clinical outcome in AML cytogenetic groups differs considerably, we hypothesized that cytogenetic risk groups of AML might differ specifically in their proteome, protein interaction pathways and posttranslational modifications (PTMs). Thus, we determined the proteome of 30 AML patients belonging to various cytogenetic groups based on two-dimensional gel electrophoresis and Nano LC coupled MALDI-TOF-TOF tandem mass spectrometry. We could identify substantial differences in the proteome, protein expression and peak pattern between cytogenetic risk groups of AML. The interactome analysis based on computational bioinformatics using Ingenuity analysis revealed major regulating networks: MAPK8 and MYC for complex aberrant karyotype AML, TP53 for t(8;21)-AML, TP53- MYC- PRKAC for 11q23-AML, JUN and MYC for inv(16)-AML. Most interestingly, peak explorer analysis revealed a modification of O-linked acetyl glucosamine of hnRNPH1 in AML patients with a 11q23 translocation, an acetylation of calreticulin in t(8;21) translocation AML, an increased intensity of dimethylated peptide of hnRNPA2/B1 in AML patients with translocations of t(8;21) and inv(16) in comparison to healthy bone marrow. We show for the first time that cytogenetic risk groups of AML differ specifically both in their proteome, interactome and PTMs. These findings lead to a new thinking about the pathogenesis of AML and has major therapeutic implications because PTMs are the primary drug targets.}
}
@article{XIE2023119,
title = {2D magnetotelluric inversion based on ResNet},
journal = {Artificial Intelligence in Geosciences},
volume = {4},
pages = {119-127},
year = {2023},
issn = {2666-5441},
doi = {https://doi.org/10.1016/j.aiig.2023.08.003},
url = {https://www.sciencedirect.com/science/article/pii/S2666544123000266},
author = {LiAn Xie and Bo Han and Xiangyun Hu and Ningbo Bai},
keywords = {Magnetotellurics, 2D inversion, Residual network, Deep learning},
abstract = {In this study, a deep learning algorithm was applied to two-dimensional magnetotelluric (MT) data inversion. Compared with the traditional linear iterative inversion methods, the MT inversion method based on convolutional neural networks (CNN) does not rely on the selection of the initial model parameters and does not fall into the local optima. Although the CNN inversion models can provide a clear electrical interface division, their inversion results may remain prone to abrupt electrical interfaces as opposed to the actual underground electrical situation. To solve this issue, a neural network with a residual network architecture (ResNet-50) was constructed in this study. With the apparent resistivity and phase pseudo-section data as the inputs and with the resistivity parameters of the geoelectric model as the training labels, the modified ResNet-50 model was trained end-to-end for producing samples according to the corresponding production strategy of the study area. Through experiments, the training of the ResNet-50 with the dice loss function effectively solved the issue of over-segmentation of the electrical interface by the cross-entropy function, avoided its abrupt inversion, and overcame the computational inefficiency of the traditional iterative methods. The proposed algorithm was validated against MT data measured from a geothermal field prospect in Huanggang, Hubei Province, which showed that the deep learning method has opened up broad prospects in the field of MT data inversion.}
}
@article{JARLEBLAD2024108930,
title = {A framework for synthetic diagnostics using energetic-particle orbits in tokamaks},
journal = {Computer Physics Communications},
volume = {294},
pages = {108930},
year = {2024},
issn = {0010-4655},
doi = {https://doi.org/10.1016/j.cpc.2023.108930},
url = {https://www.sciencedirect.com/science/article/pii/S0010465523002758},
author = {H. Järleblad and L. Stagner and M. Salewski and J. Eriksson and M. Nocente and B.S. Schmidt and M. {Rud Larsen}},
keywords = {Nuclear fusion, Fast ions, Orbits, Weight functions},
abstract = {In fusion plasma physics, the large-scale trajectories of energetic particles in magnetic confinement devices are known as orbits. To effectively and efficiently be able to work with orbits, the Orbit Weight Computational Framework (OWCF) was developed. The OWCF constitutes a set of scripts, functions and applications capable of computing, visualizing and working with quantities related to fast-ion (FI) orbits in toroidally symmetric fusion devices. The current version is highly integrated with the DRESS code, which enables the OWCF to compute and analyze the orbit sensitivity for arbitrary neutron- and gamma-diagnostics. However, the framework is modular in the sense that any future codes (e.g. FIDASIM) can be easily integrated. The OWCF can also compute projected velocity spectra for FI orbits, which play a key role in many FI diagnostics. Via interactive applications, the OWCF can function both as a tool for investigative research but also for teaching. The OWCF will be used to analyze and simulate the diagnostic results of current and future fusion experiments such as ITER. The orbit weight functions computed with the OWCF can be used to reconstruct the FI distribution in terms of FI orbits from experimental measurements using tomographic inversion.}
}
@article{TAYLOR1999943,
title = {Towards the networks of the brain: from brain imaging to consciousness},
journal = {Neural Networks},
volume = {12},
number = {7},
pages = {943-959},
year = {1999},
issn = {0893-6080},
doi = {https://doi.org/10.1016/S0893-6080(99)00044-1},
url = {https://www.sciencedirect.com/science/article/pii/S0893608099000441},
author = {J.G. Taylor},
keywords = {Brain imaging, Consciousness, Structural modelling, Motion after-effect, Planning, Thinking, Self},
abstract = {The manner in which the brain computes in various tasks is being probed at a deep level by modern brain imaging techniques, with an increasing appreciation of the different networks being used to solve these tasks. There is simultaneously developing a neural modelling technology, which attempts to explain the underlying computations being performed by this set of networks. This paper describes results from brain imaging and how they may be related to the underlying neural networks by means of structural modelling. It thereby attempts to give an initial glimpse of the emerging picture of the functionality of brain networks. It concludes with a discussion of the role of consciousness in global processing, and how particular styles of neural processing can attain this.}
}
@article{RAM2022100232,
title = {The role of ‘big data’ and ‘in silico’ New Approach Methodologies (NAMs) in ending animal use – A commentary on progress},
journal = {Computational Toxicology},
volume = {23},
pages = {100232},
year = {2022},
issn = {2468-1113},
doi = {https://doi.org/10.1016/j.comtox.2022.100232},
url = {https://www.sciencedirect.com/science/article/pii/S2468111322000202},
author = {Rebecca N. Ram and Domenico Gadaleta and Timothy E.H. Allen},
keywords = {Computational toxicology, In-silico, NAMs, New approach methodologies, Human relevant, QSAR, Read across, Chemical safety, High throughput, Adverse outcome pathways},
abstract = {In silico (computational) methods continue to evolve as part of a robust 21st century public health strategy in risk assessment, relevant to all sectors of chemical safety including preclinical drug discovery, industrial chemicals testing, food and cosmetics. Alongside in vitro methods as components of intelligent testing and pathway driven strategies, in silico models provide the potential for more human relevant solutions to the use of animals in safety testing and biomedical research. These are often termed ‘New Approach Methodologies’ (NAMs). Some NAMs incorporate the use of ‘big data’, for example the information provided from high throughput or high content in vitro screening assays or ‘omics’ technologies. Big data has increasing relevance to predictive toxicology but must be appropriately defined, particularly with regard to ‘quality vs quantity’. The purpose of this article is to provide a commentary on the progress of in silico human-based research methods within the context of NAMs, as well as discussion of the emerging use of big data with relevance to safety assessment. The current status of in silico methods is discussed, with input from researchers in the field. Scientific and legislative drivers for change are also considered, along with next steps to address challenges in funding and recognition, to achieve regulatory acceptance and uptake within the research community. To provide some wider context, the use of in silico methods alongside other relevant approaches (e.g., human-based in vitro) is also discussed.}
}
@article{ROBSON2014287,
title = {When do aquatic systems models provide useful predictions, what is changing, and what is next?},
journal = {Environmental Modelling & Software},
volume = {61},
pages = {287-296},
year = {2014},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2014.01.009},
url = {https://www.sciencedirect.com/science/article/pii/S1364815214000188},
author = {Barbara J. Robson},
keywords = {Modelling philosophy, Biogeochemical modelling, Ecological models, Developments, Progress, Knowledge gaps},
abstract = {This article considers how aquatic systems modelling has changed since 1995 and how it must change in future if we are to continue to advance. A distinction is made between mechanistic and statistical models, and the relative merits of each are considered. The question of “when do aquatic systems models provide accurate and useful predictions?” is addressed, implying some guidelines for model development. It is proposed that, in general, ecological models only provide management-relevant predictions of the behaviour of real systems when there are strong physical (as opposed to chemical or ecological) drivers. Developments over the past 15 years have included changes in technology, changes in the modelling community and changes in the context in which modelling is conducted: the implications of each are briefly discussed. Current trends include increased uptake of best practice guidelines, increasing integration of models, operationalisation, data assimilation, development of improved tools for skill assessment, and application of models to new management questions and in new social contexts. Deeper merging of statistical and mechanistic modelling approaches through such techniques as Bayesian Melding, Bayesian Hierarchical Modelling and surrogate modelling is identified as a key emerging area. Finally, it is suggested that there is a need to systematically identify areas in which our current models are inadequate. We do not yet know for which categories of problems well-implemented aquatic systems models can (or cannot) be expected to accurately predict observational data and system behaviour. This can be addressed through better modelling and publishing practices.}
}
@article{LOU2023541,
title = {Human Creativity in the AIGC Era},
journal = {She Ji: The Journal of Design, Economics, and Innovation},
volume = {9},
number = {4},
pages = {541-552},
year = {2023},
issn = {2405-8726},
doi = {https://doi.org/10.1016/j.sheji.2024.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S2405872624000054},
author = {Yongqi Lou},
keywords = {AIGC, Artificial intelligence, Meaning-making, Paradigmatic innovation, Human values},
abstract = {Recent advances in artificial intelligence raise profound questions for humanity. Is the artificial intelligence-generated content (AIGC) technology merely a tool? Or is AIGC developing a level of creativity comparable to that of human beings? This essay explores the challenges and opportunities that AIGC technology brings to creativity, industry, and the ways of living of people around the world. These questions involve scale, authenticity, choice, and wisdom. Further, this essay addresses the core capabilities of future creative workers in the era of AIGC. The author believes that the ability to create meaning—meaning making—is and will remain a distinctive strength of human creativity in the AIGC era. To build on this strength, human beings must focus on six key areas: human-centered values, paradigmatic innovation, holistic experiences, cultural awareness, situational connections, and narrative reasoning. The best outcome for the AIGC is to make machines more machine-like and humans more human. Achieving this goal requires a cultural renaissance. We must break through the limits of computational rationality with the brilliance of humanity.}
}
@incollection{MOUSTAFA202149,
title = {3 - Deductive reasoning abilities in schizophrenia and related disorders: A systematic review},
editor = {Ahmed A. Moustafa},
booktitle = {Cognitive and Behavioral Dysfunction in Schizophrenia},
publisher = {Academic Press},
pages = {49-65},
year = {2021},
isbn = {978-0-12-820005-6},
doi = {https://doi.org/10.1016/B978-0-12-820005-6.00004-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128200056000049},
author = {Ahmed A. Moustafa and Anchal Garg and Ahmed A. Helal and Eid {Abo Hamza}},
keywords = {Schizophrenia, Delusions, Hallucinations, Negative symptoms, Reasoning, Transitive inference, Wason Selection Task, Syllogism, Inductive vs. deductive reasoning},
abstract = {Background: Schizophrenia is a psychiatric disorder characterized by delusions, hallucinations, negative symptoms, and disorganized thinking. There has been a multitude of studies assessing reasoning performance in schizophrenia patients by using various reasoning tasks. Methods: We reviewed the existing literature using the following reasoning tasks in schizophrenia and related disorders: Transitive Inferences, Wason Card Selection, conditional reasoning, syllogisms, and other related tasks. Results: Some deductive reasoning studies have reported conflicting results where schizophrenia patients sometimes, outperform or underperform healthy controls. These findings are related to the plausibility, emotional content of logical sentences used in these studies. Importantly, data show that performance in deductive reasoning tasks is impacted by emotional and cognitive processes, such as theory of mind and working memory. However, neural studies report different brain mechanisms underlying different deductive reasoning task performance. Conclusions: Overall, there are differences in the findings of reasoning tasks which should be investigated in future studies as it will contribute towards an accurate understanding of reasoning processes in schizophrenia spectrum and related disorders.}
}
@article{ISMAILOVA2021341,
title = {Cognitive System to Clarify the Semantic Vulnerability and Destructive Substitutions},
journal = {Procedia Computer Science},
volume = {190},
pages = {341-360},
year = {2021},
note = {2020 Annual International Conference on Brain-Inspired Cognitive Architectures for Artificial Intelligence: Eleventh Annual Meeting of the BICA Society},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.06.044},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921012898},
author = {Larisa Ismailova and Viacheslav Wolfengagen and Sergey Kosikov},
keywords = {cognitive system, information process, knowledge stage, cognitive interference, semantic web, functor-as-object, dynamics, semantic virus, variable sets, category theory},
abstract = {The development of special mathematics capable of directly taking into account the dynamics of the problem domain, as it turns out, is a non-trivial task. Its very formulation in a refined form and the fixation of the most important features cause noticeable complications in the target formalism, significantly complicating the development of software. A constructive solution to this problem is given, obtained using the original functor-as-object construction. The concept of semantic viralization is introduced. It is expected that the obtained computational model has a high innovative potential for the development of information systems designed for intensive data exchange.}
}
@article{JIANG2021116441,
title = {Impacts of COVID-19 on energy demand and consumption: Challenges, lessons and emerging opportunities},
journal = {Applied Energy},
volume = {285},
pages = {116441},
year = {2021},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2021.116441},
url = {https://www.sciencedirect.com/science/article/pii/S030626192100009X},
author = {Peng Jiang and Yee Van Fan and Jiří Jaromír Klemeš},
keywords = {COVID-19, Energy impacts, Environmental impacts, Energy recovery, Lessons, Emerging opportunities},
abstract = {COVID-19 has caused great challenges to the energy industry. Potential new practices and social forms being facilitated by the pandemics are having impacts on energy demand and consumption. Spatial and temporal heterogeneities of impacts appear gradually due to the dynamics of pandemics and mitigation measures. This paper overviews the impacts and challenges of COVID-19 pandemics on energy demand and consumption and highlights energy-related lessons and emerging opportunities. The discussion on energy-related issues is divided into four main sections: emergency situation and its impacts, environmental impacts and stabilising energy demand, recovering energy demand, and lessons and emerging opportunities. The changes in energy requirements are compared and analysed from multiple perspectives according to available data and information. In general, although the overall energy demand declines, the spatial and temporal variations are complicated. The energy intensity has presented apparent changes, the extra energy for COVID-19 fighting is non-negligible for stabilising energy demand, and the energy recovery in different regions presents significant differences. A crucial issue has been to allocate and find energy-related emerging opportunities for the post pandemics. This study could offer a direction in opening new avenues for increasing energy efficiency and promoting energy saving.}
}
@incollection{ILLES2015735,
title = {Chapter 45 - Advances in Ethics for the Neuroscience Agenda},
editor = {Michael J. Zigmond and Lewis P. Rowland and Joseph T. Coyle},
booktitle = {Neurobiology of Brain Disorders},
publisher = {Academic Press},
address = {San Diego},
pages = {735-747},
year = {2015},
isbn = {978-0-12-398270-4},
doi = {https://doi.org/10.1016/B978-0-12-398270-4.00045-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780123982704000458},
author = {Judy Illes and Peter B. Reiner},
keywords = {animal model, biomedical science, data sharing, ethics, health, incidental finding, neuroscience, public policy, science communication},
abstract = {Critical thinking about ethics in neuroscience can be a powerful force in enabling research and translating results meaningfully for society. This chapter provides four examples of such an empowered approach to neuroscience. The authors discuss how upfront consideration of the societal implications of advances in neuroscience can shape the use of animal models. They situate ethical thinking in this era of big science and big data, reflecting on strategies for sharing databases while protecting contributors and users. They highlight how collaboration among neuroscientists, ethicists, and others can produce positive measures to resolve the problem of incidental discoveries in brain imaging research, as one example of debates on incidental findings more broadly. The mandate of neuroscience research as public service and ethical imperative is addressed by describing opportunities for neuroscientists to engage with societal issues emerging from their research, and how this deepens the discourse and adds value to the research enterprise.}
}
@article{HAUSER201778,
title = {The Universal Generative Faculty: The source of our expressive power in language, mathematics, morality, and music},
journal = {Journal of Neurolinguistics},
volume = {43},
pages = {78-94},
year = {2017},
note = {Language Evolution: On the Origin of Lexical and Syntactic Structures},
issn = {0911-6044},
doi = {https://doi.org/10.1016/j.jneuroling.2016.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S0911604416300811},
author = {Marc D. Hauser and Jeffrey Watumull},
keywords = {Domain-specificity, Evolution, Generative functions, Language faculty, Recursion, Turing machine, Universal generative faculty},
abstract = {Many have argued that the expressive power of human thought comes from language. Language plays this role, so the argument goes, because its generative computations construct hierarchically structured, abstract representations, covering virtually any content and communicated in linguistic expressions. However, language is not the only domain to implement generative computations and abstract representations, and linguistic communication is not the only medium of expression. Mathematics, morality, and music are three others. These similarities are not, we argue, accidental. Rather, we suggest they derive from a common computational system that we call the Universal Generative Faculty or UGF. UGF is, at its core, a suite of contentless generative procedures that interface with different domains of knowledge to create contentful expressions in thought and action. The representational signatures of different domains are organized and synthesized by UGF into a global system of thought. What was once considered the language of thought is, on our view, the more specific operation of UGF and its interfaces to different conceptual domains. This view of the mind changes the conversation about domain-specificity, evolution, and development. On domain-specificity, we suggest that if UGF provides the generative engine for different domains of human knowledge, then the specificity of a given domain (e.g., language, mathematics, music, morality) is restricted to its repository of primitive representations and to its interfaces with UGF. Evolutionarily, some generative computations are shared with other animals (e.g., combinatorics), both for recognition-learning and generation-production, whereas others are uniquely human (e.g., recursion); in some cases, the cross-species parallels may be restricted to recognition-learning, with no observable evidence of generation-production. Further, many of the differences observed between humans and other animals, as well as among nonhuman animals, are the result of differences in the interfaces: whereas humans promiscuously traverse (consciously and unconsciously) interface conditions so as to combine and analogize concepts across many domains, nonhuman animals are far more limited, often restricted to a specific domain as well as a specific sensory modality within the domain. Developmentally, the UGF perspective may help explain why the generative powers of different domains appear at different stages of development. In particular, because UGF must interface with domain-specific representations, which develop on different time scales, the generative power of some domains may mature more slowly (e.g., mathematics) than others (e.g., language). This explanation may also contribute to a deeper understanding of cross-cultural differences among human populations, especially cases where the generative power of a domain appears absent (e.g., cultures with only a few count words). This essay provides an introduction to these ideas, including a discussion of implications and applications for evolutionary biology, human cognitive development, cross-cultural variation, and artificial intelligence.}
}
@article{OH2015e6,
title = {The effects of simulation-based learning using standardized patients in nursing students: A meta-analysis},
journal = {Nurse Education Today},
volume = {35},
number = {5},
pages = {e6-e15},
year = {2015},
issn = {0260-6917},
doi = {https://doi.org/10.1016/j.nedt.2015.01.019},
url = {https://www.sciencedirect.com/science/article/pii/S0260691715000507},
author = {Pok-Ja Oh and Kyeong Deok Jeon and Myung Suk Koh},
keywords = {Students, Nursing, Patient simulation, Meta-analysis},
abstract = {Summary
Purpose
The aim of this study was to evaluate the effect of simulation-based learning using standardized patients (SPs) on cognitive, affective, and psychomotor domain outcomes of learning in nursing students.
Methods
MEDLINE via PubMed, Cochrane Library CENTRAL, EMBASE, CINAHL, and several Korean electronic databases (to June 2014) were searched. The RevMan 5.3 program of the Cochrane library was used for data analysis.
Results
A meta-analysis was conducted of 18 controlled trials (4 randomized and 14 non-randomized designs), with a total of 1326 nursing students. Overall, simulation-based learning using SPs appeared to have beneficial effects on the cognitive, affective, and psychomotor domains of learning. In subgroup analysis, use of SPs showed significant effects on knowledge acquisition (d=0.38, p=.05, I2=42%), communication skill (d=1.86, p<.001, I2=15%), self-efficacy (d=0.61, p<.001, I2=6%), learning motivation (d=0.77, p<.001, I2=0%) and clinical competence (d=0.72, p<.001, I2=0%). Treatment effects on critical thinking (p=.75) and learning satisfaction (p=.43) were not significant.
Conclusion
The findings of the current study suggest that simulation-based learning using SPs might have a positive impact on self efficacy and learning motivation that affects knowledge and clinical skill acquisition. Therefore, these findings demonstrate that, if integrated appropriately, an SP educational approach can be used in academic settings as an active learning methodology.}
}
@article{PIERCE2024,
title = {Identifying Factors Associated With Heightened Anxiety During Breast Cancer Diagnosis Through the Analysis of Social Media Data on Reddit: Mixed Methods Study},
journal = {JMIR Cancer},
volume = {10},
year = {2024},
issn = {2369-1999},
doi = {https://doi.org/10.2196/52551},
url = {https://www.sciencedirect.com/science/article/pii/S2369199924000569},
author = {Joni Pierce and Mike Conway and Kathryn Grace and Jude Mikal},
keywords = {breast cancer, anxiety, NLP, natural language processing, mixed methods study, cancer diagnosis, social media apps, descriptive analysis, diagnostic progression, patient-centered care},
abstract = {Background
More than 85% of patients report heightened levels of anxiety following breast cancer diagnosis. Anxiety may become amplified during the early stages of breast cancer diagnosis when ambiguity is high. High levels of anxiety can negatively impact patients by reducing their ability to function physically, make decisions, and adhere to treatment plans, with all these elements combined serving to diminish the quality of life.
Objective
This study aimed to use individual social media posts about breast cancer experiences from Reddit (r/breastcancer) to understand the factors associated with breast cancer–related anxiety as individuals move from suspecting to confirming cancer diagnosis.
Methods
We used a mixed method approach by combining natural language processing–based computational methods with descriptive analysis. Our team coded the entire corpus of 2170 unique posts from the r/breastcancer subreddit with respect to key variables, including whether the post was related to prediagnosis, diagnosis, or postdiagnosis concerns. We then used Linguistic Inquiry and Word Count (LIWC) to rank-order the codified posts as low, neutral, or high anxiety. High-anxiety posts were then retained for deep descriptive analysis to identify key themes relative to diagnostic progression.
Results
After several iterations of data analysis and classification through both descriptive and computational methods, we identified a total of 448 high-anxiety posts across the 3 diagnostic categories. Our analyses revealed that individuals experience higher anxiety before a confirmed cancer diagnosis. Analysis of the high-anxiety posts revealed that the factors associated with anxiety differed depending on an individual’s stage in the diagnostic process. Prediagnosis anxiety was associated with physical symptoms, cancer-related risk factors, communication, and interpreting medical information. During the diagnosis period, high anxiety was associated with physical symptoms, cancer-related risk factors, communication, and difficulty navigating the health care system. Following diagnosis, high-anxiety posts generally discussed topics related to treatment options, physical symptoms, emotional distress, family, and financial issues.
Conclusions
This study has practical, theoretical, and methodological implications for cancer research. Content analysis reveals several possible drivers of anxiety at each stage (prediagnosis, during diagnosis, and postdiagnosis) and provides key insights into how clinicians can help to alleviate anxiety at all stages of diagnosis. Findings provide insights into cancer-related anxiety as a process beginning before engagement with the health care system: when an individual first notices possible cancer symptoms. Uncertainty around physical symptoms and risk factors suggests the need for increased education and improved access to trained medical staff who can assist patients with questions and concerns during the diagnostic process. Assistance in understanding technical reports, scheduling, and patient-centric clinician behavior may pinpoint opportunities for improved communication between patients and providers.}
}
@article{STROBL2024104585,
title = {Counterfactual formulation of patient-specific root causes of disease},
journal = {Journal of Biomedical Informatics},
volume = {150},
pages = {104585},
year = {2024},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2024.104585},
url = {https://www.sciencedirect.com/science/article/pii/S1532046424000030},
author = {Eric V. Strobl},
keywords = {Root cause analysis, Causal inference, Precision medicine, Causal discovery, Computational medicine},
abstract = {Objective:
Root causes of disease intuitively correspond to root vertices of a causal model that increase the likelihood of a diagnosis. This description of a root cause nevertheless lacks the rigorous mathematical formulation needed for the development of computer algorithms designed to automatically detect root causes from data. We seek a definition of patient-specific root causes of disease that models the intuitive procedure routinely utilized by physicians to uncover root causes in the clinic.
Methods:
We use structural equation models, interventional counterfactuals and the recently developed mathematical formalization of backtracking counterfactuals to propose a counterfactual formulation of patient-specific root causes of disease matching clinical intuition.
Results:
We introduce a definition of patient-specific root causes of disease that climbs to the third rung of Pearl’s Ladder of Causation and matches clinical intuition given factual patient data and a working causal model. We then show how to assign a root causal contribution score to each variable using Shapley values from explainable artificial intelligence.
Conclusion:
The proposed counterfactual formulation of patient-specific root causes of disease accounts for noisy labels, adapts to disease prevalence and admits fast computation without the need for counterfactual simulation.}
}
@article{CHEN201910,
title = {An artificial intelligence based data-driven approach for design ideation},
journal = {Journal of Visual Communication and Image Representation},
volume = {61},
pages = {10-22},
year = {2019},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2019.02.009},
url = {https://www.sciencedirect.com/science/article/pii/S1047320319300604},
author = {Liuqing Chen and Pan Wang and Hao Dong and Feng Shi and Ji Han and Yike Guo and Peter R.N. Childs and Jun Xiao and Chao Wu},
keywords = {Idea generation, Artificial intelligence in design, Data-driven design, Generative adversarial networks, Semantic network analysis, Network visualisation, Computational creativity},
abstract = {Ideation is a source of innovation and creativity, and is commonly used in early stages of engineering design processes. This paper proposes an integrated approach for enhancing design ideation by applying artificial intelligence and data mining techniques. This approach consists of two models, a semantic ideation network and a visual concepts combination model, which provide inspiration semantically and visually based on computational creativity theory. The semantic ideation network aims to provoke new ideas by mining potential knowledge connections across multiple knowledge domains, and this was achieved by applying “step-forward” and “path-track” algorithms which assist in exploring forward given a concept and in tracking back the paths going from a departure concept through a destination concept. In the visual concepts combination model, a generative adversarial networks model is proposed for generating images which synthesize two distinct concepts. An implementation of these two models was developed and tested in a design case study, which indicated that the proposed approach is able to not only generate a variety of cross-domain concept associations but also advance the ideation process quickly and easily in terms of quantity and novelty.}
}
@article{ANDRADE2022102986,
title = {Writing styles and modes of engagement with the future},
journal = {Futures},
volume = {141},
pages = {102986},
year = {2022},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2022.102986},
url = {https://www.sciencedirect.com/science/article/pii/S0016328722000866},
author = {Stefan B. Andrade and Anneke Sools and Yashar Saghai},
keywords = {Anticipation, Writing style, Modes of engagement with the future, Anticipatory functions, narrative, Digital story grammar},
abstract = {This paper present a new approach to analyze how people anticipate the future in times of uncertainty. Our approach combines insights from narrative theory and the sociology of anticipatory modes of engagement with the future. We applied a mixed method approach to analyze 166 letters from a creative writing exercise where residents from five countries was asked to write retrospectively from the viewpoint of a desired post-corona future. Using the methodology of Digital Story Grammar, we first categorized the letters given their grammatical structure in terms of who are in stories (characters), what the stories are about (type of action), and to what or whom were the actions directed to (objects for the character’s actions). This resulted in four writing styles: (1) analytical-observational, (2) collective-moral, (3) dialogical-personal, and (4) sensory-emotional. Consequently, we interpreted the four writing styles qualitatively in relation to the theory of modes of engagement with the future (i.e., familiarity, plans, exploration, and justification). We conclude by reflecting on the relationships between writing styles and modes in a multi-paradigmatic approach to the study of anticipation and the relevance to scenario-building practices.}
}
@article{DO2000483,
title = {Intentions in and relations among design drawings},
journal = {Design Studies},
volume = {21},
number = {5},
pages = {483-503},
year = {2000},
issn = {0142-694X},
doi = {https://doi.org/10.1016/S0142-694X(00)00020-X},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X0000020X},
author = {Ellen Yi-Luen Do and Mark D Gross and Bennett Neiman and Craig Zimring},
keywords = {drawing(s), architectural design, case study/studies, design activity, design research},
abstract = {Designers use drawings to explore alternatives and to test ideas. We report here on two studies on design and drawing. The first study of design drawing symbols aims to determine whether and to what extent it is possible to infer, interpret, or even guess what a designer was thinking about by looking at the drawings she has made. In the second study we examined a collection of drawings for the design of a house to investigate the systems of design transformations. Drawings are characterized by drawing style, projection type, and key elements. We analyzed the relationships among the drawings and developed a notation system for documenting these relationships.}
}
@article{BATTAGLIA2025197,
title = {The paradox of the self-studying brain},
journal = {Physics of Life Reviews},
volume = {52},
pages = {197-204},
year = {2025},
issn = {1571-0645},
doi = {https://doi.org/10.1016/j.plrev.2024.12.009},
url = {https://www.sciencedirect.com/science/article/pii/S1571064524001787},
author = {Simone Battaglia and Philippe Servajean and Karl J. Friston},
keywords = {Theoretical neuroscience, Consciousness, Perception, Introspection, Neurophenomenology, Brain},
abstract = {The paradox of a brain trying to study itself presents a conundrum, raising questions about self-reference, consciousness, psychiatric disorders, and the boundaries of scientific inquiry. By which means can this complex organ shift the focus of study towards itself? We aim at unpacking the intricacies of this paradox. Historically, this question has been raised by philosophers under different frameworks. Thanks to the development of novel techniques to study the brain on a functional and structural level - as well as neurostimulation protocols that can modulate its activity in selected areas - we now possess advanced methods to progress this intricate inquiry. Nonetheless, the broader implications of the brain's pursuit of understanding itself remain unclear to this day. Ultimately, the need to employ both perception and introspection has led to different formulations of consciousness. This creates a challenge, as evidence supporting one formulation does not necessarily support the other. By deconstructing the paradoxical nature of self understanding - from a philosophical and neuroscientific point of view - we may gain insights into the human brain, which could lead to improved understanding of self-awareness and consciousness.}
}
@article{RUTHVEN2004259,
title = {Teacher representations of the successful use of computer-based tools and resources in secondary-school English, mathematics and science},
journal = {Teaching and Teacher Education},
volume = {20},
number = {3},
pages = {259-275},
year = {2004},
issn = {0742-051X},
doi = {https://doi.org/10.1016/j.tate.2004.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S0742051X04000113},
author = {Kenneth Ruthven and Sara Hennessy and Sue Brindley},
keywords = {Computer uses in education, Educational technology, Teacher attitude and cognition, Subject teaching and learning, Secondary education, England},
abstract = {This study investigated professional thinking about pedagogical aspects of technology use in mainstream classroom practice. It focuses on the systems of ideas which frame teacher accounts of the successful use of computer-based tools and resources in the core subjects of English, Mathematics and Science at secondary-school level. These accounts were elicited through group interviews with the relevant subject departments in six secondary schools in England. The analysis identifies seven broad themes in which teachers point to the contribution of technology use in: effecting working processes and improving production; supporting processes of checking, trialling and refinement; enhancing the variety and appeal of classroom activity; fostering pupil independence and peer support; overcoming pupil difficulties and building assurance; broadening reference and increasing currency of activity; and focusing on overarching issues and accentuating important features. Further examination of these themes shows how professional thinking about technology use is anchored in well-established representations of pupil motivation and classroom learning, and how contrasting subject profiles reflect corresponding differences in wider subject cultures.}
}
@article{KANSELAAR2001123,
title = {Computer supported collaborative learning Computer supported collaborative learning: cognitive and computational approaches: P. Dillenbourg (Ed.); Pergamon, Elsevier Science Ltd., Oxford, 1999, 246pp., ISBN 0-08-043073-2},
journal = {Teaching and Teacher Education},
volume = {17},
number = {1},
pages = {123-129},
year = {2001},
issn = {0742-051X},
doi = {https://doi.org/10.1016/S0742-051X(00)00042-1},
url = {https://www.sciencedirect.com/science/article/pii/S0742051X00000421},
author = {Gellof Kanselaar and Gijsbert Erkens and Jos Jaspers and Hermi (Tabachneck-) Schijf}
}
@article{XI2025106930,
title = {Depression detection based on the temporal-spatial-frequency feature fusion of EEG},
journal = {Biomedical Signal Processing and Control},
volume = {100},
pages = {106930},
year = {2025},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2024.106930},
url = {https://www.sciencedirect.com/science/article/pii/S1746809424009881},
author = {Yang Xi and Ying Chen and Tianyu Meng and Zhu Lan and Lu Zhang},
keywords = {Depression detection, EEG, Temporal-spatial-frequency feature, Channel selection, Attention mechanism},
abstract = {Depression is a prevalent affective psychiatric disorder projected to be the leading contributor to the world’s disease burden by 2030. Due to its high prevalence and low recognition rate, an objective and effective detection method is urgently needed. Deep learning methods based on electroencephalography (EEG) have shown significant potential in depression detection. However, excessive channels can increase redundancy and computational complexity in EEG, while irrelevant channels may reduce accuracy. Additionally, existing models often overlook the complementarity between the temporal-, spatial-, and frequency-domain features of EEG, limiting their detection capabilities. To address these issues, we propose a method that fuses the temporal, spatial, and frequency domain features of EEG to enhance the detection accuracy while eliminating redundant channels. We introduce an EEG channel selection method based on frequency domain weighting that automatically adjusts the channel weights to select the EEG channels that best capture spatial information across the delta, theta, alpha, beta, and gamma bands, thereby optimizing the extraction of spatial-frequency features. In addition, we designed a multiscale spatiotemporal convolutional attention network to extract the spatiotemporal features of EEG. In this network, the multiscale convolutional attention module enhanced the model’s ability to capture spatial features, whereas the temporal trend-aware self-attention module extracted long-term temporal features by analyzing global correlations across different time points. Experimental results on the MODMA dataset show that our method achieved a 97.24% detection accuracy, surpassing current state-of-the-art models. This study offers a novel approach for constructing depression detection models, providing a foundation for future research and application.}
}
@article{TSAI2017997,
title = {An empirical study on the incorporation of APP and progressive reasoning teaching materials for improving technical creativity amongst students in the subject of automatic control},
journal = {Computers in Human Behavior},
volume = {75},
pages = {997-1007},
year = {2017},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2016.10.031},
url = {https://www.sciencedirect.com/science/article/pii/S0747563216307117},
author = {Hsieh–Chih Tsai and Min Jou and JingYing Wang and Chun-Chiang Huang},
keywords = {APP, Progressive reasoning, Technical creativity, Scientific reasoning},
abstract = {This study reformed teaching materials for automatic control, a mandatory course for engineering students, and designed a set of digital teaching materials based upon progressive reasoning with hand-mind combinations. The teaching materials were mainly delivered via a hands-on APP. The authors conducted an empirical study as well as pre-tests and post-tests for a total of 118 sophomore students majoring in engineering at two Universities. Outcomes found that the progressive reasoning teaching materials designed for this course were helpful in improving student creativity and scientific reasoning. Significant improvements were also achieved in product design, technical methods, and technological ideas aspects of technological creativity and every scientific reasoning skill, with the exception of proportional reasoning. Results also identified strong correlation between technical creativity and scientific reasoning. This relationship may be further investigated in follow-up studies. This study also proposed recommendations for coordinating designs of digital teaching materials in other engineering courses with the development of student thinking.}
}
@article{SELIG2025105923,
title = {Using the kinematics of the RC linkage to find the degree of the adjoint representation of SE(3)},
journal = {Mechanism and Machine Theory},
volume = {206},
pages = {105923},
year = {2025},
issn = {0094-114X},
doi = {https://doi.org/10.1016/j.mechmachtheory.2025.105923},
url = {https://www.sciencedirect.com/science/article/pii/S0094114X25000126},
author = {J.M. Selig},
keywords = {Adjoint representation, Birational mappings, Study quadric, Assembly configurations},
abstract = {This work studies the projective algebraic variety formed from the closure of the adjoint representation of the group of rigid-body displacements, SE(3). This is motivated by asking how many assembly configurations a mechanism would have in general, if it was designed to keep six given lines in six linear line complexes. The main result is to find the degree of the variety defined by the adjoint representation and hence answer the motivating question. A simple special case is discussed, a mechanism that maintains a single given line reciprocal to three fixed lines from the regulus of a cylindrical hyperboloid of one sheet. The three dimensional variety defined in this way can be realised by an RC linkage. More specifically, the variety splits into two components each of which can be realised by an RC linkage. The homology of these 3-dimensional varieties, as subvarieties of the Study quadric, is found and used to determine the degree of the adjoint representation as an algebraic variety. The possible equations defining the variety determined by the adjoint representation of SE(3), are also discussed but no definitive result is found.}
}
@article{WADHWA2022101177,
title = {Most significant hotspot detection using improved particle swarm optimizers},
journal = {Swarm and Evolutionary Computation},
volume = {75},
pages = {101177},
year = {2022},
issn = {2210-6502},
doi = {https://doi.org/10.1016/j.swevo.2022.101177},
url = {https://www.sciencedirect.com/science/article/pii/S2210650222001444},
author = {Ankita Wadhwa and Manish Kumar Thakur},
keywords = {Hotspot detection, Emergency response planning, Scan statistics, Improved particle swarm optimization, Geospatial data},
abstract = {Significant circular hotspot detection (SCHD) aims at identifying those circular regions in a spatial space where the occurrence of a particular activity is uncommonly higher than the surrounding areas. Plentiful societal applications make SCHD a problem of utmost interest. In many domains, detection of the most significant circular hotspot (MSCHD) is of further usefulness. Well-timed detection of the most significant hotspot helps crucially for short term response planning (STRP) in situations like a disease outbreak, police vigilance, etc. State of the art methods like SaTScan, identify circular hotspots by listing all possible circles in the search area, followed by a statistical significance test, making it a very high computational cost problem. Considering their high costs, these methods are inefficient in applications related to STRP. To reduce the computational time of SaTScan, two randomized versions of the SaTScan algorithm are presented in this paper. Further, the MSCHD problem is modeled as an optimization problem and three improved variants of Particle Swarm Optimizer (PSO) namely I-PSO, HCL-PSO and Ensemble PSO are applied to detect the most significant hotspots. The comparative and sensitivity analysis are performed using synthetic datasets. The comparative analysis of SaTScan and the presented PSO based schemes is made in terms of the quality of identified hotspots and the computational time. Results reveal that the PSO based schemes (I-PSO, HCL-PSO, and Ensemble PSO) are promising and far efficient than randomized and traditional SaTScan algorithms. Further, for the datasets containing only a single hotspot, the performance of all PSO based schemes is at par with each other. However, for datasets with more than one hotspot in the study area, HCL-PSO has lower average rank than I-PSO and Ensemble PSO schemes and hence seems more promising for MSCHD. The superiority of HCL-PSO based hotspot detection is also validated using Friedman Test. Finally, the presented schemes are applied to the case study of Chicago city for identification of different types of crime hotspots.}
}
@incollection{VODOVOTZ201589,
title = {Chapter 4.2 - Data-Driven and Statistical Models: Everything Old Is New Again},
editor = {Yoram Vodovotz and Gary An},
booktitle = {Translational Systems Biology},
publisher = {Academic Press},
address = {Boston},
pages = {89-98},
year = {2015},
isbn = {978-0-12-397884-4},
doi = {https://doi.org/10.1016/B978-0-12-397884-4.00012-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780123978844000124},
author = {Yoram Vodovotz and Gary An},
keywords = {Data-driven modeling, statistical modeling, systems biology, computational biology, critical illness, inflammation},
abstract = {In this chapter, data-driven and statistical methods, and the thinking process behind them, are introduced. The development of these methods is associated with the thought process behind their use, both in the context of reductionist research as well as in the context of systems and computational biology. The concepts, advantages, and disadvantages of Big Data are discussed and contrasted with those of dynamic mechanistic modeling. Clinically translational applications of data-driven and statistical methods in the context of critical illness are presented and discussed as a gateway to true mechanistic modeling.}
}
@article{MALINVERNI2025100727,
title = {Scaffolding Children's critical reflection on intelligent technologies: Opportunities from speculative fiction},
journal = {International Journal of Child-Computer Interaction},
volume = {43},
pages = {100727},
year = {2025},
issn = {2212-8689},
doi = {https://doi.org/10.1016/j.ijcci.2025.100727},
url = {https://www.sciencedirect.com/science/article/pii/S2212868925000078},
author = {Laura Malinverni and Marie-Monique Schaper and Elisa Rubegni and Mariana Aki Tamashiro},
keywords = {AI literacy, Critical reflection, Speculative fiction, Children, Reflective AI literacy},
abstract = {Current technological development of Artificial Intelligence (AI) requires educational practices that address the social and ethical implications derived from these emerging technologies. To this end, an increasing number of educational practices are pursuing the goal of supporting children's critical reflection on these topics. Our research aims at understanding how speculative fiction-based resources can meet and respond to the goals of supporting children's critical reflection on AI technologies and their impact on society. Through revisiting relevant literature on these topics and critically analyzing our own practices in three different settings, we identify a set of opportunities and challenges oriented at guiding the design of resources capable of taking advantage of speculative fiction as a way to support critical reflection.}
}
@article{AYERS201883,
title = {The axiomatic approach to chemical concepts},
journal = {Computational and Theoretical Chemistry},
volume = {1142},
pages = {83-87},
year = {2018},
issn = {2210-271X},
doi = {https://doi.org/10.1016/j.comptc.2018.09.006},
url = {https://www.sciencedirect.com/science/article/pii/S2210271X18304237},
author = {Paul W. Ayers and Stijn Fias and Farnaz Heidar-Zadeh},
abstract = {Many concepts that are central to chemical language and thought emerge from the wealth of chemists’ historical experience and cannot be precisely defined mathematically from the underlying physics. In such cases, it is useful to take an axiomatic approach: list the chemical, mathematical and computational properties that one desires for a concept to possess, and then find the rigorous (and, if possible, elegant) mathematical formulation of the concept that satisfies those desiderata. This mathematical formulation is most useful if it relies on fundamental quantities—quantum-mechanical observables, reduced density matrices, or the N-electron wavefunction—rather than method-dependent quantities (e.g., orbitals) that are not defined for some computational approaches to the molecular electronic structure problem. This ensures that the pursuit of chemical intuition does not lead one too far from the underlying physics. It also ensures that one can interpret the results of any computational method, even methods (e.g., quantum Monte Carlo) that make no reference to any molecular-orbital or valence-bond model.}
}
@article{GUZMANURBINA2022109295,
title = {FIEMA, a system of fuzzy inference and emission analytics for sustainability-oriented chemical process design},
journal = {Applied Soft Computing},
volume = {126},
pages = {109295},
year = {2022},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2022.109295},
url = {https://www.sciencedirect.com/science/article/pii/S1568494622004859},
author = {Alexander Guzman-Urbina and Kakeru Ouchi and Hajime Ohno and Yasuhiro Fukushima},
keywords = {Sustainability engineering, Emission analytics, Fuzzy systems, Data clustering},
abstract = {In the quest to achieve sustainable development goals, developments in sustainability-oriented chemical process design are key to innovation in the chemical industry, especially important for processes aiming for sustainable fuels. One of the greatest challenges is the difficulty of modeling the highly complex interactions among the design variables, such as catalyst technology attributes, and greenhouse gas emissions. Most of the computational aids crucial to deal with the complexity of chemical processes require data that is either unavailable or uncertain at an early stage of design. The multistage integrated system for sustainable design proposed in this paper boosts these computational aids by applying data science techniques to allow uncertainty to be handled more efficiently, thereby facilitating the modeling of the interactions between the properties of new materials or processes and sustainability indicators. In this system, current data connectivity methods are used to find paths of correlation among catalysts properties and greenhouse gas emissions. The key feature of the proposed system relies on the integration through multiple stages of Fuzzy Inference systems and a data-driven technique for Emissions Analytics, FIEMA.11FIEMA: Fuzzy Inference and Emission Analytics. The algorithm in FIEMA provides a semi-supervised learning approach to emission analytics: it determines data clusters by a C-means algorithm and subsequently builds fuzzy sets for multiple stages of input–output inference. The proposed FIEMA system was demonstrated in an effort to determine the optimal configurations of the properties of catalysts to minimize the probability of associated greenhouse gas emissions for a methanol production process. The results showed the potential of this approach to reduce the search space of catalyst material designs by suggesting promising configurations for oxygen storage capacity, mechanical strength, lifetime, size, and poisoning level. The research impacts of this study contribute to the development of clean fuels by a computationally-efficient system for early design, and by the determination of catalysts development paths that assure an actual reduction of the life-cycle emissions.}
}
@article{ANGIONE2015102,
title = {Analysis and design of molecular machines},
journal = {Theoretical Computer Science},
volume = {599},
pages = {102-117},
year = {2015},
note = {Advances in Computational Methods in Systems Biology},
issn = {0304-3975},
doi = {https://doi.org/10.1016/j.tcs.2015.01.030},
url = {https://www.sciencedirect.com/science/article/pii/S0304397515000663},
author = {C. Angione and J. Costanza and G. Carapezza and P. Lió and G. Nicosia},
keywords = {Pareto optimality,  modelling, Turing machine, Molecular machine, Biological complexity, Petri nets, Register machines, Von Neumann architectures, Trade-off genetic strategies, Flux-balance analysis},
abstract = {Biologically inspired computation has been recently used with mathematical models towards the design of new synthetic organisms. In this work, we use Pareto optimality to optimize these organisms in a multi-objective fashion. We infer the best knockout strategies to perform specific tasks in bacteria, which involve concurrent maximization/minimization of multiple functions (codomain) and optimization of several decision variables (domain). Furthermore, we propose and exploit a mapping between the metabolism and a register machine. We show that optimized bacteria have computational capability and act as molecular Turing machines programmed using a Pareto optimal solution. Finally, we investigate communication between bacteria as a means to evaluate their computational capability. We report that the density and gradient of the Pareto curve are useful tools to compare models and understand their structure, while modelling organisms as computers proves useful to carry out computation using biological machines with specific input–output conditions, as well as to estimate the bacterial computational effort for specific tasks.}
}
@article{STOKLASA2021153,
title = {Possibilistic fuzzy pay-off method for real option valuation with application to research and development investment analysis},
journal = {Fuzzy Sets and Systems},
volume = {409},
pages = {153-169},
year = {2021},
note = {Games and Decision Analysis},
issn = {0165-0114},
doi = {https://doi.org/10.1016/j.fss.2020.06.012},
url = {https://www.sciencedirect.com/science/article/pii/S0165011420302475},
author = {Jan Stoklasa and Pasi Luukka and Mikael Collan},
keywords = {Finance, Transformation, Possibility theory, Real option valuation, Fuzzy pay-off method},
abstract = {This paper presents the first fully possibilistic method for real option valuation of investment projects, a new possibilistic variant of the fuzzy pay-off method for real option valuation. The new variant is derived by using the Luukka-Stoklasa-Collan transformation and is proven to be consistent with financial theory. The new variant is comparatively analyzed with the original method and the previously presented probabilistic variant. Fast computation formulae for the new variant in all use-cases in the triangular context are presented and complete fast computation formulae also for the previously presented probabilistic variant of the method are presented for the first time. The use of the new variant is illustrated with a set of numerical examples including examples of Research and Development investment analysis.}
}
@article{1989N1,
title = {Newsletter on computational and applied mathematics},
journal = {Journal of Computational and Applied Mathematics},
volume = {25},
number = {2},
pages = {N1-N18},
year = {1989},
issn = {0377-0427},
doi = {https://doi.org/10.1016/0377-0427(89)90050-2},
url = {https://www.sciencedirect.com/science/article/pii/0377042789900502}
}
@article{VIJAYALAKSHMI2022103179,
title = {Predicting Hepatitis B to be acute or chronic in an infected person using machine learning algorithm},
journal = {Advances in Engineering Software},
volume = {172},
pages = {103179},
year = {2022},
issn = {0965-9978},
doi = {https://doi.org/10.1016/j.advengsoft.2022.103179},
url = {https://www.sciencedirect.com/science/article/pii/S0965997822000898},
author = {C. Vijayalakshmi and S. Pakkir Mohideen},
keywords = {Hepatitis B, Machine learning, SVM, Stochastic gradient algorithm, Dataset},
abstract = {Hepatitis B is a viral infection which causes liver damage. It can lead to death. This hepatitis B along with Hepatitis C can cause hepatocellular carcinoma and liver cirrhosis. In this paper it is discussed about Hepatitis B found positive in a person's blood test is acute or chronic. This research work plans to code an endurance forecast model for the dataset which contains the boundaries or data of Hepatitis-B patients. At first the information will be pre-prepared, to improve fit for additional handling and for being in satisfactory configuration for the calculations. At that point, several calculations to indicate the forecast and draw out the precision of the model. What's more, further contrast those calculations with indicate the calculation with most adequacy. The precision is determined by contrasting the anticipated result and ongoing result of the patient. In light of thinking about different boundaries, the model will anticipate the danger of a patient of his endurance rate of acute or chronic infected person accuracy. In this paper we use Stochastic Gradient algorithm to find the Co-connection between boundaries of the date set, kernel approximation to finalise the resulting accuracy of the acute or choric prediction of patients and SVM method we use to clustering the kernel approximation calculation and connection analysis.}
}
@article{HO2024124656,
title = {Unraveling the complexity of amorphous solid as direct ingredient for conventional oral solid dosage form: The story of Elagolix Sodium},
journal = {International Journal of Pharmaceutics},
volume = {665},
pages = {124656},
year = {2024},
issn = {0378-5173},
doi = {https://doi.org/10.1016/j.ijpharm.2024.124656},
url = {https://www.sciencedirect.com/science/article/pii/S0378517324008901},
author = {Raimundo Ho and Richard S. Hong and Joseph Kalkowski and Kevin C. Spence and Albert W. Kruger and Jayanthy Jayanth and Nandkishor K. Nere and Samrat Mukherjee and Ahmad Y. Sheikh and Shailendra V. Bordawekar},
keywords = {Amorphous drug substance, Impinging jet precipitation, Scale-up, Glass transition, Microstructure, Physical property control, Multi-scale modeling},
abstract = {Conventional solid oral dosage form development is not typically challenged by reliance on an amorphous drug substance as a direct ingredient in the drug product, as this may result in product development hurdles arising from process design and scale-up, control of physical quality attributes, drug product processability and stability. Here, we present the Chemistry, Manufacturing and Controls development journey behind the successful commercialization of an amorphous drug substance, Elagolix Sodium, a first-in-class, orally active gonadotropin-releasing hormone antagonist. The reason behind the lack of crystalline state was assessed via Molecular Dynamics (MD) at the molecular and inter-molecular level, revealing barriers for nucleation due to prevalence of intra-molecular hydrogen bond, repulsive interactions between active pharmaceutical ingredient (API) molecules and strong solvation effects. To provide a foundational basis for the design of the API manufacturing process, we modeled the solvent-induced plasticization behavior experimentally and computationally via MD for insights into molecular mobility. In addition, we applied material science tetrahedron concepts to link API porosity to drug product tablet compressibility. Finally, we designed the API isolation process, incorporating computational fluid dynamics modeling in the design of an impinging jet mixer for precipitation and solvent-dependent glass transition relationships in the cake wash, blow-down and drying process, to enable the consistent manufacture of a porous, non-sintered amorphous API powder that is suitable for robust drug product manufacturing.}
}
@article{GONZALEZFELIU201289,
title = {Modeling Urban Goods Movement: How to be Oriented with so Many Approaches?},
journal = {Procedia - Social and Behavioral Sciences},
volume = {39},
pages = {89-100},
year = {2012},
note = {Seventh International Conference on City Logistics which was held on June 7- 9,2011, Mallorca, Spain},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2012.03.093},
url = {https://www.sciencedirect.com/science/article/pii/S1877042812005605},
author = {Jesus Gonzalez-Feliu and Jean-Louis Routhier},
keywords = {Urban goods movement, modeling approaches, systematic review, multidisciplinarity},
abstract = {This paper proposes an analysis of the different model construction and development approaches in the context of urban goods movement (UGM). We focus on the model development issues more than on the mathematical tools applied in these models. First, we explore the main UGM models in the field, identifying their main construction schemas and their features limits. From this analysis, we propose a classification of UGM modeling frameworks, synthesizing them on a table that illustrates their construction schemas. Second, we analyze their limits and find a first set of synergies between the different thinking schools. This analysis allows us to highlight the strong points and override their weaknesses, and to propose a set of recommendations for planners and modeling schools in order to find co-operative schemas that improve the models’ efficiency.}
}
@article{POLHILL2023103121,
title = {Cognition and hypocognition: Discursive and simulation-supported decision-making within complex systems},
journal = {Futures},
volume = {148},
pages = {103121},
year = {2023},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2023.103121},
url = {https://www.sciencedirect.com/science/article/pii/S0016328723000253},
author = {J. Gareth Polhill and Bruce Edmonds},
keywords = {Simulation, Cognition, Hypocognition, Divination, Ecocyborgs, Blasphemy},
abstract = {Homo sapiens is currently believed to have evolved in the African savannah several hundreds of thousands of years ago. Since then, human societies have become, through technological innovation and application, powerful influencers of the planet’s ecological, hydrological and meteorological systems – for good and ill. They have experimented with many different systems of governance, in order to manage their societies and the environments they inhabit – using computer simulations as a tool to help make decisions concerning highly complex systems, is only the most recent of these. In questioning whether, when and how computer simulations should play a role in determining decision-making in these systems of governance, it is also worth reflecting on whether, when and how humans, or groups of humans, have the capability to make such decisions without the aid of such technology. This paper looks at and compares the characteristics of natural language-based and simulation-based decision-making. We argue that computational tools for decision-making can and should be complementary to natural language discourse approaches, but that this requires that both systems are used with their limitations in mind. All tools and approaches – physical, social and mental – have dangers when used inappropriately, but it seems unlikely humankind can survive without them. The challenge is how to do so.}
}
@incollection{BERNINGER2004197,
title = {Chapter 6 - The Reading Brain in Children and Youth: A Systems Approach},
editor = {Bernice Wong},
booktitle = {Learning About Learning Disabilities (Third Edition)},
publisher = {Academic Press},
edition = {Third Edition},
address = {San Diego},
pages = {197-248},
year = {2004},
isbn = {978-0-12-762533-1},
doi = {https://doi.org/10.1016/B978-012762533-1/50009-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780127625331500093},
author = {Virginia W. Berninger},
abstract = {Publisher Summary
This chapter presents a systems approach to the reading brain in children and youth. A supervisory attentional system in the frontal lobes protects the working brain from external and internal distraction through an inhibitory mechanism that suppresses distraction. Brains are electrochemical computers whose computations create inner mental worlds and overt interactions with the external world. Complete understanding of the functional reading system will require knowledge of regionally specific localized brain activation and interconnectivity of specific regions during the computational processes that create the inner mental worlds as well as the overt reading behavior of reading brains. Visual inspection of the brains of normal and disabled readers reveals no secrets about the structural anomalies that differentiate the neural architecture of those who learn to read easily and those who struggle to learn to read. Domain-general systems that the functional reading system may draw upon include specific sensory systems, fine motor systems for the mouth and hand, attentional systems, networks of supervisory executive functions, the limbic system, and the higher-level thinking and problem solving system.}
}
@incollection{HANEES202523,
title = {Chapter 2 - The evolution of healthcare: bridging conventional and quantum computing},
editor = {Gayathri Nagasubramanian and S. Rakesh Kumar and Valentina {Emilia Balas}},
booktitle = {Quantum Computing for Healthcare Data},
publisher = {Academic Press},
pages = {23-42},
year = {2025},
series = {Advances in Biomedical Informatics},
isbn = {978-0-443-29297-2},
doi = {https://doi.org/10.1016/B978-0-443-29297-2.00011-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780443292972000113},
author = {Ahamed Lebbe Hanees and Elakkiya Elango and Gnanasankaran Natarajan and Gayathri Nagasubramanian},
keywords = {Traditional computing, quantum computing healthcare, AI (artificial intelligence), medical diagnostics, real-time data analysis, personalized medicine, treatment optimization},
abstract = {Quantum computing is going to completely transform the medical field, replacing traditional computing. Pattern identification and predictive analysis are two types of jobs that quantum computing may expedite significantly. In contrast, classical computing, which is fueled by artificial intelligence techniques including machines learning and deep learning, primarily uses enormous datasets to feed these types of operations. This development is anticipated to enable real-time visualization of complex medical records, leading to faster and more accurate diagnoses via genetics and imaging information. Through leveraging the mathematical capabilities of quantum computing, healthcare providers may anticipate significant advancements in individualized medicine, therapy optimizing, and entire patient care, that will improve the standards for the delivery of medical services. Innovative and inventive collaborations exist involving Quantum Computing and the healthcare sector. Thus it was merely an extension of decades when the field of healthcare was drastically changed by quantum computing. The development of quantum technology means that an entirely new phase of computation is about to begin. Despite being a purely scientific subject, the laws of quantum mechanics and technologies have the power to completely transform a variety of industries, including healthcare. Quantum convergence presents enormous opportunities throughout the medical sector. Furthermore, technologies in general and AI in particular have made major improvements to the healthcare sector. These advances in technology are being used and transforming the healthcare industry to provide better care, assistance, and diagnosis. In the same way, quantum computing hopes to revolutionize how it is used in the field of healthcare. These days, personalized medicine that utilizes pharmaceutical kinetics human physiology and genomics is the standard. Therefore quantum computing is an ideal way to achieve this.}
}
@article{TEMPL20249,
title = {Advancing forensic research: An examination of compositional data analysis with an application on petrol fraud detection},
journal = {Science & Justice},
volume = {64},
number = {1},
pages = {9-18},
year = {2024},
issn = {1355-0306},
doi = {https://doi.org/10.1016/j.scijus.2023.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S1355030623001223},
author = {M. Templ and J. Gonzalez-Rodriguez},
keywords = {Forensic science, Petrol data, Chemical compounds, Compositional data analysis, Classification},
abstract = {In recent years, numerous studies have examined the chemical compounds of petrol and petrol data for forensic research. Standard quantitative methods often assume that the variables or compounds do not have compositional constraints or are not part of a constrained whole, operating within an Euclidean vector space. However, chemical compounds are typically part of a whole, and the appropriate vector space for their analysis is the simplex. Biased and arbitrary results result when statistical analysis are applied on such data without proper pre-processing of such data. Compositional analysis of data has not yet been considered in forensic science. Therefore, we compare classical statistical analysis as applied in forensic research and the new proposed paradigm of compositional data analysis (CoDa). It is demonstrated how such analysis improves the analysis in petrol and forensic science. Our study shows how principal component analysis (PCA) and classification results are affected by the preprocessing steps performed on the raw data. Our results indicate that results from a log ratio analysis provides a better separation between subgroups of the data and leads to an easier interpretation of the results. In addition, with a compositional analysis a higher classification accuracy is obtained. Even a non-linear classification method - in our case a random forest - was shown to perform poorly when applied without using compositional methods. Moreover, normalization of samples due to laboratory/unit-of-measurement effects is no longer necessary, since the composition of an observation is in compositional thinking equivalent to a multiple of it, because the used (log) ratios on raw and log ratio transformed data are equal. Petrol data from different petrol stations in Brazil are used for the demonstration. This data is highly susceptible to counterfeit petrol. Forensic analysis of its chemical elements requires non-biased statistical analysis designed for compositional data to detect fraud. Based on these results, we recommend the use of compositional data methods for gasoline and petrol chemical element analysis and gasoline product characterization, authentication and fraud detection in forensic sciences.}
}
@article{BYRNE2002426,
title = {Mental models and counterfactual thoughts about what might have been},
journal = {Trends in Cognitive Sciences},
volume = {6},
number = {10},
pages = {426-431},
year = {2002},
issn = {1364-6613},
doi = {https://doi.org/10.1016/S1364-6613(02)01974-5},
url = {https://www.sciencedirect.com/science/article/pii/S1364661302019745},
author = {Ruth M.J. Byrne},
keywords = {counterfactual thinking, reasoning, imagination, emotions, if only},
abstract = {Counterfactual thoughts about what might have been (‘if only…’) are pervasive in everyday life. They are related to causal thoughts, they help people learn from experience and they influence diverse cognitive activities, from creativity to probability judgements. They give rise to emotions and social ascriptions such as guilt, regret and blame. People show remarkable regularities in the aspects of the past they mentally ‘undo’ in their counterfactual thoughts. These regularities provide clues about their mental representations and cognitive processes, such as keeping in mind true possibilities, and situations that are false but temporarily supposed to be true.}
}
@article{OBIEKE2020373,
title = {Supporting Design Problem-exploring with Emergent Technologies},
journal = {Procedia CIRP},
volume = {91},
pages = {373-381},
year = {2020},
note = {Enhancing design through the 4th Industrial Revolution Thinking},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2020.02.189},
url = {https://www.sciencedirect.com/science/article/pii/S2212827120308362},
author = {Chijioke Obieke and Jelena Milisavljevic-Syed and Ji Han},
keywords = {Creativity, Design Process, Industry 4.0, Problem-exploring},
abstract = {The goal in this study is to highlight the value of using emergent technologies to support human effort in identifying creative design problems. First, we explore the relationship between design and creativity - a popular concept and an important requirement in engineering design process. A search is conducted across repositories. This includes search in Google, Google Scholar and Google Books databases in addition to others. Findings show that the extent to which the design process requires creativity is somewhat obscure and not generally perceptible. We observe that creativity consists of two aspects: problem-solving and problem-exploring. We also observe that creativity drives the design process, not by the way of problem-solving but by the way of problem-exploring. However, currently, focus is on problem-solving than the equally important problem-exploring. For every 135 studies on problem-solving, there is only one on problem-exploring. Study on problem-exploring is limited. We research further and identify some determinants of the neglect in problem-exploring in design. These determinants are lack of motivation, significant level of difficulty and the presence of many problems yet unsolved. Using the X-Design Process model and Problem-dependent Solution model we show the importance and benefits of problem-exploring in design and why it deserves attention. Consequently, we illustrate the use of emergent technologies to support problem-exploring in design and give reasons why this is possible in Industry 4.0. These technologies include data mining, natural language processing, machine learning, duplication recognition, and so on. We indicate that these technologies will only play subordinate role to humans towards inspiring problem-exploring in design. Also, we state that a precondition to applying these technologies is a study of the human problem-exploring cognition process for subsequent simulation. Success in computational problem-exploring would lead to breakthroughs in global problem-exploring and trigger more creative solutions in coming years.}
}
@article{TUCKER199223,
title = {Deterministic and nondeterministic computation, and horn programs, on abstract data types},
journal = {The Journal of Logic Programming},
volume = {13},
number = {1},
pages = {23-55},
year = {1992},
issn = {0743-1066},
doi = {https://doi.org/10.1016/0743-1066(92)90020-4},
url = {https://www.sciencedirect.com/science/article/pii/0743106692900204},
author = {J.V. Tucker and J.I. Zucker},
abstract = {We investigate the notion of “semicomputability,” intended to generalize the notion of recursive enumerability of relations to abstract structures. Two characterizations are considered and shown to be equivalent: one in terms of “partial computable functions” (for a suitable notion of computability over abstract structures) and one in terms of definability by means of Horn programs over such structures. This leads to the formulation of a “Generalized Church-Turing Thesis” for definability of relations on abstract structures.}
}
@article{ERA2021105070,
title = {Dissociating cognitive, behavioral and physiological stress-related responses through dorsolateral prefrontal cortex inhibition},
journal = {Psychoneuroendocrinology},
volume = {124},
pages = {105070},
year = {2021},
issn = {0306-4530},
doi = {https://doi.org/10.1016/j.psyneuen.2020.105070},
url = {https://www.sciencedirect.com/science/article/pii/S0306453020304935},
author = {Vanessa Era and Luca Carnevali and Julian F. Thayer and Matteo Candidi and Cristina Ottaviani},
keywords = {Dorsolateral prefrontal cortex, Perseverative cognition, Cortisol, Heart rate variability, High-frequency repetitive transcranial magnetic stimulation},
abstract = {The left dorsolateral prefrontal cortex (dlPFC) has been implicated in the regulation of stress-related cognitive processes and physiological responses and is the principal target of noninvasive brain stimulation techniques applied to psychiatric conditions. However, existing studies are mostly correlational and causal evidence on the role of this region in mediating specific psychophysiological mechanisms underpinning stress-related responses are needed to make the application of such techniques more efficient. To fill this gap, this study used inhibitory continuous theta burst stimulation (cTBS) in healthy individuals to examine the extent to which activity of the left dlPFC is associated with cognitive (subjective focus on a tracking task), behavioral (reaction times and variability), and physiological responses (heart rate and its variability and cortisol level) following induction of perseverative cognition. Compared to sham and left ventral PreMotor area stimulation (as active control area), inhibition of left dlPFC determined sustained autonomic and neuroendocrine activation and increased the subjective perception of being task-focused, while not changing the behavioral and self-reported stress-related responses. Adopting a causative approach, we describe a role of left dlPFC in inhibitory control of the physiological stress-response associated to perseverative thinking.}
}
@article{ALI2023e14993,
title = {Small hydropower generation using pump as turbine; a smart solution for the development of Pakistan's energy},
journal = {Heliyon},
volume = {9},
number = {4},
pages = {e14993},
year = {2023},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2023.e14993},
url = {https://www.sciencedirect.com/science/article/pii/S2405844023022004},
author = {Asad Ali and Jianping Yuan and Hamza Javed and Qiaorui Si and Ibra Fall and Israel Enema Ohiemi and Fareed Konadu Osman and Rice ul Islam},
keywords = {Pump-as-turbine, Small hydropower, Renewable energy, Environment friendly, Energy resources, Energy in developing countries},
abstract = {Energy supply that is sustainable, effective, and economical has a strong association with socio-economic growth, particularly in developing countries such as Pakistan. Due to the ever-increasing gap between supply and demand, Pakistan has become an energy-deficient nation, with most people having no-to-limited access to power. Pakistan has been suffering from power shortages and an energy crisis because of its strong reliance on fossil-fuels to provide expensive electricity. Therefore, this paper offers a novel concept for developing Pakistan's energy by producing small-hydropower using Pump-As-Turbine (PAT), which is a form of Renewable-energy with lower environmental-impact and has not been used in Pakistan previously. PATs have shown several advantages over traditional hydro-turbines, such as minimum expenses, low-complexity, short delivery time, ease of spare parts, easy installation, availability in a large number of standard sizes, and massive production for broad-range of heads and flow rates. According to technical standards, any sort of pump could be used as PAT, including radial, mixed, single-stage, multi-stage etc. for power generation, which are capable of producing 5kW–1000kW of power, depending on their usage. However, Pakistan has shown little to no interest in exploring small/micro hydropower generation (PATs technology). Thus, this study offers public awareness and forward thinking regarding the use of advanced SHPs and draws the interests of legislators and different investors via solid recommendations about the cost-effective and environmental-friendly technology (PAT).}
}
@article{YAN2025129868,
title = {SPRInT: Scaling Programmatic Reasoning for INstruction Tuning in mathematics},
journal = {Neurocomputing},
volume = {634},
pages = {129868},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2025.129868},
url = {https://www.sciencedirect.com/science/article/pii/S0925231225005405},
author = {Yan Yan and Lin Li and Bo-Wen Zhang},
keywords = {Programmatic mathematical reasoning, Data augmentation, Data synthesis, Decoupled numeric dependencies, Logical inconsistencies},
abstract = {We present SPRInT, a novel approach for large-scale, cost-effective synthesis of instruction-tuning datasets, leveraging Program-of-Thoughts (PoT) to enhance mathematical reasoning capabilities. Through the SPRInT framework, we synthesized data from seven high-quality open-source math datasets (including GSM8K, MATH, AQuA), and developed InfinityMATH-a dataset containing over 100,000 samples generated from QA pairs, offering extensive coverage across various mathematical domains. The SPRInT model series, fine-tuned on InfinityMATH using open-source language and code models such as Llama2-7B, Mistral-7B, and CodeLlama-7B, achieved remarkable improvements in mathematical reasoning, with performance gains between 184.7% and 514.3%. In zero-shot settings, our SPRInT-CodeLlama-7B model surpassed MAmmoTH-Coder on widely-used benchmarks, including GSM8K (65.80% vs. 56.86%) and MATH (34.06% vs. 29.88%). To assess logical consistency in numerical transformations, we created the GSM8K+ and MATH＋ test sets by modifying the numerical values in the original datasets. While traditional models struggled with these alterations, the SPRInT models exhibited superior robustness. The InfinityMATH dataset is publicly available at https://huggingface.co/datasets/BAAI/InfinityMATH.}
}
@incollection{BLAGOJEVIC20171,
title = {Chapter One - A Systematic Approach to Generation of New Ideas for PhD Research in Computing},
editor = {Ali R. Hurson and Veljko Milutinović},
series = {Advances in Computers},
publisher = {Elsevier},
volume = {104},
pages = {1-31},
year = {2017},
booktitle = {Creativity in Computing and DataFlow SuperComputing},
issn = {0065-2458},
doi = {https://doi.org/10.1016/bs.adcom.2016.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S0065245816300572},
author = {V. Blagojević and D. Bojić and M. Bojović and M. Cvetanović and J. Đorđević and Đ. Đurđević and B. Furlan and S. Gajin and Z. Jovanović and D. Milićev and V. Milutinović and B. Nikolić and J. Protić and M. Punt and Z. Radivojević and Ž. Stanisavljević and S. Stojanović and I. Tartalja and M. Tomašević and P. Vuletić},
keywords = {PhD research, Idea generation, Research methodology, Research idea classification, Creative thinking},
abstract = {This article represents an effort to help PhD students in computer science and engineering to generate good original ideas for their PhD research. Our effort is motivated by the fact that most PhD programs nowadays include several courses, as well as the research component, that should result in journal publications and the PhD thesis, all in a timeframe of 3–6 years. In order to help PhD students in computing disciplines to get focused on generating ideas and finding appropriate subject for their PhD research, we have analyzed some state-of-the-art inventions in the area of computing, as well as the PhD thesis research of faculty members of our department, and came up with a proposal of 10 methods that could be implemented to derive new ideas, based on the existing body of knowledge in the research field. This systematic approach provides guidance for PhD students, in order to improve their efficiency and reduce the dropout rate, especially in the area of computing.}
}
@incollection{EDELMAN2015596,
title = {Marr, David (1945–80)},
editor = {James D. Wright},
booktitle = {International Encyclopedia of the Social & Behavioral Sciences (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {596-598},
year = {2015},
isbn = {978-0-08-097087-5},
doi = {https://doi.org/10.1016/B978-0-08-097086-8.61085-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780080970868610851},
author = {Shimon Edelman and Lucia M Vaina},
keywords = {Biological information processing, Brain function, Cognitive psychology, Computational theory and modeling, Neuroscience, Scientific methodology, Vision},
abstract = {David Courtnay Marr was born in 1945 in Essex, England. Marr's dissertation, written at Trinity College, Cambridge and published between 1969 and 1971, presented a theory of mammalian brain function, parts of which remain relevant to the present day, despite vast advances in neurobiology in the past decades. In 1973, Marr joined the Artificial Intelligence Laboratory at the Massachusetts Institute of Technology, where he was made a tenured full professor in 1980. Marr died in November 1980, of leukemia. His highly influential book, Vision: A Computational Investigation into the Human Representation and Processing of Visual Information, which has redefined and revitalized the study of human and machine vision, was published posthumously, in 1982, with a new edition appearing in 2010.}
}
@article{WANG2024109848,
title = {An effective DOA estimation method for low SIR in small-size hydrophone array},
journal = {Applied Acoustics},
volume = {217},
pages = {109848},
year = {2024},
issn = {0003-682X},
doi = {https://doi.org/10.1016/j.apacoust.2023.109848},
url = {https://www.sciencedirect.com/science/article/pii/S0003682X23006461},
author = {Wenbo Wang and Ye Li and TongSheng Shen and Feng Liu and DeXin Zhao},
abstract = {The estimation ability of traditional direction of arrival (DOA) estimation methods is relatively fragile in small-size hydrophone arrays with limited space. Especially in low signal to interference ratio (SIR), the strong interference signals may submerge some weak signals of interest (SOI) and make DOA estimation difficult in response to this issue. This paper introduces an improved sparse DOA estimation method for practical multi-objective DOA estimation in complex scenarios. The main work is to introduce a noise weight constraint in the sparse iterative covariance process. It leads the algorithm to output sparse peaks and smooth spatial energy spectra and achieve faster fitting while reducing the probability of false peaks. The algorithm can complete DOA estimation of the multi-target reliably without prior information of sources. Then, we propose a fast region grid refinement method based on allocation reconstruction to increase angle resolution. The method increases the accuracy of multi-objective DOA estimation while reducing computational costs. Finally, simulation and experiment have verified the method's effectiveness.}
}
@article{MITTAL2021102927,
title = {Modified-MaMeMi filter bank for efficient extraction of brainwaves from electroencephalograms},
journal = {Biomedical Signal Processing and Control},
volume = {69},
pages = {102927},
year = {2021},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2021.102927},
url = {https://www.sciencedirect.com/science/article/pii/S1746809421005243},
author = {Rakshit Mittal and A. Amalin Prince and Saif Nalband and Femi Robert and Agastinose Ronickom Jac Fredo},
keywords = {Electroencephalogram, Band-pass filter, Brainwaves, MaMeMi filter},
abstract = {Electroencephalography (EEG) is an important tool for characterizing the functioning of the brain. Studies based on EEG involve the extraction of different spectra from EEG signals. Traditional methods of extracting these brainwaves (commonly δ, θ, α, β, γ) from EEG signals, like impulse-response filtering or wavelet decomposition, are computationally inefficient or unsuitable for real-time implementation. The Maximum-Mean-Minimum (MaMeMi) filter is a signal processing algorithm that is computationally efficient for signal filtering. The response of the MaMeMi filter is dependent on pre-decided filter coefficients. An obstacle to its implementation is that the filter coefficients have to be tuned to the sampling frequency. We propose the Modified-MaMeMi (MoMaMeMi) filter, in which the choice of coefficients is independent from the sampling frequency. Furthermore, we develop a band-pass MoMaMeMi filter which is duplicated in a filter bank, to decompose EEG signals into five common brainwaves. We validate the efficiency of the proposed filter bank by the increase in Signal-to-Noise Ratio (SNR). The maximum average increase in SNR is 19.68 dB. To prove utility of the filter-bank, we statistically compare the values of windowed average power extracted from the MoMaMeMi-filtered signals, between seizure and non-seizure components of the EEG data-set. A significant difference between the distributions suggests utility for classification problems. Since EEG-signal processing algorithms are highly customised and not limited to the 5 common brainwaves reported in this paper, we also develop a program to determine filter parameters for extraction of unique frequency bands in a bespoke MoMaMeMi filter.}
}
@article{SAHU2023105206,
title = {SCZ-SCAN: An automated Schizophrenia detection system from electroencephalogram signals},
journal = {Biomedical Signal Processing and Control},
volume = {86},
pages = {105206},
year = {2023},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2023.105206},
url = {https://www.sciencedirect.com/science/article/pii/S1746809423006390},
author = {Geet Sahu and Mohan Karnati and Abhishek Gupta and Ayan Seal},
keywords = {Schizophrenia, Electroencephalography, Continuous wavelet transform, Scalogram, Convolutional neural network},
abstract = {Schizophrenia (SCZ) is a severe neurological and physiological syndrome that perverts a patient’s perception of reality. SCZ exhibits several symptoms, including hallucinations, delusions, aberrant behavior, and thinking. It affects their professional, academic, personal, and social lives. Neurologists use a variety of verbal and visual tests to determine SCZ. However, these methods are laborious, time-consuming, superficial, and vulnerable to mistakes. Therefore, it is necessary to create an automated model for SCZ detection. Convolutional neural networks have swiftly established themselves in the field of mental health care due to the growth of deep learning in recent decades. Electroencephalogram (EEG) data records the variations in the neural dynamics of human memory. Using EEG data, this study proposes an automatic SCZ detection method using separable convolution attention network (SCZ-SCAN). The proposed network employs depth-wise separable convolution and attention networks on high-level and low-level to aggregate characteristics of 2-D scalogram images acquired from the continuous wavelet transform. The depth-wise separable convolutions help to create a lightweight framework, while attention techniques concentrate on significant features and reduce futile computations by removing the transmission of irrelevant features. The proposed approach has an average classification accuracy of 99% and 95% on the IBIB-PAN and EEG data from the basic sensory task in SZ dataset. Moreover, statistical hypothesis testing is performed using Wilcoxon’s Rank-Sum test to signify the model performance and it proves that SCZ-SCAN is statistically efficient to nine cutting-edge methods. Experimental results show that the PSFAN statistically defeats 11 contemporary methods, proving its effectiveness for medical industrial applications.}
}
@article{GENNARI2023103006,
title = {Design for social digital well-being with young generations: Engage them and make them reflect},
journal = {International Journal of Human-Computer Studies},
volume = {173},
pages = {103006},
year = {2023},
issn = {1071-5819},
doi = {https://doi.org/10.1016/j.ijhcs.2023.103006},
url = {https://www.sciencedirect.com/science/article/pii/S1071581923000125},
author = {Rosella Gennari and Maristella Matera and Diego Morra and Alessandra Melonio and Mehdi Rizvi},
keywords = {Digital well-being, Social digital well-being, Responsible design, Smart-thing design, Toolkit},
abstract = {Digital well-being traditionally means limiting the effects on individuals of technology abuses. However, in a broader perspective, it can be crucial to consider the pervasiveness of technology, and the effect it can have not only on individuals but also on their peers in the context of diverse everyday-life situations. Within this view, which emphasises the social side of digital well-being, the paper argues the need of educating young generations to participate in the making of technology for a social goal and have a reflective attitude towards technology and its impact on society. It, therefore, presents a design toolkit as a means to (i) engage young generations to become active in design for social digital well-being and, thanks to the exposure to how technology works, (ii) reflect deeply on the pros and cons of technology in use in their everyday life. By presenting the results of a study with 24 high-school pupils and their teachers, the paper discusses how a phygital toolkit, which structures the design process, engages them in the rapid prototyping of their own smart things, and how it acts as a proxy for soliciting their own reflections around technology and social digital well-being.}
}
@article{BENEDEK2014125,
title = {To create or to recall? Neural mechanisms underlying the generation of creative new ideas},
journal = {NeuroImage},
volume = {88},
pages = {125-133},
year = {2014},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2013.11.021},
url = {https://www.sciencedirect.com/science/article/pii/S1053811913011130},
author = {Mathias Benedek and Emanuel Jauk and Andreas Fink and Karl Koschutnig and Gernot Reishofer and Franz Ebner and Aljoscha C. Neubauer},
keywords = {Creativity, fMRI, Human cognition, Memory retrieval, Inferior parietal cortex},
abstract = {This fMRI study investigated brain activation during creative idea generation using a novel approach allowing spontaneous self-paced generation and expression of ideas. Specifically, we addressed the fundamental question of what brain processes are relevant for the generation of genuinely new creative ideas, in contrast to the mere recollection of old ideas from memory. In general, creative idea generation (i.e., divergent thinking) was associated with extended activations in the left prefrontal cortex and the right medial temporal lobe, and with deactivation of the right temporoparietal junction. The generation of new ideas, as opposed to the retrieval of old ideas, was associated with stronger activation in the left inferior parietal cortex which is known to be involved in mental simulation, imagining, and future thought. Moreover, brain activation in the orbital part of the inferior frontal gyrus was found to increase as a function of the creativity (i.e., originality and appropriateness) of ideas pointing to the role of executive processes for overcoming dominant but uncreative responses. We conclude that the process of idea generation can be generally understood as a state of focused internally-directed attention involving controlled semantic retrieval. Moreover, left inferior parietal cortex and left prefrontal regions may subserve the flexible integration of previous knowledge for the construction of new and creative ideas.}
}
@article{CHIU2024100282,
title = {Developing and validating measures for AI literacy tests: From self-reported to objective measures},
journal = {Computers and Education: Artificial Intelligence},
volume = {7},
pages = {100282},
year = {2024},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2024.100282},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X24000857},
author = {Thomas K.F. Chiu and Yifan Chen and King Woon Yau and Ching-sing Chai and Helen Meng and Irwin King and Savio Wong and Yeung Yam},
keywords = {AI literacy, Instrument, K-12 education, AI education, Co-design process, Measures},
abstract = {The majority of AI literacy studies have designed and developed self-reported questionnaires to assess AI learning and understanding. These studies assessed students' perceived AI capability rather than AI literacy because self-perceptions are seldom an accurate account of true measures. International assessment programs that use objective measures to assess science, mathematical, digital, and computational literacy back up this argument. Furthermore, because AI education research is still in its infancy, the current definition of AI literacy in the literature may not meet the needs of young students. Therefore, this study aims to develop and validate an AI literacy test for school students within the interdisciplinary project known as AI4future. Engineering and education researchers created and selected 25 multiple-choice questions to accomplish this goal, and school teachers validated them while developing an AI curriculum for middle schools. 2390 students in grades 7 to 9 took the test. We used a Rasch model to investigate the discrimination, reliability, and validity of the items. The results showed that the model met the unidimensionality assumption and demonstrated a set of reliable and valid items. They indicate the quality of the test items. The test enables AI education researchers and practitioners to appropriately evaluate their AI-related education interventions.}
}
@article{CHANG2025100364,
title = {Co-designing AI with youth partners: Enabling ideal classroom relationships through a novel AI relational privacy ethical framework},
journal = {Computers and Education: Artificial Intelligence},
volume = {8},
pages = {100364},
year = {2025},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2025.100364},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X25000049},
author = {Michael Alan Chang and Mike Tissenbaum and Thomas M. Philip and Sidney K. D’Mello},
keywords = {Architectures for educational technology systems, Cooperative/collaborative learning, Cultural and social implications, Interdisciplinary studies, participatory design, AI-supported collaboration},
abstract = {In recent years, the design of AI-based tools for educational spaces have been largely driven by researchers who impart their past expertises, experiences, and perspectives in the design process. While this typically leads to technically feasible designs and are often well-grounded in theories of learning, youth agency is typically limited in this process. In this paper, we argue that designers have a significant ethical responsibility to incorporate youth voices – in particular, their dreams and concerns – into the design of AI tools starting from conception. This need is particularly important as new applications for AI, such as AI-supported collaboration, introduce new surveillance vectors into classroom spaces. Drawing from recent scholarship which advances ethics and relationality in participatory co-design with youth, we introduce a co-design methodology in which youth are supported in imagining expansive technical possibilities for K-12 public schools, grounded within affordances, limitations, and tradeoffs of AI and machine learning techniques. This approach is demonstrated through our Learning Futures Workshop, which brought together 30 historically minoritized youth in conversation with experts in both education and technology. Through detailed case study on the enactment of the workshop, including a thematic analysis of the activities the youth engaged in and their outputs, we identified new, expansive relational possibilities for AI, ethical commitments to support the design, and finally, developed a novel AI Relational Privacy ethical framework that supports the design of new collaborative AI platforms. We conclude by connecting these findings and frameworks to the design of newly enacted AI-based applications and underlying data infrastructures.}
}
@article{CROMWELL20112026,
title = {Rethinking the cognitive revolution from a neural perspective: How overuse/misuse of the term ‘cognition’ and the neglect of affective controls in behavioral neuroscience could be delaying progress in understanding the BrainMind},
journal = {Neuroscience & Biobehavioral Reviews},
volume = {35},
number = {9},
pages = {2026-2035},
year = {2011},
note = {Pioneering Research in Affective Neuroscience: Celebrating the Work of Dr. Jaak Panksepp},
issn = {0149-7634},
doi = {https://doi.org/10.1016/j.neubiorev.2011.02.008},
url = {https://www.sciencedirect.com/science/article/pii/S0149763411000273},
author = {Howard Casey Cromwell and Jaak Panksepp},
keywords = {Cognition, Emotion, Motivation, Perception, Concepts, Neural activity, Behavior},
abstract = {Words such as cognition, motivation and emotion powerfully guide theory development and the overall aims and goals of behavioral neuroscience research. Once such concepts are accepted generally as natural aspects of the brain, their influence can be pervasive and long lasting. Importantly, the choice of conceptual terms used to describe and study mental/neural functions can also constrain research by forcing the results into seemingly useful ‘conceptual’ categories that have no discrete reality in the brain. Since the popularly named ‘cognitive revolution’ in psychological science came to fruition in the early 1970s, the term cognitive or cognition has been perhaps the most widely used conceptual term in behavioral neuroscience. These terms, similar to other conceptual terms, have potential value if utilized appropriately. We argue that recently the term cognition has been both overused and misused. This has led to problems in developing a usable shared definition for the term and to promotion of possible misdirections in research within behavioral neuroscience. In addition, we argue that cognitive-guided research influenced primarily by top-down (cortical toward subcortical) perspectives without concurrent non-cognitive modes of bottom-up developmental thinking, could hinder progress in the search for new treatments and medications for psychiatric illnesses and neurobehavioral disorders. Overall, linkages of animal research insights to human psychology may be better served by bottom-up (subcortical to cortical) affective and motivational ‘state-control’ perspectives, simply because the lower networks of the brain are foundational for the construction of higher ‘information-processing’ aspects of mind. Moving forward, rapidly expanding new techniques and creative methods in neuroscience along with more accurate brain concepts, may help guide the development of new therapeutics and hopefully more accurate ways to describe and explain brain-behavior relationships.}
}
@article{MAYER2024115725,
title = {Site heterogeneity and broad surface-binding isotherms in modern catalysis: Building intuition beyond the Sabatier principle},
journal = {Journal of Catalysis},
volume = {439},
pages = {115725},
year = {2024},
issn = {0021-9517},
doi = {https://doi.org/10.1016/j.jcat.2024.115725},
url = {https://www.sciencedirect.com/science/article/pii/S002195172400438X},
author = {James M. Mayer},
abstract = {Learning the science of heterogeneous catalysis and electrocatalysis always starts with the simple case of a flat, uniform surface with an ideal adsorbate. It has of course been recognized for a century that real catalysts are more complicated. For the increasingly complex catalysts of the 21st century, this Perspective argues that surface heterogeneity and non-ideal binding isotherms are central features, and their implications need to be incorporated in current thinking. A variety of systems are described herein where catalyst complexity leads to broad, non-Langmuirian surface isotherms for the binding of hydrogen atoms – and this occurs even for ideal, flat Pt(111) surfaces. Modern catalysis employs nanoscale materials whose surfaces have substantial step, edge, corner, impurity, and other defect sites, and they increasingly have both metallic and non-metallic elements MnXm, including metal oxides, chalcogenides, pnictides, carbides, doped carbons, etc. The surfaces of such catalysts are often not crystal facets of the bulk phase underneath, and they typically have a variety of potential active sites. Catalytic surfaces in operando are often non-stoichiometric, amorphous, dynamic, and impure, and often vary from one part of the surface to another. Understanding of the issues that arise at such nanoscale, multi-element catalysts is just beginning to emerge. Yet these catalysts are widely discussed using Brønsted/Bell-Evans-Polanyi (BEP) relations, volcano plots, Tafel slopes, the Butler-Volmer equation, and other linear free energy relations (LFERs), which all depend on the implicit assumption that the active sites are “similar” and that surface adsorption is close to ideal. These assumptions underly the ubiquitous intuition based on the Sabatier Principle, that the fastest catalysis will occur when key intermediates have free energies of adsorption that are not too strong nor too weak. Current catalysis research often aims to minimize the complexity of non-ideal isotherms through experimental and computational design (e.g., the use of single crystal surfaces), and these studies are the foundation of the field. In contrast, this Perspective argues that the heterogeneity of binding sites and binding energies is an inherent strength of these catalysts. This diversity makes many nanoscale catalysts inherently a high-throughput screen wrapped in a tiny package. Only by making the heterogeneity part of the foundation of catalysis models, sorting the types of active sites and dissecting non-ideal binding isotherms, will modern catalysis learn to harness the inherent diversity of real catalysts. Controlling and exploiting diversity rather than avoiding it will help to optimize complex modern catalysts and catalytic conditions.}
}
@article{WU2020242,
title = {Mentalizing during social InterAction: A four component model},
journal = {Cortex},
volume = {126},
pages = {242-252},
year = {2020},
issn = {0010-9452},
doi = {https://doi.org/10.1016/j.cortex.2019.12.031},
url = {https://www.sciencedirect.com/science/article/pii/S0010945220300277},
author = {Haiyan Wu and Xun Liu and Cindy C. Hagan and Dean Mobbs},
keywords = {Metacognition, Mentalizing, Vicarious mentalizing, Co-mentalizing, Social inference},
abstract = {Mentalizing, conventionally defined as the process in which we infer the inner thoughts and intentions of others, is a fundamental component of human social cognition. Yet its role, and the nuanced layers involved, in real world social interaction are rarely discussed. To account for this lack of theory, we propose the interactive mentalizing theory (IMT) -to emphasize the role of metacognition in different mentalizing components. We discuss the connection between mentalizing, metacognition, and social interaction in the context of four elements of mentalizing: (i) Metacognition–inference of our own thought processes and social cognitions and which is central to all other components of mentalizing including: (ii) first-order mentalizing–inferring the thoughts and intentions of an agent's mind; (iii) personal second-order mentalizing–inference of other's mentalizing of one's own mind; (iv) Collective mentalizing: which takes at least two forms (a) vicarious mentalizing: adopting another's mentalizing of an agent (i.e., what we think others think of an agent) and (b) co-mentalizing: mentalizing about an agent in conjunction with others' mentalizing of that agent (i.e., conforming to others beliefs about another agent's internal states). The weights of these four elements is determined by metacognitive insight and confidence in one's own or another's mentalizing ability, yielding a dynamic interaction between these circuits. To advance our knowledge on mentalizing during live social interaction, we identify how these subprocesses can be organized by different target agents and facilitated by combining computational modeling and interactive brain approaches.}
}
@article{TANTILLO2021n/a,
title = {Dynamic effects on organic reactivity—Pathways to (and from) discomfort},
journal = {Journal of Physical Organic Chemistry},
volume = {34},
number = {6},
pages = {n/a},
year = {2021},
issn = {0894-3230},
doi = {https://doi.org/10.1002/poc.4202},
url = {https://www.sciencedirect.com/science/article/pii/S0894323022006889},
author = {Dean J. Tantillo},
keywords = {bifurcation, dynamics, entropy},
abstract = {Recent computational studies highlighting the importance of accounting for dynamic effects on organic reactivity are discussed, accompanied by descriptions of the factors that led the author to pursue these projects.}
}
@article{ATANASIU2023e20698,
title = {On the utility of Colour in shape analysis: An introduction to Colour science via palaeographical case studies},
journal = {Heliyon},
volume = {9},
number = {10},
pages = {e20698},
year = {2023},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2023.e20698},
url = {https://www.sciencedirect.com/science/article/pii/S2405844023079069},
author = {Vlad Atanasiu and Peter Fornaro},
keywords = {Colour science, Colour processing, Colour perception, Image processing, Image enhancement, Palaeography},
abstract = {In this article, we explore the use of colour for the analysis of shapes in digital images. We argue that colour can provide unique information that is not available from shape alone, and that familiarity with the interdisciplinary field of colour science is essential for unlocking the potential of colour. Within this perspective, we offer an illustrated overview of the colour-related aspects of image management and processing, perceptual psychology, and cultural studies, using for exemplary purposes case studies focused on computational palaeography. We also discuss the changing roles of colour in society and the sciences, and provide technical solutions for using digital colour effectively, highlighting the impact of human factors. The article concludes with an annotated bibliography. This work is a primer, and its intended readership are scholars and computer scientists unfamiliar with colour science.}
}