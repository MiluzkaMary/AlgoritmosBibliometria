@article{LU2024112309,
title = {The physical information LSTM surrogate model for establishing a digital twin model of reciprocating air compressors},
journal = {Applied Soft Computing},
volume = {167},
pages = {112309},
year = {2024},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2024.112309},
url = {https://www.sciencedirect.com/science/article/pii/S1568494624010834},
author = {Yingkang Lu and Yanfei Li and Gaocai Fu and Yu Jiang and Yuzhe Huang and Jiaxing Zhu and Buyun Sheng},
keywords = {Reciprocating air compressors, Digital twin, Surrogate model, Pressure prediction, Long short-term memory neural network},
abstract = {Reciprocating air compressors play a crucial role in industrial production processes. However, due to the complex structure and long operating time of reciprocating air compressors, real-time monitoring to grasp the operating status of reciprocating air compressors has become particularly important. Digital twin is a technology that can reflect the behavior of physical entities in real time, accurately predicting and evaluating the operation status of reciprocating air compressors. However, the establishment of a digital twin model for reciprocating air compressors requires a significant amount of computational resources, which can result in the inability to meet the requirements of real-time performance evaluation. To overcome this limitation, this paper proposes a method for constructing a digital twin model of reciprocating air compressors based on a surrogate model. The surrogate model is constructed based on a long short-term memory neural network with physical information(PILSTM). This model can accurately describe the changes in cylinder pressure by combining physical information. According to the characteristics of cylinder pressure changes, regularization formulas are added to ensure the smoothness of the predicted pressure. The experimental results show that the digital twin model based on the surrogate model has high prediction accuracy and real-time performance. Therefore, this model provides a new method for monitoring the operating status of reciprocating air compressors.}
}
@article{BARBOSA2025100864,
title = {An interchangeable editor to create generic and adaptable decision trees for versatile applications and game development scenarios},
journal = {Entertainment Computing},
volume = {52},
pages = {100864},
year = {2025},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2024.100864},
url = {https://www.sciencedirect.com/science/article/pii/S1875952124002325},
author = {Rafael Garcia Barbosa and Maria Andréia Formico Rodrigues},
keywords = {Decision tree editor, Standard interchange format, Interoperability and adaptability, Personalized learning, Game engine integration, Game development scenarios},
abstract = {This paper introduces a novel software tool developed to serve as an editor for the manual construction of decision trees, characterized by their generic nature, flexibility, and adaptability across a wide range of applications and game development scenarios. The editor enables straightforward definition and modification of decision tree elements and data, dynamically updating to meet changing needs and contexts. A key feature of this tool is its capability to export decision trees in a standardized interchange format, enhancing interoperability by allowing seamless integration with other computational platforms, including game engines. We demonstrate the utility and versatility of our editor with three distinct and comprehensive use cases, highlighting its potential as a significant contribution to interactive technology. The tool facilitates the development of decision trees, enhancing informed decision-making and strategic planning, and supports personalized learning to improve engagement and outcomes.}
}
@article{PANG2022118826,
title = {Uncovering the global task-modulated brain network in chunk decomposition with Chinese characters},
journal = {NeuroImage},
volume = {247},
pages = {118826},
year = {2022},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2021.118826},
url = {https://www.sciencedirect.com/science/article/pii/S1053811921010971},
author = {Jiaoyan Pang and Hanning Guo and Xiaochen Tang and Yu Fu and Zhengwu Yang and Yongchao Li and Na An and Jing Luo and Zhijun Yao and Bin Hu},
keywords = {Cognitive pattern, Beta-series correlation, Chinese characters, Thalamus, Hippocampus},
abstract = {Chunk decomposition, which requires the mental representation transformation in accordance with behavioral goals, is of vital importance to problem solving and creative thinking. Previous studies have identified that the frontal, parietal, and occipital cortex in the cognitive control network selectively activated in response to chunk tightness, however, functional localization strategy may overlook the interaction brain regions. Based on the notion of a global brain network, we proposed that multiple specialized regions have to be interconnected to maintain goal representation during the course of chunk decomposition. Therefore, the present study applied a beta-series correlation method to investigate interregional functional connectivity in the event-related design of chunk decomposition tasks using Chinese characters, which would highlight critical nodes irrespective to chunk tightness. The results reveal a network of functional hubs with highly within or between module connections, including the orbitofrontal cortex, superior/inferior parietal lobule, hippocampus, and thalamus. We speculate that the thalamus integrates information across modular as an integrative hub while the orbitofrontal cortex tracks the mental states of chunk decomposition on a moment-to-moment basis. The superior and inferior parietal lobule collaborate to manipulate the mental representation of chunk decomposition and the hippocampus associates the relationship between elements in the question and solution phase. Furthermore, the tightness of chunks is not only associated with different processors in visual systems but also leads to increased intermodular connections in right superior frontal gyrus and left precentral gyrus. To summary up, the present study first reveals the task-modulated brain network of chunk decomposition in addition to the tightness-related nodes in the frontal and occipital cortex.}
}
@article{PICCININI2004811,
title = {Functionalism, computationalism, and mental states},
journal = {Studies in History and Philosophy of Science Part A},
volume = {35},
number = {4},
pages = {811-833},
year = {2004},
issn = {0039-3681},
doi = {https://doi.org/10.1016/j.shpsa.2004.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S0039368104000883},
author = {Gualtiero Piccinini},
keywords = {Functionalism, Computationalism, Computational functionalism, Mental states, Computational theory of mind, Functional analysis},
abstract = {Some philosophers have conflated functionalism and computationalism. I reconstruct how this came about and uncover two assumptions that made the conflation possible. They are the assumptions that (i) psychological functional analyses are computational descriptions and (ii) everything may be described as performing computations. I argue that, if we want to improve our understanding of both the metaphysics of mental states and the functional relations between them, we should reject these assumptions.}
}
@incollection{REYNA199391,
title = {Chapter 3 Fuzzy Memory and Mathematics in The Classroom},
editor = {Graham M. Davies and Robert H. Logie},
series = {Advances in Psychology},
publisher = {North-Holland},
volume = {100},
pages = {91-119},
year = {1993},
booktitle = {Memory in Everyday Life},
issn = {0166-4115},
doi = {https://doi.org/10.1016/S0166-4115(08)61096-1},
url = {https://www.sciencedirect.com/science/article/pii/S0166411508610961},
author = {Valerie F. Reyna and Charles J. Brainerd},
abstract = {Publisher Summary
This chapter presents an intuitionist account of mathematical problem solving, and some of its implications for learning and teaching. The study is based primarily on research concerning the relationship between memory and reasoning, in particular, between memory for specific problem facts and gist based reasoning—for example, Reyna, in press-b. The chapter reviews that research, including the nature of representations adults and children use to solve problems, some counterintuitive findings about factors that facilitate learning, and the implications of distinguishing between competence versus bringing that competence to bear in actual situations. The chapter focuses on computational skills (e.g., mental arithmetic) and quantitative reasoning about probability and expected value (e.g., compensation), the numerosity of sets (e.g., class-inclusion reasoning), relative magnitude (e.g., transitive inference), and weights and measures (e.g., conservation). Although many factors undoubtedly contribute to the problem of underachievement in mathematics (including low expectations) observers generally agree that, beyond basic arithmetic, mathematics is not intuitive for most learners. Recent research suggests that successful mathematical reasoners call on an intuitive appreciation of the fuzzy, qualitative relationships in problem information. Thus, by emphasizing a kind of cognitive approach that seems antithetical to mathematics, one may achieve greater success in instruction. The origins of this new approach can be traced to the foundations of mathematics itself, to Brouwer and Heyting, who argued that intuition-fluid thinking that operates on the barest senses of ideas—underlies the insights of mathematicians.}
}
@article{SPEISER2012463,
title = {Why is paper-and-pencil multiplication difficult for many people?},
journal = {The Journal of Mathematical Behavior},
volume = {31},
number = {4},
pages = {463-475},
year = {2012},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2012.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S0732312312000338},
author = {Robert Speiser and Matthew H. Schneps and Amanda Heffner-Wong and Jaimie L. Miller and Gerhard Sonnert},
keywords = {Representations, Spatial schemas, Working memory, Attention, Multiplication, Algorithms},
abstract = {In school, at least in the US, we were taught to multiply by hand according to a standard algorithm. Most people find that algorithm difficult to use, and many children fail to learn it. We propose a new way to make sense of this difficulty: to treat explicit computation as perceptually supported physical and mental action. Based on recent work in neuroscience, we trace the flow of arithmetic information to emphasize demands on visual working memory and attention. We predict that algorithms that make moderate demands on memory and attention will work better than others that make stronger demands. We suggest that the judicious use of spatial schemas can reduce such cognitive demands. Experimental evidence from children in an inner-city school supports this claim. Our work suggests new ways to think about instruction. The goal should be to minimize demands that present obstacles and maximize instead what human eyes, bodies, and brains do well.}
}
@article{ISMAILOVA2021332,
title = {Equalities between Combinators to Evaluate Expressions},
journal = {Procedia Computer Science},
volume = {190},
pages = {332-340},
year = {2021},
note = {2020 Annual International Conference on Brain-Inspired Cognitive Architectures for Artificial Intelligence: Eleventh Annual Meeting of the BICA Society},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.06.058},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921013053},
author = {Larisa Ismailova and Viacheslav Wolfengagen and Sergey Kosikov},
keywords = {information process, model structure, indexed expressions, cognitive model, combinator-as-process},
abstract = {One of the aims of this work is to revise the known issues of how to obtain the value of the expression using an applicative computing system. Computation and/or symbolic transformations have undoubtedly become one of the dominant trends in modern computer science. In particular, this refers to the execution of the processes of inference of an object with given properties, which is studied with the formation of sets of equalities leading to a target computational model. At the same time, the mathematical theory of computation gets a target equational description, making the process of studying its properties and capabilities more suitable. A model structure based on the method of indexed expressions is developed and applied using the notions of evaluation mapping and assignments. As shown, this may not be considered as generic notion, but derived using the combinators. Thus, the semantics of computation can be derived from the construction of a combinator-as-process. This is the embedding into a system of combinators. A linking system of equalities between combinatorsl is established, which serving as a generic computational model. This allows us to look differently at the previous cognitive ideas of the semantics of computation disabling an evaluation map and assignments and enabling their replacement by a set of equalities between combinators.}
}
@article{MARUFA20241182,
title = {Educating Middle Adolescent through Social Media: The Impact of Early Marriage on Achieving High Education},
journal = {Procedia Computer Science},
volume = {245},
pages = {1182-1191},
year = {2024},
note = {9th International Conference on Computer Science and Computational Intelligence 2024 (ICCSCI 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.10.348},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924031569},
author = {Riza Izzati Ma'rufa and Yudhistya Ayu Kusumawati and Asri Radhitanti},
keywords = {Social Media Campaign, Early Marriege, Quality Education, Women},
abstract = {Malang has become the city that reached the highest rate of early marriage in the Province of East Java. Many data had stated that Malang remained on the first rank for two years straight which is 2021 until 2022. This issue became the main cause of students dropping out of school and hampered in achieving higher education. Some of the factors that cause early marriage to remain unresolved are society's distorted perception of early marriage. Due to this issue, this study aimed to observe how (research output) can reduce the rate of early marriage especially in Malang. To acknowledge the problem, this study uses literature review and qualitative methods for data processing. Sources of this literature review are extracted from various journals that come from trusted sources. Based on the research results, it can be sensed that the influence of early marriage has a negative impact on educational attainment of adolescents especially on women. Due to high number of early marriages in Malang and the impact on educational attainment of female adolescents, this research resulting in social media campaign to educate adolescents about how early marriage impacts quality education. Therefore, it is hoped that this Instagram Platform Based Campaign will help to spread awareness among adolescents about how early marriage impacts their education and life goals.}
}
@article{ISOLAN2024111786,
title = {Monte Carlo analysis of dosimetric issues in space exploration},
journal = {Radiation Physics and Chemistry},
volume = {221},
pages = {111786},
year = {2024},
issn = {0969-806X},
doi = {https://doi.org/10.1016/j.radphyschem.2024.111786},
url = {https://www.sciencedirect.com/science/article/pii/S0969806X24002780},
author = {Lorenzo Isolan and Valentina Sumini and Marco Sumini},
keywords = {Space habitat, MCNP6, Unstructured mesh, Topology optimization, Radiation protection},
abstract = {The Radiation protection is of paramount importance in the planning of human exploration activities in space. The related risks must be considered with respect to two aspects: devising a proper shielding and providing answers to the requirement of an effective dosimetry evaluation in astronaut's activities. Both aspects have been considered using the Monte Carlo (MC) code MCNP 6.2 as the reference tool. As case study an application devised for the National Aeronautics and Space Administration (NASA) Artemis program has been chosen. The project aims to establish a sustainable human presence on the Moon, envisioning the realization of an outpost that will serve as a steppingstone for space exploration endeavors. A Class III shelter, in situ resource utilization (ISRU) built habitat for the Moon, has been designed through computational methods and topology optimization techniques, and analyzed in terms of radiation shielding performances and the strictly related structural behavior. The outpost must be able to withstand temperature variations, micrometeorite impacts, and the absence of a substantial atmosphere. Any solution studied to respect the constraints must devise robust and innovative materials and techniques to create habitats that have as goal the shielding from the Galactic Cosmic Rays (GCR) and from the solar flares to provide a safe and habitable environment at the time scales scheduled for the missions. Moreover, the outpost design must incorporate strategies for extracting and utilizing local resources. Overcoming such challenges will pave the way for the establishment of a sustainable human presence on the Moon and serve as a crucial leap for future space exploration missions.}
}
@article{HUANG2024369,
title = {A linear-attention-combined convolutional neural network for EEG-based visual stimulus recognition},
journal = {Biocybernetics and Biomedical Engineering},
volume = {44},
number = {2},
pages = {369-379},
year = {2024},
issn = {0208-5216},
doi = {https://doi.org/10.1016/j.bbe.2024.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S0208521624000299},
author = {Junjie Huang and Wanzhong Chen and Tao Zhang},
keywords = {Brain–computer interface (BCI), Convolutional neural network (CNN), Electroencephalogram (EEG), Linear attention mechanism, Visual stimulus recognition},
abstract = {The recognition task of visual stimuli based on EEG (Electroencephalogram) has become a major and important topic in the field of Brain–Computer Interfaces (BCI) research. Although the underlying spatial features of EEG can effectively represent visual stimulus information, it still remains a highly challenging task to explore the local–global information of the underlying EEG to achieve better decoding performance. Therefore, in this paper we propose a deep learning architecture called Linear-Attention-combined Convolutional Neural Network (LACNN) for visual stimuli EEG-based classification task. The proposed architecture combines the modules of Convolutional Neural Networks (CNN) and Linear Attention, effectively extracting local and global features of EEG for decoding while maintaining low computational complexity and model parameters. We conducted extensive experiments on a public EEG dataset from the Stanford Digital Repository. The experimental results demonstrate that LACNN achieves an average decoding accuracy of 54.13% and 29.83% in 6-category and 72-exemplar classification tasks respectively, outperforming the state-of-the-art methods, which indicates that our method can effectively decode visual stimuli from EEG. Further analysis of LACNN shows that the Linear Attention module improves the separability between different category features and localizes key brain region information that aligns with the paradigm principles.}
}
@article{DENG2023104944,
title = {A VR-based BCI interactive system for UAV swarm control},
journal = {Biomedical Signal Processing and Control},
volume = {85},
pages = {104944},
year = {2023},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2023.104944},
url = {https://www.sciencedirect.com/science/article/pii/S1746809423003774},
author = {Tao Deng and Zhen Huo and Lihua Zhang and Zhiyan Dong and Lan Niu and Xiaoyang Kang and Xiuwei Huang},
keywords = {Brain-computer interface (BCI), Swarm control, Steady state visual evoked potential (SSVEP), Electroencephalogram (EEG), Unmanned Aerial Vehicle (UAV), Quadcopter, Virtual Reality (VR)},
abstract = {The traditional Unmanned Aerial Vehicle (UAV) swarm control mainly adopts the ground station method, which is too fixed, and the interaction is difficult to meet the high dynamic task requirements. There is an urgent need for new interaction methods to integrate the advantages of human thinking in dealing with uncertain problems. Nevertheless, brain-computer interface(BCI) technology is directly controlled by thoughts, one of the most promising next-generation human–computer interaction technologies. Therefore, in this study, we innovatively applied the BCI system based on Virtual Reality (VR) to the group UAV and realized a novel and intelligent group control method, which proposes new ideas and paradigms for the control of swarm UAVs in the future. Specifically, this study takes a quadcopter as an example. A modular and extensible multi-quadcopter system was created, and then a visual stimulation 3D VR scene system with a digital twin function was established. On this basis, the BCI system based on the Stable state visual evoked potential (SSVEP) paradigm was adopted for the swarm control of the quadcopter. The experimental results show that the formation control of multi-quadcopter is successfully realized by the subjects using the proposed VR-based BCI interactive system, with an accuracy rate of 90% and a good performance in information transmission rate. In addition, the immersive VR twin system established one-to-one for EEG signal acquisition allows subjects to have a better experience.}
}
@incollection{LU2024173,
title = {Chapter 13 - Collection and transmission planning for large offshore wind power base},
editor = {Zongxiang Lu and Haibo Li and Ying Qiao and Le Xie and Chanan Singh},
booktitle = {Power System Flexibility},
publisher = {Academic Press},
pages = {173-191},
year = {2024},
isbn = {978-0-323-99517-7},
doi = {https://doi.org/10.1016/B978-0-323-99517-7.00016-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780323995177000166},
author = {Zongxiang Lu and Haibo Li and Ying Qiao and Le Xie and Chanan Singh},
keywords = {Mid- to offshore wind power, hierarchical planning, steepest descent method, minimum spanning tree algorithm, transmission equipment selection, improved ant colony algorithm},
abstract = {Offshore wind power planning in China thus far has generally inherited the principles and thinking of onshore wind power planning. Mid- to offshore wind power access could incur a cost far higher than that of onshore wind generation, accounting for 15%–30% of the total investment. Sea state resources, corridor resources, landing conditions, and submarine cable wiring all face materially contrasting constraints to onshore wind power, necessitating dedicated planning efforts for offshore wind power. In this chapter, a planning framework for the sequential and cascaded development of mid- to offshore wind power bases is proposed, and a tiered master planning approach featuring on-site, cluster, and AC/DC transmission is established, filling the gap in transmission planning of large-scale mid- to offshore wind farm clusters. Firstly, location optimization of offshore hub substations based on a steepest descent method is established and explaining its specific implementation methods. Secondly, an improved minimum spanning tree algorithm is used to find a topology connection method for the collector system with the optimal length of submarine cable, which makes the cost optimal. Finally, the optimization model of transmission equipment selection considering the risk of high wind speed truncation is established, and the topology optimization modeling method of offshore wind power cluster transmission systems based on the improved ant colony optimization algorithm is proposed.}
}
@article{20152,
title = {Positive Gradients},
journal = {Cell Systems},
volume = {1},
number = {1},
pages = {2-3},
year = {2015},
issn = {2405-4712},
doi = {https://doi.org/10.1016/j.cels.2015.07.005},
url = {https://www.sciencedirect.com/science/article/pii/S2405471215000137},
abstract = {The new associate vice chancellor of computational heath sciences at the University of California San Diego reflects on the coming era of big data in medicine.}
}
@incollection{BOSE2006649,
title = {Chapter 10 - Fuzzy Logic and Applications},
editor = {Bimal K. Bose},
booktitle = {Power Electronics And Motor Drives},
publisher = {Academic Press},
address = {Burlington},
pages = {649-729},
year = {2006},
isbn = {978-0-12-088405-6},
doi = {https://doi.org/10.1016/B978-012088405-6/50012-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780120884056500121},
author = {Bimal K. Bose},
abstract = {Publisher Summary
The chapter deals with the description of Fussy Logic (FL) principles and its application in power electronics and motor-drive systems. The FL is a discipline under Artificial Intelligence (AI). AI is basically computer emulation of human thinking (called computational intelligence). The goal of AI is to mimic human intelligence. AI also includes expert systems (ESs), artificial neural networks (ANNs), or neural networks (NNWs), and genetic algorithms (GAs). All of the main areas in AI, except ES, are defined as soft computing. The concept of FL and its characteristics emerge from Zadeh's theory propounded in 1965. Any FL application uses a knowledge base that consists of multivalued membership functions (MFs) describing the fuzzy variables and the rule table consisting of “IF… THEN… statements.” The knowledge base is developed on the basis of the behavioral nature of the system. The trial-and-error approach of FL algorithms may be time consuming, but user-friendly computer programs (such as the MATLAB-based Fuzzy Logic Toolbox) help speed the process. The applications of FL principles, include speed control of induction motor vector drives, efficiency optimization of induction motor vector drives by flux programming; and wind generation systems, linearization of the transfer characteristics of thyristor converters at discontinuous conduction, induction motor stator resistance estimations, estimation of distorted waveforms and MRAC slip gain tuning control of vector drives.}
}
@article{BOWMAN2023107339,
title = {Desperately searching for something},
journal = {Communications in Nonlinear Science and Numerical Simulation},
volume = {125},
pages = {107339},
year = {2023},
issn = {1007-5704},
doi = {https://doi.org/10.1016/j.cnsns.2023.107339},
url = {https://www.sciencedirect.com/science/article/pii/S1007570423002575},
author = {Clive E. Bowman and Peter Grindrod},
abstract = {There is a growing interest in novelty search : that is, in sampling a parameter space to search for radical or unexpected behaviour(s), occurring as a consequence of parameter choice, being input to some downstream complex system, process, or service that will not yield to analysis, without imposing any specific pre-ordained objective function, or fitness function to be optimised. We mean “parameter” in the widest sense, including system learnables, non-autonomous forcing, sequencing and all inputs. Depending upon the nature of the underlying parameter space of interest one may adopt a rather wide range of search algorithms. We do consider that this search activity has meta-objectives, though: one is of achieving diversity (efficiently reaching out across the space in some way); and one is of achieving some minimum density (not leaving out large unexplored holes). These are in tension. In general, the computational costs of both of these qualities become restrictive as the dimension of the parameter spaces increase; and consequently their balance is harder to maintain. We may also wish for a substantial random element of search to provide some luck in discovery and to avoid any naive preset sampling patterns. We consider archive-based methods within a range of spaces: finite discrete spaces, where the problem is straightforward (provided we are patient with the random element); Euclidean spaces, of increasing dimension, that become very lonely places; and infinite dimensional spaces. Our aim is to discuss a raft of distinctive search concepts, that respond to identified challenges, and rely on a rather diverse range of mathematical ideas. This arms practitioners with a range of highly practical methods. However applications requiring novelty search arise, one should avoid rushing to code-up a standard evolving search algorithm and instead give some thought to the nature and requirements of the search: there is a range of effective options available. We give some considered advice.}
}
@article{SHAW2019,
title = {Artificial Intelligence and the Implementation Challenge},
journal = {Journal of Medical Internet Research},
volume = {21},
number = {7},
year = {2019},
issn = {1438-8871},
doi = {https://doi.org/10.2196/13659},
url = {https://www.sciencedirect.com/science/article/pii/S1438887119003595},
author = {James Shaw and Frank Rudzicz and Trevor Jamieson and Avi Goldfarb},
keywords = {artificial intelligence, machine learning, implementation science, ethics},
abstract = {Background
Applications of artificial intelligence (AI) in health care have garnered much attention in recent years, but the implementation issues posed by AI have not been substantially addressed.
Objective
In this paper, we have focused on machine learning (ML) as a form of AI and have provided a framework for thinking about use cases of ML in health care. We have structured our discussion of challenges in the implementation of ML in comparison with other technologies using the framework of Nonadoption, Abandonment, and Challenges to the Scale-Up, Spread, and Sustainability of Health and Care Technologies (NASSS).
Methods
After providing an overview of AI technology, we describe use cases of ML as falling into the categories of decision support and automation. We suggest these use cases apply to clinical, operational, and epidemiological tasks and that the primary function of ML in health care in the near term will be decision support. We then outline unique implementation issues posed by ML initiatives in the categories addressed by the NASSS framework, specifically including meaningful decision support, explainability, privacy, consent, algorithmic bias, security, scalability, the role of corporations, and the changing nature of health care work.
Results
Ultimately, we suggest that the future of ML in health care remains positive but uncertain, as support from patients, the public, and a wide range of health care stakeholders is necessary to enable its meaningful implementation.
Conclusions
If the implementation science community is to facilitate the adoption of ML in ways that stand to generate widespread benefits, the issues raised in this paper will require substantial attention in the coming years.}
}
@article{QUARESIMIN20122290,
title = {Strategies for the assessment of nanocomposite mechanical properties},
journal = {Composites Part B: Engineering},
volume = {43},
number = {5},
pages = {2290-2297},
year = {2012},
issn = {1359-8368},
doi = {https://doi.org/10.1016/j.compositesb.2011.12.012},
url = {https://www.sciencedirect.com/science/article/pii/S1359836812000030},
author = {Marino Quaresimin and Marco Salviato and Michele Zappalorto},
keywords = {A. Nano-structures, B. Mechanical properties, B. Fracture toughness, Three-stage strategy (TSS)},
abstract = {The assessment of nanocomposite mechanical properties is a challenging task. Due to their hierarchical structure, which spans from nano to macro length-scales, a different way of thinking from traditional approaches is needed to account for the characteristic phenomena of each length-scale and bridge their effects from the smaller scale to the macroscale. In the present work, some important issues of nanocomposite modelling are discussed. Then, a classification of the available modelling strategies is proposed, according to the scale from which the problem is addressed. This comprehensive analysis is thought as a necessary tool for the development of new effective approaches.}
}
@article{ZHENG2021102174,
title = {An Attention-based Bi-LSTM Method for Visual Object Classification via EEG},
journal = {Biomedical Signal Processing and Control},
volume = {63},
pages = {102174},
year = {2021},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2020.102174},
url = {https://www.sciencedirect.com/science/article/pii/S174680942030313X},
author = {Xiao Zheng and Wanzhong Chen},
keywords = {Deep learning, Attention mechanism, EEG, Bi-LSTM, Visual perception},
abstract = {Background and Objective
Despite many models have been proposed for brain visual perception and content understanding via electroencephalograms (EEGs), due to the lack of research on the inherent temporal relationship, EEG-based visual object classification still demands the improvement on its accuracy and computation complexity.
Methods
To take full advantage of the uneven visual feature saturation between time segments, an end-to-end attention-based Bi-LSTM Method is proposed, named Bi-LSTM-AttGW. Two attention strategies are introduced to Bi-LSTM framework. The attention gate replaces the forget gate in traditional LSTM. It is only relevant to the historical cell state, and not related to the current input. Hence, the attention gate can greatly reduce the number of training parameters. Moreover, the attention weighting method is applied to Bi-LSTM output, and it can explore the most decisive information.
Results
The best classification accuracy achieved by Bi-LSTM-AttGW model is 99.50%. Compared with the state-of-art algorithms and baseline models, the proposed method has great advantages in classification performance and computational complexity. Considering brain region level contribution on visual cognition task, we also verify our method using EEG signals collected from the frontal and occipital regions, that are highly correlated with visual perception tasks.
Conclusions
The results show promise towards the idea that human brain activity related to visual recognition can be more effectively decoded by neural networks with neural mechanism. The experimental results not only could provide strong support for the modularity theory about the brain cognitive function, but show the superiority of the proposed Bi-LSTM model with attention mechanism again.}
}
@article{XUE2025129449,
title = {Lightweight visual backbone network with enhanced comprehensive strength through context-aware dual attention mechanism},
journal = {Neurocomputing},
volume = {624},
pages = {129449},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2025.129449},
url = {https://www.sciencedirect.com/science/article/pii/S0925231225001213},
author = {Jianxin Xue and Yaohua Hu and Sicheng Hua and Minyu Chen and Ling-I Wu and Xi Chang and Guoqiang Li},
keywords = {Deep learning, Attention mechanism, Backbone network, Computer vision},
abstract = {Recent studies have increasingly concentrated on integrating visual neural networks into edge devices, especially for robots and drones that require effective real-time decision-making and autonomous path planning. Achieving this necessitates lightweight visual networks with robust overall performance. In this study, we adopt a small-scale architecture based on convolutional neural networks (CNNs) and propose a Context-Aware Decoupled Fully Connected (CADFC) attention mechanism to enhance the performance of existing CNN-based visual networks. The core concept of the CADFC attention mechanism is to incorporate both remote and local contexts, enabling the network to capture local features and their relevant remote features. Moreover, we design a novel bottleneck that integrates CADFC attention with depthwise convolution. This design thinking is to accumulate focused regions, learning from depthwise convolution, alongside their surrounding features based on CADFC attention. Experimental results demonstrate that CADFC MobileNet outperforms the recent SOTA network GhostNetV2, achieving a top-1 accuracy of 76.8% with 15% fewer parameters, surpassing GhostNetV2’s 75.3% accuracy by 1.5%, even in resource-constrained environments.}
}
@article{MITTAL2023105092,
title = {The method of harmonic balance for the Giesekus model under oscillatory shear},
journal = {Journal of Non-Newtonian Fluid Mechanics},
volume = {321},
pages = {105092},
year = {2023},
issn = {0377-0257},
doi = {https://doi.org/10.1016/j.jnnfm.2023.105092},
url = {https://www.sciencedirect.com/science/article/pii/S0377025723001040},
author = {Shivangi Mittal and Yogesh M. Joshi and Sachin Shanbhag},
keywords = {LAOS, Spectral method, Fourier series, Numerical method},
abstract = {The method of harmonic balance (HB) is a spectrally accurate method used to obtain periodic steady state solutions to dynamical systems subjected to periodic perturbations. We adapt HB to solve for the stress response of the Giesekus model under large amplitude oscillatory shear (LAOS) deformation. HB transforms the system of differential equations to a set of nonlinear algebraic equations in the Fourier coefficients. Convergence studies find that the difference between the HB and true solutions decays exponentially with the number of harmonics (H) included in the ansatz as e−mH. The decay coefficient m decreases with increasing strain amplitude, and exhibits a “U” shaped dependence on applied frequency. The computational cost of HB increases slightly faster than linearly with H. The net result of rapid convergence and modest increase in computational cost with increasing H implies that HB outperforms the conventional method of using numerical integration to solve differential constitutive equations under oscillatory shear. Numerical experiments find that HB is simultaneously about three orders of magnitude cheaper, and several orders of magnitude more accurate than numerical integration. Thus, it offers a compelling value proposition for parameter estimation or model selection.}
}
@article{ALHARRASI201958,
title = {Evidence for the involvement of a GABAergic mechanism in the effectiveness of natural and synthetically modified incensole derivatives in neuropharmacological disorders: A computational and pharmacological approach},
journal = {Phytochemistry},
volume = {163},
pages = {58-74},
year = {2019},
issn = {0031-9422},
doi = {https://doi.org/10.1016/j.phytochem.2019.04.007},
url = {https://www.sciencedirect.com/science/article/pii/S0031942218308239},
author = {Ahmed Al-Harrasi and Ajmal Khan and Najeeb Ur Rehman and Sulaiman Al-Shidhani and Nasiara Karim and Imran Khan and Sobia Ahsan Halim and Ahmed Al-Rawahi and Javid Hussain and Rene Csuk},
keywords = {Incensole, Incensone, Incensfuran, Antidepressant, Anxiolytic, Anticonvulsant, Homology modelling, Molecular docking, ADMET prediction},
abstract = {In the course of our continuing exploration for novel bioactive lead compounds (s) from the species Boswellia, we have recently reported incensole derivatives isolated from Boswellia papyrifera Hochst. Given the known antidepressant-like effects of incensole and incensole acetate, we herein present that the low dose intraperitoneal administration of incensole derivatives, namely, incensfuran and incensone, showed significant antidepressant-like effects in the forced swim test (FST) and tail suspension test (TST). Furthermore, these compounds were evaluated for their anxiolytic potential in the elevated plus maze (EPM) and light dark box (LDB) tests and anticonvulsant effects in pentylenetetrazole (PTZ)-induced seizure tests. In the EPM test, administration of these compounds led to dose-dependent increases in open arm entries and in the time spent in EPM open arms. Similar results were obtained in the LDB test, wherein compounds these caused significant increases in the number of transitions between lit and dark compartments and the time spent in the lit compartment. The anxiolytic-like effects in the EPM were not reversed by pretreatment with flumazenil, whereas PTZ and bicuculline (BIC) completely abolished the anxiolytic effects, showing the involvement of the non-benzodiazepine binding sites of GABAA receptors. All four compounds induced significantly elevated brain GABA levels, indicating the involvement of a GABAergic mechanism. Additionally, molecular docking was conducted to elucidate the mode of action for the anxiolytic and anticonvulsant effects of these derivatives. Moreover, these compounds also possess drug-like properties and excellent ADMET profiles.}
}
@article{DUAL2024596,
title = {The Future of Durable Mechanical Circulatory Support: Emerging Technological Innovations and Considerations to Enable Evolution of the Field},
journal = {Journal of Cardiac Failure},
volume = {30},
number = {4},
pages = {596-609},
year = {2024},
issn = {1071-9164},
doi = {https://doi.org/10.1016/j.cardfail.2024.01.011},
url = {https://www.sciencedirect.com/science/article/pii/S1071916424000411},
author = {Seraina A. Dual and Jennifer Cowger and Ellen Roche and Aditi Nayak},
keywords = {Mechanical circulatory support, left ventricular assist device, translation, innovation},
abstract = {The field of durable mechanical circulatory support (MCS) has undergone an incredible evolution over the past few decades, resulting in significant improvements in longevity and quality of life for patients with advanced heart failure. Despite these successes, substantial opportunities for further improvements remain, including in pump design and ancillary technology, perioperative and postoperative management, and the overall patient experience. Ideally, durable MCS devices would be fully implantable, automatically controlled, and minimize the need for anticoagulation. Reliable and long-term total artificial hearts for biventricular support would be available; and surgical, perioperative, and postoperative management would be informed by the individual patient phenotype along with computational simulations. In this review, we summarize emerging technological innovations in these areas, focusing primarily on innovations in late preclinical or early clinical phases of study. We highlight important considerations that the MCS community of clinicians, engineers, industry partners, and venture capital investors should consider to sustain the evolution of the field.}
}
@article{HU2018275,
title = {Can a machine have two systems for recognition, like human beings?},
journal = {Journal of Visual Communication and Image Representation},
volume = {56},
pages = {275-286},
year = {2018},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.09.008},
url = {https://www.sciencedirect.com/science/article/pii/S1047320318302232},
author = {Jiwei Hu and Kin-Man Lam and Ping Lou and Quan Liu and Wupeng Deng},
keywords = {Image annotation, Multi-labeling, Hierarchical tree structure, Feature-pool selection},
abstract = {Artificial Intelligence has attracted much of researchers’ attention in recent years. A question we always ask is: “Can machines replace human beings to some extent?” This paper aims to explore the knowledge learning for an image-annotation framework, which is an easy task for humans but a tough task for machines. This paper’s research is based on an assumption that machines have two systems of thinking, each of which handles the labels of images at different abstract levels. Based on this, a new hierarchical model for image annotation is introduced. We explore not only the relationships between the labels and the features used, but also the relationships between labels. More specifically, we divide labels into several hierarchies for efficient and accurate labeling, which are constructed using our Associative Memory Sharing method, proposed in this paper.}
}
@incollection{ADDIS2025501,
title = {Memory and imagination},
editor = {Jordan Henry Grafman},
booktitle = {Encyclopedia of the Human Brain (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {501-513},
year = {2025},
isbn = {978-0-12-820481-8},
doi = {https://doi.org/10.1016/B978-0-12-820480-1.00135-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128204801001352},
author = {Donna Rose Addis},
keywords = {Associative, Autobiographical memory, Default mode network, Episodic memory, Frontoparietal control network, Future thinking, Hippocampus, Imagination, Medial prefrontal cortex, Prospection, Relational processing, Schema, Simulation},
abstract = {The human brain has a remarkable capacity to not only remember events from the past but to construct a variety of imagined experiences, ranging from hypothetical and counterfactual past events, to future events and entirely fictional episodes. Both remembering and imagining is underpinned by the brain's simulation system: default mode network. I describe the theoretical beginnings of this relatively new topic in contemporary neuroscience, as well as neuroimaging investigations of autobiographical memory and prospective imagination that together provide evidence of a single “simulation system” supported primarily by core functions of the default mode network: associating elements, associative schematic processes, and buffering the emergent simulation.}
}
@article{JIN202520,
title = {Methods and reliability study of moral education assessment in universities: A machine learning-based approach},
journal = {Alexandria Engineering Journal},
volume = {125},
pages = {20-28},
year = {2025},
issn = {1110-0168},
doi = {https://doi.org/10.1016/j.aej.2025.03.095},
url = {https://www.sciencedirect.com/science/article/pii/S111001682500403X},
author = {Ting Jin},
keywords = {Reliability study, Moral education, Machine learning, Policy analysis, Statistical analysis},
abstract = {The research aims to assess the effectiveness of machine learning (ML) techniques in evaluating moral education programs at university institutions. The objective is to employ data-driven methodologies to enhance ethical assessment frameworks through improved objectivity, scalability, and consistency. This analysis utilizes Principal Component Analysis (PCA) alongside the k-Nearest Neighbor (k-NN) method, Support Vector Regression (SVR), and Artificial Neural Networks (ANN) to study student performance indices, enabling the prediction of ethical reasoning capabilities for standardized evaluation. The study demonstrates how machine learning efficiently assesses student moral education performance by leveraging PCA to identify patterns and using ML models to make accurate predictions. Findings reveal a strong correlation between subject proficiency in mathematics, reading, and writing and moral reasoning abilities, highlighting the role of academic competencies in ethical decision-making. Additionally, gender-based analysis indicates that female students tend to achieve better results in moral skills assessments than their male counterparts. Among the models tested, SVR exhibits the highest predictive accuracy, whereas k-NN returns the widest prediction errors. The study recommends the deployment of AI-based moral assessment systems in universities to ensure consistent and objective evaluation processes for policy development.}
}
@article{KHEDMATIMORASAE2024219,
title = {Advancing the discourse: A next-generation value chain-based taxonomy for circular economy key performance indicators},
journal = {Sustainable Production and Consumption},
volume = {48},
pages = {219-234},
year = {2024},
issn = {2352-5509},
doi = {https://doi.org/10.1016/j.spc.2024.05.013},
url = {https://www.sciencedirect.com/science/article/pii/S2352550924001428},
author = {Esmaeil Khedmati-Morasae and Markus Zils and Peter Hopkinson and Ryan Nolan and Fiona Charnley and Okechukwu Okorie and Halid Abu-Bakar},
keywords = {Circular economy, Key performance indicators, Taxonomy, Value chain, Systemic},
abstract = {The growth of interest in circular economy (CE) has been accompanied by different approaches to measurement of CE outcomes and impacts, leading to a wide portfolio of indicators with varying degrees of overlap, inconsistency, and convergence. The aim of this paper is to propose a unifying framework for CE indicators, as the next generation of CE taxonomies. We first undertake a scoping review of 59 review papers on CE indicators using manual and computational methods (i.e., topic modelling) to inform the taxonomy structure and content. As a result, we report on 11 clusters of approaches that have been attentive to different dimensions of CE (e.g. horizontal value chain, vertical scale of operation (macro, meso, micro), impact category (economic, biophysical, social), material vs product focus, etc.). Highlighting the strengths and weakness of these approaches, we identify gaps in dimensions related to horizontal and vertical scales of measurement, and propose an agnostic, integrative framework that builds on the scientific foundations of previous research, within a more systemic and comprehensive taxonomy. This taxonomy could be used as a guiding framework or heuristic for regulators, both nationally and internationally, and for practitioners to undertake a comprehensive measurement and assessment of CE related interventions and initiatives at scale.}
}
@incollection{STANOVICH2008251,
title = {The Development of Rational Thought: A Taxonomy of Heuristics and Biases},
editor = {Robert V. Kail},
series = {Advances in Child Development and Behavior},
publisher = {JAI},
volume = {36},
pages = {251-285},
year = {2008},
booktitle = {Advances in Child Development and Behavior},
issn = {0065-2407},
doi = {https://doi.org/10.1016/S0065-2407(08)00006-2},
url = {https://www.sciencedirect.com/science/article/pii/S0065240708000062},
author = {Keith E. Stanovich and Maggie E. Toplak and Richard F. West},
abstract = {Publisher Summary
The most well-known indicators of cognitive functioning—intelligence and cognitive ability tests—do not assess a critical aspect of thinking, which is the ability to think rationally. To think rationally means adopting appropriate goals, taking the appropriate action given one's goals and beliefs, and holding beliefs that are commensurate with available evidence. Standard intelligence tests do not assess such functions. Although intelligence tests assess the ability to focus on an immediate goal in the face of distraction, they do not assess whether a person has the tendency to develop goals that are rational in the first place. Likewise, intelligence tests are good measures of how well a person can hold beliefs in short-term memory and manipulate those beliefs, but they do not assess whether a person has the tendency to form beliefs rationally when presented with evidence. Similarly, intelligence tests are good measures of how efficiently a person processes information that has been provided, but they do not assess whether the person is a critical assessor of information as it is gathered in the natural environment.}
}
@article{EKLUND201716,
title = {Two approaches to System-of-Systems from Lative Logic point of view},
journal = {Procedia Computer Science},
volume = {119},
pages = {16-21},
year = {2017},
note = {6th International Young Scientist Conference on Computational Science, YSC 2017, 01-03 November 2017, Kotka, Finland},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2017.11.155},
url = {https://www.sciencedirect.com/science/article/pii/S1877050917323657},
author = {Patrik Eklund and Jari Kortelainen},
keywords = {Category theory, computational science, lative logic, System-of-Systems},
abstract = {The paper presents two approaches to model System-of-Systems on lative logic point of view. Lative logic is a general framework to construct building blocks of logic using Category Theory as its metalanguage. This approach reveals avenues to describe System-of-Systems themselves, and to model information and processes they posess, using some reasonable modelling languages in a computational manner, thus, touching foundations of computational science. After presenting some preliminary notes, the paper explains the main steps to construct lative logics, and then give two approaches to System-of-System modelling. Finally, the paper presents a survey to some applications.}
}
@article{ZENG2019138,
title = {Iterative optimal control syntheses illustrated on the Brockett integrator},
journal = {IFAC-PapersOnLine},
volume = {52},
number = {16},
pages = {138-143},
year = {2019},
note = {11th IFAC Symposium on Nonlinear Control Systems NOLCOS 2019},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2019.11.768},
url = {https://www.sciencedirect.com/science/article/pii/S2405896319317719},
author = {Shen Zeng},
keywords = {Nonholonomic systems, Optimal control, Computational methods},
abstract = {In this paper, we investigate computational methods for synthesizing optimal control inputs for nonholonomic control systems. We use the Brockett integrator as a benchmark example to illustrate different aspects of the computational trajectory optimization problem. The main result of this paper is the establishment of a rather attractive iterative scheme for practically constructing optimal control inputs. The functionality and efficiency of the proposed iterative scheme are illustrated on different optimal steering problems centered around the Brockett integrator.}
}
@article{QU2024102255,
title = {Unmanned combat aerial vehicle path planning in complex environment using multi-strategy sparrow search algorithm with double-layer coding},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {36},
number = {10},
pages = {102255},
year = {2024},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2024.102255},
url = {https://www.sciencedirect.com/science/article/pii/S1319157824003446},
author = {Liangdong Qu and Jingkun Fan},
keywords = {UCAV path planning, Double-layer coding, Complex environment, Sparrow search algorithm, Dynamic fitness regulation learning strategy},
abstract = {Unmanned combat aerial vehicles (UCAV) path planning in complex environments demands a substantial number of path points to determine feasible paths. Establishing an effective flight path for UCAVs requires numerous path points to account for fuel constraints, artillery threats, and radar avoidance. This increase in path points raises the dimensionality of the problem, which in turn degrades algorithm performance. To mitigate this issue, a double-layer coding (DLC) model is utilized to remove redundant path points, consequently lowering computational complexity and operational difficulties. Meanwhile, this paper introduces a novel enhanced sparrow search algorithm (MESSA) based on multi-strategy for UCAV path planning. The MESSA incorporates a novel dynamic fitness regulation learning strategy (DFRL), a random differential learning strategy (RDL), an elite example equilibrium learning strategy (EEEL), a dynamic elimination and regeneration strategy based on the elite example (DERE), and quadratic interpolation (QI). Furthermore, MESSA is compared against 11 state-of-the-art algorithms, demonstrating exceptional optimization performance and robustness. Additionally, the combination of MESSA with the DLC model (DLC-MESSA) is applied to solve the UCAV path planning problem. The experimental results from five complex environments indicate that DLC-MESSA outperforms other algorithms in 80% of the cases by achieving the lowest average cost, thereby demonstrating its superior robustness and computational efficiency.}
}
@article{BOUKHRIS201727,
title = {Co-creation in the Early Stage of Product-service System Development},
journal = {Procedia CIRP},
volume = {63},
pages = {27-32},
year = {2017},
note = {Manufacturing Systems 4.0 – Proceedings of the 50th CIRP Conference on Manufacturing Systems},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2017.03.316},
url = {https://www.sciencedirect.com/science/article/pii/S2212827117305048},
author = {Aida Boukhris and Albrecht Fritzsche and Kathrin Möslein},
keywords = {Co-creation, product-service system, prototyping},
abstract = {Co-creation is a well-established topic in manufacturing research. Since 1999 there has been a wide range of publications about the involvement of customers in the design of end products. What is new, however, is the stakeholder, particularly user, integration at the early stage of development of product-service systems (PSS) for concept co-creation. In this paper, we evaluate the methods for PSS development at the fuzzy front-end, before we derive a co-creation oriented method that relies on prototyping at selected stages of the process. The first stage deals with the generation of a shared understanding of the concept to be developed, and in the second stage we converge towards the generation of the user requirements by employing a set of electronic tools to drive process and logic flow thinking within the group of co-creators. The methodology has been evaluated within three workshops involving 42 participants who were asked to design a decentralized bike-sharing system, where bike owners can generate revenues from their bikes by renting them out when not otherwise needed. In a future phase, the results will be compared to the outcomes of a workshop organized with a bike sharing company that is currently developing this model.}
}
@article{DIETRICH2008319,
title = {Imaging the imagination: The trouble with motor imagery},
journal = {Methods},
volume = {45},
number = {4},
pages = {319-324},
year = {2008},
note = {Neuroimaging in the sports sciences},
issn = {1046-2023},
doi = {https://doi.org/10.1016/j.ymeth.2008.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S1046202308000765},
author = {Arne Dietrich},
keywords = {Brain, Cortex, Exercise, Explicit, Hypofrontality, Implicit, Mental training, Neuroimaging, Sports, Imagery},
abstract = {Sports and exercise psychology finds itself in a most unfortunate situation these days. While all other branches of the psychological sciences help themselves freely to the glitzy new toys of modern neuroscience—MRI and PET, mostly—exploring the neural underpinnings of whatever cognitive function they are interested in exploring, the sport sciences are left out of the fun for the simple reason that these imaging instruments preclude motion—the very thing then that is the subject of interest to them. There are several legitimate ways around this problem but the one that seems to be most popular is, I think, not—legitimate, that is. The basic idea, unduly sharpened here, is the following. Neuroimaging studies have shown that imagined and actual motion share the same neural substrates or, alternatively, imagining an action corresponds to a subliminal activation of the same brain areas required for its execution. It follows from this, the arguments runs, that motor imagery can be used as a proxy for real motor performance, et voilà, the sports sciences can go wild with all the snazzy brain imaging tools after all—just like everyone else. This notion is, I believe, misbegotten, a house of cards that threatens to cast a long shadow over the field. The present article, then, is, to be frank, intended to put a machete to this kind of thinking. It does this by exposing this conclusion to be based on an unholy marriage of selective data reporting and gross overgeneralization. The result is a wild goose chase fueled by wishful thinking.}
}
@article{KELLER2006357,
title = {A practical view of ‘druggability’},
journal = {Current Opinion in Chemical Biology},
volume = {10},
number = {4},
pages = {357-361},
year = {2006},
note = {Next-generation therapeutics},
issn = {1367-5931},
doi = {https://doi.org/10.1016/j.cbpa.2006.06.014},
url = {https://www.sciencedirect.com/science/article/pii/S1367593106000834},
author = {Thomas H Keller and Arkadius Pichota and Zheng Yin},
abstract = {The introduction of Lipinski's ‘Rule of Five’ has initiated a profound shift in the thinking paradigm of medicinal chemists. Understanding the difference between biologically active small molecules and drugs became a priority in the drug discovery process, and the importance of addressing pharmacokinetic properties early during lead optimization is a clear result. These concepts of ‘drug-likeness’ and ‘druggability’ have been extended to proteins and genes for target identification and selection. How should these concepts be integrated practically into the drug discovery process? This review summarizes the recent advances in the field and examines the usefulness of ‘the rules of the game’ in practice from a medicinal chemist's standpoint.}
}
@article{SAMPAYO2022100348,
title = {CPSD2: A new approach for cyber-physical systems design and development},
journal = {Journal of Industrial Information Integration},
volume = {28},
pages = {100348},
year = {2022},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2022.100348},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X22000206},
author = {M. Sampayo and P. Peças},
keywords = {Cyber-physical system, Product design and development, Business model, Operational and production needs, Design of cyber-physical systems, Industry 4.0},
abstract = {Cyber-physical systems (CPS) allow the integration of computation to physical contexts, unlocking more sophisticated capabilities in engineering systems. A methodology is needed to properly guide the entire process of creating, designing, building and implementing a CPS. Methodologies addressing the essential aspects of CPS design already exist. However, none was found that properly concerns the company's business model and its way of market interaction, in order to identify opportunities where the use of CPS could improve the performance. Therefore, this article proposes CPSD2: a CPS design and development methodology approach based on generic product design and development methods.}
}
@article{BROSOLO2025104063,
title = {Through the static: Demystifying malware visualization via explainability},
journal = {Journal of Information Security and Applications},
volume = {91},
pages = {104063},
year = {2025},
issn = {2214-2126},
doi = {https://doi.org/10.1016/j.jisa.2025.104063},
url = {https://www.sciencedirect.com/science/article/pii/S2214212625001000},
author = {Matteo Brosolo and Vinod P. and Mauro Conti},
keywords = {Malware, Explainability, Replicability, Neural networks, Class Activation Maps, CNN},
abstract = {Security researchers face growing challenges in rapidly identifying and classifying malware strains for effective protection. While Convolutional Neural Networks (CNNs) have emerged as powerful visual classifiers for this task, critical issues of robustness and explainability, well-studied in domains like medicine, remain underaddressed in malware analysis. Although these models achieve strong performance without manual feature engineering, their replicability and decision-making processes remain poorly understood. Two technical barriers have limited progress: first, the lack of obvious methods for selecting and evaluating explainability techniques due to their inherent complexity, and second the substantial computational resources required for replicating and tuning these models across diverse environments, which requires extensive computational power and time investments often beyond typical research constraints. Our study addresses these gaps through comprehensive replication of six CNN architectures, evaluating both performance and explainability using Class Activation Maps (CAMs) including GradCAM and HiResCAM. We conduct experiments across standard datasets (MalImg, Big2015) and our new VX-Zoo collection, systematically comparing how different models interpret inputs. Our analysis reveals distinct patterns in malware family identification while providing concrete explanations for CNN decisions. Furthermore, we demonstrate how these interpretability insights can enhance Visual Transformers, achieving F1-score yielding substantial improvements in F1 score, ranging from 2% to 8%, across the datasets compared to benchmark values.}
}
@article{MIRTSOPOULOS2023103518,
title = {Structural topology exploration through policy-based generation of equilibrium representations},
journal = {Computer-Aided Design},
volume = {160},
pages = {103518},
year = {2023},
issn = {0010-4485},
doi = {https://doi.org/10.1016/j.cad.2023.103518},
url = {https://www.sciencedirect.com/science/article/pii/S0010448523000507},
author = {Ioannis Mirtsopoulos and Corentin Fivet},
keywords = {Design space exploration, Generative design, Rule-based design, Topology, Structural design, Strut-and-tie},
abstract = {Mainstream approaches to design spatial architectural forms that are structurally relevant consist either in adapting well-known and catalogued conventional types or in searching for close-to-optimum solutions of well-defined problems. Few means exist to explore structural forms detached from these routines. The approach in this paper generates diverse non-triangulated structural topologies that do not result from optimization procedures. The process incrementally transforms interim networks of bars and forces by means of a parametric policy (–) that maintains the static equilibrium of the network at every single step, (–) that ensures growth of the network within specified (non-)convex geometric boundaries, and (–) whose high-level abstract description controls all design parameters. The successive policy application aims at decreasing the number of interim forces while increasing the number of nodes and bars in compression or tension. The entire process ends when no interim force exists anymore, which is always achievable thanks to the permanence of the static equilibrium condition. From a designer perspective, the approach opens up the generative design black box by providing geometrical and topological control and partial automation of the generation process, while not resorting to common topology patterns – e.g. triangulated bar networks. This paper describes the conceptualization and its implementation into a computational framework, named Policy-based Exploration of Equilibrium Representations (PEER). It illustrates the potential of the approach to unveil unprecedented, unexpected, but statically-valid, structural topologies. Opportunities for further development are eventually discussed.}
}
@incollection{VHORA2024709,
title = {Investigating Fluid Flow Dynamics in Triply Periodic Minimal Surfaces (TPMS) Structures Using CFD Simulation},
editor = {Flavio Manenti and Gintaras V. Reklaitis},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {53},
pages = {709-714},
year = {2024},
booktitle = {34th European Symposium on Computer Aided Process Engineering / 15th International Symposium on Process Systems Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-443-28824-1.50119-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780443288241501198},
author = {Kasimhussen Vhora and Tanya Neeraj and Dominique Thévenin and Gábor Janiga and Kai Sundmacher},
keywords = {Computational Fluid Dynamic, TPMS Structure, Pressure Drop, LBM},
abstract = {Efficient absorption processes require optimized packed bed column structures, which affect gas-liquid contact, flow distribution, and pressure drop. An optimal setup ensures efficient mass transfer with high surface area while keeping down the pressure drop, which leads to energy savings and better absorption. TPMS structures such as the Gyroid, Schwarz-P, and Schwarz-D were investigated in this study, with a focus on balancing porosity and surface area to achieve reduced pressure drops and optimal phase contact. Single-phase flow simulations were conducted using the commercial software STAR- CCM+, compared to the lattice Boltzmann method (LBM) to provide an alternative perspective on fluid dynamics. Validation, analysis of the results and identification of possible improvements were achieved through these comparisons. According to the results, the Schwarz-D structure with 70% porosity and 2 mm unit cell leads to the best performance, exhibiting a pressure drop of 655 Pa m-1 and a specific surface area of 1776 m2 m-3 when analysed with STAR-CCM+. The predicted pressure drop was successfully confirmed using LBM simulations, adding robustness to the findings.}
}
@article{VARUGHESE2023684,
title = {The intersection of space and sustainability: The need for a transdisciplinary and bi-cultural approach},
journal = {Acta Astronautica},
volume = {211},
pages = {684-701},
year = {2023},
issn = {0094-5765},
doi = {https://doi.org/10.1016/j.actaastro.2023.07.009},
url = {https://www.sciencedirect.com/science/article/pii/S0094576523003600},
author = {Carolle Varughese and Lena Henry and Adam Morris and Sarah Bickerton and Nicholas Rattenbury and Cody Mankelow and Alice Gorman and Stevie Katavich-Barton and Priyanka Dhopade},
keywords = {Max of six for acta astronautica, Space sustainability, Transdisciplinary, Indigenous sustainability, Terrestrial sustainability, Space debris},
abstract = {Aotearoa New Zealand's emerging New Space economy provides an opportunity for key actors to focus on space and sustainability issues beyond space debris. The conflict between competing definitions and paradigms of sustainability highlights the importance of diverse values, assumptions, and drivers of change that shape the normative understanding of space sustainability issues. This paper recognises that Indigenous knowledges and practices are in parallel with systems-thinking and transdisciplinary approaches to space and sustainability. The aim of this paper is to describe how current actions can have long term impacts on using and accessing space commercially, scientifically, and culturally.}
}
@article{JI2023106379,
title = {Scalable incomplete multi-view clustering via tensor Schatten p-norm and tensorized bipartite graph},
journal = {Engineering Applications of Artificial Intelligence},
volume = {123},
pages = {106379},
year = {2023},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2023.106379},
url = {https://www.sciencedirect.com/science/article/pii/S0952197623005638},
author = {Guangyan Ji and Gui-Fu Lu and Bing Cai},
keywords = {Scalable incomplete multi-view clustering, Tensorized bipartite graph, Graph completion, Tensor low-rank constraint},
abstract = {Graph-based incomplete multi-view clustering (IMVC) methods have drawn considerable attention due to their good performance in exploring the nonlinear structure of data. However, they still have the following shortcomings. First, graph construction and eigen decomposition of the Laplacian matrix included in the IMVC methods generally have high computational complexity. Second, most methods do not consider the impact of missing views and neglect the potential relationships between different views. Third, few algorithms consider both intra-view and inter-view information for clustering. Therefore, we innovatively propose a scalable incomplete multi-view clustering via the tensor Schatten p-norm and tensorized bipartite graph (SIMVC/TSTBG) method, which combines tensorized bipartite graph, graph completion, and tensor low-rank constraint into a joint framework. Concretely, we first construct bipartite graphs based on the selected m anchor points and the n data points, reducing the size of the graph from n×n to n×m(m<<n), which considerably reduces the computational complexity. Second, we adaptively complete the missing bipartite graph, which reduces the effect of missing view information on the clustering results. Third, to explore connections between missing views and mine high-order information between views, we splice the bipartite graphs into a tensor and impose a tensor low-rank constraint, i.e., the tensor Schatten p-norm, on it. At the same time, we also design an efficient algorithm to solve SIMVC/TSTBG. To our knowledge, we are the first successful practice to integrate the tensor technique with the scalable IMVC method. Compared with other IMVC methods, the results on seven datasets fully show the high efficiency and effectiveness of SIMVC/TSTBG.}
}
@article{QIN2025102994,
title = {A comprehensive taxonomy of machine consciousness},
journal = {Information Fusion},
volume = {119},
pages = {102994},
year = {2025},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2025.102994},
url = {https://www.sciencedirect.com/science/article/pii/S1566253525000673},
author = {Ruilin Qin and Changle Zhou and Mengjie He},
keywords = {Consciousness, Machine consciousness, Artificial consciousness, Conscious robot, Qualia, Large language model},
abstract = {Machine consciousness (MC) is the ultimate challenge to artificial intelligence. Although great progress has been made in artificial intelligence and robotics, consciousness is still an enigma and machines are far from having it. To clarify the concepts of consciousness and the research directions of machine consciousness, in this review, a comprehensive taxonomy for machine consciousness is proposed, categorizing it into seven types: MC-Perception, MC-Cognition, MC-Behavior, MC-Mechanism, MC-Self, MC-Qualia and MC-Test, where the first six types aim to achieve a certain kind of conscious ability, and the last type aims to provide evaluation methods and criteria for machine consciousness. For each type, the specific research contents and future developments are discussed in detail. Especially, the machine implementations of three influential consciousness theories, i.e. global workspace theory, integrated information theory and higher-order theory, are elaborated in depth. Moreover, the challenges and outlook of machine consciousness are analyzed in detail from both theoretical and technical perspectives, with emphasis on new methods and technologies that have the potential to realize machine consciousness, such as brain-inspired computing, quantum computing and hybrid intelligence. The ethical implications of machine consciousness are also discussed. Finally, a comprehensive implementation framework of machine consciousness is provided, integrating five suggested research perspectives: consciousness theories, computational methods, cognitive architectures, experimental systems, and test platforms, paving the way for the future developments of machine consciousness.}
}
@incollection{AI2019853,
title = {Study on the formation of chemical wave patterns for the Belousov–Zhabotinsky reaction system},
editor = {Anton A. Kiss and Edwin Zondervan and Richard Lakerveld and Leyla Özkan},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {46},
pages = {853-858},
year = {2019},
booktitle = {29th European Symposium on Computer Aided Process Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-12-818634-3.50143-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128186343501430},
author = {Jiali Ai and Wei Sun and Chi Zhai},
keywords = {far-from thermodynamic equilibrium, instabilities, reaction-diffusion system, Hopf bifurcation},
abstract = {The Belousov–Zhabotinsky (BZ) reaction system is famous because it can generate self-organized patterns, also known as “chemical waves”. Pattern formation out of an initially homogeneous system is seemingly violating the 2nd-law of thermodynamics (order is produced out of disorder), while in fact, the BZ reaction is an open, far-from thermodynamic equilibrium system, where instability is the cause of morphogenesis and Hopf bifurcation of the reaction kinetics can generate self-oscillatory state trajectories. In this paper, the evolution of the BZ reaction in a two dimensional diffusion system is studied by the numerical computation methods, for the purpose of reconstructing the chemical wave patterns. The similarity of the chemical waves to many complex systems in biology, ecology and engineering makes current study potentially significant. With the study of the pattern formation, we hope provide some thoughts on complex system theory, thermodynamics of the self-oscillatory reaction system, and numerical computation methods on complex patterns, etc.}
}
@article{GERSTENBERG2024924,
title = {Counterfactual simulation in causal cognition},
journal = {Trends in Cognitive Sciences},
volume = {28},
number = {10},
pages = {924-936},
year = {2024},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2024.04.012},
url = {https://www.sciencedirect.com/science/article/pii/S1364661324001074},
author = {Tobias Gerstenberg},
keywords = {counterfactuals, causality, mental simulation, intuitive physics, theory of mind},
abstract = {How do people make causal judgments and assign responsibility? In this review article, I argue that counterfactual simulations are key. To simulate counterfactuals, we need three ingredients: a generative mental model of the world, the ability to perform interventions on that model, and the capacity to simulate the consequences of these interventions. The counterfactual simulation model (CSM) uses these ingredients to capture people’s intuitive understanding of the physical and social world. In the physical domain, the CSM predicts people’s causal judgments about dynamic collision events, complex situations that involve multiple causes, omissions as causes, and causes that sustain physical stability. In the social domain, the CSM predicts responsibility judgments in helping and hindering scenarios.}
}
@article{WOLTHUSEN2018178,
title = {Correlation Between Levels of Delusional Beliefs and Perfusion of the Hippocampus and an Associated Network in a Non–Help-Seeking Population},
journal = {Biological Psychiatry: Cognitive Neuroscience and Neuroimaging},
volume = {3},
number = {2},
pages = {178-186},
year = {2018},
issn = {2451-9022},
doi = {https://doi.org/10.1016/j.bpsc.2017.06.007},
url = {https://www.sciencedirect.com/science/article/pii/S2451902217301180},
author = {Rick P.F. Wolthusen and Garth Coombs and Emily A. Boeke and Stefan Ehrlich and Stephanie N. DeCross and Shahin Nasr and Daphne J. Holt},
keywords = {Arterial spin labeling, Delusion, fMRI, Hippocampus, Perfusion, Psychosis},
abstract = {Background
Delusions are a defining and common symptom of psychotic disorders. Recent evidence suggests that subclinical and clinical delusions may represent distinct stages on a phenomenological and biological continuum. However, few studies have tested whether subclinical psychotic experiences are associated with neural changes that are similar to those observed in clinical psychosis. For example, it is unclear if overactivity of the hippocampus, a replicated finding of neuroimaging studies of schizophrenia, is also present in individuals with subclinical psychotic symptoms.
Methods
To investigate this question, structural and pulsed arterial spin labeling scans were collected in 77 adult participants with no psychiatric history. An anatomical region of interest approach was used to extract resting perfusion of the hippocampus, and 15 other regions, from each individual. A self-report measure of delusional ideation was collected on the day of scanning.
Results
The level of delusional thinking (number of beliefs [r = .27, p = .02]), as well as the associated level of distress (r = .29, p = .02), was significantly correlated with hippocampal perfusion (averaged over right and left hemispheres). The correlations remained significant after controlling for age, hippocampal volume, symptoms of depression and anxiety, and image signal-to-noise ratio, and they were confirmed in a voxelwise regression analysis. The same association was observed in the thalamus and parahippocampal, lateral temporal, and cingulate cortices.
Conclusions
Similar to patients with schizophrenia, non–help-seeking individuals show elevated perfusion of a network of limbic regions in association with delusional beliefs.}
}
@article{PAVLOVA2025103832,
title = {A developmental perspective on mind wandering and its relation to goal-directed thought},
journal = {Consciousness and Cognition},
volume = {129},
pages = {103832},
year = {2025},
issn = {1053-8100},
doi = {https://doi.org/10.1016/j.concog.2025.103832},
url = {https://www.sciencedirect.com/science/article/pii/S105381002500025X},
author = {Maria K. Pavlova},
keywords = {Attention development, Direct memory retrieval, Executive control development, Motivated attention, Spontaneous thought, Stimulus-independent thought, Task-unrelated thought},
abstract = {Mind wandering (i.e., thoughts drifting from one topic to another, with no immediate connection to the perceptual field or the ongoing task) is a widespread cognitive phenomenon. There has been increasing research interest in mind wandering in children and adolescents. However, the developmental origins of this phenomenon remain largely unknown. In the present article, I summarize the purported cognitive mechanisms of mind wandering in adults and review the empirical findings on mind wandering and automatic memory retrieval in children and adolescents. I propose a comprehensive account of the emergence of mind wandering in early and middle childhood, covering the development of its central components identified in the adult literature: motivational and emotional processes, episodic and semantic processes, perceptual decoupling, and meta-awareness. Paying special attention to the roles of developing motivation and executive control, I then address the relationship between mind wandering and goal-directed thought in children.}
}
@article{SALAHSHOORI2024123888,
title = {Simulation-based approaches for drug delivery systems: Navigating advancements, opportunities, and challenges},
journal = {Journal of Molecular Liquids},
volume = {395},
pages = {123888},
year = {2024},
issn = {0167-7322},
doi = {https://doi.org/10.1016/j.molliq.2023.123888},
url = {https://www.sciencedirect.com/science/article/pii/S0167732223026958},
author = {Iman Salahshoori and Mahdi Golriz and Marcos A.L. Nobre and Shahla Mahdavi and Rahime {Eshaghi Malekshah} and Afsaneh Javdani-Mallak and Majid {Namayandeh Jorabchi} and Hossein {Ali Khonakdar} and Qilin Wang and Amir H. Mohammadi and Seyedeh {Masoomeh Sadat Mirnezami} and Farshad Kargaran},
keywords = {Computational fluid dynamics, Drug delivery systems, Molecular simulations, Optimization, Molecular dynamics simulation, Monte Carlo simulation},
abstract = {Efficient drug delivery systems (DDSs) play a pivotal role in ensuring pharmaceuticals’ targeted and effective administration. However, the intricate interplay between drug formulations and delivery systems poses challenges in their design and optimization. Simulations have emerged as indispensable tools for comprehending these interactions and enhancing DDSs performance to address this complexity. This comprehensive review explores the latest advancements in simulation techniques for DDSs and provides a detailed analysis. The review encompasses various simulation methodologies, including molecular dynamics (MD), Monte Carlo (MC), finite element analysis (FEA), computational fluid dynamics (CFD), density functional theory (DFT), machine learning (ML), and dissipative particle dynamics (DPD). These techniques are critically examined in the context of drug delivery research. The article presents illustrative case studies involving liposomal, polymer-based, nano-particulate, and implantable DDSs, demonstrating the influential role of simulations in optimizing these systems. Furthermore, the review addresses the advantages and limitations of simulations in drug delivery research. It also identifies future directions for research and development, such as integrating multiple simulation techniques, refining and validating models for greater accuracy, overcoming computational limitations, and exploring applications of simulations in personalized medicine and innovative DDSs. Simulations employing various techniques like MD, MC, FEA, CFD, DFT, ML, and DPD offer crucial insights into drug behaviour, aiding in DDS design and optimization. Despite their advantages, including rapid and cost-effective screening, simulations require validation and addressing computational limitations. Future research should focus on integrating techniques, refining models, and exploring personalized medicine applications to enhance drug delivery outcomes. This paper underscores the indispensable contribution of simulations to drug research and development, emphasizing their role in providing valuable insights into drug behaviour, facilitating the development and optimization of DDSs, and ultimately enhancing patient outcomes. As we continue to explore and enhance simulation techniques, their impact on advancing drug discovery and improving DDSs is expected to be profound.}
}
@article{LEVENSPIEL20024691,
title = {Modeling in chemical engineering},
journal = {Chemical Engineering Science},
volume = {57},
number = {22},
pages = {4691-4696},
year = {2002},
note = {Festschrift in Honour of Dr Winn van Swaaij},
issn = {0009-2509},
doi = {https://doi.org/10.1016/S0009-2509(02)00280-4},
url = {https://www.sciencedirect.com/science/article/pii/S0009250902002804},
author = {Octave Levenspiel},
abstract = {In its 90 year life what has chemical engineering (ChE) contributed to society? Firstly, we have invented and developed processes to create new materials, more gently and more efficiently, so as to make life easier for all. Secondly, ChE has changed our accepted concepts and our ways of thinking in science and technology. Here modeling stands out as the primary development. Let us consider this.}
}
@article{CAVEDON201514,
title = {“C׳Mon dude!”: Users adapt their behaviour to a robotic agent with an attention model},
journal = {International Journal of Human-Computer Studies},
volume = {80},
pages = {14-23},
year = {2015},
issn = {1071-5819},
doi = {https://doi.org/10.1016/j.ijhcs.2015.02.012},
url = {https://www.sciencedirect.com/science/article/pii/S1071581915000452},
author = {Lawrence Cavedon and Christian Kroos and Damith Herath and Denis Burnham and Laura Bishop and Yvonne Leung and Catherine J. Stevens},
keywords = {Human–robot interaction, Attention model, Social interaction, Evaluation, Engagement},
abstract = {Social cues facilitate engagement between interaction participants, whether they be two (or more) humans or a human and an artificial agent such as a robot. Previous work specific to human–agent/robot interaction has demonstrated the efficacy of implemented social behaviours, such as eye-gaze or facial gestures, for demonstrating the illusion of engagement and positively impacting interaction with a human. We describe the implementation of THAMBS, The Thinking Head Attention Model and Behavioural System, which is used to model attention controlling how a virtual agent reacts to external audio and visual stimuli within the context of an interaction with a human user. We evaluate the efficacy of THAMBS for a virtual agent mounted on a robotic platform in a controlled experimental setting, and collect both task- and behavioural-performance variables, along with self-reported ratings of engagement. Our results show that human subjects noticeably engaged more often, and in more interesting ways, with the robotic agent when THAMBS was activated, indicating that even a rudimentary display of attention by the robot elicits significantly increased attention by the human. Back-channelling had less of an effect on user behaviour. THAMBS and back-channelling did not interact and neither had an effect on self-report ratings. Our results concerning THAMBS hold implications for the design of successful human–robot interactive behaviours.}
}
@article{NIEMYSKA2024168455,
title = {Discovery of a trefoil knot in the RydC RNA: Challenging previous notions of RNA topology},
journal = {Journal of Molecular Biology},
volume = {436},
number = {6},
pages = {168455},
year = {2024},
issn = {0022-2836},
doi = {https://doi.org/10.1016/j.jmb.2024.168455},
url = {https://www.sciencedirect.com/science/article/pii/S0022283624000214},
author = {Wanda Niemyska and Sunandan Mukherjee and Bartosz A. Gren and Szymon Niewieczerzal and Janusz M. Bujnicki and Joanna I. Sulkowska},
keywords = {Entanglement of biomolecules, Topology in soft matter, RNA 3D structure, RNA folding, Molecular dynamics},
abstract = {Knots are very common in polymers, including DNA and protein molecules. Yet, no genuine knot has been identified in natural RNA molecules to date. Upon re-examining experimentally determined RNA 3D structures, we discovered a trefoil knot 31, the most basic non-trivial knot, in the RydC RNA. This knotted RNA is a member of a small family of short bacterial RNAs, whose secondary structure is characterized by an H-type pseudoknot. Molecular dynamics simulations suggest a folding pathway of the RydC RNA that starts with a native twisted loop. Based on sequence analyses and computational RNA 3D structure predictions, we postulate that this trefoil knot is a conserved feature of all RydC-related RNAs. The first discovery of a knot in a natural RNA molecule introduces a novel perspective on RNA 3D structure formation and on fundamental research on the relationship between function and spatial structure of biopolymers.}
}
@article{VUNJAKNOVAKOVIC20214597,
title = {Organs-on-a-chip models for biological research},
journal = {Cell},
volume = {184},
number = {18},
pages = {4597-4611},
year = {2021},
issn = {0092-8674},
doi = {https://doi.org/10.1016/j.cell.2021.08.005},
url = {https://www.sciencedirect.com/science/article/pii/S0092867421009478},
author = {Gordana Vunjak-Novakovic and Kacey Ronaldson-Bouchard and Milica Radisic},
abstract = {Summary
We explore the utility of bioengineered human tissues—individually or connected into physiological units—for biological research. While much smaller and simpler than their native counterparts, these tissues are complex enough to approximate distinct tissue phenotypes: molecular, structural, and functional. Unlike organoids, which form spontaneously and recapitulate development, “organs-on-a-chip” are engineered to display some specific functions of whole organs. Looking back, we discuss the key developments of this emerging technology. Thinking forward, we focus on the challenges faced to fully establish, validate, and utilize the fidelity of these models for biological research.}
}
@article{KARIMISANI2025102651,
title = {Drug repositioning for Parkinson’s disease: An emphasis on artificial intelligence approaches},
journal = {Ageing Research Reviews},
volume = {104},
pages = {102651},
year = {2025},
issn = {1568-1637},
doi = {https://doi.org/10.1016/j.arr.2024.102651},
url = {https://www.sciencedirect.com/science/article/pii/S1568163724004690},
author = {Iman Karimi-Sani and Mehrdad Sharifi and Nahid Abolpour and Mehrzad Lotfi and Amir Atapour and Mohammad-Ali Takhshid and Amirhossein Sahebkar},
keywords = {Neurodegenerative diseases, Parkinson’s disease, Levodopa induced dyskinesia, Drug repositioning, Artificial intelligence, Machine learning, Deep learning},
abstract = {Parkinson’s disease (PD) is one of the most incapacitating neurodegenerative diseases (NDDs). PD is the second most common NDD worldwide which affects approximately 1–2 percent of people over 65 years. It is an attractive pursuit for artificial intelligence (AI) to contribute to and evolve PD treatments through drug repositioning by repurposing existing drugs, shelved drugs, or even candidates that do not meet the criteria for clinical trials. A search was conducted in three databases Web of Science, Scopus, and PubMed. We reviewed the data related to the last years (1975-present) to identify those drugs currently being proposed for repositioning in PD. Moreover, we reviewed the present status of the computational approach, including AI/Machine Learning (AI/ML)-powered pharmaceutical discovery efforts and their implementation in PD treatment. It was found that the number of drug repositioning studies for PD has increased recently. Repositioning of drugs in PD is taking off, and scientific communities are increasingly interested in communicating its results and finding effective treatment alternatives for PD. A better chance of success in PD drug discovery has been made possible due to AI/ML algorithm advancements. In addition to the experimentation stage of drug discovery, it is also important to leverage AI in the planning stage of clinical trials to make them more effective. New AI-based models or solutions that increase the success rate of drug development are greatly needed.}
}
@article{CHATANAKA2022100007,
title = {Immunoinformatics: Pushing the boundaries of immunology research and medicine},
journal = {ImmunoInformatics},
volume = {5},
pages = {100007},
year = {2022},
issn = {2667-1190},
doi = {https://doi.org/10.1016/j.immuno.2021.100007},
url = {https://www.sciencedirect.com/science/article/pii/S2667119021000070},
author = {Miyo K. Chatanaka and Antigona Ulndreaj and Dorsa Sohaei and Ioannis Prassas},
keywords = {Immunoinformatics, Immunology, Informatics, Perspective},
abstract = {Immunology has come a long way, from its early religious beginnings thousands of years ago, to the explosion of immunological data in the 21st century. Thanks to discoveries in immunology, our world has seen tremendous progress in how we understand and treat disease. However, a lot of unmet clinical needs remain, which require focused, real-time collaboration at the clinical and scientific research forefronts. Moreover, the current exponential growth in the generation of research data makes it impossible to handle, analyze, visualize, and interpret such data without the use of advanced computational tools. We think immunoinformatics- a discipline at the intersection of immunology and computer science- will greatly increase efficiency in research productivity and disease treatment. This perspective paper aims to emphasize the role of immunoinformatics toward pushing the boundaries of immunology research. It will also illustrate its clinical applications, including disease prevention, diagnosis, prognosis, treatment, monitoring, as well as in drug discovery. We believe informatics approaches will be implemented increasingly more frequently in research. Thus, here we also discuss a set of fundamental prerequisites to facilitate the efficient and ethical integration of informatics in research and ensure immunological advancements provide maximum benefits to society.}
}
@article{MAKINDE2020368,
title = {An approach to estimate the back order penalty cost of a manufacturing company},
journal = {Procedia Manufacturing},
volume = {43},
pages = {368-374},
year = {2020},
note = {Sustainable Manufacturing - Hand in Hand to Sustainability on Globe: Proceedings of the 17th Global Conference on Sustainable Manufacturing},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.02.175},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920307551},
author = {Olasumbo Makinde and Thomas Munyai},
keywords = {Backorder cost, Economic Order Quantity, Customer disappointment index, Inventory Models},
abstract = {The classical inventory models solely rely on accurate estimate of the back order cost, with a view to establish economic order quantity (EOQ) that must be placed by a customer. Recognizing and quantifying the adverse effects of loss of customer goodwill owing to the inability of a raw material or product supplier organisation to meet customer demands should not only focus on direct penalty cost computation, but should also incorporate change in customers’ future demand owing to this backordering phenomenon. A lot of classical and mathematical approaches focused on the computation of the back order penalty cost coefficient; which gives an organisation a clue of the customer disappointment index, and not the estimated back order cost required for EOQ computation. In light of this, this paper proposes an approach that could be utilized to accurately compute the back order penalty cost of an organisation. The approach considers: (i) the number of times backordering phenomenon have occurred in an organisation, (ii) the decision a customer takes when backordering occur once or couple of times during the ordering phases of an organisation and (iii) myriads of penalties that a customer bestow on a raw material or product supplier organisation for backordering its order, to establish the backorder cost of this organisation. The approach proposed in this study serve as a useful information to suppliers in ascertaining the raw material backorder cost based on their customer responses to backordering, with a view to ensure sustainable raw material supply.}
}
@article{HICKENDORFF2020101311,
title = {Fourth graders’ adaptive strategy use in solving multidigit subtraction problems},
journal = {Learning and Instruction},
volume = {67},
pages = {101311},
year = {2020},
issn = {0959-4752},
doi = {https://doi.org/10.1016/j.learninstruc.2020.101311},
url = {https://www.sciencedirect.com/science/article/pii/S0959475219307327},
author = {Marian Hickendorff},
keywords = {Mathematics education, Multidigit subtraction, Strategy flexibility, Strategy adaptivity, Choice/no-choice method},
abstract = {Using the choice/no-choice methodology we investigated Dutch fourth graders’ (N = 124) adaptive use of the indirect addition strategy to solve subtraction problems. Children solved multidigit subtraction problems in one choice condition, in which they were free to choose between direct subtraction and indirect addition, and in two no-choice conditions, in which they had to use either direct subtraction or indirect addition. Furthermore, children were randomly assigned to mental computation, written computation, or free choice between mental and written computation. One third of the children adaptively switched their strategy according to the number characteristics of the problems, whereas the remaining children consistently used the same strategy. The likelihood to adaptively switch strategies decreased when written computation was allowed or required, compared to mandatory mental computation. On average, children were adaptive to their own speed differences but not to the accuracy differences between the strategies.}
}
@article{KULAKOVA2024103762,
title = {Comparing third-party responsibility with intention attribution: An fMRI investigation of moral judgment},
journal = {Consciousness and Cognition},
volume = {125},
pages = {103762},
year = {2024},
issn = {1053-8100},
doi = {https://doi.org/10.1016/j.concog.2024.103762},
url = {https://www.sciencedirect.com/science/article/pii/S1053810024001296},
author = {Eugenia Kulakova and Sofia Bonicalzi and Adrian L. Williams and Patrick Haggard},
keywords = {Moral responsibility, Intention, Social cognition, Causality},
abstract = {Neuroimaging studies demonstrate that moral responsibility judgments activate the social cognition network, presumably reflecting mentalising processes. Conceptually, establishing an agent’s intention is a sub-process of responsibility judgment. However, the relationship between both processes on a neural level is poorly understood. To date, neural correlates of responsibility and intention judgments have not been compared directly. The present fMRI study compares neural activation elicited by third-party judgments of responsibility and intention in response to animated pictorial stimuli showing harm events. Our results show that the social cognition network, in particular Angular Gyrus (AG) and right Temporo-Parietal Junction (RTPJ), showed stronger activation during responsibility vs. intention evaluation. No greater activations for the reverse contrast were observed. Our imaging results are consistent with conceptualisations of intention attribution as a sub-process of responsibility judgment. However, they question whether the activation of the social cognition network, particularly AG/RTPJ, during responsibility judgment is limited to intention evaluation.}
}
@article{LEON2009539,
title = {The future of computer-aided innovation},
journal = {Computers in Industry},
volume = {60},
number = {8},
pages = {539-550},
year = {2009},
note = {Computer Aided Innovation},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2009.05.010},
url = {https://www.sciencedirect.com/science/article/pii/S0166361509001286},
author = {Noel Leon},
keywords = {Computer Aided Innovation, TRIZ, QFD},
abstract = {A new category of tools known as CAI (computer-aided innovation) is an emerging domain in the array of computer-aided technologies. CAI has been growing as a response to greater industry demands for reliability in new products. Some initial CAI ideas and concepts focused on assisting product designers in the early stage of the design process, but now a more comprehensive vision conceives CAI systems as beginning at the fuzzy front end of perceiving business opportunities and customer demands, then continuing during the creative stage in developing inventions and, further on, providing help up to the point of turning inventions into successful innovations in the marketplace. CAI methods and tools are partially inspired by Innovation Theories, such as TRIZ, QFD (Quality Function Development), Axiomatic Design, Synectics, General Theory of Innovation, Mind Mapping, Brain Storming, Lateral Thinking, and Kansei Engineering, among others. The goal of these new CAI tools under development is to assist innovators, inventors, designers, process developers and managers in their creative performance, with the expectation of changes in paradigms through the use of this new category of software tools. CAI, therefore, stands out as a departure from the usual trends. The latest approaches are presented and analyzed to derive conclusions regarding the present status and the future of these emerging tools.}
}
@article{TOWERS201025,
title = {An ecological reading of mathematical language in a Grade 3 classroom: A case of learning and teaching measurement estimation},
journal = {The Journal of Mathematical Behavior},
volume = {29},
number = {1},
pages = {25-40},
year = {2010},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2009.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S073231230900056X},
author = {Jo Towers and Kim Hunter},
keywords = {Classroom mathematical language, Ecological thinking, Measurement, Estimation},
abstract = {In our work in teacher education and professional development, we aim to help teachers to learn to participate in, and create, classroom ecologies that support students’ learning. In this article we focus on the challenges of developing a classroom ecology that provides mathematical sustenance for students. We pay particular attention to the ways in which classroom language can impede the development of a classroom ecology—one where all students are heard and where knowing is understood as participatory. We present recommendations for teaching practice drawn from an ecological reading of the classroom discourse during a series of lessons on measurement in a Grade 3 classroom.}
}
@article{CARPENTER2020100064,
title = {Bridging Domain and Data},
journal = {Patterns},
volume = {1},
number = {4},
pages = {100064},
year = {2020},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2020.100064},
url = {https://www.sciencedirect.com/science/article/pii/S2666389920300842},
author = {Anne E. Carpenter},
abstract = {Dr. Anne Carpenter addresses her career path from cell biology toward computation. Why would a researcher move outside their comfort zone into a different field, from a domain into data science? What is the best way to bridge domain and data? What is challenging about moving from domain toward data? What is amazing about bridging domain and data?}
}
@article{KOWALCZUK2020103562,
title = {Interpretation and modeling of emotions in the management of autonomous robots using a control paradigm based on a scheduling variable},
journal = {Engineering Applications of Artificial Intelligence},
volume = {91},
pages = {103562},
year = {2020},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2020.103562},
url = {https://www.sciencedirect.com/science/article/pii/S0952197620300518},
author = {Zdzisław Kowalczuk and Michał Czubenko and Tomasz Merta},
keywords = {Emotions, Decision-making systems, Cognitive modeling, Human mind, Computational models, Fuzzy approach, Intelligent systems, Autonomous agents},
abstract = {The paper presents a technical introduction to psychological theories of emotions. It highlights a usable idea implemented in a number of recently developed computational systems of emotions, and the hypothesis that emotion can play the role of a scheduling variable in controlling autonomous robots. In the main part of this study, we outline our own computational system of emotion – xEmotion – designed as a key structural element in the developed target device, being an Intelligent System of Decision-making (ISD) for autonomous and robotic units. The ISD system has a cognitive architecture based on the principles of human psychology. The main purpose of building such a system is to prepare a framework for autonomous units used in system engineering (Kowalczuk and Czubenko, 2011; Czubenko et al., 2015). In particular, ISD is based on the concepts of cognitive psychology (in information processing) and motivation theory, which includes the system of needs (for decision-making). The xEmotion subsystem, however, focuses on modeling an alternative approach based on emotion. The xEmotion implementation covers aspects of somatic, appraisal and evolutionary theories of emotions using fuzzy sets. In this article, we also illustrate the core emotional behavior of the ISD system using simulation. The first application is a user interface for identifying emotions and predicting human behavior. The second is an eSailor simulation, which illustrates the possible behavior of the xEmotion subsystem. The last is an xDriver simulation experiment, which is to prove the validity of the concept of using emotion-based systems, according to the SVC principle. In summary, we also discuss other possible applications of the xEmotion system.}
}
@article{DUGGAN2024100102,
title = {The digital geographies of tact},
journal = {Digital Geography and Society},
volume = {7},
pages = {100102},
year = {2024},
issn = {2666-3783},
doi = {https://doi.org/10.1016/j.diggeo.2024.100102},
url = {https://www.sciencedirect.com/science/article/pii/S2666378324000242},
author = {Mike Duggan},
keywords = {Tact, Tactics, Tactility, Touch, Social behaviour, Judgement, Space, Digital media, Artificial intelligence},
abstract = {This article outlines a research agenda for the spatialities of tact produced by, through and of digital spaces. As a discipline interested in what and who characterises digital space, and in how different relations come to produce space, the article puts forward a proposition for geographers to take tact seriously as an inherently spatial concept useful for theorising the production of space in our digital society. The paper identifies three strands of tact from the literature, 1) tact and social behaviour, 2) tact and touch, 3) tact and judgement, and outlines what they can offer geography in terms of a novel framework for studying digital society. It raises questions of how and why digital spaces and practices produce new trajectories for displays of tact in everyday life, how digital spaces modulate our understanding and experiences of touch, as well as asking whether algorithmic decision making technologies such as Artificial Intelligence have a capacity for tact, and what that means for the geographies these systems shape. The work makes a contribution to the discipline's long standing interests in spatial tactics and socio-spatial behaviour, in touch and sensory geographies, and more recently to algorithmic decision making.}
}
@article{WILKINSON2024168584,
title = {Environmental impacts of earth observation data in the constellation and cloud computing era},
journal = {Science of The Total Environment},
volume = {909},
pages = {168584},
year = {2024},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2023.168584},
url = {https://www.sciencedirect.com/science/article/pii/S0048969723072121},
author = {R. Wilkinson and M.M. Mleczko and R.J.W. Brewin and K.J. Gaston and M. Mueller and J.D. Shutler and X. Yan and K. Anderson},
keywords = {Cloud computing, Satellite, Data centre, Carbon intensity, Environmental impacts},
abstract = {Numbers of Earth Observation (EO) satellites have increased exponentially over the past decade reaching the current population of 1193 (January 2023). Consequently, EO data volumes have mushroomed and data storage and processing have migrated to the cloud. Whilst attention has been given to the launch and in-orbit environmental impacts of satellites, EO data environmental footprints have been overlooked. These issues require urgent attention given data centre water and energy consumption, high carbon emissions for computer component manufacture, and difficulty of recycling computer components. Doing so is essential if the environmental good of EO is to withstand scrutiny. We provide the first assessment of the EO data life-cycle and estimate that the current size of the global EO data collection is ~807 PB, increasing by ~100 PB/year. Storage of this data volume generates annual CO2 equivalent emissions of 4101 t. Major state-funded EO providers use 57 of their own data centres globally, and a further 178 private cloud services, with considerable duplication of datasets across repositories. We explore scenarios for the environmental cost of performing EO functions on the cloud compared to desktop machines. A simple band arithmetic function applied to a Landsat 9 scene using Google Earth Engine (GEE) generated CO2 equivalent (e) emissions of 0.042–0.69 g CO2e (locally) and 0.13–0.45 g CO2e (European data centre; values multiply by nine for Australian data centre). Computation-based emissions scale rapidly for more intense processes and when testing code. When using cloud services such as GEE, users have no choice about the data centre used and we push for EO providers to be more transparent about the location-specific impacts of EO work, and to provide tools for measuring the environmental cost of cloud computation. The EO community as a whole needs to critically consider the broad suite of EO data life-cycle impacts.}
}
@incollection{VOIRONCANICIO202185,
title = {Chapter 4 - Methods and tools in geoprospective},
editor = {Emmanuel Garbolino and Christine Voiron-Canicio},
booktitle = {Ecosystem and Territorial Resilience},
publisher = {Elsevier},
pages = {85-122},
year = {2021},
isbn = {978-0-12-818215-4},
doi = {https://doi.org/10.1016/B978-0-12-818215-4.00004-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128182154000043},
author = {Christine Voiron-Canicio and Emmanuel Garbolino and Giovanni Fusco and Jean-Christophe Loubier},
keywords = {Geoprospective approach, simulations, modeling, uncertainty, land change, decision-making tool, uncertain causal model, geovizualization, graphic modeling, 3D simulation},
abstract = {This chapter illustrates the diversity of methods and tools available for developing a geoprospective approach, and, through them, the variety of ways to introduce the spatial dimension in scenarios, simulations, and collective thinking. A number of methods, such as modeling, are not specific to geoprospective. The perspective adopted is to shine the light on what their use in geoprospective entails: the specific constraints and the new questions raised relating to the weight of past evolutions, to the unforeseen, to uncertainty. In addition to the models of the land use and cover change (LUCC) type and companion modeling, this chapter gives much importance to the following new approaches which are hitherto hardly used in geoprospective: scenarios integrating various territorial scales, modeling of the decision-making process coupled with prospective spatial modeling, geoprospective based on causal probabilistic models, graphic modeling, prospective choremes, immersive and 3D simulation in landscapes of the future.}
}
@article{ZHANG2017427,
title = {Genomic Energy Landscapes},
journal = {Biophysical Journal},
volume = {112},
number = {3},
pages = {427-433},
year = {2017},
issn = {0006-3495},
doi = {https://doi.org/10.1016/j.bpj.2016.08.046},
url = {https://www.sciencedirect.com/science/article/pii/S0006349516307743},
author = {Bin Zhang and Peter G. Wolynes},
abstract = {Energy landscape theory, developed in the context of protein folding, provides, to our knowledge, a new perspective on chromosome architecture. We review what has been learned concerning the topology and structure of both the interphase and mitotic chromosomes from effective energy landscapes constructed using Hi-C data. Energy landscape thinking raises new questions about the nonequilibrium dynamics of the chromosome and gene regulation.}
}
@incollection{EVETT1994115,
title = {Chapter 6 - Providing Computationally Effective Knowledge Representation via Massive Parallelism},
editor = {Laveen N. KANAL and Vipin KUMAR and Hiroaki KITANO and Christian B. SUTTNER},
series = {Machine Intelligence and Pattern Recognition},
publisher = {North-Holland},
volume = {14},
pages = {115-135},
year = {1994},
booktitle = {Parallel Processing for Artificial Intelligence},
issn = {0923-0459},
doi = {https://doi.org/10.1016/B978-0-444-81704-4.50012-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780444817044500120},
author = {Matthew P. Evett and William A. Andersen and James A. Hendler},
abstract = {PARKA is a frame-based knowledge representation system implemented on the Connection Machine. PARKA provides a representation language consisting of concept descriptions (frames) and binary relations on those descriptions (slots). The system is designed explicitly to provide extremely fast property inheritance inference capabilities. In particular, PARKA can perform fast “recognition” queries of the form “find all frames satisfying m property constraints” in O(d + m) time—proportional only to the depth (d) of the knowledge base (KB), and independent of its size. For conjunctive queries of this type, PARKA's performance is measured in tenths of a second, even for KBs with more than 100,000 frames. We show similar results for timings on on large IS-A networks derived from the Cyc commonsense KB, and for queries involving knowledge structure pattern matching in support of case-based planning. With such run-time performance, PARKA is possibly the “fastest knowledge representation system in the world”.}
}
@article{NOSS201263,
title = {The design of a system to support exploratory learning of algebraic generalisation},
journal = {Computers & Education},
volume = {59},
number = {1},
pages = {63-81},
year = {2012},
note = {CAL 2011},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2011.09.021},
url = {https://www.sciencedirect.com/science/article/pii/S0360131511002387},
author = {Richard Noss and Alexandra Poulovassilis and Eirini Geraniou and Sergio Gutierrez-Santos and Celia Hoyles and Ken Kahn and George D. Magoulas and Manolis Mavrikis},
keywords = {Exploratory learning, Algebraic generalisation, Intelligent support, Teacher assistance, Design},
abstract = {This paper charts the design and application of a system to support 11–14 year old students’ learning of algebraic generalisation, presenting students with the means to develop their understanding of the meaning of generality, see its power for mathematics and develop algebraic ways of thinking. We focus squarely on design, while taking account of both technical and pedagogical issues and challenges, and provide an account of how we have designed and built a system with a very close fit to our knowledge of students’ difficulties with the subject matter. We report the challenges involved in building a system that is both intelligent and exploratory, a learning environment in which both student and teacher are supported without explicit tutoring.}
}
@article{CHEN2021107754,
title = {2D multi-area coverage path planning using L-SHADE in simulated ocean survey},
journal = {Applied Soft Computing},
volume = {112},
pages = {107754},
year = {2021},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2021.107754},
url = {https://www.sciencedirect.com/science/article/pii/S156849462100675X},
author = {Guanzhong Chen and Yue Shen and Yixiao Zhang and Wenfeng Zhang and Dianrui Wang and Bo He},
keywords = {L-SHADE, Multi-area, Coverage path planning, Sub-area path planning, Mutation process},
abstract = {Ocean environmental surveys typically involve multi-area coverage path planning tasks. The most important problem is improving the coverage efficiency of the task. A new path planning method based on Successful History-Based Adaptive Differential Evolution variants with Linear population size reduction(L-SHADE) is presented to solve this problem. The method comprises two parts: the part of sub area coverage path planning and the part of finding the optimized sequence of sub area start points. The key idea is establishing the relationship between the starting point of each sub area and the optimized multi-area path. We implement the method through numbering the possible starting point of sub area path and proposing a computing formula. In addition, the results of L-SHADE mutation process are optimized which make L-SHADE possible to apply in multi-area coverage path planning. This method avoids area discretization and exponential growth of computational quantities, and it is suitable for complex areas as well as multi-area. The simulation results with MATLAB showed the improvement of coverage path planning task execution efficiency. Compared with the method thinking of the sub area as the center of it, our method reduced the multi-area coverage path length by 4%–7%. From the simulations and analysis, we concluded that the method is able to improve the efficiency and stability of multi-area coverage path planning.}
}
@article{MILLER2025105387,
title = {Biological mechanisms contradict AI consciousness: The spaces between the notes},
journal = {BioSystems},
volume = {247},
pages = {105387},
year = {2025},
issn = {0303-2647},
doi = {https://doi.org/10.1016/j.biosystems.2024.105387},
url = {https://www.sciencedirect.com/science/article/pii/S0303264724002727},
author = {William B. Miller and František Baluška and Arthur S. Reber and Predrag Slijepčević},
keywords = {Consciousness, Cellular basis of consciousness, Cognition-based evolution, Information field, Artificial intelligence, Senome},
abstract = {The presumption that experiential consciousness requires a nervous system and brain has been central to the debate on the possibility of developing a conscious form of artificial intelligence (AI). The likelihood of future AI consciousness or devising tools to assess its presence has focused on how AI might mimic brain-centered activities. Currently, dual general assumptions prevail: AI consciousness is primarily an issue of functional information density and integration, and no substantive technical barriers exist to prevent its achievement. When the cognitive process that underpins consciousness is stipulated as a cellular attribute, these premises are directly contradicted. The innate characteristics of biological information and how that information is managed by individual cells have no parallels within machine-based AI systems. Any assertion of computer-based AI consciousness represents a fundamental misapprehension of these crucial differences.}
}
@article{GREENLEE20201043,
title = {Kinetic and Thermodynamic Control in Dynamic Covalent Synthesis},
journal = {Trends in Chemistry},
volume = {2},
number = {12},
pages = {1043-1051},
year = {2020},
issn = {2589-5974},
doi = {https://doi.org/10.1016/j.trechm.2020.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S258959742030232X},
author = {Andrew J. Greenlee and Chloe I. Wendell and Morgan M. Cencer and Summer D. Laffoon and Jeffrey S. Moore},
keywords = {dynamic, covalent, reversible, kinetic, thermodynamic},
abstract = {In recent years, dynamic covalent chemistry (DCC) has seen the synthesis of increasingly complex cyclooligomers, polymers, and diverse compound libraries. The reversible formation of covalent bonds characteristic of DCC reactions favors thermodynamic product distributions for simple unitopic reactions; however, kinetic effects are increasingly influential in reactions of multitopic precursors. In this review, we explore the interplay between thermodynamic and kinetic considerations when planning a DCC synthesis. Computational models, typically based on reaction thermodynamics, have aided in predicting DCC reaction outcomes with moderate success. A clear direction for the field is to develop more robust computational tools informed by thermodynamic and kinetic driving forces that can predict product distributions in DCC reactions.}
}
@article{EHRENFELD2019102525,
title = {Online Public Spheres in the Era of Fake News: Implications for the Composition Classroom},
journal = {Computers and Composition},
volume = {54},
pages = {102525},
year = {2019},
issn = {8755-4615},
doi = {https://doi.org/10.1016/j.compcom.2019.102525},
url = {https://www.sciencedirect.com/science/article/pii/S875546151830029X},
author = {Dan Ehrenfeld and Matt Barton},
keywords = {fake news, public sphere, social media, composition, misinformation, disinformation, critical thinking, media literacy},
abstract = {This article revisits Matt Barton's 2005 article "The Future of Rational-Critical Debate in Online Public Spheres" in light of recent debates around misinformation and disinformation, data-driven influence campaigns, the blurring line between social media and news media, and the algorithmic incentivization of “fake news.” While today’s social media platforms exhibit many of the qualities that C.W. Mills and Jürgen Habermas associate with a healthy public sphere—communication between strangers is participatory, immediate, accessible, and decentralized—this article raises questions about the extent to which everyday digital writing and circulation practices align with broader democratic aspirations. The goal of this article is to explore not only what these social and technological developments mean for the health of public discourse, but also how we, as teachers of writing, can meaningfully engage with them in our classrooms. An appendix includes ideas for assignments that engage students in critical reflection about their own participation in today’s online public spheres.}
}
@article{MAVERS2002187,
title = {Interpreting the externalised images of pupils’ conceptions of ICT: methods for the analysis of concept maps},
journal = {Computers & Education},
volume = {38},
number = {1},
pages = {187-207},
year = {2002},
issn = {0360-1315},
doi = {https://doi.org/10.1016/S0360-1315(01)00074-4},
url = {https://www.sciencedirect.com/science/article/pii/S0360131501000744},
author = {Diane Mavers and Bridget Somekh and Jane Restorick},
keywords = {Concept mapping, Representations, Learning, Networked technologies, Phenomenography},
abstract = {The ImpacT2 evaluation is using image based concept mapping as one method of exploring the impact of networked technologies on students' learning. In a pre-test administered in June 2000, students in three cohorts aged 10–11, 13–14 and 15–16, produced around 2000 ‘maps’. Entitled ‘Computers in My World’, these provide a means of students externalising mental representations of networked technologies. Using a phenomenographic approach, the study aims to identify qualitatively different patterns of thinking and trends in the development of pupils' concepts. Five quanititative measures emerged from heuristic analysis of the maps: nodes, links, connectivity, ‘Spheres of Thinking’ and ‘Zones of Use’. Analysis of the pre-test maps was carried out alongside analysis of pre-test questionnaires, using SPSS. The outcomes suggest correlations between pupils' experience and the constent of their maps. Phenomenographic interviewing of selected 11 year old pupils, which entailed handling control over to interviewees through the use of open-ended questions, enabled further exploration of their experiences and understandings of those experiences. A method for in-depth interviewing of young students is described. Data suggest that pupils have sophisticated ‘secondary artifacts’ or mental models of the nature of networked technologies and their role in today's world. This has implications for the way that ICT is used in schools and for its potential as a tool for students' learning.}
}
@incollection{COBB200685,
title = {Constructivism},
editor = {Keith Brown},
booktitle = {Encyclopedia of Language & Linguistics (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {85-87},
year = {2006},
isbn = {978-0-08-044854-1},
doi = {https://doi.org/10.1016/B0-08-044854-2/01593-5},
url = {https://www.sciencedirect.com/science/article/pii/B0080448542015935},
author = {T. Cobb},
keywords = {constructivism, constructivist, educational reform, language technologies, learner-as-linguist, objectivism, second language acquisition},
abstract = {Constructivism, the notion that knowledge must be assembled from pieces rather than assimilated whole, has been a principal learning theory in psychology for about 20 years and in psycholinguistics for 10. The theory is now making a strong entry into educational thinking, but in language education it is less in evidence. That is because applied linguists have always been constructivists, implicitly, and have already confronted some of the implementation problems facing constructivism in mathematics or science education. Nevertheless, a more explicit understanding of the constructivist approach is useful within language education, particularly in providing a framework for exploiting information technologies.}
}
@article{SCAGLIOTTI2025113811,
title = {Normalizing flows as approximations of optimal transport maps via linear-control neural ODEs},
journal = {Nonlinear Analysis},
volume = {257},
pages = {113811},
year = {2025},
issn = {0362-546X},
doi = {https://doi.org/10.1016/j.na.2025.113811},
url = {https://www.sciencedirect.com/science/article/pii/S0362546X25000653},
author = {A. Scagliotti and S. Farinelli},
keywords = {Optimal transport, Optimal control, Γ-convergence, Linear-control neural ODEs},
abstract = {In this paper, we consider the problem of recovering the W2-optimal transport map T between absolutely continuous measures μ,ν∈P(Rn) as the flow of a linear-control neural ODE, where the control depends only on the time variable and takes values in a finite-dimensional space. We first show that, under suitable assumptions on μ,ν and on the controlled vector fields governing the neural ODE, the optimal transport map is contained in the Cc0-closure of the flows generated by the system. Then, we tackle the problem under the assumption that only discrete approximations of μN,νN of the original measures μ,ν are available: we formulate approximated optimal control problems, and we show that their solutions give flows that approximate the original optimal transport map T. In the framework of generative models, the approximating flow constructed here can be seen as a ‘Normalizing Flow’, which usually refers to the task of providing invertible transport maps between probability measures by means of deep neural networks. We propose an iterative numerical scheme based on the Pontryagin Maximum Principle for the resolution of the optimal control problem, resulting in a method for the practical computation of the approximated optimal transport map, and we test it on a two-dimensional example.}
}
@article{FUNG20243519,
title = {Chemical education in digital chemistry},
journal = {Chem},
volume = {10},
number = {12},
pages = {3519-3525},
year = {2024},
issn = {2451-9294},
doi = {https://doi.org/10.1016/j.chempr.2024.10.010},
url = {https://www.sciencedirect.com/science/article/pii/S2451929424005369},
author = {Fun Man Fung and Magdalena Lederbauer and Yvonne S.L. Choo and Timo Gehring and Kevin Maik Jablonka and Kjell Jorner and Philippe Schwaller and Michael B. Sullivan and Andrea Volkamer and Matthew S. Sigman and Kuangbiao Liao and Charles Windle},
abstract = {In this digital age where machine learning has won the Nobel Prizes in both Physics and Chemistry, it is ever more important to give chemistry students an educational advantage that will enable them to use the tools of artificial intelligence and machine learning to enhance both their study experience and their future research. In this Voices article, chemistry education and research experts gather to share their implementation and utilization of these data-driven tools in classes and in labs.}
}
@article{BROOKS2013947,
title = {The Primate Cerebellum Selectively Encodes Unexpected Self-Motion},
journal = {Current Biology},
volume = {23},
number = {11},
pages = {947-955},
year = {2013},
issn = {0960-9822},
doi = {https://doi.org/10.1016/j.cub.2013.04.029},
url = {https://www.sciencedirect.com/science/article/pii/S0960982213004375},
author = {Jessica X. Brooks and Kathleen E. Cullen},
abstract = {Summary
Background
The ability to distinguish sensory signals that register unexpected events (exafference) from those generated by voluntary actions (reafference) during self-motion is essential for accurate perception and behavior. The cerebellum is most commonly considered in relation to its contributions to the fine tuning of motor commands and sensorimotor calibration required for motor learning. During unexpected motion, however, the sensory prediction errors that drive motor learning potentially provide a neural basis for the computation underlying the distinction between reafference and exafference.
Results
Recording from monkeys during voluntary and applied self-motion, we demonstrate that individual cerebellar output neurons encode an explicit and selective representation of unexpected self-motion by means of an elegant computation that cancels the reafferent sensory effects of self-generated movements. During voluntary self-motion, the sensory responses of neurons that robustly encode unexpected movement are canceled. Neurons with vestibular and proprioceptive responses to applied head and body movements are unresponsive when the same motion is self-generated. When sensory reafference and exafference are experienced simultaneously, individual neurons provide a precise estimate of the detailed time course of exafference.
Conclusions
These results provide an explicit solution to the longstanding problem of understanding mechanisms by which the brain anticipates the sensory consequences of our voluntary actions. Specifically, by revealing a striking computation of a sensory prediction error signal that effectively distinguishes between the sensory consequences of self-generated and externally produced actions, our findings overturn the conventional thinking that the sensory errors coded by the cerebellum principally contribute to the fine tuning of motor activity required for motor learning.}
}
@article{BARR1986183,
title = {New Approaches in Water Balance Computations.},
journal = {Journal of Arid Environments},
volume = {11},
number = {2},
pages = {183-184},
year = {1986},
issn = {0140-1963},
doi = {https://doi.org/10.1016/S0140-1963(18)31233-3},
url = {https://www.sciencedirect.com/science/article/pii/S0140196318312333},
author = {D.I.H. Barr}
}
@article{GREEN2017125,
title = {Fluid reasoning predicts future mathematical performance among children and adolescents},
journal = {Journal of Experimental Child Psychology},
volume = {157},
pages = {125-143},
year = {2017},
issn = {0022-0965},
doi = {https://doi.org/10.1016/j.jecp.2016.12.005},
url = {https://www.sciencedirect.com/science/article/pii/S0022096516302995},
author = {Chloe T. Green and Silvia A. Bunge and Victoria {Briones Chiongbian} and Maia Barrow and Emilio Ferrer},
keywords = {Children, Math, Cognitive development, Fluid reasoning, Working memory, Problem solving},
abstract = {The aim of this longitudinal study was to determine whether fluid reasoning (FR) plays a significant role in the acquisition of mathematics skills above and beyond the effects of other cognitive and numerical abilities. Using a longitudinal cohort sequential design, we examined how FR measured at three assessment occasions, spaced approximately 1.5years apart, predicted math outcomes for a group of 69 participants between ages 6 and 21years across all three assessment occasions. We used structural equation modeling (SEM) to examine the direct and indirect relations between children’s previous cognitive abilities and their future math achievement. A model including age, FR, vocabulary, and spatial skills accounted for 90% of the variance in future math achievement. In this model, FR was the only significant predictor of future math achievement; age, vocabulary, and spatial skills were not significant predictors. Thus, FR was the only predictor of future math achievement across a wide age range that spanned primary school and secondary school. These findings build on Cattell’s conceptualization of FR as a scaffold for learning, showing that this domain-general ability supports the acquisition of rudimentary math skills as well as the ability to solve more complex mathematical problems.}
}
@article{NORROS201461,
title = {Developing human factors/ergonomics as a design discipline},
journal = {Applied Ergonomics},
volume = {45},
number = {1},
pages = {61-71},
year = {2014},
note = {Systems Ergonomics/Human Factors},
issn = {0003-6870},
doi = {https://doi.org/10.1016/j.apergo.2013.04.024},
url = {https://www.sciencedirect.com/science/article/pii/S0003687013000975},
author = {Leena Norros},
keywords = {Design thinking, Technology-in-use, Naturalistic approach, Core-task modelling},
abstract = {This paper deals with internal challenges that the human factors/ergonomics (HFE) research faces when wishing to strengthen its contribution to development of work systems. Three established characteristics of high-quality HFE, i.e., HFE takes a systems approach, HFE is design-driven, and HFE focuses on two closely related outcomes, performance and well-being, are taken as a starting point of a methodological discussion, in which conceptual innovations, e.g. adopting the technology-in-use perspective, are proposed to support development of HFE towards the high-quality aims. The feasibility of the proposed conceptual choices is demonstrated by introducing a naturalistic HFE analysis approach including four HFE functions. The gained experience of the use of this approach in a number of complex work domains allows the conclusion that becoming design-driven appears as that most difficult quality target for HFE to reach. Creating an own design discipline identity in a multi-voiced collaboration is the key internal challenge for human factors/ergonomics.}
}
@article{HOLMES201743,
title = {Motor cognition and neuroscience in sport psychology},
journal = {Current Opinion in Psychology},
volume = {16},
pages = {43-47},
year = {2017},
note = {Sport psychology},
issn = {2352-250X},
doi = {https://doi.org/10.1016/j.copsyc.2017.03.009},
url = {https://www.sciencedirect.com/science/article/pii/S2352250X16301646},
author = {Paul S Holmes and David J Wright},
abstract = {Advances in technology have allowed research in cognitive neuroscience to contribute significantly to the discipline of sport psychology. In most cases, the research has become more rigorous and has directed current thinking on the mechanisms subserving a number of psychological theories and models of practice. Currently, the three most common neuroscience techniques informing sport and exercise research are electroencephalography, transcranial magnetic stimulation and functional magnetic resonance imaging. In this review, we highlight and discuss the contributions to sport psychology that have been made in recent years by applying these techniques, with a focus on the development of expertise, motor cognition, motor imagery and action observation.}
}
@article{YE2024116982,
title = {A fast cosine transformation accelerated method for predicting effective thermal conductivity},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {426},
pages = {116982},
year = {2024},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2024.116982},
url = {https://www.sciencedirect.com/science/article/pii/S004578252400238X},
author = {Changqing Ye and Shubin Fu and Eric T. Chung},
keywords = {Effective thermal conductivity, Preconditioner, Fast cosine transformation, CUDA, GPU},
abstract = {Predicting effective thermal conductivity by solving a Partial Differential Equation (PDE) defined on a high-resolution Representative Volume Element (RVE) is a computationally intensive task. In this paper, we tackle the task by proposing an efficient and implementation-friendly computational method that can fully leverage the computing power offered by hardware accelerators, namely, graphical processing units (GPUs). We first employ the Two-Point Flux-Approximation scheme to discretize the PDE and then utilize the preconditioned conjugate gradient method to solve the resulting algebraic linear system. The construction of the preconditioner originates from FFT-based homogenization methods, and an engineered linear programming technique is utilized to determine the homogeneous reference parameters. The fundamental observation presented in this paper is that the preconditioner system can be effectively solved using multiple Fast Cosine Transformations (FCT) and parallel tridiagonal matrix solvers. Regarding the fact that default multiple FCTs are unavailable on the CUDA platform, we detail how to derive FCTs from FFTs with nearly optimal memory usage. Numerical experiments including the stability comparison with standard preconditioners are conducted for 3D RVEs. Our performance reports indicate that the proposed method can achieve a 5-fold acceleration on the GPU platform over the pure CPU platform and solve the problems with 5123 degrees of freedom and reasonable contrast ratios in less than 30 s.}
}
@article{GALLISTEL201287,
title = {On rationalism and optimality: Responses to the Miller and Nevin Commentaries},
journal = {Behavioural Processes},
volume = {90},
number = {1},
pages = {87-88},
year = {2012},
note = {Society for the Quantitative Analyses of Behavior: Extinction},
issn = {0376-6357},
doi = {https://doi.org/10.1016/j.beproc.2012.02.015},
url = {https://www.sciencedirect.com/science/article/pii/S0376635712000514},
author = {C.R. Gallistel},
keywords = {Rationalism, Optimality, Information theory, Contingency, Cue competition, Assignment of credit},
abstract = {Modern materialist rationalism is the doctrine that principles governing behaviorally important aspects of the world have become implicit in the structure of purpose-specific information-processing mechanisms through evolution by natural selection. These principles are mostly, but not entirely mathematical. Because the evolutionary process tends to optimize, the computations performed by these mechanisms tend to approximate the optimal computation. This doctrine does not imply that animals always make rational and/or optimal choices.}
}
@article{BORG202041,
title = {On “the application of science to science itself:” chemistry, instruments, and the scientific labor process},
journal = {Studies in History and Philosophy of Science Part A},
volume = {79},
pages = {41-56},
year = {2020},
issn = {0039-3681},
doi = {https://doi.org/10.1016/j.shpsa.2019.05.008},
url = {https://www.sciencedirect.com/science/article/pii/S0039368118300529},
author = {George Borg},
keywords = {Instrumental revolution, Labor, Scientific revolution, Structure determination, Technology, Progress, Chemistry, Mechanization},
abstract = {The “Instrumental Revolution” in chemistry refers to a transitional period in the mid-20th century during which sophisticated instrumentation based on physical principles was introduced to solve chemical problems. Historical and philosophical reflection on whether the revolution was a scientific one has been dominated by general models of scientific revolution, in particular, those proposed by Thomas Kuhn, I. B. Cohen and Ian Hacking. In this article I propose that the Industrial Revolution is a useful model for understanding the transformation wrought by the increasingly important role of machines in chemical research. Drawing on Marx's analysis of that event, I argue that that the Instrumental Revolution bears a striking resemblance to the industrial one. I offer grounds for thinking that the resemblance is not fortuitous, but rather reflects a general pattern of development involving the mechanization of the labor process. It is suggested that the cognitive consequences of radical changes in the means of production, as exemplified in the Instrumental Revolution, warrant the consideration of whether the latter is an instance of a kind of revolution in science rather than a singular episode.}
}
@incollection{LEEFRANCISS2025237,
title = {Chapter 13 - Generative artificial intelligence in genetics: A comprehensive review},
editor = {Khalid Raza},
booktitle = {Deep Learning in Genetics and Genomics},
publisher = {Academic Press},
pages = {237-247},
year = {2025},
isbn = {978-0-443-27523-4},
doi = {https://doi.org/10.1016/B978-0-443-27523-4.00005-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780443275234000056},
author = {Nicholas {Lee Franciss}},
keywords = {Generative artificial intelligence, Generative pretrained transformers, Large language models, Model architecture, Transformers},
abstract = {Generative artificial intelligence (GenAI) is revolutionizing genetics by applying the computational capabilities of predictive algorithms to unveil the genome's intricate complexities. From protein prediction to gene discovery and motif detection, GenAI techniques are transforming our understanding of genetic processes that were not previously possible. Here we explore how Markov chains, long-standing predecessors of more modern technologies like large language models (LLMs) and generative pretrained transformers (GPTs), have been complemented by these advanced methods, empowering researchers to extract unprecedented levels of information from DNA sequences, including regulatory networks that govern gene expression. We dive deep into how the individual model architectures enable their capability to implicitly understand and generate biological data. The cultural and intellectual implications of DeepMind's AlphaFold on the prediction of three-dimensional protein structures and, with it, its cultural impact on generative approaches in protein design and is also explored.}
}
@article{VAR2025e41447,
title = {A new strategy for constructing alternative consumer confidence indexes to explain household consumption: A fuzzy DEMATEL approach},
journal = {Heliyon},
volume = {11},
number = {2},
pages = {e41447},
year = {2025},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e41447},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024174786},
author = {Özge Var and Alptekin Durmuşoğlu and Türkay Dereli},
keywords = {Consumer confidence index, Consumer surveys, Fuzzy DEMATEL, Household consumption, Lasso regression},
abstract = {Background
Consumer Confidence Index (CCI) is a measure obtained from consumer surveys (CS) that gauges assessments and expectations of the economic environment. Common practice uses 4 of the 12 questions in CCI calculation. However, efforts to find best set of questions continue, such as the European Commission swapping two questions in 2019. Literature studies employ different combinations of questions; however all-alternative combinations take too much time and computational power. The questions also exhibit cause-and-effect relationships as household consumption predictors and are not statistically independent of one another.
Objective
We suggest classifying the CS questions as "Causes" and "Effects." It makes sense that inquiries in the cause group should provide a better explanation of household consumption. If this theory turns out to be correct, a smaller solution space will be able to be used to find the ideal substitute CCI.
Method
A fuzzy DEMATEL (Decision-Making Trial and Evaluation Laboratory), a reliable method to present causal relationships, is used to classification. The prediction power of cause group (in terms of explaining household expenditures) is measured with the Lasso regression (Least Absolute Shrinkage and Selection Operator), which provides more interpretable regression models. This approach was applied to European Union dataset from 2007Q3 to 2021Q2.
Results
The cause group included four CS questions and explained the 75% variability of the consumption expenditures. It is performed comparably to earlier studies that took into account all possible question combinations. The Türkiye case, covering data from 2007 to 2021, supported the finding of EU case, explaining 84% variation in consumption expenditures.
Conclusion
These encouraging results suggest that comparable prediction power can be attained with a significant reduction in effort (in comparison to all brute force). Therefore, this approach would provide shortcut for constructing alternative CCIs to the authorities.}
}
@article{CHANDRASEGARAN2013204,
title = {The evolution, challenges, and future of knowledge representation in product design systems},
journal = {Computer-Aided Design},
volume = {45},
number = {2},
pages = {204-228},
year = {2013},
note = {Solid and Physical Modeling 2012},
issn = {0010-4485},
doi = {https://doi.org/10.1016/j.cad.2012.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S0010448512001741},
author = {Senthil K. Chandrasegaran and Karthik Ramani and Ram D. Sriram and Imré Horváth and Alain Bernard and Ramy F. Harik and Wei Gao},
keywords = {Knowledge representation, Knowledge capture, Knowledge management, Product design, Computational tools, Ontology, Systems engineering, Design rationale, Multidisciplinary modeling, Virtual reality, Collaborative engineering, Simulation},
abstract = {Product design is a highly involved, often ill-defined, complex and iterative process, and the needs and specifications of the required artifact get more refined only as the design process moves toward its goal. An effective computer support tool that helps the designer make better-informed decisions requires efficient knowledge representation schemes. In today’s world, there is a virtual explosion in the amount of raw data available to the designer, and knowledge representation is critical in order to sift through this data and make sense of it. In addition, the need to stay competitive has shrunk product development time through the use of simultaneous and collaborative design processes, which depend on effective transfer of knowledge between teams. Finally, the awareness that decisions made early in the design process have a higher impact in terms of energy, cost, and sustainability, has resulted in the need to project knowledge typically required in the later stages of design to the earlier stages. Research in design rationale systems, product families, systems engineering, and ontology engineering has sought to capture knowledge from earlier product design decisions, from the breakdown of product functions and associated physical features, and from customer requirements and feedback reports. VR (Virtual reality) systems and multidisciplinary modeling have enabled the simulation of scenarios in the manufacture, assembly, and use of the product. This has helped capture vital knowledge from these stages of the product life and use it in design validation and testing. While there have been considerable and significant developments in knowledge capture and representation in product design, it is useful to sometimes review our position in the area, study the evolution of research in product design, and from past and current trends, try and foresee future developments. The goal of this paper is thus to review both our understanding of the field and the support tools that exist for the purpose, and identify the trends and possible directions research can evolve in the future.}
}
@incollection{KUMBALE2021306,
title = {Models for Personalized Medicine},
editor = {Olaf Wolkenhauer},
booktitle = {Systems Medicine},
publisher = {Academic Press},
address = {Oxford},
pages = {306-317},
year = {2021},
isbn = {978-0-12-816078-7},
doi = {https://doi.org/10.1016/B978-0-12-801238-3.11349-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128012383113492},
author = {Carla M. Kumbale and Jacob D. Davis and Eberhard O. Voit},
keywords = {Dynamic model, Health simplex, Health trajectory, Machine learning, Modeling, Networks, Personalized medicine, Precision medicine, Systems biology, Theranostics},
abstract = {The customization of medicine to specific individuals promises clear improvements in disease treatment, but also faces substantial challenges, many of which have their roots in the complexity of the human body. This complexity cannot be grasped with intuition alone and is not appropriately captured by reductionist methods, which have been dominating biology and medicine for the past decades. Experimental and computational systems biology have the potential of generating adequate datasets and analyzing them in a manner that captures the complexity of health and disease systems in a personalized manner. This potential has not yet fully materialized, but examples and case studies provide a glimpse of the power these approaches are likely to have in the future.}
}
@article{BARBIAN2024122160,
title = {Flow and mass transfer prediction in anisotropic TPMS-structures as extracorporeal oxygenator membranes using reduced order modeling},
journal = {Journal of Membrane Science},
volume = {690},
pages = {122160},
year = {2024},
issn = {0376-7388},
doi = {https://doi.org/10.1016/j.memsci.2023.122160},
url = {https://www.sciencedirect.com/science/article/pii/S0376738823008165},
author = {Kai P. Barbian and Lukas T. Hirschwald and John Linkhorst and Michael Neidlin and Ulrich Steinseifer and Matthias Wessling and Bettina Wiegmann and Sebastian V. Jansen},
keywords = {Membrane oxygenator, TPMS, Reduced order modeling, Gas transfer simulation, Anisotropic},
abstract = {Currently, hollow fiber membranes are the standard technology for extracorporeal membrane oxygenators. Apart from the inevitable contact of the circulating blood with the artificial material, a suboptimal flow distribution within the oxygenator favors thrombus formation which leads to a rapid loss of gas exchange capacity. The current advancement in additive manufacturing allows the design of three-dimensional membrane-structures, based on triply periodic minimal surfaces (TPMS). One of their unique advantages is local geometry variation to manipulate the flow distribution. But how this anisotropy influences the overall device performance is non-trivial and requires numerical simulation. The aim of this study was to develop a reduced order model (ROM) that is able to efficiently predict three-dimensional flow distribution and gas transfer inside TPMS-structures. We performed a parametric study using a validated micro scale computational fluid dynamics (CFD) model. Afterwards, two different modeling approaches of Sherwood-correlations and artificial neural networks (NN) were compared to characterize flow and mass transfer from the simulated data. To create the ROM, the NN modeling strategy was then implemented into a porous medium CFD model. The developed ROM was also validated. Finally, an anisotropic TPMS-membrane-structure was compared numerically with an isotropic predicate. The NN fitting strategy showed superior accuracy over the Sherwood-correlations for characterizing mass transfer in TPMS-structures. With the ROM, the gas transfer rates of oxygen and carbon dioxide from the micro CFD model could be predicted within relative root mean squared errors (RRMSE) of 2.7% and 5.1%. The pressure drop in the experiments was predicted with an RRMSE accuracy of 7.6%. In comparison with the isotropic TPMS-structure, the anisotropic structure showed a homogenized flow distribution and increased gas transfer rates of 4% to 5% for oxygen and 2.3% to 7.4% for carbon dioxide at the simulated flow rates.}
}
@article{NNAJI2019106672,
title = {Modelling and management of smart microgrid for rural electrification in sub-saharan Africa: The case of Nigeria},
journal = {The Electricity Journal},
volume = {32},
number = {10},
pages = {106672},
year = {2019},
issn = {1040-6190},
doi = {https://doi.org/10.1016/j.tej.2019.106672},
url = {https://www.sciencedirect.com/science/article/pii/S1040619019302775},
author = {Eunice C. Nnaji and Donald Adgidzi and Michael O. Dioha and Daniel R.E. Ewim and Zhongjie Huan},
keywords = {Energy access, Off-grid rural electrification, HOMER, Simulink, Nigeria},
abstract = {Access to electricity is still a challenge in many parts of sub-Saharan Africa. In Nigeria, over 70% of the rural dwellers do not have access to electricity. The purpose of this paper is to examine the potential of a smart microgrid for off-grid rural electrification in Nigeria. A combination of design thinking and model-based design methodology is employed to select a suitable microgrid configuration and to develop a smart microgrid model. A system consisting of a solar photovoltaic array, battery energy storage and a diesel generator is selected, and the model is developed in Simulink. Demand data from 10 rural communities in Nigeria are used to validate the performance of the model and the potential for demand management is considered. The use of energy efficient light bulbs is found to reduce the peak electricity demand of the case study communities by 42 to 76%. Combining the proposed system with the use of LED bulbs makes the system to have 56 to 81% less net present cost than a system with a diesel generator alone and incandescent light bulbs. The proposed smart microgrid is found to be more suitable for off-grid rural electrification in Nigeria than diesel generators which are currently used for off-grid electrification in Nigeria.}
}
@article{WU2025113281,
title = {Graph knowledge tracing in cognitive situation: Validation of classic assertions in cognitive psychology},
journal = {Knowledge-Based Systems},
volume = {315},
pages = {113281},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.113281},
url = {https://www.sciencedirect.com/science/article/pii/S0950705125003284},
author = {Qianxi Wu and Weidong Ji and Guohui Zhou and Yingchun Yang},
keywords = {Knowledge Tracing, Cognitive situation, Hyper-Graph Neural Network, Directed Graph Convolutional Neural Network, Cognitive psychology},
abstract = {Knowledge Tracing (KT) is a fundamental and challenging task in intelligent education, aiming to trace learners’ knowledge states and learning processes, providing better support and guidance for teaching and addressing mental factors. Previous KT tasks have focused on considering learners’ exposure to extrinsic environmental factors while ignoring the influence of intrinsic psychological factors. Moreover, previous methods have adopted a single perspective in modeling learners’ knowledge states, ignoring the diversity of states in the learning process. To address these issues, we define the concept of cognitive situation through the guidance of cognitive psychology theory to help to explain the extrinsic influence and intrinsic cognition of learners within complex learning environments. Moreover, we design a Cognitive Situation-based Graph KT (CSGKT) model to quantify learners’ influences in the cognitive process by modeling schemas capturing intrinsic characteristics and extrinsic factors through Hyper-Graph Neural Networks (HGNN). Second, we utilize a Directed Graph Convolutional Neural Network (DGCNN) to capture the correlation information between knowledge concepts and structure the learner’s cognitive activities and knowledge states, adding a detailed representation of multiple states of the learning process. In addition, we use the Erase-add Gate to filter out the knowledge states that do not match the learner’s current cognitive activities to stabilize the learner’s due cognition. In our experiments, we selected nine baseline models from three mainstream approaches, including sequence-based approaches, Transformer-based approaches, and complex structure-based approaches. The experimental results show that our models outperform these baseline models. At the same time, we also verify two classic assertions in cognitive psychology, namely, the “short-term memory forgetting of knowledge concepts is mainly caused by interference rather than memory trace fading” and the “cognitive imagery and perceptual function play an equivalent role in the cognitive process”, which further support the feasibility of the model.}
}
@article{MAROWKA2020199,
title = {On the performance difference between theory and practice for parallel algorithms},
journal = {Journal of Parallel and Distributed Computing},
volume = {138},
pages = {199-210},
year = {2020},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2019.12.020},
url = {https://www.sciencedirect.com/science/article/pii/S0743731519304964},
author = {Ami Marowka},
keywords = {Python, Teaching parallel programming, Quicksort, Performance modeling},
abstract = {The performance of parallel algorithms is often inconsistent with their preliminary theoretical analyses. Indeed, the difference is increasing between the ability to theoretically predict the performance of a parallel algorithm and the results measured in practice. This is mainly due to the accelerated development of advanced parallel architectures, whereas there is still no agreed model for parallel computation, which has implications for the design of parallel algorithms and for the manner in which parallel programming should be taught. In this study, we examined the practical performance of Cormen’s Quicksort parallel algorithm. We determined the performance of the algorithm with different parallel programming approaches and examine the capacity of theoretical performance analyses of the algorithm for predicting the actual performance. This algorithm is used for teaching theoretical and practical aspects of parallel programming to undergraduate students. We considered the pedagogic implications that may arise when the algorithm is used as a learning resource for teaching parallel programming.}
}
@article{ROSENBAUM2018510,
title = {Stress-related dysfunction of the right inferior frontal cortex in high ruminators: An fNIRS study},
journal = {NeuroImage: Clinical},
volume = {18},
pages = {510-517},
year = {2018},
issn = {2213-1582},
doi = {https://doi.org/10.1016/j.nicl.2018.02.022},
url = {https://www.sciencedirect.com/science/article/pii/S2213158218300561},
author = {David Rosenbaum and Mara Thomas and Paula Hilsendegen and Florian G. Metzger and Florian B. Haeussinger and Hans-Christoph Nuerk and Andreas J. Fallgatter and Vanessa Nieratschker and Ann-Christine Ehlis},
keywords = {Trier Social Stress Test (TSST), Functional near-infrared spectroscopy (fNIRS), Inferior frontal gyrus (IFG), Functional connectivity, Rumination, Cognitive control network (CCN)},
abstract = {Repetitive thinking styles such as rumination are considered to be a key factor in the development and maintenance of mental disorders. Different situational triggers (e.g., social stressors) have been shown to elicit rumination in subjects exhibiting such habitual thinking styles. At the same time, the process of rumination influences the adaption to stressful situations. The study at hand aims to investigate the effect of trait rumination on neuronal activation patterns during the Trier Social Stress Test (TSST) as well as the physiological and affective adaptation to this high-stress situation.
Methods
A sample of 23 high and 22 low ruminators underwent the TSST and two control conditions while their cortical hemodynamic reactions were measured with functional near-infrared spectroscopy (fNIRS). Additional behavioral, physiological and endocrinological measures of the stress response were assessed.
Results
Subjects showed a linear increase from non-stressful control conditions to the TSST in cortical activity of the cognitive control network (CCN) and dorsal attention network (DAN), comprising the bilateral dorsolateral prefrontal cortex (dlPFC), inferior frontal gyrus (IFG) and superior parietal cortex/somatosensory association cortex (SAC). During stress, high ruminators showed attenuated cortical activity in the right IFG, whereby deficits in IFG activation mediated group differences in post-stress state rumination and negative affect.
Conclusions
Aberrant activation of the CCN and DAN during social stress likely reflects deficits in inhibition and attention with corresponding negative emotional and cognitive consequences. The results shed light on possible neuronal underpinnings by which high trait rumination may act as a risk factor for the development of clinical syndromes.}
}
@incollection{FOWLER20131,
title = {Chapter 1 - Introduction},
editor = {Bruce A. Fowler},
booktitle = {Computational Toxicology},
publisher = {Academic Press},
address = {San Diego},
pages = {1-4},
year = {2013},
isbn = {978-0-12-396461-8},
doi = {https://doi.org/10.1016/B978-0-12-396461-8.00001-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780123964618000014},
author = {Bruce A. Fowler},
keywords = {Computational toxicology, risk assessment, chemical mixtures, emergency responses, animal–human extrapolations, data extrapolations from  test systems to human risk assessments, systems biology approaches, data mining, hypothesis generation},
abstract = {This book is intended to be an introduction to various applications of computational toxicology and to show how these approaches are currently being used effectively for risk assessment purposes in the near term. It is important to note that the field of computational toxicology is rapidly evolving and that subsequent editions of this book will take up new methods that are currently under development, such as high-throughput screening, and others that are still in a conceptual stage. There are many advantages for including computational toxicology approaches in the risk assessment process. Among these are reducing costs, minimizing use of animals in toxicology testing, improving speed in providing answers regarding chemicals in emergency situations such as the Gulf Oil spill, and dealing with the common problem of decision making for chemical mixtures. In addition, computational methods may be used for extrapolating or translating data from both in vitro and in vivo experimental animal test systems for human risk assessments of chemicals and drugs. In addition, computational methods may be used for focusing laboratory studies into productive areas by data mining the published literature and developing testable hypotheses by application of systems biology approaches to identify chemical interactions with functional molecular pathways to generate a more comprehensive picture of likely primary and secondary modes of chemical or drug activity. In summary, there is much that computational toxicology is now contributing to helping make better societal risk assessment decisions about chemicals and drugs. The future for these approaches is optimistic and limited only by human ingenuity and availability of resources.}
}
@article{ZHOU2025121348,
title = {Turning waste into energy through a solar-powered multi-generation system with novel machine learning-based life cycle optimization},
journal = {Chemical Engineering Science},
volume = {307},
pages = {121348},
year = {2025},
issn = {0009-2509},
doi = {https://doi.org/10.1016/j.ces.2025.121348},
url = {https://www.sciencedirect.com/science/article/pii/S000925092500171X},
author = {Jianzhao Zhou and Jingzheng Ren and Liandong Zhu and Chang He},
keywords = {Medical waste, Waste-to-energy, Multi-generation system, Machine learning, Comprehensive optimization},
abstract = {This study presents an innovative solar-powered multi-generation system aiming at converting waste into diverse forms of energy, including dimethyl ether (DME), hydrogen, power, and heat. Concurrently, a systematic and computationally efficient optimization framework is developed to unlock the maximum potential of this complex waste-to-energy system. The system integrates plasma gasification, DME synthesis, combining heat and power generation, solar-driven electrolysis and desalination. Life cycle assessment and techno-economic assessment have been implemented for system comprehensive optimization which is formulated as a large-scale nonlinear program (NLP) model. Based on rigorous process simulation results, a machine learning-based framework is proposed to accelerate optimization. Using medical waste treatment as a case study, the solution of the NLP problem reveals optimal levelized costs per kWh energy range from $0.1064 to $0.1304, with total life cycle carbon emissions ranging from 0.2748 to 0.5083 kg CO2-eq/kWh energy. The findings demonstrate the proposed system’s environmental sustainability and economic viability.}
}
@article{HALTAUFDERHEIDE2023,
title = {Cultural Implications Regarding Privacy in Digital Contact Tracing Algorithms: Method Development and Empirical Ethics Analysis of a German and a Japanese Approach to Contact Tracing},
journal = {Journal of Medical Internet Research},
volume = {25},
year = {2023},
issn = {1438-8871},
doi = {https://doi.org/10.2196/45112},
url = {https://www.sciencedirect.com/science/article/pii/S1438887123004831},
author = {Joschka Haltaufderheide and Davide Viero and Dennis Krämer},
keywords = {digital contact tracing, algorithms, methodology, empirical ethics, privacy, culture-sensitive ethics, mobile phone},
abstract = {Background
Digital contact tracing algorithms (DCTAs) have emerged as a means of supporting pandemic containment strategies and protecting populations from the adverse effects of COVID-19. However, the impact of DCTAs on users’ privacy and autonomy has been heavily debated. Although privacy is often viewed as the ability to control access to information, recent approaches consider it as a norm that structures social life. In this regard, cultural factors are crucial in evaluating the appropriateness of information flows in DCTAs. Hence, an important part of ethical evaluations of DCTAs is to develop an understanding of their information flow and their contextual situatedness to be able to adequately evaluate questions about privacy. However, only limited studies and conceptual approaches are currently available in this regard.
Objective
This study aimed to develop a case study methodology to include contextual cultural factors in ethical analysis and present exemplary results of a subsequent analysis of 2 different DCTAs following this approach.
Methods
We conducted a comparative qualitative case study of the algorithm of the Google Apple Exposure Notification Framework as exemplified in the German Corona Warn App and the Japanese approach of Computation of Infection Risk via Confidential Locational Entries (CIRCLE) method. The methodology was based on a postphenomenological perspective, combined with empirical investigations of the technological artifacts within their context of use. An ethics of disclosure approach was used to focus on the social ontologies created by the algorithms and highlight their connection to the question about privacy.
Results
Both algorithms use the idea of representing a social encounter of 2 subjects. These subjects gain significance in terms of risk against the background of a representation of their temporal and spatial properties. However, the comparative analysis reveals 2 major differences. Google Apple Exposure Notification Framework prioritizes temporality over spatiality. In contrast, the representation of spatiality is reduced to distance without any direction or orientation. However, the CIRCLE framework prioritizes spatiality over temporality. These different concepts and prioritizations can be seen to align with important cultural differences in considering basic concepts such as subject, time, and space in Eastern and Western thought.
Conclusions
The differences noted in this study essentially lead to 2 different ethical questions about privacy that are raised against the respective backgrounds. These findings have important implications for the ethical evaluation of DCTAs, suggesting that a culture-sensitive assessment is required to ensure that technologies fit into their context and create less concern regarding their ethical acceptability. Methodologically, our study provides a basis for an intercultural approach to the ethics of disclosure, allowing for cross-cultural dialogue that can overcome mutual implicit biases and blind spots based on cultural differences.}
}
@article{WOLFENGAGEN2020276,
title = {Capturing information processes with variable domains},
journal = {Procedia Computer Science},
volume = {169},
pages = {276-283},
year = {2020},
note = {Postproceedings of the 10th Annual International Conference on Biologically Inspired Cognitive Architectures, BICA 2019 (Tenth Annual Meeting of the BICA Society), held August 15-19, 2019 in Seattle, Washington, USA},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.02.177},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920303008},
author = {Viacheslav Wolfengagen and Larisa Ismailova and Sergey Kosikov},
keywords = {semantic information processing, computational model, variable domains},
abstract = {An approach to the construction of a computational model in which information processes are presented in the framework of theories without types, and they, in turn, are considered as special parts of typed theories, is proposed. Similar mixing was used in model studies for lambda-calculus. In contrast to them, in the present work, information processes correspond to parameterized metadata objects, which are variable domain constructs. Transformations of variable domains correspond to the spread of the process. Directional transformation provides the generation of metadata targets in the form of parameterized concepts. This simulates the evolving of the process, which allows the interpretation of the hidden time factor. The emerging model is purely process-based and provides a conceptual framework. The possibility of coding this framework with a system of interdependent lambda-terms is shown.}
}
@article{SAVILLE201577,
title = {Application of information and communication technology and data sharing management scheme for the coastal fishery using real-time fishery information},
journal = {Ocean & Coastal Management},
volume = {106},
pages = {77-86},
year = {2015},
issn = {0964-5691},
doi = {https://doi.org/10.1016/j.ocecoaman.2015.01.019},
url = {https://www.sciencedirect.com/science/article/pii/S0964569115000289},
author = {Ramadhona Saville and Katsumori Hatanaka and Minoru Sano and Masaaki Wada},
keywords = {Catchable stock index, Self-management support, Real-time data sharing, Cloud computing, ICT, Swept area method, Sea cucumber},
abstract = {In this paper, we propose an automatic computation and data sharing scheme to support management system in coastal fishery using real-time fishery information through information and communication technology (ICT). In Japan, several species of fisheries commodity have not been specified in Total Allowable Catch policy, causing a lot of confusion on fishery cooperatives and fishermen on how to set the catch limit. To deal with the problem, in the previous study, we developed catchable stock index, a method to estimate a certain extent of resource via the swept area method. However, as the calculation of the index was computed on a GIS software manually, it was very time consuming, costly and unable to give an immediate evaluation of the fishing operation. This study aims to support management system in a coastal fishery through the development of automatic catchable stock index algorithm. In this study, ICT was utilized to obtain and transmit the real-time data sharing of fishery information as well as to distribute the computation results to the fishermen and fishery cooperative. The data used were vessels' trajectories and catch records, which included the start/end time and catch amount of each fishing operation. The catchable stock index was automatically computed in an originally developed cloud computing service. We have conducted the test run of the present method in sea cucumber dredge-net fishery on the coast of Rumoi City, Hokkaido, Japan. Data were collected from the entire vessels in Rumoi (16 vessels) during the 2012 and 2013 fishing seasons. The results were returned to the fishermen via the Internet each day during the fishing season, therefore, fishermen were able to immediately evaluate their catch. The estimated catchable stock index for the 2012 and 2013 seasons was 85.5 tons and 92.3 tons, respectively. By referring to the present system, fishermen voluntarily stopped the 2012 and 2013 fishing season several weeks earlier than their initial schedule to avoid overfishing. Moreover, in the previous study, the spacing of the grid has been decided empirically, but in this study, the adequate grid size could be evaluated due to the fast computation through ratio of the area of a grid cell to the total dredged area. In light of the evidence, the present automatic algorithm provided useful information for supporting the self-management of this coastal fishery.}
}
@incollection{MCCALL20231025,
title = {Chapter 57 - Advances in ethics for the neuroscience agenda∗},
editor = {Michael J. Zigmond and Clayton A. Wiley and Marie-Francoise Chesselet},
booktitle = {Neurobiology of Brain Disorders (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
pages = {1025-1045},
year = {2023},
isbn = {978-0-323-85654-6},
doi = {https://doi.org/10.1016/B978-0-323-85654-6.00053-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780323856546000538},
author = {Iris Coates McCall and Veljko Dubljević},
keywords = {Animal models, Biomedical science, Data sharing, Ethics, Health, Incidental finding, Neuroscience, Public policy, Science communication},
abstract = {Critical thinking about ethics in neuroscience can be a powerful force in enabling research and translating results meaningfully for society. In this chapter, we strive to give those new to neuroscience investigation, as well as more experienced investigators, an introduction to the field of neuroethics such that they may begin to incorporate it into their future work from its very inception. We begin with an overview of the regulatory and oversight mechanisms currently in place that guide the ethical conduct of neuroscience research, followed by an introduction to four of the most salient neuroethics topics relevant to neuroscience research—research with animals, data sharing, incidental findings, and neuroscience communication. First, we discuss how upfront consideration of the societal implications of advances in neuroscience can shape the use of animal models. We situate ethical thinking in this era of big science and big data, reflecting on strategies for sharing databases while protecting contributors and users. Next, we highlight how collaboration among neuroscientists, ethicists, and others can produce positive measures to resolve the problem of incidental discoveries in brain imaging research, as one example of debates on incidental findings more broadly. Third, we discuss the importance of effective and responsible communication of neuroscience research and information. Finally, the mandate of neuroscience research as public service and ethical imperative is addressed by describing opportunities for neuroscientists to engage with societal issues emerging from their research and how this deepens the discourse and adds value to the research enterprise.}
}
@article{SPEER201099,
title = {Collegiate mathematics teaching: An unexamined practice},
journal = {The Journal of Mathematical Behavior},
volume = {29},
number = {2},
pages = {99-114},
year = {2010},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2010.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S0732312310000052},
author = {Natasha M. Speer and John P. Smith and Aladar Horvath},
keywords = {Collegiate mathematics, Teaching practice},
abstract = {Though written accounts of collegiate mathematics teaching exist (e.g., mathematicians’ reflections and analyses of learning and teaching in innovative courses), research on collegiate teachers’ actual classroom teaching practice is virtually non-existent. We advance this claim based on a thorough review of peer-reviewed journals where scholarship on collegiate mathematics teaching is published. To frame this review, we distinguish between instructional activities and teaching practice and present six categories of published scholarship that consider collegiate teaching but are not descriptive empirical research on teaching practice. Empirical studies can reveal important differences among teachers’ thinking and actions, promote discussions of practice, and support learning about teaching. To support such research, we developed a preliminary framework of cognitively oriented dimensions of teaching practice based on our review of empirical research on pre-college and college teaching.}
}
@incollection{CHOQUET1995421,
title = { - Viscous flow computations on the connection machine by a finite element Petrov-Galerkin scheme},
editor = {A. Ecer and J. Hauser and P. Leca and J. Periaux},
booktitle = {Parallel Computational Fluid Dynamics 1993},
publisher = {North-Holland},
address = {Amsterdam},
pages = {421-428},
year = {1995},
isbn = {978-0-444-81999-4},
doi = {https://doi.org/10.1016/B978-044481999-4/50175-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780444819994501754},
author = {Remi Choquet and Penelope Leyland},
abstract = {Publisher Summary
This chapter studies the numerical solution of the compressible Navier–Stokes equations with matrix-free implicit schemes, and its implementation on the Connection Machine CM-200 in the framework of unstructured grids. First, a Petrov Galerkin formulation is chosen following ideas of Hughes, which give at each time step a non-linear problem to be solved F(u)=0. Regarding the cost side of such a resolution, the main difficulties are due to communications and to the nonlinear solver. This chapter considers such problems. In the resolution process, either the jacobian of F is needed or the product of such jacobian by a vector. To obtain either of them, elementary contributions are assembled. Then at some stage of the algorithm of resolution, the new solution obtained globally is explored. Each of these operations involves communications among processors, which are quite costly on a single instruction, multiple data (SIMD) computer. So, existing libraries are selected because they are both easy to use and reasonably efficient compared to more sophisticated approaches. This enables to focus the study on the algorithmic part of the code. The nonlinear equations are solved by Newton-generalized minimal residual method (GMRES). Newton iterations are applied to linearize the problem and the induced linear system is solved by the iterative scheme GMRES, which only needs matrix vector product and is well-suited to nonsymmetric large matrices.}
}
@article{NGUYEN2015257,
title = {Identifiability Challenges in Mathematical Models of Viral Infectious Diseases**This work was supported by iMed - the Helmholtz Initiative on Pesonalized Medicine.},
journal = {IFAC-PapersOnLine},
volume = {48},
number = {28},
pages = {257-262},
year = {2015},
note = {17th IFAC Symposium on System Identification SYSID 2015},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2015.12.135},
url = {https://www.sciencedirect.com/science/article/pii/S2405896315027597},
author = {Van Kinh Nguyen and Esteban A. Hernandez-Vargas},
keywords = {parameter estimation, identifiability, viral infections},
abstract = {Nowadays, infections by viral pathogens are one of the biggest health threats to mankind. The development of new avenues of thinking to integrate the complexity of infectious diseases and the immune system is urgently needed. Recently mathematical modelling has emerged as a tool to interpret experimental results on quantitative grounds providing relevant insights to understand several infectious diseases. Nevertheless, modelling the complex mechanisms between viruses and the immune system can result in models with a large number of parameters to be estimated. Furthermore, experimental measurements have the problem to be sparse (in time) and highly noisy. Therefore, structural and practical identifiability are key obstacles to overcome towards mathematical models with predictive value. This paper addresses the identifiability limitations in the most common mathematical model to represent viral infections. Additionally, numerical simulations reveal how initial conditions of differential equations and fixing parameter values can alter the profile likelihood.}
}
@incollection{AGARWAL2024625,
title = {4.10 - Digital twin},
editor = {Kenneth S. Ramos},
booktitle = {Comprehensive Precision Medicine (First Edition)},
publisher = {Elsevier},
edition = {First Edition},
address = {Oxford},
pages = {625-638},
year = {2024},
isbn = {978-0-12-824256-8},
doi = {https://doi.org/10.1016/B978-0-12-824010-6.00051-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128240106000514},
author = {Sarvesh Agarwal and Vijay Pratap Singh and Paulamy Ganguly and Pujita Munnangi and Claire Collins and Sadmaan Sarker and Jason Shenoi and Scott Heston and Shruti Pandita and Tej K. Pandita and Michael Moreno and Douglas A. Baxter and Roderick I. Pettigrew and Shameer Khader and Kamlesh K. Yadav},
keywords = {Artificial intelligence, Biomarker, Clinical trial, Digital twin, Healthcare, Industrial digital twin, Manufacturing, NASA, Oncology, Personalized medicine, Supply chain},
abstract = {Since the very beginning of space exploration, NASA has been building actual size replicas of spaceships and rovers to help them troubleshoot issues when the vehicles are out in outer space. Furthermore, real-time data captured and processed during spaceship launches has helped with timely maneuvering decisions. Similar concepts are currently utilized in various industrial processes such as manufacturing, oil and gas industry, and supply chain to name a few. A good example of the use of Digital twin technology is the GPS-based navigation system where maps are overlaid with location coordinates and real-time traffic data to make decisions on the best available routes. In this chapter we describe the use of digital twin technology in various industries such as manufacturing (including drug and vaccine development), pharmaceutical, healthcare and the practice of medicine. We further describe the rapid development of computation technologies and better structuring of electronic health records which in turn has ushered the emergence of digital health in medicine. Lastly, we provide examples of the use of digital twin technology in the areas of personalized medicine (immunology, dementia, and oncology), biomarker development (Multiple sclerosis, Chron's and Cardiovascular diseases), and clinical trials (Alzheimer's and breast cancer).}
}
@incollection{BARTO199135,
title = {On the Computational Economics of Reinforcement Learning},
editor = {David S. Touretzky and Jeffrey L. Elman and Terrence J. Sejnowski and Geoffrey E. Hinton},
booktitle = {Connectionist Models},
publisher = {Morgan Kaufmann},
pages = {35-44},
year = {1991},
isbn = {978-1-4832-1448-1},
doi = {https://doi.org/10.1016/B978-1-4832-1448-1.50010-X},
url = {https://www.sciencedirect.com/science/article/pii/B978148321448150010X},
author = {Andrew G. Barto and Satinder Pal Singh},
abstract = {Following terminology used in adaptive control, we distinguish between indirect learning methods, which learn explicit models of the dynamic structure of the system to be controlled, and direct learning methods, which do not. We compare an existing indirect method, which uses a conventional dynamic programming algorithm, with a closely related direct reinforcement learning method by applying both methods to an infinite horizon Markov decision problem with unknown state-transition probabilities. The simulations show that although the direct method requires much less space and dramatically less computation per control action, its learning ability in this task is superior to, or compares favorably with, that of the more complex indirect method. Although these results do not address how the methods’ performances compare as problems become more difficult, they suggest that given a fixed amount of computational power available per control action, it may be better to use a direct reinforcement learning method augmented with indirect techniques than to devote all available resources to a computationally costly indirect method. Comprehensive answers to the questions raised by this study depend on many factors making up the economic context of the computation.}
}