@article{MIYAMOTO2023869,
title = {An evaluation of homeostatic plasticity for ecosystems using an analytical data science approach},
journal = {Computational and Structural Biotechnology Journal},
volume = {21},
pages = {869-878},
year = {2023},
issn = {2001-0370},
doi = {https://doi.org/10.1016/j.csbj.2023.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S2001037023000016},
author = {Hirokuni Miyamoto and Jun Kikuchi},
keywords = {Biodiversity, Environmental analysis, Network, Machine learning, Statistical inference},
abstract = {The natural world is constantly changing, and planetary boundaries are issuing severe warnings about biodiversity and cycles of carbon, nitrogen, and phosphorus. In other views, social problems such as global warming and food shortages are spreading to various fields. These seemingly unrelated issues are closely related, but it can be said that understanding them in an integrated manner is still a step away. However, progress in analytical technologies has been recognized in various fields and, from a microscopic perspective, with the development of instruments including next-generation sequencers (NGS), nuclear magnetic resonance (NMR), gas chromatography-mass spectrometry (GC/MS), and liquid chromatography-mass spectrometry (LC/MS), various forms of molecular information such as genome data, microflora structure, metabolome, proteome, and lipidome can be obtained. The development of new technology has made it possible to obtain molecular information in a variety of forms. From a macroscopic perspective, the development of environmental analytical instruments and environmental measurement facilities such as satellites, drones, observation ships, and semiconductor censors has increased the data availability for various environmental factors. Based on these background, the role of computational science is to provide a mechanism for integrating and understanding these seemingly disparate data sets. This review describes machine learning and the need for structural equations and statistical causal inference of these data to solve these problems. In addition to introducing actual examples of how these technologies can be utilized, we will discuss how to use these technologies to implement environmentally friendly technologies in society.}
}
@article{MOHAMED2022152376,
title = {The Search for Efficient and Stable Metal-Organic Frameworks for Photocatalysis: Atmospheric Fixation of Nitrogen},
journal = {Applied Surface Science},
volume = {583},
pages = {152376},
year = {2022},
issn = {0169-4332},
doi = {https://doi.org/10.1016/j.apsusc.2021.152376},
url = {https://www.sciencedirect.com/science/article/pii/S0169433221033948},
author = {Amro M.O. Mohamed and Yusuf Bicer},
keywords = {Computational screening, Electronic properties, Green ammonia, Life cycle assessment, Molecular simulation, Photoactivity, Solar energy},
abstract = {Recent research targets the low-pressure synthesis of ammonia via a light-initiated catalytic process. Despite the importance of materials selection for photocatalysis, computational efforts to guide candidate materials’ nomination ahead of experiments are lacking. The purpose of this study is to employ computational screening, using density functional theory and molecular simulations, to select and evaluate metal–organic frameworks (MOFs) as nitrogen fixation photocatalysts and further deduce correlations for the prediction of MOFs’ electronic properties. First, MOFs with appropriate electronic and structural properties are identified. The top candidates have been examined from the perspective of adsorption, diffusion, and mechanical and chemical stability properties. Four MOFs, Fe2Cl2(BBTA), Fe2(mDOBDC), Zn2(mDOBDC), and Ni-BTP, have been selected based on their band edges, while only Fe2Cl2(BBTA) MOF exhibited a bandgap less than 3 eV. Fe2(mDOBDC) exhibited the highest shear modulus of approximately 31 GPa. In addition, a life cycle assessment of the four MOFs showed that Ni-BTP has the lowest environmental impact. A set of 48 MOFs’ combinations are proposed for heterojunction application to enhance charge carriers’ separation. Intriguingly, we demonstrated the predictability of MOF’s bandgap and edges from MOF’s organic linker bandgap and metal node type (oxidation state and corresponding electronic configuration) for MOF families.}
}
@article{FU2015159,
title = {An iterative method for discovering feasible management interventions and targets conjointly using uncertainty visualizations},
journal = {Environmental Modelling & Software},
volume = {71},
pages = {159-173},
year = {2015},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2015.05.017},
url = {https://www.sciencedirect.com/science/article/pii/S1364815215001656},
author = {Baihua Fu and Joseph H.A. Guillaume and Anthony J. Jakeman},
keywords = {Uncertainty, Environmental flows, Visualization, Wetlands, Decision making, Environmental management, Systems},
abstract = {This paper presents a generic method, referred to as Iterative Discovery, to guide deliberation with analysis where the aim is to plan refinements to management interventions with difficult-to-define objectives, often due to system uncertainties and diverse stakeholder positions. The method can be initiated by evaluating a scenario describing the current-best intervention. This provides the starting point for three evaluation cycles, focusing on model assumptions, alternative interventions and management targets. The outcome of this method is a list of management targets that can and cannot be achieved, the potential interventions that correspond to these targets, and the assumptions and uncertainties associated with these interventions. It was applied to a case study for environmental flow management in the Macquarie Marshes, Australia. We identified feasible management targets based on ecological outcomes in flood suitability across different locations, climate conditions and species, and the suitable environmental flow volumes that correspond to these targets.}
}
@article{THEOFILIDIS2024219,
title = {Mental Imagery: Investigating the Limits of Mental Partitioning},
journal = {Revista Colombiana de Psiquiatría},
volume = {53},
number = {3},
pages = {219-228},
year = {2024},
issn = {0034-7450},
doi = {https://doi.org/10.1016/j.rcp.2024.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S003474502400009X},
author = {Antonios Theofilidis and Maria-Valeria Karakasi and Filippos Kargopoulos},
keywords = {Mental imagery, Mental partitioning, Memory, Cognition, Neuroscience, Imaginería mental, Partición mental, Memoria, Cognición, Neurociencia},
abstract = {Introduction
Do we form mental models which bear an analogical relation to the real world like those of a photograph? Has the language of thought an analogue nature (it makes use of mental imagery) or whether it is exclusively of digital nature like language?
Objectives
The basic aim of the present study is to contribute to the ongoing work on mental imagery by extending the research to an unexplored area that of mental partitioning.
Methods
The present research sample consisted of 498 participants (234 males and 264 females). We used the SPSS software package in order to analyze our data.
Results
According to our results, we detected significant peculiarities in the cognitive performance of the participants in the tasks of mental partitioning of the Moebius strip, indicating certain limitations inherent in human thinking.
Conclusions
The position we are led to adopt is closer to that of Pylyshyn (2003), who maintained that visual mental imagery depends on abstract form of thought and on previous knowledge. Specifically, it rests on previous abstract propositional thought and knowledge rather than on concrete perceptual processes like the ones proposed by Kosslyn and Sheppard. The present work investigates a potentially valuable theoretical basis in imagery research for understanding maladaptive imagery across various related clinical disorders, while encouraging multidisciplinary approaches among cognitive psychological/neuroscientific and clinical domains.
Resumen
Introducción
¿Formamos modelos mentales que guardan una relación analógica con el mundo real como los de una fotografía? ¿Tiene el lenguaje del pensamiento una naturaleza analógica (hace uso de imágenes mentales) o es exclusivamente de naturaleza digital como el lenguaje?
Objetivos
El objetivo básico del presente estudio es contribuir al trabajo en curso sobre la imaginería mental extendiendo la investigación a un área inexplorada que es la partición mental.
Métodos
La muestra de la presente investigación estuvo compuesta por 498 participantes (234 varones y 264 mujeres). Usamos el paquete de software SPSS® para analizar nuestros datos.
Resultados
De acuerdo con nuestros resultados, detectamos peculiaridades significativas en el desempeño cognitivo de los participantes en las tareas de partición mental de la tira de Moebius, indicando ciertas limitaciones inherentes al pensamiento humano.
Conclusiones
La posición a la que nos vemos llevados a adoptar se acerca más a la de Pylyshyn (2003), quien sostenía que la imaginería mental visual depende de formas abstractas de pensamiento y de conocimientos previos. Específicamente, se basa en el pensamiento y el conocimiento proposicionales abstractos previos más que en procesos de percepción concretos como los propuestos por Kosslyn y Sheppard. El presente trabajo investiga una base teórica potencialmente valiosa en la investigación de imágenes para comprender las imágenes desadaptativas en varios trastornos clínicos relacionados, al tiempo que fomenta enfoques multidisciplinarios entre los dominios cognitivos psicológicos/neurocientíficos y clínicos.}
}
@article{BRIANTHOROMAN2020104859,
title = {An integrated approach to near miss analysis combining AcciMap and Network Analysis},
journal = {Safety Science},
volume = {130},
pages = {104859},
year = {2020},
issn = {0925-7535},
doi = {https://doi.org/10.1016/j.ssci.2020.104859},
url = {https://www.sciencedirect.com/science/article/pii/S0925753520302563},
author = {M.S. {Brian Thoroman} and Paul Salmon},
keywords = {Near miss, Systems thinking, AcciMap, Network analysis, Incident analysis},
abstract = {Contemporary safety philosophies, such as Safety II, promote the importance of understanding effective work practices as well as those leading to adverse events. Despite this, little safety research has focussed on identifying and representing these protective practices in incident analysis. This study combined AcciMap and network analysis methods to identify and evaluate the system-wide protective practices occurring within a set of led outdoor activity domain near miss incidents. The analysis was based on subject matter expert surveys and interviews regarding near miss incidents. The findings revealed a large set of interrelated protective factors. These factors were about communication, policy and procedure, individual and organisational behavioural influences, and environmental conditions across the sociotechnical system. It is argued that network analysis should be combined with AcciMap in future incident analyses.}
}
@article{LEBERRE2022103122,
title = {Systemic vulnerability of coastal territories to erosion and marine flooding: A conceptual and methodological approach applied to Brittany (France)},
journal = {International Journal of Disaster Risk Reduction},
volume = {78},
pages = {103122},
year = {2022},
issn = {2212-4209},
doi = {https://doi.org/10.1016/j.ijdrr.2022.103122},
url = {https://www.sciencedirect.com/science/article/pii/S2212420922003417},
author = {Iwan {Le Berre} and Catherine Meur-Ferec and Véronique Cuq and Elisabeth Guillou and Thibaud Lami and Nicolas {Le Dantec} and Pauline Letortu and Caroline Lummert and Manuelle Philippe and Mathias Rouan and Camille Noûs and Alain Hénaff},
keywords = {Coastal risks, Erosion, Flooding, Vulnerability, Web-GIS interface},
abstract = {The attractiveness of the coasts tends to increase their exposure to erosion and marine flooding risks. This exposure is exacerbated by the effects of climate change, in particular sea level rise. To contribute to strategic thinking on the vulnerability of coastal areas, it is essential to develop, share and collectively maintain relevant knowledge on risks. This article will present the thinking behind the setting up of a coastal risks observatory in Brittany, a region located in north-western France. It relies on a conceptual approach to systemic vulnerability based on four components: hazards, assets, management, and social representations. Hazards and assets underpin the notion of risk and tend to increase the vulnerability, management tends to mitigate it, and representations can play a part in increasing or decreasing it depending on the context. To understand and analyse this system of vulnerability, our approach is based on the generation of a set of 62 indicators combined into different types of indices. A web-GIS interface was developed to navigate through and map this system of vulnerability. The difficulties associated with this type of synthetic approach will be discussed, whether they are related to data availability, to the links between scientific research and operational territorial management requirements, or to an understanding of the dynamics of all of the vulnerability components and their interactions. Ultimately, the approach developed has been successful in mobilising scientific and operational stakeholders around the co-construction of a diagnosis of territories with regard to their vulnerability to coastal risks.}
}
@article{LANG20151369,
title = {Beyond the Golden Era of public health: charting a path from sanitarianism to ecological public health},
journal = {Public Health},
volume = {129},
number = {10},
pages = {1369-1382},
year = {2015},
issn = {0033-3506},
doi = {https://doi.org/10.1016/j.puhe.2015.07.042},
url = {https://www.sciencedirect.com/science/article/pii/S0033350615003029},
author = {Tim Lang and Geof Rayner},
keywords = {Public health, Prosperity, Economic growth, Human progress, Societal & ecosystem costs, Public policy, 21st century challenges, Ecological public health, Institutional reform},
abstract = {The paper considers the long-term trajectory of public health and whether a ‘Golden Era’ in Public Health might be coming to an end. While successful elements of the 20th century policy approach need still to be applied in the developing world, two significant flaws are now apparent within its core thinking. It assumes that continuing economic growth will generate sufficient wealth to pay for the public health infrastructure and improvement needed in the 21st century when, in reality, externalised costs are spiralling. Secondly, there is evidence of growing mismatch between ecosystems and human progress. While 20th century development has undeniably improved public health, it has also undermined the capacity to maintain life on a sustainable basis and has generated other more negative health consequences. For these and other reasons a rethink about the role, purpose and direction of public health is needed. While health has to be at the heart of any viable notion of progress the dominant policy path offers new versions of the ‘health follows wealth’ position. The paper posits ecological public health as a radical project to reshape the conditions of existence. Both of these broad paths require different functions and purposes from their institutions, professions and politicians. The paper suggests that eco-systems pressures, including climate change, are already adding to pressure for a change of course.}
}
@article{CHIHAB2023106810,
title = {Thermal performance and energy efficiency of the composite clay and hemp fibers},
journal = {Journal of Building Engineering},
volume = {73},
pages = {106810},
year = {2023},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2023.106810},
url = {https://www.sciencedirect.com/science/article/pii/S2352710223009890},
author = {Yassine Chihab and Najma Laaroussi and Mohammed Garoum},
keywords = {Dynamic thermal, Composite materials, Clay bricks, Hemp fibers, Energy-savings},
abstract = {The purpose of this research is to demonstrate that incorporating hemp fibers into earth bricks can provide an adequate level of thermal comfort by enhancing the material's dynamic thermal characteristics. The main goal is to find the optimal thickness of a clay-hemp wall to attain the highest thermal inertia values. First, the flash method was used to estimate thermal diffusivity, and the state hot plate method was used to measure the thermal conductivity of a clay brick. Using the experimental data as input, computational analysis is performed to investigate the relationship between the thermal performance of composite materials and their microstructures, with the goal of predicting the effective thermal conductivity of the composite clay-hemp. Using the Random Sequential Addition algorithm, a two-phase, three-dimensional composite clay-hemp microstructure was produced. The effective thermal conductivity of these composites was assessed using the finite volume method. The predicted thermophysical characteristics were then utilized to simulate the transient heat transfer across clay-hemp walls. The results demonstrate that when the hemp volume fraction increased, the thermal conductivity, thermal diffusivity, and thermal volumetric capacity all decreased by approximately 52%, 27%, and 35%, respectively. Furthermore, incorporating hemp fibers improved the bricks' dynamic thermal characteristics. Finally, this study has revealed that a wall composed of clay-hemp bricks of 22 cm thick and an insulating layer of 6 cm thick allows for limiting the risk of overheating during the summer months (time lag between 10 and 12 h) while also satisfying the Moroccan Thermal Construction Regulation requirement for Marrakech city.}
}
@article{JI2024109648,
title = {Research on 3D printed titanium alloy scaffold structure induced osteogenesis: Mechanics and in vitro testing},
journal = {Materials Today Communications},
volume = {40},
pages = {109648},
year = {2024},
issn = {2352-4928},
doi = {https://doi.org/10.1016/j.mtcomm.2024.109648},
url = {https://www.sciencedirect.com/science/article/pii/S2352492824016295},
author = {Yuchen Ji and Huiming Zhang and Zhixiu Jiang and Danyu Liu and Yuhao Yang and Chenxu Guan and Yucheng Su and Xinyu Wang and Feng Duan},
keywords = {Selective laser melting, Structural titanium alloy scaffold, Finite element analysis, Mechanical experiment, Osteogenesis},
abstract = {Several methods exist for repairing mandibular segmental bone defects, typically employing the implant method to accomplish the repair. Following a comparative analysis, the Ti6Al4V structural scaffold implant was chosen for bone defect repair. Triply periodic minimal surface (TPMS) is characterized by a high surface area to volume ratio, an average curvature of zero, and other notable advantages, providing a new line of thinking for bone tissue scaffolds. In this work, the in vitro osteogenesis of a cell unit measuring 4 mm was investigated. First, the finite element analysis (FEA) method and the mechanical experiment method were employed to screen the elasticity modulus of the cancellous bone of the mandible. Subsequently, the selective laser melting (SLM) technique was adopted to prepare three different structures precisely - Gyroid, octahedron, and cube - each with wall thicknesses of 0.3 mm, 0.4 mm, and 0.5 mm. In the in vitro osteogenic experiments, it was observed through confocal laser scanning microscopy (CLSM) that each scaffold displayed favorable cell spreading at 1 day and 3 days. Moreover, osteoblast cell adhesion and proliferation assays revealed improved cell adhesion and proliferation with prolonged co-cultivation time, signifying the excellent biocompatibility of the structural titanium alloy scaffolds. Furthermore, findings from cell differentiation and bioactivity assays indicated that the Gyroid structure exhibited superior osteogenesis compared to the cube and octahedron structures. However, no statistically significant difference was noted between varying wall thicknesses within the same structure.}
}
@article{TUTHILL2019259,
title = {Decision Support to Enhance Automated Laboratory Testing by Leveraging Analytical Capabilities},
journal = {Clinics in Laboratory Medicine},
volume = {39},
number = {2},
pages = {259-267},
year = {2019},
note = {Clinical Decision Support: Tools, Strategies, and Emerging Technologies},
issn = {0272-2712},
doi = {https://doi.org/10.1016/j.cll.2019.01.005},
url = {https://www.sciencedirect.com/science/article/pii/S0272271219300058},
author = {J. Mark Tuthill},
keywords = {Business analytics, Clinical decision support, Laboratory automation, Dashboards, Artificial intelligence, Learning health systems}
}
@article{STEVENS2022108887,
title = {Hyper-differential sensitivity analysis for inverse problems governed by ODEs with application to COVID-19 modeling},
journal = {Mathematical Biosciences},
volume = {351},
pages = {108887},
year = {2022},
issn = {0025-5564},
doi = {https://doi.org/10.1016/j.mbs.2022.108887},
url = {https://www.sciencedirect.com/science/article/pii/S0025556422000827},
author = {Mason Stevens and Isaac Sunseri and Alen Alexanderian},
keywords = {Inverse problems, Sensitivity analysis, Uncertainty quantification, Design of experiments, Computational epidemiology},
abstract = {We consider inverse problems governed by systems of ordinary differential equations (ODEs) that contain uncertain parameters in addition to the parameters being estimated. In such problems, which are common in applications, it is important to understand the sensitivity of the solution of the inverse problem to the uncertain model parameters. It is also of interest to understand the sensitivity of the inverse problem solution to different types of measurements or parameters describing the experimental setup. Hyper-differential sensitivity analysis (HDSA) is a sensitivity analysis approach that provides tools for such tasks. We extend existing HDSA methods by developing methods for quantifying the uncertainty in the estimated parameters. Specifically, we propose a linear approximation to the solution of the inverse problem that allows efficiently approximating the statistical properties of the estimated parameters. We also explore the use of this linear model for approximate global sensitivity analysis. As a driving application, we consider an inverse problem governed by a COVID–19 model. We present comprehensive computational studies that examine the sensitivity of this inverse problem to several uncertain model parameters and different types of measurement data. Our results also demonstrate the effectiveness of the linear approximation model for uncertainty quantification in inverse problems and for parameter screening.}
}
@article{GARDNER2013167,
title = {Dark matter studies entrain nuclear physics},
journal = {Progress in Particle and Nuclear Physics},
volume = {71},
pages = {167-184},
year = {2013},
note = {Fundamental Symmetries in the Era of the LHC},
issn = {0146-6410},
doi = {https://doi.org/10.1016/j.ppnp.2013.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S0146641013000203},
author = {Susan Gardner and George M. Fuller},
keywords = {Dark matter, Nuclear astrophysics, Neutrinos},
abstract = {We review theoretically well-motivated dark-matter candidates, and pathways to their discovery, in the light of recent results from collider physics, astrophysics, and cosmology. Taken in aggregate, these encourage broader thinking in regards to possible dark-matter candidates — dark-matter need not be made of “WIMPs”, i.e., elementary particles with weak-scale masses and interactions. Facilities dedicated to nuclear physics are well-poised to investigate certain non-WIMP models. In parallel to this, developments in observational cosmology permit probes of the relativistic energy density at early epochs and thus provide new ways to constrain dark-matter models, provided nuclear physics inputs are sufficiently well-known. The emerging confluence of accelerator, astrophysical, and cosmological constraints permit searches for dark-matter candidates in a greater range of masses and interaction strengths than heretofore possible.}
}
@article{DEMAZIERE2024113028,
title = {Enhancing higher education through hybrid and flipped learning: Experiences from the GRE@T-PIONEeR project},
journal = {Nuclear Engineering and Design},
volume = {421},
pages = {113028},
year = {2024},
issn = {0029-5493},
doi = {https://doi.org/10.1016/j.nucengdes.2024.113028},
url = {https://www.sciencedirect.com/science/article/pii/S0029549324001286},
author = {C. Demazière and C. Stöhr and Y. Zhang and O. Cabellos and S. Dulla and N. Garcia-Herranz and R. Miró and R. Macian and M. Szieberth and C. Lange and M. Hursin and S. Strola},
keywords = {Nuclear education and training, Computational reactor physics, Experimental reactor physics, Flipped classroom, Active learning, Hybrid teaching, Online learning},
abstract = {GRE@T-PIONEeR is a Horizon 2020 project coordinated by Chalmers University of Technology, running over the period 2020–2024. 18 university teachers from 8 different universities located in 6 different countries gathered forces to develop and offer advanced courses in computational and experimental nuclear reactor physics and safety. All courses are flipped hybrid courses, i.e., students work on online preparatory activities at their own pace before attending a set of interactive sessions organized on five consecutive days. Those sessions can be attended either onsite or remotely. During the academic year 2022/2023, 8 different courses were offered, and 185 students successfully completed the courses, with a success rate of 87.7% for the students taking at least one activity during the interactive sessions. Student behaviour and performance were monitored via the Learning Management System (LMS) used in all courses. This paper presents an analysis of various metrics from the LMS and demonstrates a high level of engagement of the students committed to the courses and a high success rate for those students. Whereas all students are equally engaged in the online preparatory work and perform equally well, significant differences exist during the interactive sessions between the students who opted for onsite participation and those who attended the sessions online, with the onsite students outperforming the online students.}
}
@article{MOALLEMI2018205,
title = {A participatory exploratory modelling approach for long-term planning in energy transitions},
journal = {Energy Research & Social Science},
volume = {35},
pages = {205-216},
year = {2018},
note = {Energy and the Future},
issn = {2214-6296},
doi = {https://doi.org/10.1016/j.erss.2017.10.022},
url = {https://www.sciencedirect.com/science/article/pii/S221462961730350X},
author = {Enayat A. Moallemi and Shirin Malekpour},
keywords = {Exploratory modelling, Policy analysis, Sustainability transitions, Energy policy, Uncertainty},
abstract = {Energy transitions are complex transformation processes, which involve different actors and unfold in a deeply uncertain future. These features make the long-term planning of energy transitions a wicked problem. Traditional strategic planning approaches fail to address this wickedness as they have a predictive, deterministic, and reactive standpoint to future issues. Modelling approaches that are used within conventional contexts are perceived to be inadequate too. They often simplify the qualitative characteristics of transitions and cannot cope with deeply uncertain futures. More recently, new ways of qualitative participatory planning, as well as new approaches to quantitative modelling have emerged to enable policy analysis under deep uncertainty. We argue that qualitative participatory and quantitative modelling approaches can be complementary to each other in different ways. We operationalise their coupling in the form of a practical approach to be used for long-term planning of energy transitions. The suggested approach enables energy decision makers to test various policy interventions under numerous possibilities with a computational model and in a participatory process. We explain our approach with illustrative examples mostly from transitions in electricity sectors. However, our approach is applicable to different forms of energy transitions, and to the broader context of transition in any societal system, such as water and transportation.}
}
@article{SU2024121016,
title = {Detecting anomalies with granular-ball fuzzy rough sets},
journal = {Information Sciences},
volume = {678},
pages = {121016},
year = {2024},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2024.121016},
url = {https://www.sciencedirect.com/science/article/pii/S0020025524009307},
author = {Xinyu Su and Zhong Yuan and Baiyang Chen and Dezhong Peng and Hongmei Chen and Yingke Chen},
keywords = {Granular computing, Fuzzy rough sets, Granular-ball, Anomaly detection, Outlier detection},
abstract = {Most of the existing anomaly detection methods are based on a single and fine granularity input pattern, which is susceptible to noisy data and inefficient for detecting anomalies. Granular-ball computing, as a novel multi-granularity representation and computation method, can effectively compensate for these shortcomings. We utilize the fuzzy rough sets to mine the potential uncertainty information in the data efficiently. The combination of granular-ball computing and fuzzy rough sets takes into account the benefits of both methods, providing great application and research value. However, this novel combination still needs to be explored, especially for unsupervised anomaly detection. In this study, we first propose the granular-ball fuzzy rough set model, and the relevant definitions in the model are given. Subsequently, we pioneeringly present an unsupervised anomaly detection method based on granular-ball fuzzy rough sets called granular-ball fuzzy rough sets-based anomaly detection (GBFRD). Our method introduces the granular-ball fuzzy rough granules-based outlier factor to characterize the outlier degree of an object effectively. The experimental results demonstrate that GBFRD exhibits superior performance compared to the state-of-the-art methods. The code is publicly available at https://github.com/Mxeron/GBFRD.}
}
@incollection{HOWARD2025291,
title = {Types of AI},
editor = {David Baker and Lucy Ellis},
booktitle = {Encyclopedia of Libraries, Librarianship, and Information Science (First Edition)},
publisher = {Academic Press},
edition = {First Edition},
address = {Oxford},
pages = {291-300},
year = {2025},
isbn = {978-0-323-95690-1},
doi = {https://doi.org/10.1016/B978-0-323-95689-5.00274-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780323956895002741},
author = {Katherine Howard},
keywords = {AI, Artificial intelligence, Deep learning, Machine learning, Types of AI},
abstract = {The world of artificial intelligence (AI) is complex. As a relatively young discipline that is continually evolving, our understanding of how we categorize the various facets of AI also must evolve and adapt. AI falls into just two categories: AI based on capability, and AI based on functionality. AI based on capability has three sub-categories, two of which remain as purely theoretical concepts. AI that is based on functionality is distinguished by four sub-categories, with each one targeted at different problems, and solving them.}
}
@article{HUBALOVSKY2019691,
title = {Assessment of the influence of adaptive E-learning on learning effectiveness of primary school pupils},
journal = {Computers in Human Behavior},
volume = {92},
pages = {691-705},
year = {2019},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2018.05.033},
url = {https://www.sciencedirect.com/science/article/pii/S0747563218302590},
author = {S. Hubalovsky and M. Hubalovska and M. Musilek},
keywords = {Learning analytics, Cognitive computing, Adaptive e-learning, Primary education, Learning effectiveness, Bloom's taxonomy},
abstract = {The paper deals with assessment of the influence of adaptive e-learning as a part of learning analytics on learning effectiveness of primary school pupils. E-learning exercises containing implemented adaptive elements were created in accordance with the Bloom's Taxonomy. Within the pilot study the authors detected high percentage success rate during e-learning exercise completion. This leads to formulation of the question „Can any e-learning exercise of lower cognitive levels of Bloom's taxonomy be skipped without affecting the cognitive thinking for solution of the e-learning exercises on upper cognitive levels of Bloom's taxonomy?” To answer the question, the algorithm of adaptive e-learning was defined and hypotheses were established. The research was carried out as pedagogical experiment comparing the results of both experimental and control groups of pupils. The research hypotheses were confirmed by statistical analysis of the research data. The results confirm that adaptive features of e-learning can be implemented in the primary education. The research results confirm the fact that educational objectives can be achieved with some pupils more effectively. Consequently, the implementation of adaptive elements into e-learning at the primary school supports an individual approach when completing e-learning exercises according to the principle of cognitive computing.}
}
@article{ZAHOOR2025100931,
title = {Child computer interactions: Cognitive development and segmenting unsafe video contents: A review},
journal = {Entertainment Computing},
volume = {53},
pages = {100931},
year = {2025},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2025.100931},
url = {https://www.sciencedirect.com/science/article/pii/S1875952125000114},
author = {Irwa Zahoor and Sajaad Ahmed Lone},
keywords = {Child Computer Interaction (CCI), Artificial Intelligence (AI) based CCI, Cognitive development, Segmentation of inappropriate video content, and Systematic review on Child Computer Interaction},
abstract = {Computer Technology (CT) is now an integral part of our daily lives, influencing various aspects of human activity, particularly those of children. Child Computer Interactions a specialized area within CT, focuses on enhancing children’s physical activities, psychology, education, and communication through diverse computer applications. CCI is a steadily growing field that focuses on children as a prominent and emergent user group. This review article highlights the lack of research regarding the effective use of CCI technologies to promote cognitive development while reducing the risks linked to harmful digital content. The paper systematically examines the influence of CCI technologies on the cognitive development of children aged 0 to 12 years, addressing the research problem of inadequate filtering methods for unsafe video content that children are exposed to in today’s screen-dominated environment. It highlights how technological innovations, particularly in gaming, artificial intelligence, and media applications, are designed to enhance children’s skills while safeguarding their digital environment. A critical aspect of this review is the assessment of methods to filter and mitigate exposure to unsafe video content, a growing concern in today’s screen-dominated environment. The findings reveal that CCI programs significantly enhance children’s knowledge and skills with high accuracy. Moreover, the review underscores the importance of machine ethics in guiding the moral behavior of machines and ensuring the usability and safety of these technologies. This comprehensive analysis provides valuable insights into the role of CCI in fostering cognitive development and protecting children from inappropriate content.}
}
@article{DEVGUN2022143,
title = {Pre-cath Laboratory Planning for Left Atrial Appendage Occlusion – Optional or Essential?},
journal = {Interventional Cardiology Clinics},
volume = {11},
number = {2},
pages = {143-152},
year = {2022},
note = {Left Atrial Appendage Occlusion},
issn = {2211-7458},
doi = {https://doi.org/10.1016/j.iccl.2021.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S2211745821001073},
author = {Jasneet Devgun and Tom {De Potter} and Davide Fabbricatore and Dee Dee Wang},
keywords = {Left atrial appendage occlusion, Left atrial appendage, Atrial fibrillation, Cardiac CT, 3D printing, Imaging, Structural heart disease}
}
@incollection{MCKAY2017,
title = {Behavior Therapy: Theoretical Bases☆},
booktitle = {Reference Module in Neuroscience and Biobehavioral Psychology},
publisher = {Elsevier},
year = {2017},
isbn = {978-0-12-809324-5},
doi = {https://doi.org/10.1016/B978-0-12-809324-5.05242-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128093245052421},
author = {D. McKay and W.W. Tryon},
keywords = {Acceptance and Commitment Therapy, Applied behavior analysis, Behavior therapy, Behavioral contextualism, Cognitive therapy, Computational neuropsychology, Conditioning, Connectionist neural networks, Dialectical behavior therapy, Mindfulness based cognitive therapy, Rational emotive behavior therapy},
abstract = {Behavior therapy has become a dominant approach to treating psychological conditions. The theories underlying this approach have been broadly defined by three conceptual frameworks, or ‘waves’. The first wave is based on classical and operant conditioning. The second wave is based on targeting cognitions, or dysfunctional beliefs, that are assumed to contribute to distressing emotional experiences and problematic behaviors. The third wave is based, broadly, on acceptance and mindfulness. Collectively, all three waves may be understood from a computational neuropsychology perspective that is based on connectionist neural network models.}
}
@article{OBYRNE2022820,
title = {How critical is brain criticality?},
journal = {Trends in Neurosciences},
volume = {45},
number = {11},
pages = {820-837},
year = {2022},
issn = {0166-2236},
doi = {https://doi.org/10.1016/j.tins.2022.08.007},
url = {https://www.sciencedirect.com/science/article/pii/S0166223622001643},
author = {Jordan O’Byrne and Karim Jerbi},
keywords = {phase transition, edge of chaos, neuronal avalanche, cognition, scale-free, complexity},
abstract = {Criticality is the singular state of complex systems poised at the brink of a phase transition between order and randomness. Such systems display remarkable information-processing capabilities, evoking the compelling hypothesis that the brain may itself be critical. This foundational idea is now drawing renewed interest thanks to high-density data and converging cross-disciplinary knowledge. Together, these lines of inquiry have shed light on the intimate link between criticality, computation, and cognition. Here, we review these emerging trends in criticality neuroscience, highlighting new data pertaining to the edge of chaos and near-criticality, and making a case for the distance to criticality as a useful metric for probing cognitive states and mental illness. This unfolding progress in the field contributes to establishing criticality theory as a powerful mechanistic framework for studying emergent function and its efficiency in both biological and artificial neural networks.}
}
@article{LI2025115048,
title = {Linking firm performance with innovation culture: An algorithmic approach towards theory building},
journal = {Journal of Business Research},
volume = {187},
pages = {115048},
year = {2025},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2024.115048},
url = {https://www.sciencedirect.com/science/article/pii/S0148296324005526},
author = {Wanqing Li and Jiang Yu and Feng Chen},
keywords = {Data-driven analysis, Theory building, Firm performance management, Innovation culture},
abstract = {To address the growing demand for theory development in computational social science research, this study employs an algorithm-driven approach to formulate a comprehensive six-step theory generation process. By applying this original research method, a new theoretical model—the “Dynamic Resource-Culture Synergy Theory” is proposed which enhances the explanatory power regarding how firms maintain competitiveness in rapidly changing environments by emphasizing the pivotal role of culture in resource integration and innovation processes. Drawing on empirical data from 887 Chinese high-tech manufacturing firms, our analysis identifies key drivers of organizational performance, with a particular focus on the role of organizational culture, especially innovation culture, as a mediating force. Utilizing the GWO-SVM technique, we gain a nuanced understanding of how different cultural traits interact with innovation and leverage, uncovering how the initial enhancement of innovation culture positively impacts performance metrics such as ROA. The findings confirm that innovation, facilitated by organizational culture, significantly enhances performance outcomes. Furthermore, this study considers factors such as leverage and the proportion of technical personnel, investigating their moderating effects on the relationship between innovation culture and firm performance. This study not only deepens the understanding of how innovation and culture interact to influence firm performance but also provides significant theoretical and practical contributions to the study of the dynamics of high-tech manufacturing enterprises.}
}
@article{FOELLMI2019103136,
title = {Loss aversion at the aggregate level across countries and its relation to economic fundamentals},
journal = {Journal of Macroeconomics},
volume = {61},
pages = {103136},
year = {2019},
issn = {0164-0704},
doi = {https://doi.org/10.1016/j.jmacro.2019.103136},
url = {https://www.sciencedirect.com/science/article/pii/S0164070419301028},
author = {Reto Foellmi and Adrian Jaeggi and Rina Rosenblatt-Wisch},
abstract = {Preferences are important when thinking about macroeconomic problems and questions. Differences in preferences might, for example, explain cross-country variations in economic fundamentals. In recent years, differences in preferences across countries and cultures have been studied more frequently, usually concentrating on micro evidence. However, it is an open question as to how differences in average preferences affect the aggregate economy. Coming from a macroeconomic perspective, we test whether preferences stated in Kahneman and Tversky’s prospect theory, namely, reference point dependence and loss aversion, prevail on the aggregate and whether the average degree of loss aversion differs across countries. We find evidence of loss aversion for a broad set of OECD countries, while the average loss aversion clearly differs across these countries. We find little evidence that these differences could be linked to micro evidence. Furthermore, we analyse whether the different degrees of loss aversion correlate with economic fundamentals such as the level of GDP and consumption per capita. We find that indeed loss aversion is negatively correlated with GDP and consumption per capita and positively correlated with consumption smoothing.}
}
@incollection{SCHEINER2005831,
title = {Chapter 29 - The CH···O hydrogen bond: A historical account},
editor = {Clifford E. Dykstra and Gernot Frenking and Kwang S. Kim and Gustavo E. Scuseria},
booktitle = {Theory and Applications of Computational Chemistry},
publisher = {Elsevier},
address = {Amsterdam},
pages = {831-857},
year = {2005},
isbn = {978-0-444-51719-7},
doi = {https://doi.org/10.1016/B978-044451719-7/50072-X},
url = {https://www.sciencedirect.com/science/article/pii/B978044451719750072X},
author = {Steve Scheiner},
abstract = {Publisher Summary
This chapter presents a history of the problem, how the CH· · ·O H-bond went from a nonissue to one of recognized importance. The history also includes a discussion on the contributions made by computational chemistry along the way, at each stage. The chapter discusses a unique and surprising property of the CH· · ·O bond that led some to initially deny its characterization as a H-bond at all, and others to go so far as to dub it an “anti-H-bond”. This property has been analyzed and placed into proper perspective through the power of modern quantum chemistry. The early definition of a H-bond paired a proton donor group, typically OH or NH, with an acceptor that contained a nonbonded electron pair. It was the OH· · ·O, OH· · ·N, and NH· · ·O sorts of H-bonds that dominated most thinking about H-bonds. Nonetheless, the weaker sorts of H-bonds were not completely ignored: a smaller number of studies considered the H-bonding abilities of F, Cl, S, and so on.}
}
@article{MOFIDI2020110192,
title = {Intelligent buildings: An overview},
journal = {Energy and Buildings},
volume = {223},
pages = {110192},
year = {2020},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2020.110192},
url = {https://www.sciencedirect.com/science/article/pii/S0378778819338289},
author = {Farhad Mofidi and Hashem Akbari},
keywords = {Intelligent building, Comfort, Energy conservation, Integrated control, Optimization, Productivity, Behavior modeling, Building simulation},
abstract = {The objective of this paper is to review the topics related to the optimized operation of intelligent buildings with respect to occupant comfort and energy consumption. To simultaneously optimize energy costs and indoor environmental quality, intelligent buildings should consider several continuously changing inputs including energy exchange processes across the building, sets of indoor and outdoor environmental parameters, energy prices, occupants’ presence, preferences, and behavior inside the building. Therefore, a well-structured framework supported by computational intelligence and optimization methods, environmental monitoring, and behavior modeling techniques, as well as comfort, productivity, and behavioral studies, are required to make optimal decisions for the indoor environment. In this paper, the main concepts, challenges, the latest studies, findings, and developments related to the six topics of (1) Occupant comfort conditions; (2) Occupant productivity; (3) Building control; (4) Computational optimization; (5) Occupant behavior modeling; (6) Environmental monitoring and analysis, in offices, commercial and residential buildings are reviewed. Moreover, future directions and challenges related to the optimized operation of intelligent buildings are discussed.}
}
@article{CHEN2006103,
title = {Toward development of activity coefficient models for process and product design of complex chemical systems},
journal = {Fluid Phase Equilibria},
volume = {241},
number = {1},
pages = {103-112},
year = {2006},
note = {A Festschrift in Honor of John M. Prausnitz},
issn = {0378-3812},
doi = {https://doi.org/10.1016/j.fluid.2006.01.006},
url = {https://www.sciencedirect.com/science/article/pii/S0378381206000331},
author = {Chau-Chyun Chen},
keywords = {Complex chemical systems, Electrolytes, Nonelectrolytes, Polymers, Pharmaceuticals, Activity coefficient models, Excess Gibbs energy},
abstract = {Molecular thermodynamics, an engineering science for quantitative representation of thermophysical properties and phase behavior for mixtures, has served as a core scientific foundation for process modeling and process and product design in the industries. This paper presents a personal adventure through molecular thermodynamics that follows the footprints of John Prausnitz and leads toward the development of activity coefficient models for process modeling and process and product design of complex chemical systems. In this scientific expedition, passion and endurance, industrial applications, molecular insights, and out-of-the-box thinking all play key roles. We venerate past accomplishments that serve industrial needs and cherish new opportunities that await future exploration by adventurous souls.}
}
@article{TONNANG201788,
title = {Advances in crop insect modelling methods—Towards a whole system approach},
journal = {Ecological Modelling},
volume = {354},
pages = {88-103},
year = {2017},
issn = {0304-3800},
doi = {https://doi.org/10.1016/j.ecolmodel.2017.03.015},
url = {https://www.sciencedirect.com/science/article/pii/S030438001630549X},
author = {Henri E.Z. Tonnang and Bisseleua D.B. Hervé and Lisa Biber-Freudenberger and Daisy Salifu and Sevgan Subramanian and Valentine B. Ngowi and Ritter Y.A. Guimapi and Bruce Anani and Francois M.M. Kakmeni and Hippolyte Affognon and Saliou Niassy and Tobias Landmann and Frank T. Ndjomatchoua and Sansao A. Pedro and Tino Johansson and Chrysantus M. Tanga and Paulin Nana and Komi M. Fiaboe and Samira F. Mohamed and Nguya K. Maniania and Lev V. Nedorezov and Sunday Ekesi and Christian Borgemeister},
keywords = {Insect modelling approaches, Integrated pest management, Crop production, Climate change, Impact assessment, Yield losses, System thinking},
abstract = {A wide range of insects affect crop production and cause considerable yield losses. Difficulties reside on the development and adaptation of adequate strategies to predict insect pests for their timely management to ensure enhanced agricultural production. Several conceptual modelling frameworks have been proposed, and the choice of an approach depends largely on the objective of the model and the availability of data. This paper presents a summary of decades of advances in insect population dynamics, phenology models, distribution and risk mapping. Existing challenges on the modelling of insects are listed; followed by innovations in the field. New approaches include artificial neural networks, cellular automata (CA) coupled with fuzzy logic (FL), fractal, multi-fractal, percolation, synchronization and individual/agent-based approaches. A concept for assessing climate change impacts and providing adaptation options for agricultural pest management independently of the United Nations Intergovernmental Panel on Climate Change (IPCC) emission scenarios is suggested. A framework for estimating losses and optimizing yields within crop production system is proposed and a summary on modelling the economic impact of pests control is presented. The assessment shows that the majority of known insect modelling approaches are not holistic; they only concentrate on a single component of the system, i.e. the pest, rather than the whole crop production system. We suggest system thinking as a possible approach for linking crop, pest, and environmental conditions to provide a more comprehensive assessment of agricultural crop production.}
}
@article{CHAUDHARI201687,
title = {Traffic and mobility aware resource prediction using cognitive agent in mobile ad hoc networks},
journal = {Journal of Network and Computer Applications},
volume = {72},
pages = {87-103},
year = {2016},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2016.06.010},
url = {https://www.sciencedirect.com/science/article/pii/S108480451630131X},
author = {Shilpa Shashikant Chaudhari and Rajashekhar C. Biradar},
keywords = {Mobile ad hoc network, Wavelet neural network, Resource prediction, Cognitive agent, Belief desire intention architecture},
abstract = {Mobile Ad hoc NETwork (MANET) characteristics such as limited resources, shared channel, unpredictable mobility, improper load balancing, and variation in signal strength affect the routing of real-time multimedia data that requires Quality of Service (QoS) provisioning. Accurate prediction of the resource availability assists efficient resource allocation before the routing of such data. Most of the published work on resource prediction in MANET focuses on either bandwidth or energy without considering mobility effects. Adoption of intelligent software agent such as Cognitive Agent (CA) for the accurate resource prediction has a significant potential to solve the challenges of resource prediction in MANET. The intelligence provided in CA is similar to the logical thinking like a human for decision-making. The predominant CA architecture is the Belief-Desire-Intention (BDI) model, which performs the various tasks on behalf of the human user as an assistant. In this paper, we propose a CA-based Resource Prediction mechanism considering Mobility (CA-RPM) that predicts the resources using agents through the resource prediction agency consisting of one static agent, one cognitive agent and two mobile agents. Agents predict the traffic, mobility, buffer space, energy, and bandwidth effectively that is necessary for efficient resource allocation to support real-time and multimedia communications. The mobile agents collect and distribute network traffic statistics over MANET whereas a static agent collects the local statistics. CA creates static/mobile agent during the process of resource prediction. Initially, the designed time-series Wavelet Neural Networks (WNNs) predict traffic and mobility. Buffer space, energy, and bandwidth prediction use the predicted mobility and traffic. Simulation results show that the predicted resources closely match with the real values at the cost of little overheads due to the usage of agents. Simulation analysis of predicted traffic and mobility also shows the improvement compared to recurrent WNN in terms of mean square error, covariance, memory overhead, agent overhead and computation overhead. We plan to use these predicted resources for its efficient utilization in QoS routing is our future work.}
}
@article{CHIZUBEM2025487,
title = {Real-time monitoring using digital platforms for enhanced safety in hydrogen facilities – Current perspectives and future directions},
journal = {International Journal of Hydrogen Energy},
volume = {98},
pages = {487-499},
year = {2025},
issn = {0360-3199},
doi = {https://doi.org/10.1016/j.ijhydene.2024.12.128},
url = {https://www.sciencedirect.com/science/article/pii/S036031992405331X},
author = {Benson Chizubem and Ajith Subbiah and Obasi Chukwuma Izuchukwu and Kamara Sidikie Musa},
keywords = {Digital tools, Hydrogen safety, Safety design, Risk assessment},
abstract = {Hydrogen, as a versatile and clean energy carrier, holds immense potential for addressing climate change and transitioning towards sustainable energy systems. However, ensuring safety throughout the hydrogen lifecycle remains paramount to widespread adoption. In this context, digital tools have emerged as game-changers, offering innovative solutions to enhance safety practices and mitigate risks associated with hydrogen technologies. This paper provides comprehensive information on the impacts of digital tools on hydrogen safety assessment. Consequently, digital tools have offered a valuable means of monitoring and reporting Health, Safety, and Environmental (HSE) concerns and compliance within hydrogen applications in the operational facility. Through these tools, the presence of hydrogen leaks, fire, and explosion can all be effectively tracked and managed. Also, tools such as computational fluid dynamics (CFD) simulations and machine learning algorithms, virtual reality (VR) environments, and Internet of Things (IoT) sensors for assessing release scenarios, enhanced understanding of hydrogen behaviour in different environments, and real-time monitoring of operational parameters to mitigate potential risks. The tools enable the establishment of predictive maintenance strategies within hydrogen facilities. The outcomes of this study shed light on the significance of digital tools in bolstering hydrogen safety measures and enhancing safety protocols, redefining operational efficiency, and proactively identifying and mitigating potential hazards, thereby facilitating the safety and promoting the development of hydrogen infrastructure and their sustainable deployment of hydrogen as a key enabler of the future energy transition.}
}
@article{MOLINS2022113953,
title = {Stressed individuals exhibit pessimistic bursting beliefs and a lower risk preference in the balloon analogue risk task},
journal = {Physiology & Behavior},
volume = {256},
pages = {113953},
year = {2022},
issn = {0031-9384},
doi = {https://doi.org/10.1016/j.physbeh.2022.113953},
url = {https://www.sciencedirect.com/science/article/pii/S0031938422002591},
author = {Francisco Molins and Mónica Paz and Liza Rozman and Nour {Ben Hassen} and Miguel Ángel Serrano},
keywords = {Decision-making, Balloon analogue risk task, Computational modelling, Stress, Trier social stress test},
abstract = {Stress alters decision-making by usually promoting risk-taking and reward-seeking, which could be advantageous in a context where risk is rewarded, such as the Balloon Analogue Risk Task (BART). However, previous studies addressing this issue showed inconsistencies which could emerge from assessing decision-making as a single dimension. Our aim is to study through computational modelling how stress influences cognitive subprocesses of the decision-making during the BART. For this purpose, 94 healthy participants were submitted to BART, but only half were exposed to the virtual Trier Social Stress Test (TSST-VR). The Experimental-Weight Mean-Variance (EWMV) model was used to gain insight into the subprocesses involved in risk-taking during BART. Rather than reward-seeking, our results showed a pessimistic prior belief about the balloons bursting likelihood, and a lower risk preference in the stressed participants. This cautious attitude could be attributable to an alertness state promoted by stress. Yet, since risk is rewarded in BART, it could also evidence a maladaptive decision-making derived from learning difficulties and altered feedback-processing under stress.}
}
@article{CARDENASROBLEDO2019299,
title = {A holistic self-regulated learning model: A proposal and application in ubiquitous-learning},
journal = {Expert Systems with Applications},
volume = {123},
pages = {299-314},
year = {2019},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2019.01.007},
url = {https://www.sciencedirect.com/science/article/pii/S0957417419300089},
author = {Leonor Adriana Cárdenas-Robledo and Alejandro Peña-Ayala},
keywords = {Technology enhanced learning, Ubiquitous–Learning, Self–regulated learning, metacognition, cognitive load},
abstract = {Technology enhanced learning (TEL) represents an expert and intelligent paradigm in which technological affordances are used to facilitate learners' acquisition of domain knowledge (DK), where Ubiquitous–Learning (u–learning) is a TEL instance that recreates situated and immersive settings. However in such settings, learners are stressed by diverse, heterogeneous, and simultaneous stimuli that challenge their cognitive skills, increase the cognitive load, trigger emotional reactions, and bias conduct. Thus this research proposes a smart Sequencing approach that enables TEL systems to lead students to regulate their learning process. The essence of the proposal is a holistic self–regulated learning (SRL) model that encourage students to develop higher–order thinking through the practice of metacognitive skills, motivational factors, and behavioral affairs to become aware of their own learning endeavors. The approach was applied as part of the sequencing module of a u–Learning system, where students follow its suggested cognitive strategies to assist them during the programming of control equipment. Results show that, although experimental subjects had to deal with higher cognitive load, they are expected to reach higher learning achievements than their control peers. The experience reveals how TEL systems are enabled to foster learners to handle their own apprenticeship processes. As a consequence, the smart sequencing functionality of TEL is cognitively enhanced to facilitate students the simultaneal acquisition DK and development of higher–order thinking.}
}
@article{KIRCHER2018515,
title = {Formal thought disorders: from phenomenology to neurobiology},
journal = {The Lancet Psychiatry},
volume = {5},
number = {6},
pages = {515-526},
year = {2018},
issn = {2215-0366},
doi = {https://doi.org/10.1016/S2215-0366(18)30059-2},
url = {https://www.sciencedirect.com/science/article/pii/S2215036618300592},
author = {Tilo Kircher and Henrike Bröhl and Felicitas Meier and Jennifer Engelen},
abstract = {Summary
Formal thought disorder (FTD) is present in most psychiatric disorders and in some healthy individuals. In this Review, we present a comprehensive, integrative, and multilevel account of what is known about FTD, covering genetic, cellular, and neurotransmitter effects, environmental influences, experimental psychology and neuropsychology, brain imaging, phenomenology, linguistics, and treatment. FTD is a dimensional, phenomenologically defined construct, which can be clinically subdivided into positive versus negative and objective versus subjective symptom clusters. Because FTDs have been traditionally linked to schizophrenia, studies in other diagnoses are scarce. Aetiologically, FTD is the only symptom under genetic influence in schizophrenia as shown in linkage studies, but familial communication patterns (allusive thinking) have also been associated with the condition. Positive FTDs are related to synaptic rarefication in the glutamate system of the superior and middle lateral temporal cortices. Cortical volume of the left superior temporal gyrus is decreased in patients with schizophrenia who have positive FTD in structural MRI studies and shows reversed hemispheric (right more than left) activation in functional MRI experiments during speech production. Semantic network dysfunction in positive FTD has been demonstrated in experiments of indirect semantic hyperpriming (reaction time). In acute positive FTD, antipsychotics are effective, but a subgroup of patients have treatment-resistant, chronic, positive or negative FTD. Specific psychotherapy as treatment for FTD has not yet been developed. With this solid data on the pathogenesis of FTD, we can now implement clinical studies to treat this condition.}
}
@article{VASQUEZ2019306,
title = {Curriculum change for graduate-level control engineering education at the Universidad Pontificia Bolivariana},
journal = {IFAC-PapersOnLine},
volume = {52},
number = {9},
pages = {306-311},
year = {2019},
note = {12th IFAC Symposium on Advances in Control Education ACE 2019},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2019.08.225},
url = {https://www.sciencedirect.com/science/article/pii/S2405896319305622},
author = {Rafael E. Vásquez and Fabio Castrillón and Santiago Rúa and Norha L. Posada and Carlos A. Zuluaga},
keywords = {Control engineering, control education, active learning, curriculum change},
abstract = {This paper addresses the graduate-level control engineering curriculum change performed at the Universidad Pontificia Bolivariana (UPB), Medellin, Colombia. New proposed methodologies include active learning activities using a new multipurpose experimental test bed that was developed with industrial components. The renovated graduate-level control engineering related courses include: Continuous Processes, Discrete Processes, Fuzzy Logic, Neural Networks and Genetic Algorithms, Linear Control, Nonlinear Control, and Optimal Estimation. The new experimental station was developed for teaching, research, and industrial training activities for the School of Engineering at the UPB. In this work, we report the use of the station in an Optimal Estimation course to replace a traditional homework/exams evaluation approach with an applied work that required independent study, the implementation of different observers in a real lab-scale industrial plant, and a paper-style written report. Increasing independent study activities resulted in academic discussions that are valuable for the learning process of the student. The use of the experimental station and the real comparison of estimation algorithms, implemented by using industrial controllers and high-level programming environments, provided the student skills that cannot be acquired by using only simulations in which real implementation restrictions/challenges do not appear. This work represents one of the first approaches for the implementation of the new curriculum model at the UPB for graduate education. The methodology used in the Optimal Estimation class promoted independent learning, critical thinking and writing skills through significant learning activities.}
}
@article{SOLARI2008106,
title = {Confabulation Theory},
journal = {Physics of Life Reviews},
volume = {5},
number = {2},
pages = {106-120},
year = {2008},
issn = {1571-0645},
doi = {https://doi.org/10.1016/j.plrev.2008.03.003},
url = {https://www.sciencedirect.com/science/article/pii/S1571064508000122},
author = {Soren Solari and Andrew Smith and Rupert Minnett and Robert Hecht-Nielsen},
keywords = {Confabulation Theory},
abstract = {Confabulation Theory [Hecht-Nielsen R. Confabulation theory. Springer-Verlag; 2007] is the first comprehensive theory of human and animal cognition. Here, we briefly describe Confabulation Theory and discuss experimental results that suggest the theory is correct. Simply put, Confabulation Theory proposes that thinking is like moving. In humans, the theory postulates that there are roughly 4000 thalamocortical modules, the “muscles of thought”. Each module performs an internal competition (confabulation) between its symbols, influenced by inputs delivered via learned axonal associations with symbols in other modules. In each module, this competition is controlled, as in an individual muscle, by a single graded (i.e., analog) thought control signal. The final result of this confabulation process is a single active symbol, the expression of which also results in launching of action commands that trigger and control subsequent movements and/or thought processes. Modules are manipulated in groups under coordinated, event-contingent control, in a similar manner to our 700 muscles. Confabulation Theory hypothesizes that the control of thinking is a direct evolutionary outgrowth of the control of movement. Establishing a complete understanding of Confabulation Theory will require launching and sustaining a massive new phalanx of confabulation neuroscience research.}
}
@article{FENG2017150,
title = {Parallel programming with pictures is a Snap!},
journal = {Journal of Parallel and Distributed Computing},
volume = {105},
pages = {150-162},
year = {2017},
note = {Keeping up with Technology: Teaching Parallel, Distributed and High-Performance Computing},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2017.01.018},
url = {https://www.sciencedirect.com/science/article/pii/S0743731517300242},
author = {Annette Feng and Mark Gardner and Wu-chun Feng},
keywords = {Explicit parallel computing, Computer science education, Block-based programming, Visual programming, Parallel computational patterns, Pedagogical tools, Programming environments, Languages for PDC and HPC},
abstract = {For decades, computing speeds seemingly doubled every 24 months by increasing the processor clock speed, thus giving software a “free ride” to better performance. This free ride, however, effectively ended by the mid-2000s. With clock speeds having plateaued and computational horsepower instead increasing due to increasing the number of cores per processor, the vision for parallel computing, which started more than 40 years ago, is a revolution that has now (ubiquitously) arrived. In addition to traditional supercomputing clusters, parallel computing with multiple cores can be found in desktops, laptops, and even mobile smartphones. This ubiquitous parallelism in hardware presents a major challenge: the difficulty in easily extracting parallel performance via current software abstractions. Consequently, this paper presents an approach that reduces the learning curve to parallel programming by introducing such concepts into a visual (but currently sequential) programming language called Snap!, which was inspired by MIT’s Scratch project. Furthermore, our proposed visual abstractions can automatically generate parallel code for the end user to run in parallel on a variety of platforms from personal computing devices to supercomputers. Ultimately, this work seeks to increase parallel programming literacy so that users, whether novice or experienced, may leverage a world of ubiquitous parallelism to enhance productivity in all walks of life, including the sciences, engineering, commerce, and liberal arts.}
}
@article{TROCHIM2017176,
title = {Hindsight is 20/20: Reflections on the evolution of concept mapping},
journal = {Evaluation and Program Planning},
volume = {60},
pages = {176-185},
year = {2017},
issn = {0149-7189},
doi = {https://doi.org/10.1016/j.evalprogplan.2016.08.009},
url = {https://www.sciencedirect.com/science/article/pii/S0149718916301690},
author = {William M. Trochim},
keywords = {Concept mapping, Construct validity, Pattern matching, Theory of conceptualization, Multidimensional scaling, Cluster analysis, Bridging analysis, Go-zone plot, Bibliometric analysis},
abstract = {This paper considers the origins and development of the concept mapping methodology, a summary of its growth, and its influence in a variety of fields. From initial discussions with graduate students, through the rise of the theory-driven approach to program evaluation and the development of a theoretical framework for conceptualization methodology, the paper highlights some of the key early efforts and pilot projects that culminated in a 1989 special issue on the method in Evaluation and Program Planning that brought the method to the attention of the field of evaluation. The paper details the thinking that led to the standard version of the method (the analytic sequence, “bridging” index, and pattern matching) and the development of the software for accomplishing it. A bibliometric analysis shows that the rate of citation continues to increase, where it has grown geographically and institutionally, that the method has been used in a wide variety of disciplines and specialties, and that the literature had an influence on the field. The article concludes with a critical appraisal of some of the key aspects of the approach that warrant further development.}
}
@article{DEGRAAF2006181,
title = {Fall and rise of behavioural pharmacology},
journal = {Drug Discovery Today: Technologies},
volume = {3},
number = {2},
pages = {181-185},
year = {2006},
issn = {1740-6749},
doi = {https://doi.org/10.1016/j.ddtec.2006.06.007},
url = {https://www.sciencedirect.com/science/article/pii/S174067490600031X},
author = {Joop S. {de Graaf}},
abstract = {Since the 1970s, a fortunate ensemble of technological and scientific developments has radically changed pharmacology, both in practice and imaginative thinking, towards a predominantly molecular science. Economic and political forces contributed to the undervaluation of in vivo experiments. The present generation of bioscientists, undertrained in whole animal, particularly behavioural pharmacology, now faces the challenge to interpret and translate an interminable hoard of molecular data into understandable and applicable medicine. The article provides a retrospection in four decades of progress.}
}
@article{BOSCHETTI201671,
title = {Modelling and attitudes towards the future},
journal = {Ecological Modelling},
volume = {322},
pages = {71-81},
year = {2016},
issn = {0304-3800},
doi = {https://doi.org/10.1016/j.ecolmodel.2015.11.009},
url = {https://www.sciencedirect.com/science/article/pii/S0304380015005311},
author = {Fabio Boschetti and Iain Walker and Jennifer Price},
keywords = {Forecasting, Futures Studies, Ecological modelling, Natural resource management, Cognitive science},
abstract = {The outputs of ecological models often need to be projected several years, or decades, into the future. The psychological literature tells us that stakeholders rarely think of such a distant future and when they do, they employ cognitive styles different from the ones commonly used for planning and decision making, which the ecological models are designed to facilitate. This may affect the reception of modelling efforts in several ways. Stakeholders may question the very purpose of trying to say anything meaningful about such a distant future; may consider model outputs as irrelevant to planning; or may provide emotional, often unconscious, responses motivated by deeply held fears and aspirations. Modellers too may display some of these behaviours. Here, we review the relevant literature and describe a questionnaire a modeller could use to explore these issues within a stakeholder group. We also report an experiment which shows how the very act of answering the questionnaire can significantly change the perception of future time horizon and future concerns and discuss the possible implications for modelling projects.}
}
@article{IVANOV201952,
title = {Infinite lattice models by an expansion with a non-Gaussian initial approximation},
journal = {Physics Letters B},
volume = {796},
pages = {52-58},
year = {2019},
issn = {0370-2693},
doi = {https://doi.org/10.1016/j.physletb.2019.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S0370269319304460},
author = {Aleksandr Ivanov and Vasily Sazonov},
abstract = {Recently, a convergent series employing a non-Gaussian initial approximation was constructed and shown to be an effective computational tool for the finite size lattice models with a polynomial interaction. Here we show that the Borel summability is a sufficient condition for the correctness of the convergent series applied to infinite lattice models. We test the numerical workability of the convergent series method by examining one- and two-dimensional ϕ4-infinite lattice models. The comparison of the convergent series computations and the infinite lattice extrapolations of the Monte Carlo simulations reveals an agreement between two approaches.}
}
@article{BOLANOS201926,
title = {Energy, uncertainty, and entrepreneurship: John D Rockefeller’s sequential approach to transaction costs management in the early oil industry},
journal = {Energy Research & Social Science},
volume = {55},
pages = {26-34},
year = {2019},
issn = {2214-6296},
doi = {https://doi.org/10.1016/j.erss.2019.04.020},
url = {https://www.sciencedirect.com/science/article/pii/S2214629618304444},
author = {Jose A. Bolanos},
keywords = {Uncertainty, Rockefeller, Standard Oil Company, Entrepreneurship, Transaction costs},
abstract = {This article delves into the challenge of successful entrepreneurship in the energy industry under conditions of uncertainty by examining the case of John D Rockefeller’s Standard Oil Company, which rapidly seized control of an initially-uncertain industry. It finds that Rockefeller cemented control through a willingness to internalise contextual uncertainty (related to the nature of the energy business) as a stepping stone to managing contractual uncertainty (related to transactions with other parties). This finding suggests that thinking sequentially about the management of contextual and contractual uncertainty aids entrepreneurial success in the field of energy. This suggestion accords with standing calls in the transaction costs literature, which means that findings may generalise to some extent. However, the exploratory nature of the analysis implies the need for further research about the argument’s compatibility with modern energy practices and its generalisability.}
}
@article{SALAMATI2014283,
title = {Personal Wellness: Complex and Elusive Product and Distributed Self-services},
journal = {Procedia CIRP},
volume = {16},
pages = {283-288},
year = {2014},
note = {Product Services Systems and Value Creation. Proceedings of the 6th CIRP Conference on Industrial Product-Service Systems},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2014.02.016},
url = {https://www.sciencedirect.com/science/article/pii/S2212827114001310},
author = {Farzaneh Salamati and Zbigniew J. Pasek},
keywords = {Consumer-personalized medicine, Quantified self-tracking, Health social networks},
abstract = {In many countries across the world a universal issue of growing concern is increasing demand for health services and corresponding escalating costs. While there are many reasons for these two trends, reasonable solutions are nowhere in sight and a subject of heated debates. One potential source of relief for the health care systems is to shift some (if not majority – but in long term) of responsibilities to patients themselves. To do so effectively, however, better definition of personal well-being is needed, supported by medical knowledge transfer to the consumer and creation of some personal health management tools. Service engineering concepts, such as service package, are useful in decoupling all elements needed to develop an infrastructure in support of wellness as a core product and addressed by variety of limited-focus services. This paper reviews the emerging health care paradigms, in particular health care networks, consumer-personalized medicine and quantified self-tracking. With the Quantified Self movement on the rise for the past several years and a corresponding growth in offering of tools for variety of personal data collection (both hardware- and software-based), the obvious question arises how effective they are and what impact they actually have. The discussion also addresses the question whether it is possible to reframe the personal health issues by applying both design thinking and service engineering approaches aimed at individual's own well-being.}
}
@article{BUCHANAN2019332,
title = {Metal 3D printing in construction: A review of methods, research, applications, opportunities and challenges},
journal = {Engineering Structures},
volume = {180},
pages = {332-348},
year = {2019},
issn = {0141-0296},
doi = {https://doi.org/10.1016/j.engstruct.2018.11.045},
url = {https://www.sciencedirect.com/science/article/pii/S0141029618307958},
author = {C. Buchanan and L. Gardner},
keywords = {3D printing, Additive manufacturing, Applications, Concrete, Metal, Polymers, Research, Review, Stainless steel, Structural engineering},
abstract = {3D printing, more formally known as additive manufacturing (AM), has the potential to revolutionise the construction industry, with foreseeable benefits including greater structural efficiency, reduction in material consumption and wastage, streamlining and expedition of the design-build process, enhanced customisation, greater architectural freedom and improved accuracy and safety on-site. Unlike traditional manufacturing methods for construction products, metal 3D printing offers ready opportunities to create non-prismatic sections, internal stiffening, openings, functionally graded elements, variable microstructures and mechanical properties through controlled heating and cooling and thermally-induced prestressing. Additive manufacturing offers many opportunities for the construction sector, but there will also be fresh challenges and demands, such as the need for more digitally savvy engineers, greater use of advanced computational analysis and a new way of thinking for the design and verification of structures, with greater emphasis on inspection and load testing. It is envisaged that AM will complement, rather than replace, conventional production processes, with clear potential for hybrid solutions and structural strengthening and repairs. These opportunities and challenges are explored in this paper as part of a wider review of different methods of metal 3D printing, research and early applications of additive manufacturing in the construction industry. Lessons learnt for metal 3D printing in construction from additive manufacturing using other materials and in other industries are also presented.}
}
@article{FORTESCUE197967,
title = {Why the ‘language of thought’ is not a language: Some inconsistencies of the computational analogy of thought},
journal = {Journal of Pragmatics},
volume = {3},
number = {1},
pages = {67-80},
year = {1979},
issn = {0378-2166},
doi = {https://doi.org/10.1016/0378-2166(79)90006-7},
url = {https://www.sciencedirect.com/science/article/pii/0378216679900067},
author = {Michael Fortescue}
}
@article{MIYATA2018370,
title = {Emergence of symbolic inference based on value-driven intuitive inference via associative memory},
journal = {Procedia Computer Science},
volume = {145},
pages = {370-375},
year = {2018},
note = {Postproceedings of the 9th Annual International Conference on Biologically Inspired Cognitive Architectures, BICA 2018 (Ninth Annual Meeting of the BICA Society), held August 22-24, 2018 in Prague, Czech Republic},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.11.087},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918323755},
author = {Masahiro Miyata and Takashi Omori},
keywords = {Human inference, Associative memory, Model, Symbolic inference, Intuitive inference},
abstract = {Humans use two types of inferences: intuitive and logical. However, they are studied separately. A site for logical inference has not been found in the brain, but modeling it as a distributed neural network form is desirable. In this study, we propose an inference model of an intuitive search process in continuous and distributed associative memory (AM), and it switches to a symbolic mode, in which each step of association converges to a stable state of self-recollection, realizing step-by-step logic. Switching is evoked by biasing the associative gain upon finding a valued state during the intuitive inference. In this study, we show the computational model of symbolic inference via AM, and we verify its practicality by solving a maze task. We show the emergence of a tree search-like behavior with pruning.}
}
@article{JIANG2015154,
title = {Defining least community as a homogeneous group in complex networks},
journal = {Physica A: Statistical Mechanics and its Applications},
volume = {428},
pages = {154-160},
year = {2015},
issn = {0378-4371},
doi = {https://doi.org/10.1016/j.physa.2015.02.029},
url = {https://www.sciencedirect.com/science/article/pii/S0378437115001326},
author = {Bin Jiang and Ding Ma},
keywords = {Head/tail breaks, ht-index, Scaling, k-means, Natural breaks, Classification},
abstract = {This paper introduces a new concept of least community that is as homogeneous as a random graph, and develops a new community detection algorithm from the perspective of homogeneity or heterogeneity. Based on this concept, we adopt head/tail breaks–a newly developed classification scheme for data with a heavy-tailed distribution–and rely on edge betweenness given its heavy-tailed distribution to iteratively partition a network into many heterogeneous and homogeneous communities. Surprisingly, the derived communities for any self-organized and/or self-evolved large networks demonstrate very striking power laws, implying that there are far more small communities than large ones. This notion of far more small things than large ones constitutes a new fundamental way of thinking for community detection.}
}
@article{WU2025105996,
title = {A multi-sensor interval fusion adaptive regularization data assimilation model for wind direction prediction},
journal = {Journal of Wind Engineering and Industrial Aerodynamics},
volume = {257},
pages = {105996},
year = {2025},
issn = {0167-6105},
doi = {https://doi.org/10.1016/j.jweia.2024.105996},
url = {https://www.sciencedirect.com/science/article/pii/S0167610524003593},
author = {Yuang Wu and Shuo Liu and Jiachen Huang},
keywords = {Data assimilation, Reduced-order modelling, Regularization, Weather forecasting},
abstract = {Real-time forecasting of wind fields is an essential prerequisite for computational fluid predictions of pollutant transport. In the domain of data assimilation for real-time weather forecasting, obtaining high-quality meteorological data measurements poses a challenge that significantly impacts prediction accuracy. Predicting wind direction through data assimilation presents an inverse problem, and low-quality wind direction data resulting from suboptimal sensor placement can lead to ill-posedness when constructing proxy models. Consequently, previous research has extensively investigated the optimal placement of meteorological sensors. However, the data assimilation experiment has thus introduced uncertainties associated with the positions of the sensors. To achieve this goal, this study proposes a adaptive data assimilation model. This model introduces the concept of local convergence intervals on reduced-order response model, and deconstructs ill-posed intervals into well-posed intervals, and obtains a unique solution interval by regularization through the convergence range distance fusing. Finally, the model selects sensors using adaptive local weights, and implements the data assimilation process using inverse Ensemble Kalman Filter. This paper employs data from the Huailai Test Station to design simulated wind direction experiments.The results indicate that the method is capable of overcoming the shortcomings of sensor placement and can enhance the accuracy of prediction.}
}
@article{ZHANG202340,
title = {MCHA-Net: A multi-end composite higher-order attention network guided with hierarchical supervised signal for high-resolution remote sensing image change detection},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {202},
pages = {40-68},
year = {2023},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2023.05.033},
url = {https://www.sciencedirect.com/science/article/pii/S0924271623001570},
author = {Haiming Zhang and Guorui Ma and Yongxian Zhang and Bin Wang and Heng Li and Lunjun Fan},
keywords = {Change detection, Higher-order attention, Multi-end network, High-resolution remote sensing image, Hierarchical supervision},
abstract = {Change detection (CD) is the main way to detect changes in the Earth’s surface features in a timely and accurate manner and to understand the interactions between humans and nature, CD is also an important scientific tool for supporting decision-making. Many convolution-based methods and Transformer-based methods have gained remarkable success in the field of CD with high-resolution remote sensing images (HRSIs) due to their powerful feature extraction capability and global information modeling capability, respectively. However, the diversity and complexity of HRSIs render CD methods constantly challenging, e.g., off-nadir angle imaging, seasonal turnover, and simultaneous changes in multiple feature types. Common convolution-based or Transformer-based encoding–decoding networks have a single way of data modeling and a low degree of feature fusion, resulting in poor applicability to different remote sensing data, poor recognition of different semantic targets, and limited accuracy. To improve the generalizability and detection accuracy of the network, we developed a composite higher-order attention network with multiple encoding paths named MCHA-Net. MCHA-Net has four encoding backbones, namely, the Siamese-learning path, Residual-learning path, and Transformer-learning path. Different encoding ways are equivalent to different thinking ends, and the integration can make the network have a stronger feature representation capability, forming a local–global-cross domain data modeling approach and making the network have powerful data sensing and mining capability. The decoding end aggregates the semantic information of each layer and integrates them into a unified linearized feature mapping module to achieve full modeling of the separability of information in the target domain. In addition, we propose a new higher-order attention mechanism to perform adaptive feature refinement for each layer of each encoding end, to guide the network in focusing on the targeted region and to guarantee the boundary integrity and internal compactness of the detection results. Moreover, we design a hierarchical network supervision schema that adds supervision signals at different feature abstraction levels to impose differentiated soft constraints on each layer of the network, ensuring high semantic consistency of features across layers and facilitating fast network fitting. Experimental results on three benchmark datasets (S2Looking, SVCD, and SYSU-CD) and one transfer application dataset (Google Dataset) show that MCHA-Net outperforms state-of-the-art methods in both visual interpretation and quantitative evaluation, and exhibits strong generalization and robustness against few-shot learning.}
}
@article{MACK2000307,
title = {Long-term effects of building on informal knowledge in a complex content domain: the case of multiplication of fractions},
journal = {The Journal of Mathematical Behavior},
volume = {19},
number = {3},
pages = {307-332},
year = {2000},
issn = {0732-3123},
doi = {https://doi.org/10.1016/S0732-3123(00)00050-X},
url = {https://www.sciencedirect.com/science/article/pii/S073231230000050X},
author = {Nancy K. Mack},
keywords = {Fractions, Informal knowledge, Long-term effects},
abstract = {Four students participated in a 2-year study (fifth and sixth grades) that examined the development of their understanding of multiplication of fractions. During both years, students received individualized instruction that encouraged them to build on their informal knowledge of partitioning to understand and solve problems involving multiplication of fractions. Students also received classroom instruction on algorithmic procedures for multiplication of fractions during the second year. In the long term, students consistently drew on their informal knowledge of partitioning to reconceptualize and partition units to solve problems involving multiplication of fractions in meaningful ways. At times, students' thinking was also dominated by their knowledge of algorithmic procedures for multiplication of fractions.}
}
@article{LUDGERIO2023e10,
title = {Pedagogical practices developed with children through hospital classes: An integrative literature review},
journal = {Journal of Pediatric Nursing},
volume = {72},
pages = {e10-e18},
year = {2023},
issn = {0882-5963},
doi = {https://doi.org/10.1016/j.pedn.2023.05.014},
url = {https://www.sciencedirect.com/science/article/pii/S0882596323001227},
author = {Muanna Jéssica Batista Ludgério and Cleide Maria Pontes and Bárbara Letícia Cruz {dos Santos} and Eliza Cristina Macedo and Maria Wanderleya {de Lavor Coriolano Marinus} and Luciana Pedrosa Leal},
keywords = {Special education, Hospital education department, Hospitalized child, Child rearing, Teaching},
abstract = {Objective
To analyze the evidence available in the literature on the pedagogical practices developed with children through hospital classes.
Method
An integrative review was conducted on July 20, 2022, in Scopus, MEDLINE/PubMed, CINAHL, LILACS, Web of Science, ERIC, Educ@, and Scielo using the following descriptors in English, Portuguese, and Spanish, extracted from DECS/MeSH, CINAHL, Brased/INEP, and ERIC Thesaurus: “Child, Hospitalized”, “Education, Special”, “Education Department, Hospital”, “Hospital Classroom”, “Hospital Class”, “Child Rearing”, “Educational Practices”, “Early Childhood Education”, “Education”, “Hospital Pedagogy”, and “Hospital Special Class”. No time restriction was applied. The EndNot Web reference manager and the Rayyan software were used to select studies, and later, the methodological rigor and level of evidence were assessed.
Results
The 22 articles described pedagogical practices, including ludic activities, individualized work, working with regular school content, stimulation activities, pedagogical and dialogic listening, learning based on the exchange of knowledge, video games, computational robotics, and theatrical performance.
Conclusion
Although difficulties were identified in implementing pedagogical practices in the hospital, they were shown to allow educational continuity and clinical improvement of hospitalized children.
Implications for practice
Studies on the educational process within the hospital setting can contribute to the development of public policies and the guarantee of the right to education for hospitalized children.
Descriptors
Special education; Hospital education department; Hospitalized child; Child rearing; Teaching.}
}
@article{LAUTENBACHER2024129583,
title = {Petz recovery maps for qudit quantum channels},
journal = {Physics Letters A},
volume = {512},
pages = {129583},
year = {2024},
issn = {0375-9601},
doi = {https://doi.org/10.1016/j.physleta.2024.129583},
url = {https://www.sciencedirect.com/science/article/pii/S0375960124002779},
author = {Lea Lautenbacher and Vinayak Jagadish and Francesco Petruccione and Nadja K. Bernardes},
keywords = {Petz recovery map, Qudit channels, Non-unital maps},
abstract = {This study delves into the efficacy of the Petz recovery map within the context of two paradigmatic quantum channels: dephasing and amplitude-damping. While prior investigations have predominantly focused on qubits, our research extends this inquiry to higher-dimensional systems. We introduce a novel, state-independent framework based on the Choi-Jamiołkowski isomorphism to evaluate the performance of the Petz map. By analyzing different channels and the (non-)unital nature of these processes, we emphasize the pivotal role of the reference state selection in determining the map's effectiveness. Furthermore, our analysis underscores the considerable impact of suboptimal choices on performance, prompting a broader consideration of factors such as system dimensionality.}
}
@article{PAN2025103422,
title = {A context-aware KG-LLM collaborated conceptual design approach for personalized products: A case in lower limbs rehabilitation assistive devices},
journal = {Advanced Engineering Informatics},
volume = {66},
pages = {103422},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103422},
url = {https://www.sciencedirect.com/science/article/pii/S1474034625003155},
author = {Xinyu Pan and Weibin Zhuang and Sijie Wen and Weigang Yu and Jinsong Bao and Xinyu Li},
keywords = {Conceptual design, Knowledge graph, Large language model, Lower limb rehabilitation assistive devices, User requirement mining},
abstract = {With the rapid increase in demand for personalized Rehabilitation Assistive Devices (RADs), significant challenges have emerged in their design processes. Particularly in practical applications, designers face challenges such as ambiguity in user requirements, inefficiencies in cross-domain knowledge sharing, and deviations of generated solutions from actual user needs. To address these issues, this paper proposes a Context-Aware Conceptual design method based on Knowledge graph (KG) and Large language models (LLM), named CACKL. Firstly, to address the high complexity involved in eliciting user requirements, user profiles are constructed by integrating multi-source data, and fine-grained “requirement-function” mappings are extracted using fine-tuned LLM, thereby reducing the cost associated with manual intervention. Secondly, a KG-LLM collaborated reasoning mechanism guided by a Chain-of-Thought (CoT) prompting approach is proposed to align structured domain knowledge with implicit semantic representations from LLM, thus enhancing the contextual relevance and practical effectiveness of concept generation, aiming to improve the efficiency of personalized conceptual design. In a practical case involving lower-limb RADs, the proposed CACKL method was evaluated regarding user requirement mining and conceptual design. Experimental results demonstrated significant advantages in the automatic generation of personalized design solutions, particularly in enhancing design efficiency and meeting user requirements, thereby validating its effectiveness in real-world applications. This study provides an innovative paradigm for the intelligent design of RADs by integrating dynamic knowledge constraints with natural language interaction.}
}
@article{BLAU2020100722,
title = {How does the pedagogical design of a technology-enhanced collaborative academic course promote digital literacies, self-regulation, and perceived learning of students?},
journal = {The Internet and Higher Education},
volume = {45},
pages = {100722},
year = {2020},
issn = {1096-7516},
doi = {https://doi.org/10.1016/j.iheduc.2019.100722},
url = {https://www.sciencedirect.com/science/article/pii/S1096751619304403},
author = {Ina Blau and Tamar Shamir-Inbal and Orit Avdiel},
keywords = {Digital literacy skills, Pedagogical design, Online communication and collaboration, Psychological ownership, Self-regulated learning strategies, Cognitive, emotional and social perceived learning},
abstract = {The wide expansion of digital technologies in higher education has introduced the need for an examination of the added value of various technological tools for quality teaching and active individual and collaborative learning. The current study explored whether and how the pedagogical design of an academic course, which developed a variety of digital literacy competencies, supported students in regulating collaborative technology-enhanced learning and helped them cope with the sense of psychological ownership over collaborative learning outcomes. In addition, we examined how these issues were expressed in cognitive, emotional and social aspects of students' perceived learning (Caspi & Blau, 2011). During four semesters, we conducted a qualitative analysis on reflective learning diaries, written by 78 graduate students studying education (N = 1870 codes). The bottom-up analysis focused on learning processes that enabled the development of various digital literacies conceptualized by the Digital Literacy Framework (DLF; Eshet-Alkalai, 2012): photo-visual, information, reproduction, branching, social-emotional, and real-time thinking skills. Furthermore, findings highlighted the importance of self-regulation and learning new technologies as an integral part of digital literacies. In addition, social-emotional statements expressed the development of effective communication and collaboration that enable students to cope with a sense of ownership over learning outcomes, and present different levels of teamwork: sharing, cooperation, and collaboration. Qualitative coding provided a more granulated perspective on perceived learning by differentiating between positive and negative aspects of emotional and social retrospection during the learning process. The findings contribute to educational theory by extending DLF and by providing new insights to the literature on students' perceived learning. We discuss the implications for instructional design and adoption of innovative pedagogy in higher education.}
}
@article{LI20232,
title = {Adult acquired flatfoot deformity: an update in classification},
journal = {Orthopaedics and Trauma},
volume = {37},
number = {1},
pages = {2-10},
year = {2023},
issn = {1877-1327},
doi = {https://doi.org/10.1016/j.mporth.2022.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S187713272200121X},
author = {James Li and Chandra Pasapula and Vivekanandan Dhukaram},
keywords = {AAFD, adult acquired flatfoot deformity, classification, flatfoot},
abstract = {Adult acquired flatfoot deformity (AAFD) involves a complex spectrum of pathologies, arising primarily from failure of static restrains, leading to collapse of the medial longitudinal arch and further subsequent deformities in the foot. The landmark paper and classification by Johnson et al. proposed that pathology in the posterior tibial tendon (PTT) was key in the development of AAFD. Since then, the understanding of AAFD has evolved and advanced. Multiple structures aside from the PTT, such as the spring ligament, plantar fascia and deltoid ligament, have been identified to play a similarly key role in disease development and progression. Classification systems have also evolved to incorporate this new understanding. These include modifications to Johnson's classification (Myerson, Bluman) as well as new systems which aim to incorporate modern thinking, capture the wide spectrum of presentations or utilize modern advancements in imaging modalities. Current classification systems continue to aid understanding and management of AAFD, despite their increasing complexity. Future classifications should aim to provide a succinct way to describe and understand AAFD, as well as guiding prognosis and management.}
}
@article{SOBEL20101060,
title = {Interactions between causal models, theories, and social cognitive development},
journal = {Neural Networks},
volume = {23},
number = {8},
pages = {1060-1071},
year = {2010},
note = {Social Cognition: From Babies to Robots},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2010.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S0893608010001176},
author = {David M. Sobel and David W. Buchanan and Jesse Butterfield and Odest Chadwicke Jenkins},
keywords = {Causal models, Pretense, Learning from testimony, Markov random fields, Causal generalization, Rational Model of Categorization},
abstract = {We propose a model of social cognitive development based not on a single modeling framework or the hypothesis that a single model accounts for children’s developing social cognition. Rather, we advocate a Causal Model approach (cf. Waldmann, 1996), in which models of social cognitive development take the same position as theories of social cognitive development, in that they generate novel empirical hypotheses. We describe this approach and present three examples across various aspects of social cognitive development. Our first example focuses on children’s understanding of pretense and involves only considering assumptions made by a computational framework. The second example focuses on children’s learning from “testimony”. It uses a modeling framework based on Markov random fields as a computational description of a set of empirical phenomena, and then tests a prediction of that description. The third example considers infants’ generalization of action learned from imitation. Here, we use a modified version of the Rational Model of Categorization to explain children’s inferences. Taken together, these examples suggest that research in social cognitive development can be assisted by considering how computational modeling can lead researchers towards testing novel hypotheses.}
}
@article{MCCARTHY2019152,
title = {Shaking Gordon Wilson Flats: early seismic engineering research in New Zealand},
journal = {Proceedings of the Institution of Civil Engineers - Engineering History and Heritage},
volume = {172},
number = {4},
pages = {152-163},
year = {2019},
issn = {1757-9430},
doi = {https://doi.org/10.1680/jenhh.18.00030},
url = {https://www.sciencedirect.com/science/article/pii/S1757943019000066},
author = {Christine McCarthy},
keywords = {buildings, structures & design, history, seismic engineering},
abstract = {In the late 1950s and early 1960s, Gordon Wilson Memorial Flats (Wellington, New Zealand, 19551959) was instrumented for seismic engineering research and subjected to vibration testing. The research was prompted by new thinking about architectural design in the mid-twentieth century (i.e. modernism) that had caused a mismatch between structural assumptions in building codes (which relied on significant amounts of uncalculated stiffness inherent in 1920s building design) and the structural characteristics of new buildings that had, for example, greater areas of glazing. This type of research led to the revision of New Zealand building codes in the 1960s and informed Japanese processes for permitting buildings higher than 100 ft (305m). This paper outlines the research conducted and provides the context for understanding its significance. It is particularly topical given current proposals to instrument 400 Wellington buildings, creating the highest density of seismic instrumentation in any city.}
}
@article{KNIGHT2020100624,
title = {Researching the future of purchasing and supply management: The purpose and potential of scenarios},
journal = {Journal of Purchasing and Supply Management},
volume = {26},
number = {3},
pages = {100624},
year = {2020},
issn = {1478-4092},
doi = {https://doi.org/10.1016/j.pursup.2020.100624},
url = {https://www.sciencedirect.com/science/article/pii/S1478409220300777},
author = {Louise Knight and Joanne Meehan and Efstathios Tapinos and Laura Menzies and Alexandra Pfeiffer},
keywords = {Scenario planning, Procurement, Future studies, Prescience, Critical management, Covid-19 coronavirus},
abstract = {Drawing on prior research, the value of scenario planning as a methodology for researching the future of purchasing and supply management (PSM) is explored. Using three criteria of research quality – rigour, originality and significance – it is shown how developing scenarios and analysing their implications present new, important research opportunities for PSM academics, practitioners, and leaders of the profession. Researching the future of PSM supports the identification of uncertainties and anticipates change across many units and levels of analysis of interest to PSM scholars and practitioners, such as the profession/discipline, markets/sectors, or organisations. Scenarios are particularly effective for: considering how the complex interaction of macro-environmental factors affects the PSM context; avoiding incremental thinking; surfacing assumptions and revealing significant blind spots. PSM research using scenarios aligns with Corley and Gioia's (2011) call for prescience-oriented research in which academics aim for more impactful research, enhancing sense-giving potential and theoretical relevance to practice, to better perform their adaptive role in society.}
}
@article{QAYYUM20231739,
title = {Flexible Global Aggregation and Dynamic Client Selection for Federated Learning in Internet of Vehicles},
journal = {Computers, Materials and Continua},
volume = {77},
number = {2},
pages = {1739-1757},
year = {2023},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2023.043684},
url = {https://www.sciencedirect.com/science/article/pii/S1546221823006549},
author = {Tariq Qayyum and Zouheir Trabelsi and Asadullah Tariq and Muhammad Ali and Kadhim Hayawi and Irfan {Ud Din}},
keywords = {IoT, Federated Learning, sensors, IoV, OMNeT++, edge computing},
abstract = {Federated Learning (FL) enables collaborative and privacy-preserving training of machine learning models within the Internet of Vehicles (IoV) realm. While FL effectively tackles privacy concerns, it also imposes significant resource requirements. In traditional FL, trained models are transmitted to a central server for global aggregation, typically in the cloud. This approach often leads to network congestion and bandwidth limitations when numerous devices communicate with the same server. The need for Flexible Global Aggregation and Dynamic Client Selection in FL for the IoV arises from the inherent characteristics of IoV environments. These include diverse and distributed data sources, varying data quality, and limited communication resources. By employing dynamic client selection, we can prioritize relevant and high-quality data sources, enhancing model accuracy. To address this issue, we propose an FL framework that selects global aggregation nodes dynamically rather than a single fixed aggregator. Flexible global aggregation ensures efficient utilization of limited network resources while accommodating the dynamic nature of IoV data sources. This approach optimizes both model performance and resource allocation, making FL in IoV more effective and adaptable. The selection of the global aggregation node is based on workload and communication speed considerations. Additionally, our framework overcomes the constraints associated with network, computational, and energy resources in the IoV environment by implementing a client selection algorithm that dynamically adjusts participants according to predefined parameters. Our approach surpasses Federated Averaging (FedAvg) and Hierarchical FL (HFL) regarding energy consumption, delay, and accuracy, yielding superior results.}
}
@article{ZHANG202044,
title = {From mathematical equivalence such as Ma equivalence to generalized Zhang equivalency including gradient equivalency},
journal = {Theoretical Computer Science},
volume = {817},
pages = {44-54},
year = {2020},
issn = {0304-3975},
doi = {https://doi.org/10.1016/j.tcs.2019.07.027},
url = {https://www.sciencedirect.com/science/article/pii/S0304397519304621},
author = {Yunong Zhang and Min Yang and Binbin Qiu and Jian Li and Mingjie Zhu},
keywords = {Mathematical equivalence, Physical equivalency, Ma equivalence, Generalized Zhang equivalency, Gradient equivalency},
abstract = {The authors carried out time-varying problems solving (TVPS) including robot problems solving in 2001, and began to figure out the reasons for the problems solving via diverse layers. After eight years' thinking, i.e., in 2009, the authors began to manifest, put forward and carry out the thought of “physical equivalency”. By another eight years' practicing and experimenting, i.e., in 2017, the authors basically finished establishing the framework of Zhang equivalency. Now, it is the time to establish the complete theory in a brief manner. Therefore, concepts about mathematical equivalence simply termed equivalence are presented firstly including Ma equivalence (especially for robotics), and then concepts about physical equivalency simply termed equivalency are proposed. Meanwhile, concepts about Zhang equivalency as a kind of equivalency are further proposed, and concepts about gradient-dynamics equivalency simply termed gradient equivalency as a kind of equivalency are proposed as well. Furthermore, two specific applications are considered and investigated, which substantiate the efficacy of Zhang equivalency.}
}
@incollection{KHAN2025213,
title = {Chapter 13 - The metaverse beyond the hype: Interdisciplinary perspectives on applications, tools, techniques, opportunities, and challenges of the metaverse},
editor = {Deepika Koundal and Naveen Kumar},
booktitle = {Exploring the Metaverse},
publisher = {Academic Press},
pages = {213-223},
year = {2025},
isbn = {978-0-443-24132-1},
doi = {https://doi.org/10.1016/B978-0-443-24132-1.00013-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780443241321000132},
author = {Yusera Farooq Khan and Bilal Mir and Deepika koundal},
keywords = {Artificial intelligence, Augmented reality, Edge computing, Metaverse, Virtual reality},
abstract = {The concept of the metaverse has surged to the forefront of technological discourse, fuelled by both visionary promises and speculative hype. This chapter aims explores the metaverse in depth, delving beneath the superficial attention to provide a multivariate analysis from various interdisciplinary perspectives. The future of the metaverse is intricately explored, emphasising the applications, tools, approaches, possibilities, and difficulties that characterise its trajectory. The chapter highlights the revolutionary effects of the metaverse in areas like entertainment, education, business, and social interaction by conducting a methodical dissection of its possible uses. It investigates the central role immersive experiences, augmented reality, virtual collaboration, and decentralised technologies play in redefining user engagement and interaction paradigms. Furthermore, the chapter reveals the intricate tools and techniques that serve as the foundation for the evolution of the metaverse. It investigates 3D modelling, spatial computation, haptic feedback systems, artificial intelligence-driven simulations, and other technological enablers that support the creation and maintenance of the metaverse ecosystem. The chapter dives deep into the prospects that the metaverse offers, such as different ways to express imaginative abilities, and better digital experiences. However, these opportunities are accompanied with a tangle of difficulties. The technological challenges of interoperability, scalability, and cross-platform synchronisation are analysed alongside ethical concerns such as data protection, identity preservation, and digital ownership. Through an interdisciplinary view, this book chapter equips a introductory outline for evaluating the potential, complexities, and implications of the imminent incorporation of the metaverse into our digital landscape by navigating its multifaceted dimensions.}
}
@incollection{GOYAMARTINEZ2016171,
title = {Chapter 8 - The Emulation of Emotions in Artificial Intelligence: Another Step into Anthropomorphism},
editor = {Sharon Y. Tettegah and Safiya Umoja Noble},
booktitle = {Emotions, Technology, and Design},
publisher = {Academic Press},
address = {San Diego},
pages = {171-186},
year = {2016},
series = {Emotions and Technology},
isbn = {978-0-12-801872-9},
doi = {https://doi.org/10.1016/B978-0-12-801872-9.00008-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128018729000089},
author = {Mariana Goya-Martinez},
keywords = {Emotions, Artificial intelligence, Anthropomorphism, Human emulation},
abstract = {The conceptualization of emotions as unique to biological organisms and a flaw in human intelligence has been challenged in recent times. Artificial intelligence researchers defy the idea of emotions as solely biological and try to incorporate them in their agents' design. Emotions and feelings are now considered cognitive percepts that play an important role in decision making processes. Based on artificial intelligence discourses and inventions, this chapter explores how emotions are defined in the realm of artificial intelligence, what is their role in the agents' performance, and how and why they are being emulated. From improving human-machine interaction and achieving empathy, to providing machines with cognitive shortcuts for rational thinking, emotions could be a key element in building a coherent system of thought capable of organizing several kinds of knowledge. This could provide a way to finally pass the Turing test or to provide a smooth transformation of the human nature when we finally merge with the machines. The emulation of emotions is just another step in the objective of replicating the human, promoting a higher level of anthropomorphism in the field of artificial intelligence.}
}
@incollection{GANESAN2025411,
title = {Chapter 13 - Role of quantum computing in accelerating drug discovery process},
editor = {Shubham Mahajan and Amit Kant Pandit},
booktitle = {Innovations in Biomedical Engineering},
publisher = {Academic Press},
pages = {411-435},
year = {2025},
isbn = {978-0-443-30146-9},
doi = {https://doi.org/10.1016/B978-0-443-30146-9.00013-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780443301469000137},
author = {Nirmala Ganesan and R. Rahul and S. {Sibi Sidharth}},
keywords = {Quantum computing, Drug discovery, Quantum algorithms, Pharmaceutical research, Quantum mechanics},
abstract = {This chapter investigates the application of a variational quantum eigen solver and other quantum algorithms to drug development. The uses and constraints of quantum algorithms, such as the variational quantum eigen solver, are discusses. This article examines the usefulness of variational quantum eigen solver and other quantum algorithms to drug development, while also highlighting the limitations and application areas of quantum algorithms. A paradigm shift has occurred in the pharmaceutical business with the introduction of quantum computing, as formerly unimaginable computational powers to expedite the drug discovery process have become achievable. Examining how quantum computing is changing the field, this research shows how it can significantly increase the accuracy and effectiveness of drug discovery, virtual screening, and molecular simulations. Quantum computers use quantum physics techniques to perform calculations that are more complex than possible with their classical equivalents. These systems, by utilizing quantum parallelism and entanglement, may probe huge chemical areas much faster than classical approaches. This study investigates the application of quantum algorithms, particularly variational quantum eigen solvers, in drug development. Hybrid quantum-classical technologies present a viable solution to surmount the constraints of quantum technology and enhance the scalability of drug development processes. These technologies blend quantum computing with classical approaches. The report discusses the most recent advancements in this hybrid paradigm and highlights collaborative efforts between experts in quantum computing and academics researching drugs. As technology develops, quantum computing becomes more and more significant for drug discovery. The groundbreaking effects of quantum computing on drug development and its potential to drastically alter the pharmaceutical research landscape are highlighted in the conclusion of this chapter. Through the application of quantum computing, drug development processes stand to benefit greatly from a new age of efficacy and efficiency brought about by addressing current problems and embracing innovative ideas.}
}
@article{DELGIUDICE201644,
title = {The evolutionary future of psychopathology},
journal = {Current Opinion in Psychology},
volume = {7},
pages = {44-50},
year = {2016},
note = {Evolutionary psychology},
issn = {2352-250X},
doi = {https://doi.org/10.1016/j.copsyc.2015.07.010},
url = {https://www.sciencedirect.com/science/article/pii/S2352250X15001888},
author = {Marco {Del Giudice}},
abstract = {Evolutionary approaches to psychopathology have made considerable progress over the last years. In this paper, I review recent advances in the field focusing on three core themes: the role of trade-offs and conflicts in the origins mental disorders, the evolution of developmental mechanisms, and the emergence of alternative classification systems based on life history theory. I situate these advances in the context of current research in psychopathology, and highlight their connections with other innovative approaches such as developmental psychopathology and computational psychiatry. In total, I argue that evolutionary psychopathology offers an integrative framework for the study of mental disorders, and allows complementary approaches to connect and cross-fertilize.}
}
@article{HAFRI2021475,
title = {The Perception of Relations},
journal = {Trends in Cognitive Sciences},
volume = {25},
number = {6},
pages = {475-492},
year = {2021},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2021.01.006},
url = {https://www.sciencedirect.com/science/article/pii/S1364661321000085},
author = {Alon Hafri and Chaz Firestone},
keywords = {visual psychophysics, core cognition, intuitive physics, compositionality, structured representations, role-binding},
abstract = {The world contains not only objects and features (red apples, glass bowls, wooden tables), but also relations holding between them (apples contained in bowls, bowls supported by tables). Representations of these relations are often developmentally precocious and linguistically privileged; but how does the mind extract them in the first place? Although relations themselves cast no light onto our eyes, a growing body of work suggests that even very sophisticated relations display key signatures of automatic visual processing. Across physical, eventive, and social domains, relations such as support, fit, cause, chase, and even socially interact are extracted rapidly, are impossible to ignore, and influence other perceptual processes. Sophisticated and structured relations are not only judged and understood, but also seen — revealing surprisingly rich content in visual perception itself.}
}
@article{TANAKA201664,
title = {Modeling the motor cortex: Optimality, recurrent neural networks, and spatial dynamics},
journal = {Neuroscience Research},
volume = {104},
pages = {64-71},
year = {2016},
note = {Body representation in the brain},
issn = {0168-0102},
doi = {https://doi.org/10.1016/j.neures.2015.10.012},
url = {https://www.sciencedirect.com/science/article/pii/S0168010215002631},
author = {Hirokazu Tanaka},
keywords = {Body representation, Motor control, Computational modeling, Optimization, Neural computation},
abstract = {Specialization of motor function in the frontal lobe was first discovered in the seminal experiments by Fritsch and Hitzig and subsequently by Ferrier in the 19th century. It is, however, ironical that the functional and computational role of the motor cortex still remains unresolved. A computational understanding of the motor cortex equals to understanding what movement variables the motor neurons represent (movement representation problem) and how such movement variables are computed through the interaction with anatomically connected areas (neural computation problem). Electrophysiological experiments in the 20th century demonstrated that the neural activities in motor cortex correlated with a number of motor-related and cognitive variables, thereby igniting the controversy over movement representations in motor cortex. Despite substantial experimental efforts, the overwhelming complexity found in neural activities has impeded our understanding of how movements are represented in the motor cortex. Recent progresses in computational modeling have rekindled this controversy in the 21st century. Here, I review the recent developments in computational models of the motor cortex, with a focus on optimality models, recurrent neural network models and spatial dynamics models. Although individual models provide consistent pictures within their domains, our current understanding about functions of the motor cortex is still fragmented.}
}
@incollection{ZIEDCHAARI202565,
title = {Chapter 3 - Basics of automation},
editor = {Kishor Kumar Sadasivuni and Nebojsa Bacanin and Jaehwan Kim and Neha B Vashisht},
booktitle = {Harnessing Automation and Machine Learning for Resource Recovery and Value Creation},
publisher = {Elsevier},
pages = {65-90},
year = {2025},
isbn = {978-0-443-27374-2},
doi = {https://doi.org/10.1016/B978-0-443-27374-2.00003-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780443273742000030},
author = {Mohamed {Zied Chaari} and Oleg Serov and Gilroy Pereira and Aghzout Otman},
keywords = {Artificial intelligence, automation engineering, machine learning, systems engineering, sustainable development},
abstract = {Automation uses different technologies to manufacture products or complete tasks with minimal human involvement. It is often used to perform repetitive tasks so that humans can prioritize vital and complex work. Automation engineering is the development, analysis, and implementation of such complex systems. An example that comes to most people’s minds when thinking about automation is robotic arms picking and placing objects onto conveyor belts. While this example certainly fits the definition of automation, it is a modern take on automation. Automation systems have existed since ancient times, such as the steam engine of ancient Greece, and shortly, artificial intelligence (AI)-powered automation will become an essential part of our daily lives. In this chapter, we will explore the concepts of automation and automation engineering, look at the evolution of automation from ancient times to the present, and examine two case studies that highlight the use of automation. The first case study uses automation to monitor and provide water to crops, while the second uses deep learning to sort waste. Lastly, we will explore how AI will change the field of automation.}
}
@incollection{QUEK20021831,
title = {49 - Tap: An Inquiry Teaching Shell Using Both Rule-Based and State-Space Approaches},
editor = {Cornelius T. Leondes},
booktitle = {Expert Systems},
publisher = {Academic Press},
address = {Burlington},
pages = {1831-1895},
year = {2002},
isbn = {978-0-12-443880-4},
doi = {https://doi.org/10.1016/B978-012443880-4/50093-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780124438804500934},
author = {C. Quek and L.H. Wong and C.K. Looi},
abstract = {Publisher Summary
Inquiry teaching is similar to a scientific discovery process. The essence of inquiry teaching lies in the “higher-level Thinking” or “scientific-systematic reasoning” that helps in coping with and making proper use of any causal, rule-based, and procedural knowledge. The core of the process of inquiry teaching is the loop consisting of “forming-debugging hypothesis” and “testing hypothesis.” Once a hypothesis is successfully verified, one exits the loop and proceeds to study the generalizations or the applications of the rule. Inquiry teaching can be divided into three inquiry levels characterized by some degree of expectation about the students' performance. At level 1, students are guided to go through the form-debug-test loop of inquiry teaching for learning each concept. The hypothesizing and testing procedures-experiments-methods are based on processes predefined by the teacher or the intelligent tutoring system (ITS). The students learn the “core loop” of systematic method. At level 2, students experience the core loop while learning a rule. On verifying the rule, they proceed to learn about how to “make novel predictions” based on the rule and/or “apply the rule in the real world.” The aim at this level is to familiarize the student with the complete systematic reasoning process. Inquiry level 3 is introduced as a hypothetical level, where students encounter all the steps involved at level 2 but there are no predefined procedures. Students are required to propose their own methods for hypothesizing and testing. The main challenge at this level is how to validate the proposed methods. A human teacher is able to carry out such a task, whereas, an ITS is not. Building a computer system that has the intelligence to understand, analyze, and critique a user-defined method is an important computational problem.}
}
@article{GIGERENZER2004587,
title = {Mindless statistics},
journal = {The Journal of Socio-Economics},
volume = {33},
number = {5},
pages = {587-606},
year = {2004},
note = {Statistical Significance},
issn = {1053-5357},
doi = {https://doi.org/10.1016/j.socec.2004.09.033},
url = {https://www.sciencedirect.com/science/article/pii/S1053535704000927},
author = {Gerd Gigerenzer},
keywords = {Rituals, Collective illusions, Statistical significance, Editors, Textbooks},
abstract = {Statistical rituals largely eliminate statistical thinking in the social sciences. Rituals are indispensable for identification with social groups, but they should be the subject rather than the procedure of science. What I call the “null ritual” consists of three steps: (1) set up a statistical null hypothesis, but do not specify your own hypothesis nor any alternative hypothesis, (2) use the 5% significance level for rejecting the null and accepting your hypothesis, and (3) always perform this procedure. I report evidence of the resulting collective confusion and fears about sanctions on the part of students and teachers, researchers and editors, as well as textbook writers.}
}
@article{HUMPHREYS2020101346,
title = {Explaining short-term memory phenomena with an integrated episodic/semantic framework of long-term memory},
journal = {Cognitive Psychology},
volume = {123},
pages = {101346},
year = {2020},
issn = {0010-0285},
doi = {https://doi.org/10.1016/j.cogpsych.2020.101346},
url = {https://www.sciencedirect.com/science/article/pii/S001002852030075X},
author = {Michael S. Humphreys and Gerald Tehan and Oliver Baumann and Shayne Loft},
keywords = {Short-term memory, Long-term memory, Episodic memory, Semantic memory, Systems consolidation, Associative interference, Context},
abstract = {Current thinking about human memory is dominated by distinctions between episodic and semantic memory and between short-term memory (STM) and long-term memory (LTM). However, many memory phenomena seem to cut across these distinctions. This article attempts to set the groundwork for the issues that need to be resolved in generating an integrated model of long-term memory that incorporates semantic, episodic, and short-term memory. We contrast Nairne’s (2002, Annual Review of Psychology) consensus account of short-term memory with a relatively generic theory of an integrated episodic-semantic memory. The later consists primarily of a list of principles which we and others argue are necessary to include in any theory of long-term memory. We then add some more specific assumptions to outline a modern theory of forgetting. We then turn to the issue of much of the phenomena thought to necessitate a dedicated short-term memory can be explained by an integrated theory of episodic and semantic memory. Our conclusion is that an integrated theory of long-term memory must be augmented to explain a small number of outstanding memory phenomena. Finally, we ask whether the augmentation needs to involve a dedicated mnemonic system, or sensory or language-based systems, which also have mnemonic capabilities.}
}
@article{CHA2011990,
title = {Measuring achievement of ICT competency for students in Korea},
journal = {Computers & Education},
volume = {56},
number = {4},
pages = {990-1002},
year = {2011},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2010.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S0360131510003246},
author = {Seung Eun Cha and Soo Jin Jun and Dai Yong Kwon and Han Sung Kim and Seung Bum Kim and Ja Mee Kim and Young Ae Kim and Sun Gwan Han and Soon Sik Seo and Woo Cheon Jun and Hyun Cheol Kim and Won Gyu Lee},
keywords = {Elementary education, Pedagogical issues, Teaching/learning strategies},
abstract = {In the current information society, the need for securing human resources acquired with ICT competency is becoming a very important issue. In USA, England, Japan, India and Israel improving students’ ICT competency has become a pedagogical issue. Accordingly, education on ICT competency is changing in many countries emphasizing the basis of computer science. The Korean government revised the ICT curriculum of 2001 focused on the basic concepts and principles of computer science as educational policy in 2005. However, it is still difficult to determine a student’s ICT competency level and the outcome of ICT curriculum based on changed direction. Thereupon, this study has developed test tool for measuring the level of Korean elementary school students’ ICT competency based on computer science. In this study, ‘Content’ and ‘Information processing’ are established as the two axes of the test frame standard through literature research, consideration and discussion. The validity and reliability of questions are verified though the preliminary test and the main test tool has completed through question revisions considering the distribution of answers. About 40,000 students, roughly 1% of the total elementary school students, are selected for the main test. There were several findings made in this study. Korea’s elementary school students have a weakness in ‘algorithm and modeling’. Information processing stage has been found to vary by grade. A modified ‘Angoff method’ is used to confirm the spread of the ICT competency levels of the target students. From the results, the cutoff score employed to divide the subjects into three levels, excellent, average and below average, the ratio of excellent levels decreases and the ratio of below average increases in higher grades. To solve these problems, we need to emphasize algorithmic thinking oriented more principal of computer science in ICT curriculum. For more effective ICT elementary education, teaching and learning strategies appropriate for young children to teach computer science should be introduced.}
}
@incollection{HARSTON199029,
title = {3 - THE NEUROLOGICAL BASIS FOR NEURAL COMPUTATIONS},
editor = {Alianna J. Maren and Craig T. Harston and Robert M. Pap},
booktitle = {Handbook of Neural Computing Applications},
publisher = {Academic Press},
pages = {29-44},
year = {1990},
isbn = {978-0-12-546090-3},
doi = {https://doi.org/10.1016/B978-0-12-546090-3.50007-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780125460903500073},
author = {Craig T. Harston},
abstract = {Publisher Summary
This chapter focuses on using the human nervous system as a model for computer simulations. The brain, for example, can be better understood with computer simulations. Applying ideas from the human nervous system can solve difficult problems. There are several major design principles that can be found underlying the structural organization of different areas of the brain and that may offer long-term potential as models for design of artificial neural systems. The three major design principles are (1) layers of processing elements, (2) columns of processing elements, and (3) specialization of neural tissue into both specific and nonspecific systems. Several dynamic processes that occur in biological neural systems are integrally linked to the structures of computer simulations. These dynamics form the basis from which the higher properties of the system emerge. These structurally linked dynamic processes include distributed representation of information, temporal encoding of information, role of inhibition, and feedforward and feedback processing loops.}
}
@article{SOEMER201912,
title = {Text difficulty, topic interest, and mind wandering during reading},
journal = {Learning and Instruction},
volume = {61},
pages = {12-22},
year = {2019},
issn = {0959-4752},
doi = {https://doi.org/10.1016/j.learninstruc.2018.12.006},
url = {https://www.sciencedirect.com/science/article/pii/S095947521830389X},
author = {Alexander Soemer and Ulrich Schiefele},
keywords = {Mind wandering, Reading comprehension, Interest, Text difficulty},
abstract = {The present article deals with the question of how the difficulty of a text affects a reader's tendency to engage in task-unrelated thinking (mind wandering) during reading, and the potential role of topic interest as a mediator of the relation between text difficulty and mind wandering. Two-hundred and sixteen participants read three texts with each text either being easy, moderate, or difficult in terms of readability and cohesion. From time to time during reading, participants were interrupted and required to indicate whether they were voluntarily or involuntarily engaging in mind wandering. After reading each text, they rated their interest in and familiarity with the topic, and subsequently answered a number of comprehension questions. The results revealed that reading difficult texts increased both voluntary and involuntary mind wandering and this increase partially explained the negative relation between text difficulty and comprehension. Furthermore, topic interest fully mediated the effect of text difficulty on both forms of mind wandering.}
}
@article{QUESTATORTEROLO2025102594,
title = {A case of teaching in multigrade classrooms in Uruguay: Challenges and opportunities for learning and teaching in inclusive environments},
journal = {International Journal of Educational Research},
volume = {131},
pages = {102594},
year = {2025},
issn = {0883-0355},
doi = {https://doi.org/10.1016/j.ijer.2025.102594},
url = {https://www.sciencedirect.com/science/article/pii/S0883035525000680},
author = {Mariela Questa-Torterolo and Claudia {Cabrera Borges} and Camila {Fajardo Puentes}},
keywords = {Multigraded classes, Teaching methods, Inclusion, Teacher education},
abstract = {This study aimed to investigate the pedagogical practices and perceptions of an experienced teacher in the context of multigrade preschool and primary education classrooms in Uruguay. The focus was on understanding how these practices contribute to inclusive education and promote equity. To achieve this objective, a qualitative case study methodology was employed, collecting data through three in-depth interviews with the teacher and a thorough analysis of twelve pedagogical documents, which were examined using thematic content analysis. The results reveal the inclusive teaching strategies implemented by the teacher, the curricular adaptations applied in the classroom, and the management practices developed in response to various emerging situations. Although based on a single case, these findings suggest that the multigrade classroom modality presents both challenges and opportunities for achieving quality teaching and meaningful learning for students through authentic inclusion.}
}
@article{VALENTI2025978,
title = {Costs and benefits of item reduction: The abridgment of the Emotion Regulation Questionnaire (ERQ)},
journal = {Journal of Affective Disorders},
volume = {369},
pages = {978-985},
year = {2025},
issn = {0165-0327},
doi = {https://doi.org/10.1016/j.jad.2024.10.079},
url = {https://www.sciencedirect.com/science/article/pii/S0165032724017749},
author = {Giusy Danila Valenti and Palmira Faraci},
keywords = {Short-scale development, Item reduction, Psychometric properties, Emotion Regulation Questionnaire (ERQ)},
abstract = {Shortening existing instruments is a highly required procedure, as short scales may have several advantages over the long versions, especially in time and/or resources restrictions. However, abbreviated forms may be weaker than their parent versions from both content coverage and psychometric robustness. Also, the abridgment of instruments is often lacking in methodological strictness, and the potential drawbacks of the shortened scales are rarely reported. The current study aims to describe the whole process of scale shortening, emphasizing the potential costs and benefits, in terms of balance between time-resource savings and loss of validity and reliability. We shortened the Emotion Regulation Questionnaire (ERQ), involving a sample of 459 participants (53.2% males). Item reduction was driven by searching to preserve the content breadth of the construct and scale's psychometric quality. Our results supported the two-factor structure of the measure (Cognitive Reappraisal and Expressive Suppression), χ2(8) = 11.357 ns, CFI = 0.995, TLI = 0.990, RMSEA = 0.030 (0.000–0.067), SRMR = 0.031, and three items were selected for each subscale. The two intended factors showed good levels of reliability (α > 0.710). A latent variable model was performed to evaluate how the original ERQ and our proposed short version (ERQS) were related to depression, anxiety, and stress: A similar pattern of associations was found, with Cognitive Reappraisal (negatively) and Expressive Suppression (positively) reporting significant but weak associations. The ERQ-S can be beneficial over the original version, as it effectively assesses the two emotion regulation strategies with a trivial loss in reliability and predictive validity.}
}
@incollection{UTEVSKY2015231,
title = {Social Decision Making},
editor = {Arthur W. Toga},
booktitle = {Brain Mapping},
publisher = {Academic Press},
address = {Waltham},
pages = {231-234},
year = {2015},
isbn = {978-0-12-397316-0},
doi = {https://doi.org/10.1016/B978-0-12-397025-1.00185-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780123970251001858},
author = {A.V. Utevsky and S.A. Huettel},
keywords = {Altruism, Decision making, Frontal cortex, Mentalizing, Reward, Social, Temporoparietal junction, Theory of mind},
abstract = {Many decisions are made in a social context. These decisions may be interactive and involve cooperation or conflict, or they may be made individually but have consequences for others. The brain processes underlying social decisions are complex, requiring not only computations associated with valuation and comparison but also social-cognitive processes like inferring others' mental states and predicting their behaviors. Accordingly, progress in understanding social decision making comes from combinations of techniques, primarily from the fields of economics and neuroscience. A standard model has arisen that contends that interpreting social information relies on a network of brain regions – including the medial prefrontal cortex, the posterior cingulate cortex, and the temporoparietal junction – and that information in turn feeds into the brain's system for valuation. Here, we review different types of social decisions, as well as the neural regions underlying their components.}
}
@article{SPELDA2020102531,
title = {The future of human-artificial intelligence nexus and its environmental costs},
journal = {Futures},
volume = {117},
pages = {102531},
year = {2020},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2020.102531},
url = {https://www.sciencedirect.com/science/article/pii/S0016328720300215},
author = {Petr Spelda and Vit Stritecky},
keywords = {Machine learning, Artificial intelligence, Inductive generalisations, Anthropocene, Climate change},
abstract = {The environmental costs and energy constraints have become emerging issues for the future development of Machine Learning (ML) and Artificial Intelligence (AI). So far, the discussion on environmental impacts of ML/AI lacks a perspective reaching beyond quantitative measurements of the energy-related research costs. Building on the foundations laid down by Schwartz et al. (2019) in the GreenAI initiative, our argument considers two interlinked phenomena, the gratuitous generalisation capability and the future where ML/AI performs the majority of quantifiable inductive inferences. The gratuitous generalisation capability refers to a discrepancy between the cognitive demands of a task to be accomplished and the performance (accuracy) of a used ML/AI model. If the latter exceeds the former because the model was optimised to achieve the best possible accuracy, it becomes inefficient and its operation harmful to the environment. The future dominated by the non-anthropic induction describes a use of ML/AI so all-pervasive that most of the inductive inferences become furnished by ML/AI generalisations. The paper argues that the present debate deserves an expansion connecting the environmental costs of research and ineffective ML/AI uses (the issue of gratuitous generalisation capability) with the (near) future marked by the all-pervasive Human-Artificial Intelligence Nexus.}
}
@article{BRIDGERS2025106131,
title = {Loopholes: A window into value alignment and the communication of meaning},
journal = {Cognition},
volume = {261},
pages = {106131},
year = {2025},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2025.106131},
url = {https://www.sciencedirect.com/science/article/pii/S001002772500071X},
author = {Sophie Bridgers and Peng Qian and Kiera Parece and Maya Taliaferro and Laura Schulz and Tomer D. Ullman},
keywords = {Loopholes, Pragmatic reasoning, Utility, Cooperation, Communication},
abstract = {Intentional misunderstandings take advantage of the ambiguity of language to do what someone said, instead of what they actually wanted. These purposeful misconstruals or loopholes are a familiar facet of fable, law, and everyday life. Engaging with loopholes requires a nuanced understanding of goals (your own and those of others), ambiguity, and social alignment. As such, loopholes provide a unique window into the normal operations of cooperation and communication. Despite their pervasiveness and utility in social interaction, research on loophole behavior is scarce. Here, we combine a theoretical analysis with empirical data to give a framework of loophole behavior. We first establish that loopholes are widespread, and exploited most often in equal or subordinate relationships (Study 1). We show that people reliably distinguish loophole behavior from both compliance and non-compliance (Study 2), and that people predict that others are most likely to exploit loopholes when their goals are in conflict with their social partner’s and there is a cost for non-compliance (Study 3). We discuss these findings in light of other computational frameworks for communication and joint-planning, as well as discuss how loophole behavior might develop and the implications of this work for human–machine alignment.}
}
@article{NIXON2024100992,
title = {Using machine learning to predict investors’ switching behaviour},
journal = {Journal of Behavioral and Experimental Finance},
volume = {44},
pages = {100992},
year = {2024},
issn = {2214-6350},
doi = {https://doi.org/10.1016/j.jbef.2024.100992},
url = {https://www.sciencedirect.com/science/article/pii/S2214635024001072},
author = {Paul Nixon and Evan Gilbert},
keywords = {Supervised machine learning, Random forest, Risk behaviour, Risk perception},
abstract = {Individual investors’ decisions to switch investments very often lead to significantly lower investment returns so having an effective predictive model of these switches would be of value to clients, advisors and investment managers. A random forest algorithm was applied to a new dataset of over 20 million observations relating to 95,685 clients on Momentum Investments’ platform between 2018 and 2024. It identified a combination of investor characteristics (number of holdings, past switching behaviour, total assets) and external features (past returns, macroeconomic variables) as the key features of investor switch behaviour. This model exceeds commercially accepted standards in respect of the AUC and Gini metrics showcasing the model’s strength in its ranking capability. It can thus provide a useful basis for client segmentation and engagement by financial advisors.}
}
@article{TANG2025103900,
title = {A multi-task deep reinforcement learning approach to real-time railway train rescheduling},
journal = {Transportation Research Part E: Logistics and Transportation Review},
volume = {194},
pages = {103900},
year = {2025},
issn = {1366-5545},
doi = {https://doi.org/10.1016/j.tre.2024.103900},
url = {https://www.sciencedirect.com/science/article/pii/S1366554524004915},
author = {Tao Tang and Simin Chai and Wei Wu and Jiateng Yin and Andrea D’Ariano},
keywords = {Real-time train rescheduling, High-speed railway, Train delay time, Multi-task deep reinforcement learning, Quadratic assignment programming},
abstract = {In high-speed railway systems, unexpected disruptions can result in delays of trains, significantly affecting the quality of service for passengers. Train Timetable Rescheduling (TTR) is a crucial task in the daily operation of high-speed railways to maintain punctuality and efficiency in the face of such unforeseen disruptions. Most existing studies on TTR are based on integer programming (IP) techniques and are required to solve IP models repetitively in case of disruptions, which however may be very time-consuming and greatly limit their usefulness in practice. Our study first proposes a multi-task deep reinforcement learning (MDRL) approach for TTR. Our MDRL is constructed and trained offline with a large number of historical disruptive events, enabling to generate TTR decisions in real-time for different disruption cases. Specifically, we transform the TTR problem into a Markov decision process considering the retiming and rerouting of trains. Then, we construct the MDRL framework with the definition of state, action, transition, reward, and value function approximations with neural networks for each agent (i.e., rail train), by considering the information of different disruption events as tasks. To overcome the low training efficiency and huge memory usage in the training of MDRL, given a large number of disruptive events in the historical data, we develop a new and high-efficient training method based on a Quadratic assignment programming (QAP) model and a Frank-Wolfe-based algorithm. Our QAP model optimizes only a small number but most “representative” tasks from the historical data, while the Frank-Wolfe-based algorithm approximates the nonlinear terms in the value function of MDRL and updates the model parameters among different training tasks concurrently. Finally, based on the real-world data from the Beijing–Zhangjiakou high-speed railway systems, we evaluate the performance of our MDRL approach by benchmarking it against state-of-the-art approaches in the literature. Our computational results demonstrate that an offline-trained MDRL is able to generate near-optimal TTR solutions in real-time against different disruption scenarios, and it evidently outperforms state-of-art models regarding solution quality and computational time.}
}
@article{LIEBERMAN2020355,
title = {Comparison of intelligent transportation systems based on biocybernetic vehicle control systems},
journal = {Transportation Research Procedia},
volume = {50},
pages = {355-362},
year = {2020},
note = {XIV International Conference on Organization and Traffic Safety Management in Large Cities (OTS-2020)},
issn = {2352-1465},
doi = {https://doi.org/10.1016/j.trpro.2020.10.042},
url = {https://www.sciencedirect.com/science/article/pii/S2352146520307900},
author = {Irina Lieberman and Pavel Klachek and Sergei Korjagin},
keywords = {intelligent transportation system, artificial intelligence vehicle, biocybernetic control system, sensor network, traffic safety, control module},
abstract = {Prompt thinking and quick reaction in complex dynamically changing traffic situations are determinant factors of road accidents. A key part of modern and promising intelligent transportation systems (ITSs) can be represented by VANET vehicular ad hoc network and its equivalents, whose nodes are represented by vehicles with installed special communication modules and new-generation intelligent on-board control systems. A concept of VANET network development is presented, which is based on biocybernetic systems of vehicle control and can serve as a starting point for building conceptually new ITSs and allow solving the primary ITS task at a totally new level, which lies in obtaining optimal prompt decisions (when driving a vehicle) in a short period of time, during which neither human no automated system can make a safe decision. We consider the architecture and basics of creating an intelligent on-board information-and-control module of a vehicle, based on the integration of a biocybernetic human-machine interface and elements of VANET vehicular ad hoc network. We also consider the basics of creating a bank of mathematical models as a central element of the intelligent on-board information-and-control module. Based on the analysis of completed experiments, we can conclude that the use of biocybernetic approaches based on sensor networks and corresponding applied vehicle control systems helps solving crucial traffic safety issues, namely: obtaining optimal prompt decisions (when driving a vehicle) in a short period of time, which significantly improves traffic safety and increases traffic intensity.}
}
@article{VANDENENDE2022107201,
title = {A review of mathematical modeling of addiction regarding both (neuro-) psychological processes and the social contagion perspectives},
journal = {Addictive Behaviors},
volume = {127},
pages = {107201},
year = {2022},
issn = {0306-4603},
doi = {https://doi.org/10.1016/j.addbeh.2021.107201},
url = {https://www.sciencedirect.com/science/article/pii/S0306460321003865},
author = {Maarten W.J. {van den Ende} and Sacha Epskamp and Michael H. Lees and Han L.J. {van der Maas} and Reinout W. Wiers and Peter M.A. Sloot},
keywords = {Computational modeling, Addiction, Dynamical systems, Agent-based modeling, Formal theories, Review},
abstract = {Addiction is a complex biopsychosocial phenomenon, impacted by biological predispositions, psychological processes, and the social environment. Using mathematical and computational models that allow for surrogative reasoning may be a promising avenue for gaining a deeper understanding of this complex behavior. This paper reviews and classifies a selection of formal models of addiction focusing on the intra- and inter-individual dynamics, i.e., (neuro) psychological models and social models. We find that these modeling approaches to addiction are too disjoint and argue that in order to unravel the complexities of biopsychosocial processes of addiction, models should integrate intra- and inter-individual factors.}
}
@article{LI2012276,
title = {Predicting sRNAs and Their Targets in Bacteria},
journal = {Genomics, Proteomics & Bioinformatics},
volume = {10},
number = {5},
pages = {276-284},
year = {2012},
issn = {1672-0229},
doi = {https://doi.org/10.1016/j.gpb.2012.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S1672022912000745},
author = {Wuju Li and Xiaomin Ying and Qixuan Lu and Linxi Chen},
keywords = {Bacterial, sRNA, Target, Bioinformatics, Prediction},
abstract = {Bacterial small RNAs (sRNAs) are an emerging class of regulatory RNAs of about 40–500 nucleotides in length and, by binding to their target mRNAs or proteins, get involved in many biological processes such as sensing environmental changes and regulating gene expression. Thus, identification of bacterial sRNAs and their targets has become an important part of sRNA biology. Current strategies for discovery of sRNAs and their targets usually involve bioinformatics prediction followed by experimental validation, emphasizing a key role for bioinformatics prediction. Here, therefore, we provided an overview on prediction methods, focusing on the merits and limitations of each class of models. Finally, we will present our thinking on developing related bioinformatics models in future.}
}
@article{HOLSAPPLE2014130,
title = {A unified foundation for business analytics},
journal = {Decision Support Systems},
volume = {64},
pages = {130-141},
year = {2014},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2014.05.013},
url = {https://www.sciencedirect.com/science/article/pii/S0167923614001730},
author = {Clyde Holsapple and Anita Lee-Post and Ram Pakath},
keywords = {Analytics, Business analytics, Business intelligence, Decision making, Decision support, Evidence-based},
abstract = {Synthesizing prior research, this paper designs a relatively comprehensive and holistic characterization of business analytics – one that serves as a foundation on which researchers, practitioners, and educators can base their studies of business analytics. As such, it serves as an initial ontology for business analytics as a field of study. The foundation has three main parts dealing with the whence and whither of business analytics: identification of dimensions along which business analytics possibilities can be examined, derivation of a six-class taxonomy that covers business analytics perspectives in the literature, and design of an inclusive framework for the field of business analytics. In addition to unifying the literature, a major contribution of the designed framework is that it can stimulate thinking about the nature, roles, and future of business analytics initiatives. We show how this is done by deducing a host of unresolved issues for consideration by researchers, practitioners, and educators. We find that business analytics involves issues quite aside from data management, number crunching, technology use, systematic reasoning, and so forth.}
}
@article{HUNG20131,
title = {Conceptual Recombination: A method for producing exploratory and transformational creativity in creative works},
journal = {Knowledge-Based Systems},
volume = {53},
pages = {1-12},
year = {2013},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2013.07.007},
url = {https://www.sciencedirect.com/science/article/pii/S0950705113002098},
author = {Edward C.K. Hung and Clifford S.T. Choy},
keywords = {Conceptual Recombination, Creative work ontology, Creative method, Computational creativity, Creativity Support Tools},
abstract = {Computational creativity researchers have long been searching for a reliable creative method of generating transformational creativity in Creativity Support Tools, in vain, especially when these systems are supposed to take in a user’s unfinished creative work and produce representational and creative outputs as continuations to the user input. In this paper we propose a new creative method called Conceptual Recombination to take up this challenge. We first define creative work for this study followed by creative work ontology to be the theoretical background of Conceptual Recombination. We further refer to application ontology and regard Conceptual Recombination as the task model for creative work ontology. In this task model there are three levels of prediction leading to the formations of output features, output structures, and their combinations as the final system outputs constrained by rules, biases, and homeomorphism. Furthermore, this new creative method allows the use of exploratory creativity on structures and transformational creativity on features to attain a balance between usefulness and novelty in system outputs. A 7-tuple computational model and the search mechanisms for exploratory and transformational creativity are also defined for it. Lastly, we evaluate Conceptual Recombination with our case study about producing a 2-dimensional asymmetrical shape with a given symmetrical shape to demonstrate its practicality and conclude that it not only offers a new reliable creative method for Creativity Support Tools, but also provides an objective evaluation method for transformational creativity.}
}
@article{KENETT201879,
title = {Driving the brain towards creativity and intelligence: A network control theory analysis},
journal = {Neuropsychologia},
volume = {118},
pages = {79-90},
year = {2018},
note = {The neural bases of creativity and intelligence: common grounds and differences},
issn = {0028-3932},
doi = {https://doi.org/10.1016/j.neuropsychologia.2018.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S0028393218300010},
author = {Yoed N. Kenett and John D. Medaglia and Roger E. Beaty and Qunlin Chen and Richard F. Betzel and Sharon L. Thompson-Schill and Jiang Qiu},
keywords = {Creativity, Intelligence, Network control theory, Cognitive control},
abstract = {High-level cognitive constructs, such as creativity and intelligence, entail complex and multiple processes, including cognitive control processes. Recent neurocognitive research on these constructs highlight the importance of dynamic interaction across neural network systems and the role of cognitive control processes in guiding such a dynamic interaction. How can we quantitatively examine the extent and ways in which cognitive control contributes to creativity and intelligence? To address this question, we apply a computational network control theory (NCT) approach to structural brain imaging data acquired via diffusion tensor imaging in a large sample of participants, to examine how NCT relates to individual differences in distinct measures of creative ability and intelligence. Recent application of this theory at the neural level is built on a model of brain dynamics, which mathematically models patterns of inter-region activity propagated along the structure of an underlying network. The strength of this approach is its ability to characterize the potential role of each brain region in regulating whole-brain network function based on its anatomical fingerprint and a simplified model of node dynamics. We find that intelligence is related to the ability to “drive” the brain system into easy to reach neural states by the right inferior parietal lobe and lower integration abilities in the left retrosplenial cortex. We also find that creativity is related to the ability to “drive” the brain system into difficult to reach states by the right dorsolateral prefrontal cortex (inferior frontal junction) and higher integration abilities in sensorimotor areas. Furthermore, we found that different facets of creativity—fluency, flexibility, and originality—relate to generally similar but not identical network controllability processes. We relate our findings to general theories on intelligence and creativity.}
}
@article{KASALICA20212157,
title = {APE in the Wild: Automated Exploration of Proteomics Workflows in the bio.tools Registry},
journal = {Journal of Proteome Research},
volume = {20},
number = {4},
pages = {2157-2165},
year = {2021},
issn = {1535-3907},
doi = {https://doi.org/10.1021/acs.jproteome.0c00983},
url = {https://www.sciencedirect.com/science/article/pii/S1535390721002031},
author = {Vedran Kasalica and Veit Schwämmle and Magnus Palmblad and Jon Ison and Anna-Lena Lamprecht},
keywords = {proteomics, scientific workflows, computational pipelines, workflow exploration, automated workflow composition, semantic tool annotation},
abstract = {The bio.tools registry is a main catalogue of computational tools in the life sciences. More than 17 000 tools have been registered by the international bioinformatics community. The bio.tools metadata schema includes semantic annotations of tool functions, that is, formal descriptions of tools’ data types, formats, and operations with terms from the EDAM bioinformatics ontology. Such annotations enable the automated composition of tools into multistep pipelines or workflows. In this Technical Note, we revisit a previous case study on the automated composition of proteomics workflows. We use the same four workflow scenarios but instead of using a small set of tools with carefully handcrafted annotations, we explore workflows directly on bio.tools. We use the Automated Pipeline Explorer (APE), a reimplementation and extension of the workflow composition method previously used. Moving “into the wild” opens up an unprecedented wealth of tools and a huge number of alternative workflows. Automated composition tools can be used to explore this space of possibilities systematically. Inevitably, the mixed quality of semantic annotations in bio.tools leads to unintended or erroneous tool combinations. However, our results also show that additional control mechanisms (tool filters, configuration options, and workflow constraints) can effectively guide the exploration toward smaller sets of more meaningful workflows.
}
}
@article{SCHULZ20197,
title = {The algorithmic architecture of exploration in the human brain},
journal = {Current Opinion in Neurobiology},
volume = {55},
pages = {7-14},
year = {2019},
note = {Machine Learning, Big Data, and Neuroscience},
issn = {0959-4388},
doi = {https://doi.org/10.1016/j.conb.2018.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S0959438818300904},
author = {Eric Schulz and Samuel J. Gershman},
abstract = {Balancing exploration and exploitation is one of the central problems in reinforcement learning. We review recent studies that have identified multiple algorithmic strategies underlying exploration. In particular, humans use a combination of random and uncertainty-directed exploration strategies, which rely on different brain systems, have different developmental trajectories, and are sensitive to different task manipulations. Humans are also able to exploit sophisticated structural knowledge to aid their exploration, such as information about correlations between options. New computational models, drawing inspiration from machine learning, have begun to formalize these ideas and offer new ways to understand the neural basis of reinforcement learning.}
}
@article{MITTERAUER199899,
title = {An interdisciplinary approach towards a theory of consciousness},
journal = {Biosystems},
volume = {45},
number = {2},
pages = {99-121},
year = {1998},
issn = {0303-2647},
doi = {https://doi.org/10.1016/S0303-2647(97)00070-1},
url = {https://www.sciencedirect.com/science/article/pii/S0303264797000701},
author = {Bernhard Mitterauer},
keywords = {Reflection processes, Glial–neuronal interaction, Glial boundary-setting function, Tree of reflection, Self-systems},
abstract = {Instead of attacking the difficult problem of consciousness or self-consciousness directly, the theory is based on the more basic concept of reflection. A concept of reflection is suggested on four levels (recursion, reflective thinking, self-reflection, intersubjective reflection). We propose the glial–neuronal interaction as a neurobiological substrate for reflection processes. It is assumed that glia have a boundary-setting function (scaffolding, compartmentalization) in the spatio–temporal interaction with the neurons. This function could be a possible mechanism of `dividing' the brain into different self-systems each with their own capacity of self-organization. Although the brain's different self-systems are normally integrated, they may disintegrate and show themselves in special states of the brain (e.g. multiple personality disorder). A tree of reflection consisting of a number of places (ontological loci) on which reflection processes of varying complexity take place, is suggested as the formal model. Finally, the problem of self-conscious qualitative experience (Qualia) is discussed in terms of the reflection model.}
}
@article{CHECHURIN2016119,
title = {Understanding TRIZ through the review of top cited publications},
journal = {Computers in Industry},
volume = {82},
pages = {119-134},
year = {2016},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2016.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S0166361516301129},
author = {Leonid Chechurin and Yuri Borgianni},
keywords = {TRIZ, Conceptual design, Industrial practice, Information processing, Computer-Aided Innovation},
abstract = {The development of the Theory of Inventive Problem Solving (TRIZ) has not followed the usual patterns of scientific validation required by engineering methods. Consequently, its outreach within engineering design is interpreted differently in the scholarly community. At the same time, the claimed powerful support in tackling technical problems of any degree of difficulty conflicts with TRIZ diffusion in industrial settings, which is relatively low according to insights into product development practices. The mismatch between ambitious goals and moderate spill-over benefits in the industry ranges among the various open issues concerning TRIZ, its way of thinking, its effectiveness, the usability of its tools. In order to provide a general overview of TRIZ in science, the authors have attempted to analyse reliable and influential sources from the literature. The performed survey includes the top 100 indexed publications concerning TRIZ, according to the number of received citations. Variegated and poorly interconnected research directions emerge in the abundant literature that tackles TRIZ-related topics. The outcomes of the investigation highlight the successful implementation of TRIZ within, among the others, biomimetics and information processing. The traditional borders of mechanical and industrial engineering have been frequently crossed, as the use of TRIZ is also witnessed in the domain of business and services. At the same time, computer-aided platforms represent diffused attempts to boost TRIZ diffusion and applicability.}
}
@article{HUANG2019592,
title = {Challenges, opportunities and paradigm of applying big data to production safety management: From a theoretical perspective},
journal = {Journal of Cleaner Production},
volume = {231},
pages = {592-599},
year = {2019},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2019.05.245},
url = {https://www.sciencedirect.com/science/article/pii/S0959652619317810},
author = {Lang Huang and Chao Wu and Bing Wang},
keywords = {Big data, Production safety management, Big-data-driven, Challenges, Opportunities},
abstract = {Big data has caused the scientific community to re-examine the scientific research methodologies and has triggered a revolution in scientific thinking. As a branch of scientific research, production safety management is also exploring methods to take advantage of big data. This research aims to provide a theoretical basis for promoting the application of big data in production safety management. First, four different types of production safety management paradigms were identified, namely small-data-based, static-oriented, interpretation-based and causal-oriented paradigm, and the challenges to these paradigms in the presence of big data were introduced. Second, the opportunities of employing big data in production safety management were identified from four aspects, including better predict the future production safety phenomena, promote production safety management highlight relevance, achieve the balance between deductive and inductive approaches and promote the interdisciplinary development of production safety management. Third, the paradigm shifting trend of production safety management was concluded, and the discipline foundation of the new paradigm was considered as the integration of data science, production management and safety science. Fourth, a new big-data-driven production safety management paradigm was developed, which consists of the logical line of production safety management, the macro-meso-micro data spectrum, the key big data analytics, and the four-dimensional morphology. At last, the strengths (e.g., supporting better-informed safety description, safety inquisition, safety prediction) and future research direction (e.g., theory research focuses on safety-related data mining/capturing/cleansing) of the new paradigm were discussed. The research results not only can provide theoretical and practical basis for big-data-driven production safety management, but also can offer advice to managerial consideration and scholarly investigation.}
}
@article{ARBIB201883,
title = {From cybernetics to brain theory, and more: A memoir},
journal = {Cognitive Systems Research},
volume = {50},
pages = {83-145},
year = {2018},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2018.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S1389041718301360},
author = {Michael A. Arbib},
keywords = {Action-oriented perception, Ape, Architecture, Artificial intelligence, Automata theory, Basal ganglia, Brain theory, Cerebellum, Cerebral cortex, Cognitive science, Computational neuroscience, Cybernetics, Frog, Hippocampus, Human, Language evolution, Linguistics, Monkey, Rat, Robotics, Schema theory, Social implications, Systems theory, Theological implications},
abstract = {While structured as an autobiography, this memoir exemplifies ways in which classic contributions to cybernetics (e.g., by Wiener, McCulloch & Pitts, and von Neumann) have fed into a diversity of current research areas, including the mathematical theory of systems and computation, artificial intelligence and robotics, computational neuroscience, linguistics, and cognitive science. The challenges of brain theory receive special emphasis. Action-oriented perception and schema theory complement neural network modeling in analyzing cerebral cortex, cerebellum, hippocampus, and basal ganglia. Comparative studies of frog, rat, monkey, ape and human not only deepen insights into the human brain but also ground an EvoDevoSocio view of “how the brain got language.” The rapprochement between neuroscience and architecture provides a recent challenge. The essay also assesses some of the social and theological implications of this broad perspective.}
}
@incollection{BARTHEYE2025373,
title = {16 - Self-visualization for the human–machine mind–body problem},
editor = {William Lawless and Ranjeev Mittu and Donald Sofge and Hesham Fouad},
booktitle = {Interdependent Human-Machine Teams},
publisher = {Academic Press},
pages = {373-402},
year = {2025},
isbn = {978-0-443-29246-0},
doi = {https://doi.org/10.1016/B978-0-443-29246-0.00013-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780443292460000134},
author = {Olivier Bartheye and Laurent Chaudron},
keywords = {Consistency, Descartes' states, Human–machine, Local monism, Maximum entropy production, Meta-consciousness, Mind/body problem, Structural entropy production, Trialism},
abstract = {Recent machine-learning results and consciousness, while impressive, have nothing in common while rich human/machine interactions require an interpretation of sense-making. From their experience in symbolic AI, the authors attempt to address this global issue from an old philosophical question, the so-called mind-body problem, according to three distinct points of view: the logical one from proof theory; the informational one according to computer science; and, finally, the third: the physical one (including quantum physics and relativity), to study this mysterious missing link, and its corresponding aporia. The intention is to propose a formal mathematical framework to unify these three points of view by giving absolute priority to virtual representations, that is, abstract representations free from “real” considerations. In effect, our original motivation concerns the necessity to qualify precisely what a decision means to be automatized, a quite old question proposed from the early days of Artificial Intelligence at the Dartmouth summer research project in 1956. The artificial feature justifies a start from natural models based on experimentation, but relaxing them as much as possible to favor axioms from abstract models disconnected from reality. Unsurprisingly, all unsolvable mathematical, logical, physical, and metaphysical issues reappear, notably the dualism versus monism debate, to be reinterpreted on neutral grounds. Starting from our assumption in previous works, where a decision is a process able to fill a causal break, our proposal orients the representation of a decision as a collapse of the wave function as a measurement process acting on twisted consciousness, a pulsating form of consciousness. Applied mathematically to the human–machine problem, which is our goal, we expect to find an energy-entropy tradeoff: Well-structured interactions require less energy, operating at a lower cost characterized by low entropy; as a result, the interacting human–machine team is able to apply its maximum available energy to its performance, characterized by greater entropy, reaching a maximum.}
}
@article{LAPIDUS202183,
title = {The road less traveled in protein folding: evidence for multiple pathways},
journal = {Current Opinion in Structural Biology},
volume = {66},
pages = {83-88},
year = {2021},
note = {Centrosomal Organization and Assemblies ● Folding and Binding},
issn = {0959-440X},
doi = {https://doi.org/10.1016/j.sbi.2020.10.012},
url = {https://www.sciencedirect.com/science/article/pii/S0959440X20301809},
author = {Lisa J Lapidus},
abstract = {Free Energy Landscape theory of Protein Folding, introduced over 20 years ago, implies that a protein has many paths to the folded conformation with the lowest free energy. Despite the knowledge in principle, it has been remarkably hard to detect such pathways. The lack of such observations is primarily due to the fact that no one experimental technique can detect many parts of the protein simultaneously with the time resolution necessary to see such differences in paths. However, recent technical developments and employment of multiple experimental probes and folding prompts have illuminated multiple folding pathways in a number of proteins that had all previously been described with a single path.}
}
@article{ZHAO2023100521,
title = {Toward parallel intelligence: An interdisciplinary solution for complex systems},
journal = {The Innovation},
volume = {4},
number = {6},
pages = {100521},
year = {2023},
issn = {2666-6758},
doi = {https://doi.org/10.1016/j.xinn.2023.100521},
url = {https://www.sciencedirect.com/science/article/pii/S2666675823001492},
author = {Yong Zhao and Zhengqiu Zhu and Bin Chen and Sihang Qiu and Jincai Huang and Xin Lu and Weiyi Yang and Chuan Ai and Kuihua Huang and Cheng He and Yucheng Jin and Zhong Liu and Fei-Yue Wang},
abstract = {The growing complexity of real-world systems necessitates interdisciplinary solutions to confront myriad challenges in modeling, analysis, management, and control. To meet these demands, the parallel systems method rooted in the artificial systems, computational experiments, and parallel execution (ACP) approach has been developed. The method cultivates a cycle termed parallel intelligence, which iteratively creates data, acquires knowledge, and refines the actual system. Over the past two decades, the parallel systems method has continuously woven advanced knowledge and technologies from various disciplines, offering versatile interdisciplinary solutions for complex systems across diverse fields. This review explores the origins and fundamental concepts of the parallel systems method, showcasing its accomplishments as a diverse array of parallel technologies and applications while also prognosticating potential challenges. We posit that this method will considerably augment sustainable development while enhancing interdisciplinary communication and cooperation.}
}
@article{CYSEWSKI201623,
title = {Efficacy of bi-component cocrystals and simple binary eutectics screening using heat of mixing estimated under super cooled conditions},
journal = {Journal of Molecular Graphics and Modelling},
volume = {68},
pages = {23-28},
year = {2016},
issn = {1093-3263},
doi = {https://doi.org/10.1016/j.jmgm.2016.06.003},
url = {https://www.sciencedirect.com/science/article/pii/S1093326316300870},
author = {Piotr Cysewski},
keywords = {Cocrystals, Simple binary eutectics, Theoretical screening, Heat of mixing, Confusion matrix, COSMO-RS, COSMOtherm},
abstract = {The values of excess heat characterizing sets of 493 simple binary eutectic mixtures and 965 cocrystals were estimated under super cooled liquid condition. The application of a confusion matrix as a predictive analytical tool was applied for distinguishing between the two subsets. Among seven considered levels of computations the BP-TZVPD-FINE approach was found to be the most precise in terms of the lowest percentage of misclassified positive cases. Also much less computationally demanding AM1 and PM7 semiempirical quantum chemistry methods are likewise worth considering for estimation of the heat of mixing values. Despite intrinsic limitations of the approach of modeling miscibility in the solid state, based on components affinities in liquids under super cooled conditions, it is possible to define adequate criterions for classification of coformers pairs as simple binary eutectics or cocrystals. The predicted precision has been found as 12.8% what is quite accepted, bearing in mind simplicity of the approach. However, tuning theoretical screening to such precision implies the exclusion of many positive cases and this wastage exceeds 31% of cocrystals classified as false negatives.}
}
@article{SHAH201494,
title = {Towards a Managed Aquifer Recharge strategy for Gujarat, India: An economist’s dialogue with hydro-geologists},
journal = {Journal of Hydrology},
volume = {518},
pages = {94-107},
year = {2014},
note = {Creating Partnerships Between Hydrology and Social Science: A Priority for Progress},
issn = {0022-1694},
doi = {https://doi.org/10.1016/j.jhydrol.2013.12.022},
url = {https://www.sciencedirect.com/science/article/pii/S0022169413009190},
author = {Tushaar Shah},
keywords = {Managed Aquifer Recharge, Energy-irrigation nexus, Groundwater depletion, Groundwater economics},
abstract = {Summary
Gujarat state in Western India exemplifies all challenges of an agrarian economy founded on groundwater overexploitation sustained over decades by perverse energy subsidies. Major consequences are: secular decline in groundwater levels, deterioration of groundwater quality, rising energy cost of pumping, soaring carbon footprint of agriculture and growing financial burden of energy subsidies. In 2009, Government of Gujarat asked the present author, an economist, to chair a Taskforce of senior hydro-geologists and civil engineers to develop and recommend a Managed Aquifer Recharge (MAR) strategy for the state. This paper summarizes the recommended strategy and its underlying logic. It also describes the imperfect fusion of socio-economic and hydro-geologic perspectives that occurred in course of the working of the Taskforce and highlights the need for trans-disciplinary perspectives on groundwater governance.}
}
@article{JI2023126734,
title = {Experimental and numerical investigation on a radiative cooling driving thermoelectric generator system},
journal = {Energy},
volume = {268},
pages = {126734},
year = {2023},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2023.126734},
url = {https://www.sciencedirect.com/science/article/pii/S0360544223001287},
author = {Yishuang Ji and Song Lv},
keywords = {Radiative cooling, Thermoelectric generator, Hybrid system, Thermal-electrical properties, Numerical analysis},
abstract = {Thermoelectric (TE) technology and radiative sky cooling (RSC) technology have proven to be a promising and green way to harvest energy from the environment. Combining RSC technology with Thermoelectric generator (TEG) device for passive power generation at night is meaningful and remains a challenge. Here, a radiative sky cooling driving thermoelectric generator (RSC-TE) system integrated by a doped modified TiO2/PMMA radiative cooling film, a commercial TEG, and an aluminum heat sink is developed, with a simple structure, low cost and high efficiency. The thermal-electrical performance of the RSC-TE system was evaluated through a consecutive nighttime experiment. Experimental results show that the temperature of the cold side of the TEG in contact with the radiative cooler is 2.7–4.2 °C lower than the ambient temperature, and the temperature difference between the hot and cold sides of TEG is 2.3–3.2 °C. The temperature difference at 00:00 can reach 2.5 °C, which corresponds to an open circuit voltage of 87 mV. Furthermore, a 3D model has been established by COMSOL software to investigate the effects of different environmental parameters and component-related parameters on system performance, which has guiding significance for the improvement and optimization of the experimental setup. This study can provide a new thinking and some practical guidelines for the design and application of the RSC-TE system.}
}
@article{MAHY201468,
title = {How and where: Theory-of-mind in the brain},
journal = {Developmental Cognitive Neuroscience},
volume = {9},
pages = {68-81},
year = {2014},
issn = {1878-9293},
doi = {https://doi.org/10.1016/j.dcn.2014.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S1878929314000048},
author = {Caitlin E.V. Mahy and Louis J. Moses and Jennifer H. Pfeifer},
keywords = {Theory of mind, Neuroimaging, Modularity, Theory theory, Simulation, Executive functioning},
abstract = {Theory of mind (ToM) is a core topic in both social neuroscience and developmental psychology, yet theory and data from each field have only minimally constrained thinking in the other. The two fields might be fruitfully integrated, however, if social neuroscientists sought evidence directly relevant to current accounts of ToM development: modularity, simulation, executive, and theory theory accounts. Here we extend the distinct predictions made by each theory to the neural level, describe neuroimaging evidence that in principle would be relevant to testing each account, and discuss such evidence where it exists. We propose that it would be mutually beneficial for both fields if ToM neuroimaging studies focused more on integrating developmental accounts of ToM acquisition with neuroimaging approaches, and suggest ways this might be achieved.}
}
@article{CARDONAVASQUEZ2024110267,
title = {Enhancing time series aggregation for power system optimization models: Incorporating network and ramping constraints},
journal = {Electric Power Systems Research},
volume = {230},
pages = {110267},
year = {2024},
issn = {0378-7796},
doi = {https://doi.org/10.1016/j.epsr.2024.110267},
url = {https://www.sciencedirect.com/science/article/pii/S037877962400155X},
author = {David Cardona-Vasquez and Thomas Klatzer and Bettina Klinz and Sonja Wogrin},
keywords = {Power systems optimization, Mathematical modeling, Dimensionality reduction, Renewable energy sources, Time series aggregation, Linear programming},
abstract = {In this paper, we extend a recently developed Basis-Oriented time series aggregation approach for aggregating input-data in power system optimization models which has proven to be exact in simple economic dispatch problems. We extend this methodology to include network and ramping constraints, for the latter, to handle temporal linking, we developed a heuristic that, in its current version, relies on the dual solution to find a partition of the input data, which is then aggregated. Our numerical results, for a 3-bus system, show that with network constraints only, we reduced the number of hours needed for an exact approximation by a factor of 1747, and a factor of 12 with network and ramping constraints. Moreover, our findings suggest that in the presence of temporal linking, aggregations of variable length must be employed to obtain an exact result (i.e., the same objective function value in the aggregated model) while maintaining the computational tractability. Our findings also imply that better performing aggregations do not necessarily correspond to commonly used lengths like days or weeks; additionally, we also prove that this input-data partition, based on the dual information, is always possible for these models independent of their size.}
}
@article{LAU2017241,
title = {The many worlds hypothesis of dopamine prediction error: implications of a parallel circuit architecture in the basal ganglia},
journal = {Current Opinion in Neurobiology},
volume = {46},
pages = {241-247},
year = {2017},
note = {Computational Neuroscience},
issn = {0959-4388},
doi = {https://doi.org/10.1016/j.conb.2017.08.015},
url = {https://www.sciencedirect.com/science/article/pii/S0959438817301587},
author = {Brian Lau and Tiago Monteiro and Joseph J Paton},
abstract = {Computational models of reinforcement learning (RL) strive to produce behavior that maximises reward, and thus allow software or robots to behave adaptively [1]. At the core of RL models is a learned mapping between ‘states’—situations or contexts that an agent might encounter in the world—and actions. A wealth of physiological and anatomical data suggests that the basal ganglia (BG) is important for learning these mappings [2, 3]. However, the computations performed by specific circuits are unclear. In this brief review, we highlight recent work concerning the anatomy and physiology of BG circuits that suggest refinements in our understanding of computations performed by the basal ganglia. We focus on one important component of basal ganglia circuitry, midbrain dopamine neurons, drawing attention to data that has been cast as supporting or departing from the RL framework that has inspired experiments in basal ganglia research over the past two decades. We suggest that the parallel circuit architecture of the BG might be expected to produce variability in the response properties of different dopamine neurons, and that variability in response profile may not reflect variable functions, but rather different arguments that serve as inputs to a common function: the computation of prediction error.}
}
@article{WANG2025127400,
title = {Research on product design improvement method based on online review and improvement importance performance competitor analysis},
journal = {Expert Systems with Applications},
volume = {279},
pages = {127400},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.127400},
url = {https://www.sciencedirect.com/science/article/pii/S095741742501022X},
author = {Zeng Wang and Shi-jie Hu and Shi-fan Niu and Si-yi Li and Wei-dong Liu and Ling-yu Huang},
keywords = {Online reviews, Product design improvement, Importance-performance competitor analysis, Deep learning, TRIZ},
abstract = {To mitigate the shortcomings observed in current online review-based Importance-Performance Competitor Analysis (IPCA) model and the limitations inherent in extant performance evaluation algorithms, this research proposes a novel method for product design improvement leveraging online review-based Textual Importance-Performance Competitor Analysis (TIPCA). Initially, product attributes and their respective sentiment polarities within online reviews are scrutinized using the GRU-CAP, a composite deep learning model. Subsequently, the TIPCA model facilitates the computation of performance metrics and significance, leading to the formulation of a three-dimensional analysis graph that delineates critical avenues for product enhancement. The application of TRIZ then guides the derivation of an optimized product design proposal. Empirical assessments utilizing a neck massager and a new energy vehicle demonstrate the method’s adeptness in accurately identifying and articulating pathways for product enhancement. This approach provides designers with a robust and innovative blueprint for design, enhancing creative development. The method facilitates a meticulous analysis and strategic refinement of product designs, anchored in a comprehensive theoretical framework and corroborated by practical implementation.}
}