@article{SUN2019104141,
title = {Formal system interactive failure analysis method based on systems theoretic process analysis model},
journal = {Engineering Failure Analysis},
volume = {106},
pages = {104141},
year = {2019},
issn = {1350-6307},
doi = {https://doi.org/10.1016/j.engfailanal.2019.08.007},
url = {https://www.sciencedirect.com/science/article/pii/S1350630718314973},
author = {Rui Sun and Deming Zhong and Weigang Li},
keywords = {Engine failures, Aircraft failures, Failure analysis},
abstract = {Interactive failures are failures caused by two or more components that often occur in complex systems when the system is modified, upgraded, or simply designed inadequately. However, the official guideline provided, namely, common cause analysis, cannot discover these problems. It cannot establish a complex interactive system model, nor can it provide a unified analysis method for all parts of the system. Another method, systems theoretic process analysis, is limited to the control system. To solve this problem, a method called system theoretic formal analysis method (STFAM) is proposed in this paper. STFAM establishes a system-component-interactive model that provides an abundance of interactive information for failure analysis and presents a unified model to support the analysis of multiple components in the system. It is divided into three steps. First, a hierarchical system structure is built and then transformed into a formalized state machine. Next, the interactive failures are determined and converted into a linear temporal logic or computation tree logic model. Finally, NuSMV is used to verify the model and record the results. To evaluate the proposed method, a practical problem that occurred in full-authority digital engine control, in which in some cases, the valve closes for unknown reasons until the system is reset is presented. An analysis of the issue demonstrates the effectiveness of our method.}
}
@article{MOHAMED2010317,
title = {Investigating Number Sense Among Students},
journal = {Procedia - Social and Behavioral Sciences},
volume = {8},
pages = {317-324},
year = {2010},
note = {International Conference on Mathematics Education Research 2010 (ICMER 2010)},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2010.12.044},
url = {https://www.sciencedirect.com/science/article/pii/S187704281002149X},
author = {Mohini Mohamed and Jacinta Johnny},
keywords = {Number sense, Mental computation, Number sense test, Number sense framework},
abstract = {Number sense can be described as good intuition about numbers and their relationships. Individuals with good number sense tend to exhibit the following characteristics when performing mental computations; sense-making approach, planning and control, flexibility and appropriateness sense of reasonableness. This is a very important skill to be mastered by every individual to enable them to handle numerical problems in their daily life. Students rarely face problems with algorithms. Unfortunately, many studies have showed that students have poor understanding in making sense on numbers when tested on their competency in number sense component. This study aims to investigate if there is a relationship between student performance in number sense and mathematics achievement and to explore the components of number sense that students are weak in.}
}
@incollection{HASIJA2023247,
title = {Chapter 11 - Bioinformatics workflow management systems},
editor = {Yasha Hasija},
booktitle = {All About Bioinformatics},
publisher = {Academic Press},
pages = {247-265},
year = {2023},
isbn = {978-0-443-15250-4},
doi = {https://doi.org/10.1016/B978-0-443-15250-4.00006-X},
url = {https://www.sciencedirect.com/science/article/pii/B978044315250400006X},
author = {Yasha Hasija},
keywords = {Galaxy, GenePattern, Image analysis, KNIME, LINCS tools, NextFlow},
abstract = {In the discipline of bioinformatics, a flow of work, or a sequence of computational or analytical tasks, is managed by a bioinformatics workflow management system, which is a subtype of a workflow automation system. This type of system is used to construct and manage the flow of work. There are numerous different work process situations available at this time. Some of them have been developed with the intention that scholars in a variety of subjects, such as cosmology and geology, will be able to make use of them as frameworks for logical work processes. Workflow frameworks such as Galaxy, GenePattern, KNIME, LINCS Tools, image analysis, and NextFlow are discussed in this chapter.}
}
@article{FIROMSAWAKUMA2024,
title = {Advanced tuberculosis diagnosis system: Integrating case-based reasoning with nearest neighbor algorithm},
journal = {Indian Journal of Tuberculosis},
year = {2024},
issn = {0019-5707},
doi = {https://doi.org/10.1016/j.ijtb.2024.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S0019570724002464},
author = {Amelework {Firomsa Wakuma}},
keywords = {Tuberculosis, Expert system, Case based reasoning, Nearest Neighbor Algorithm},
abstract = {Background
A serious infectious illness with a high morbidity and death rate worldwide, tuberculosis (TB) is more prevalent in low- and middle-income nations. Although there are a number of diagnostic techniques, the most only address tuberculosis in the lung and ignore drug-resistant strains (MDR-TB, XDR-TB) as well as tuberculosis lymphadenitis. A thorough diagnostic system that covers all types of tuberculosis is essential.
Objectives
To enhance TB diagnosis, particularly pulmonary TB, lymphadenitis, and drug-resistant TB, this study offers an expert system based on Case-Based Reasoning (CBR) and the Nearest Neighbor Algorithm.
Methods
Information was gathered from hospital records of prior tuberculosis cases, including 43 cases from Debre Tabor General Hospital. In addition to document analysis, information was acquired through both structured and unstructured interviews with medical specialists. The R4 model—Retrieve, Reuse, Revise, and Retain—is followed by the system architecture. Recall, expert acceptance, and precision were among the evaluation metrics.
Results
The system had an 86.5% expert acceptance rate, 84.7% precision, and 75.3% recall. Compared to previous medical diagnostic methods, it shown a notable improvement, especially in diagnosing mental health and hypertension.
Conclusion
By combining Case-Based Reasoning and the Nearest Neighbor Algorithm, it is possible to diagnose tuberculosis (TB) more effectively and with greater accuracy. This integration also makes it possible to diagnose cases that are resistant to drugs. In order to improve the system's performance even more, future research may investigate the integration of additional reasoning strategies.}
}
@article{WANG2016377,
title = {ACP-based social computing and parallel intelligence: Societies 5.0 and beyond},
journal = {CAAI Transactions on Intelligence Technology},
volume = {1},
number = {4},
pages = {377-393},
year = {2016},
issn = {2468-2322},
doi = {https://doi.org/10.1016/j.trit.2016.11.005},
url = {https://www.sciencedirect.com/science/article/pii/S246823221630083X},
author = {Xiao Wang and Lingxi Li and Yong Yuan and Peijun Ye and Fei-Yue Wang},
keywords = {Social computing, Societies 5.0, Parallel intelligence, Knowledge automation, Cyber-physical-social system, Artificial societies, Computational experiments, Parallel execution},
abstract = {Social computing, as the technical foundation of future computational smart societies, has the potential to improve the effectiveness of open-source big data usage, systematically integrate a variety of elements including time, human, resources, scenarios, and organizations in the current cyber-physical-social world, and establish a novel social structure with fair information, equal rights, and a flat configuration. Meanwhile, considering the big modeling gap between the model world and the physical world, the concept of parallel intelligence is introduced. With the help of software-defined everything, parallel intelligence bridges the big modeling gap by means of constructing artificial systems where computational experiments can be implemented to verify social policies, economic strategies, and even military operations. Artificial systems play the role of “social laboratories” in which decisions are computed before they are executed in our physical society. Afterwards, decisions with the expected outputs are executed in parallel in both the artificial and physical systems to interactively sense, compute, evaluate and adjust system behaviors in real-time, leading system behaviors in the physical system converging to those proven to be optimal in the artificial ones. Thus, the smart guidance and management for our society can be achieved.}
}
@article{PYKA2024107668,
title = {Unlocking the potential of higher-molecular-weight 5-HT7R ligands: Synthesis, affinity, and ADMET examination},
journal = {Bioorganic Chemistry},
volume = {151},
pages = {107668},
year = {2024},
issn = {0045-2068},
doi = {https://doi.org/10.1016/j.bioorg.2024.107668},
url = {https://www.sciencedirect.com/science/article/pii/S004520682400573X},
author = {Patryk Pyka and Sabrina Garbo and Aleksandra Murzyn and Grzegorz Satała and Artur Janusz and Michał Górka and Wojciech Pietruś and Filip Mituła and Delfina Popiel and Maciej Wieczorek and Biagio Palmisano and Alessia Raucci and Andrzej J. Bojarski and Clemens Zwergel and Ewa Szymańska and Katarzyna Kucwaj-Brysz and Cecilia Battistelli and Jadwiga Handzlik and Sabina Podlewska},
keywords = {Serotonin receptor 5-HT, G protein-coupled receptors, ADMET properties, Docking, Molecular modelling,  experiments, MTS assay, Gene expression assay},
abstract = {An increasing number of drugs introduced to the market and numerous repositories of compounds with confirmed activity have posed the need to revalidate the state-of-the-art rules that determine the ranges of properties the compounds should possess to become future drugs. In this study, we designed a series of two chemotypes of aryl-piperazine hydantoin ligands of 5-HT7R, an attractive target in search for innovative CNS drugs, with higher molecular weight (close to or over 500). Consequently, 14 new compounds were synthesised and screened for their receptor activity accompanied by extensive docking studies to evaluate the observed structure–activity/properties relationships. The ADMET characterisation in terms of the biological membrane permeability, metabolic stability, hepatotoxicity, cardiotoxicity, and protein plasma binding of the obtained compounds was carried out in vitro. The outcome of these studies constituted the basis for the comprehensive challenge of computational tools for ADMET properties prediction. All the compounds possessed high affinity to the 5-HT7R (Ki below 250 nM for all analysed structures) with good selectivity over 5-HT6R and varying affinity towards 5-HT2AR, 5-HT1AR and D2R. For the best compounds of this study, the expression profile of genes associated with neurodegeneration, anti-oxidant response and anti-inflammatory function was determined, and the survival of the cells (SH-SY5Y as an in vitro model of Alzheimer’s disease) was evaluated. One 5-HT7R agent (32) was characterised by a very promising ADMET profile, i.e. good membrane permeability, low hepatotoxicity and cardiotoxicity, and high metabolic stability with the simultaneous high rate of plasma protein binding and high selectivity over other GPCRs considered, together with satisfying gene expression profile modulations and neural cell survival. Such encouraging properties make it a good candidate for further testing and optimisation as a potential agent in the treatment of CNS-related disorders.}
}
@article{DIAZBERRIOS2022100953,
title = {High school student understanding of exponential and logarithmic functions},
journal = {The Journal of Mathematical Behavior},
volume = {66},
pages = {100953},
year = {2022},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2022.100953},
url = {https://www.sciencedirect.com/science/article/pii/S0732312322000219},
author = {Tomás Díaz-Berrios and Rafael Martínez-Planell},
keywords = {APOS theory, Exponentiation, Logarithm, Rational exponents, Exponential and logarithmic functions},
abstract = {We use Action-Process-Object-Schema theory (APOS) to study high school student understanding of exponentiation and their construction of exponential and logarithmic functions. We extend didactic materials similar to those of Ferrari-Escolá et al. (2016) to include exponentials on the rational numbers and to help students construct logarithms as numbers. Qualitative data from the problem-solving activities of two groups of eight students each during a series of teaching episodes suggests that some students can use these materials successfully. The data analysis enabled us to give specific suggestions on how to help other students do some of the constructions needed to understand these functions. Research shows these constructions are difficult for students. The findings of our study led to contributing a new and detailed genetic decomposition that can be tested and improved in future research cycles.}
}
@article{UTHAMACUMARAN2020759,
title = {Cancer: A turbulence problem},
journal = {Neoplasia},
volume = {22},
number = {12},
pages = {759-769},
year = {2020},
issn = {1476-5586},
doi = {https://doi.org/10.1016/j.neo.2020.09.008},
url = {https://www.sciencedirect.com/science/article/pii/S1476558620301548},
author = {Abicumaran Uthamacumaran},
keywords = {Cancer, Complexity, Chaos, Nonlinear dynamics, Fractals, Chemical turbulence},
abstract = {Cancers are complex, adaptive ecosystems. They remain the leading cause of disease-related death among children in North America. As we approach computational oncology and Deep Learning Healthcare, our mathematical models of cancer dynamics must be revised. Recent findings support the perspective that cancer-microenvironment interactions may consist of chaotic gene expressions and turbulent protein flows during pattern formation. As such, cancer pattern formation, protein-folding and metastatic invasion are discussed herein as processes driven by chemical turbulence within the framework of complex systems theory. To conclude, cancer stem cells are presented as strange attractors of the Waddington landscape.}
}
@article{DANCKWARDTLILLIESTROM2025100906,
title = {Travelling through time in a process drama on plastic pollution – temporality in teaching about the complexity of wicked problems},
journal = {Learning, Culture and Social Interaction},
volume = {52},
pages = {100906},
year = {2025},
issn = {2210-6561},
doi = {https://doi.org/10.1016/j.lcsi.2025.100906},
url = {https://www.sciencedirect.com/science/article/pii/S221065612500025X},
author = {Kerstin Danckwardt-Lillieström and Maria Andrée and Carl-Johan Rundgren},
keywords = {Historying, Futuring, Process drama, Wicked problems, Chemistry education, Upper secondary school},
abstract = {The understanding of sustainability issues and preparedness to take action towards a sustainable future involves abilities to navigate between past, present, and future. This paper explores how the use of imaginary transitions in time – in the form of historying, and futuring in process drama – may afford student understanding of the wicked problem of plastics. The study draws on a design-based research study on process drama in upper-secondary school chemistry teaching which was conducted in collaboration with two teachers. During the process drama, the students and teachers travel in time to explore the uses of plastic; the motives and needs for using plastic as well as the consequences of plastic use in the form of plastic pollution today and in the future. The collected data consist of video- and audio recordings, which were analysed through qualitative content analysis that discerned how the students connected the temporalities, and which dimensions of the plastic problem were made visible in the temporal movements in the process drama. Our findings indicate that the temporal transitions made visible several dimensions of the plastic issue, and contributed to adding layers of complexity to the issue of plastics.}
}
@article{NYBERG2022394,
title = {Spatial goal coding in the hippocampal formation},
journal = {Neuron},
volume = {110},
number = {3},
pages = {394-422},
year = {2022},
issn = {0896-6273},
doi = {https://doi.org/10.1016/j.neuron.2021.12.012},
url = {https://www.sciencedirect.com/science/article/pii/S0896627321010291},
author = {Nils Nyberg and Éléonore Duvelle and Caswell Barry and Hugo J. Spiers},
keywords = {hippocampus, entorhinal cortex, navigation, goal, wayfinding, spatial memory, reinforcement learning, rodent, human},
abstract = {Summary
The mammalian hippocampal formation contains several distinct populations of neurons involved in representing self-position and orientation. These neurons, which include place, grid, head direction, and boundary cells, are thought to collectively instantiate cognitive maps supporting flexible navigation. However, to flexibly navigate, it is necessary to also maintain internal representations of goal locations, such that goal-directed routes can be planned and executed. Although it has remained unclear how the mammalian brain represents goal locations, multiple neural candidates have recently been uncovered during different phases of navigation. For example, during planning, sequential activation of spatial cells may enable simulation of future routes toward the goal. During travel, modulation of spatial cells by the prospective route, or by distance and direction to the goal, may allow maintenance of route and goal-location information, supporting navigation on an ongoing basis. As the goal is approached, an increased activation of spatial cells may enable the goal location to become distinctly represented within cognitive maps, aiding goal localization. Lastly, after arrival at the goal, sequential activation of spatial cells may represent the just-taken route, enabling route learning and evaluation. Here, we review and synthesize these and other evidence for goal coding in mammalian brains, relate the experimental findings to predictions from computational models, and discuss outstanding questions and future challenges.}
}
@incollection{GAINOTTI2025421,
title = {Chapter 27 - Emotion: An evolutionary model of lateralization in the human brain},
editor = {Costanza Papagno and Paul Corballis},
series = {Handbook of Clinical Neurology},
publisher = {Elsevier},
volume = {208},
pages = {421-432},
year = {2025},
booktitle = {Cerebral Asymmetries},
issn = {0072-9752},
doi = {https://doi.org/10.1016/B978-0-443-15646-5.00001-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780443156465000014},
author = {Guido Gainotti},
keywords = {Emotional lateralization, Adaptive systems, Animal asymmetries, Evolutionary perspective, Covert language, Consciousness levels, Automatic functioning},
abstract = {Since several reviews have recently discussed the lateralization of emotions, this chapter will take into account the possible evolutionary meaning of this lateralization. The organization of the chapter will be based on the following steps. I will first propose that emotions must be considered as a complex adaptive system, complementary to the more phylogenetically advanced cognitive system. Second, I will remind historical aspects and consolidated results on the lateralization of emotions. Then I will discuss the phylogenetic aspects of the problem, trying to evaluate if emotional asymmetries concern only humans and some nonhuman primates or are part of a continuum between humans and many phylogenetically distant animal species. After having reviewed various aspects of emotional lateralization across different animal species and (more specifically) in nonhuman primates, I will propose a general model of hemispheric asymmetries in the human brain, based on theoretical models and empiric data. Theoretical models stem from the influence that the presence or the absence of language can have on concomitant hemispheric functions, whereas supporting neuropsychologic data have been gathered in patients with unilateral brain damage.}
}
@article{PALMIERI2020106074,
title = {Dataset of active avoidance in Wistar-Kyoto and Sprague Dawley rats: Experimental data and reinforcement learning model code and output},
journal = {Data in Brief},
volume = {32},
pages = {106074},
year = {2020},
issn = {2352-3409},
doi = {https://doi.org/10.1016/j.dib.2020.106074},
url = {https://www.sciencedirect.com/science/article/pii/S2352340920309689},
author = {John Palmieri and Kevin M. Spiegler and Kevin C.H. Pang and Catherine E. Myers},
keywords = {Avoidance learning, Reinforcement learning, Neurosciences, Computational modelling, Computational biology, Strain differences, Wistar Kyoto rat},
abstract = {Data were collected from 40 Wistar-Kyoto (WKY) and 40 Sprague Dawley (SD) rats during an active escape-avoidance experiment. Footshock could be avoided by pressing a lever during a danger period prior to onset of shock. If avoidance did not occur, a series of footshocks was administered, and the rat could press a lever to escape (terminate shocks). For each animal, data were simplified to the presence or absence of lever press and stimuli in each 12-second time frame. Using the pre-processed dataset, a reinforcement learning (RL) model, based on an actor-critic architecture, was utilized to estimate several different model parameters that best characterized each rat's behaviour during the experiment. Once individual model parameters were determined for all 80 rats, behavioural recovery simulations were run using the RL model with each animal's “best-fit” parameters; the simulated behaviour generated avoidance data (percent of trials avoided during a given experimental session) that could be compared across simulated rats, as is customarily done with empirical data. The datasets representing both the experimental data and the model-generated data can be interpreted in various ways to gain further insight into rat behaviour during avoidance and escape learning. Furthermore, the estimated parameters for each individual rat can be compared across groups. Thus, possible between-strain differences in model parameters can be detected, which might provide insights into strain differences in learning. The software implementing the RL model can also be applied to or serve as a template for other experiments involving acquisition learning. Reference for Co-Submission: K.M. Spiegler, J. Palmieri, K.C.H. Pang, C.E. Myers, A reinforcement-learning model of active avoidance behavior: Differences between Sprague-Dawley and Wistar-Kyoto rats. Behav. Brain Res. (2020 Jun 22[epub ahead of print])  doi: 10.1016/j.bbr.2020.112784}
}
@article{KASTELLAKIS201519,
title = {Synaptic clustering within dendrites: An emerging theory of memory formation},
journal = {Progress in Neurobiology},
volume = {126},
pages = {19-35},
year = {2015},
issn = {0301-0082},
doi = {https://doi.org/10.1016/j.pneurobio.2014.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S0301008214001373},
author = {George Kastellakis and Denise J. Cai and Sara C. Mednick and Alcino J. Silva and Panayiota Poirazi},
keywords = {Plasticity, Active dendrites, Associative memory, Synapse clustering, Synaptic tagging and capture},
abstract = {It is generally accepted that complex memories are stored in distributed representations throughout the brain, however the mechanisms underlying these representations are not understood. Here, we review recent findings regarding the subcellular mechanisms implicated in memory formation, which provide evidence for a dendrite-centered theory of memory. Plasticity-related phenomena which affect synaptic properties, such as synaptic tagging and capture, synaptic clustering, branch strength potentiation and spinogenesis provide the foundation for a model of memory storage that relies heavily on processes operating at the dendrite level. The emerging picture suggests that clusters of functionally related synapses may serve as key computational and memory storage units in the brain. We discuss both experimental evidence and theoretical models that support this hypothesis and explore its advantages for neuronal function.}
}
@article{MADORE2022707,
title = {Readiness to remember: predicting variability in episodic memory},
journal = {Trends in Cognitive Sciences},
volume = {26},
number = {8},
pages = {707-723},
year = {2022},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2022.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S1364661322001127},
author = {Kevin P. Madore and Anthony D. Wagner},
keywords = {episodic retrieval, attention lapsing, goal processing, arousal, locus coeruleus, posterior alpha},
abstract = {Learning and remembering are fundamental to our lives, so what causes us to forget? Answers often highlight preparatory processes that precede learning, as well as mnemonic processes during the act of encoding or retrieval. Importantly, evidence now indicates that preparatory processes that precede retrieval attempts also have powerful influences on memory success or failure. Here, we review recent work from neuroimaging, electroencephalography, pupillometry, and behavioral science to propose an integrative framework of retrieval-period dynamics that explains variance in remembering in the moment and across individuals as a function of interactions among preparatory attention, goal coding, and mnemonic processes. Extending this approach, we consider how a ‘readiness to remember’ (R2R) framework explains variance in high-level functions of memory and mnemonic disruptions in aging.}
}
@article{ZHU2020706,
title = {Cognitive-inspired Computing: Advances and Novel Applications},
journal = {Future Generation Computer Systems},
volume = {109},
pages = {706-709},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.03.017},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X20308384},
author = {Rongbo Zhu and Lu Liu and Maode Ma and Hongxiang Li},
keywords = {Cognitive-inspired computing, Systems, Intelligent health analysis, Security and privacy, Novel applications},
abstract = {Cognition is emerging as a new and promising methodology with the development of cognitive-inspired computing and interaction systems, which enables a large class of applications and has emerged with a significance to change our life. However, recent advances on artificial intelligence (AI), edge computing, big data, and cognitive computational theory show that multidisciplinary cognitive-inspired computing still struggles with fundamental, long-standing problems, such as computational models and decision-making mechanisms based on the neurobiological processes of the brain, cognitive sciences, and psychology. How to enhance human cognitive performance with machine learning (ML), common sense, intelligent interaction, privacy security and novel applications is worth exploring. The objective of this special issue is to report high-quality state-of-the-art research contributions that address these key aspects of cognitive-inspired computing and novel applications. By presenting a selection of papers on various topics related to cognitive-inspired computing and applications, we hope to shed light on the multiple aspects of this emerging multidisciplinary paradigm. The papers included in this issue propose solutions for cognitive-inspired systems, AI-assisted computing, intelligent health analysis, security and privacy issues, as well as novel applications.}
}
@article{VELICHKOVSKY201735,
title = {Consciousness and working memory: Current trends and research perspectives},
journal = {Consciousness and Cognition},
volume = {55},
pages = {35-45},
year = {2017},
issn = {1053-8100},
doi = {https://doi.org/10.1016/j.concog.2017.07.005},
url = {https://www.sciencedirect.com/science/article/pii/S1053810017301654},
author = {Boris B. Velichkovsky},
keywords = {Consciousness, Working memory, Visual masking, Attentional blink, Implicit working memory},
abstract = {Working memory has long been thought to be closely related to consciousness. However, recent empirical studies show that unconscious content may be maintained within working memory and that complex cognitive computations may be performed on-line. This promotes research on the exact relationships between consciousness and working memory. Current evidence for working memory being a conscious as well as an unconscious process is reviewed. Consciousness is shown to be considered a subset of working memory by major current theories of working memory. Evidence for unconscious elements in working memory is shown to come from visual masking and attentional blink paradigms, and from the studies of implicit working memory. It is concluded that more research is needed to explicate the relationship between consciousness and working memory. Future research directions regarding the relationship between consciousness and working memory are discussed.}
}
@article{COONEY1992237,
title = {The influence of verbal protocol methods on children's mental computation},
journal = {Learning and Individual Differences},
volume = {4},
number = {3},
pages = {237-257},
year = {1992},
issn = {1041-6080},
doi = {https://doi.org/10.1016/1041-6080(92)90004-X},
url = {https://www.sciencedirect.com/science/article/pii/104160809290004X},
author = {John B. Cooney and Stephen F. Ladd},
abstract = {The purpose of this study was to investigate the validity of children's verbal reports about the cognitive processes underlying their mental arithmetic. A within-subject comparison was made with respect to the data that could be obtained with retrospective verbal report, concurrent verbal report, and no verbal report conditions. The results of the investigation indicated that children's verbal reports of strategy use may not be veridical. The source of the nonveridicality was incompleteness rather than fabrication. It was also found that immediately retrospective and concurrent verbal reports increased students' solution accuracy relative to a no verbal report condition. Thus, the primary mental operations underlying children's mental arithmetic are reactive to giving verbal reports. It was concluded that empirical checks for reactivity and refinements to protocol procedures to reveal the progression of strategy use are needed in future research.}
}
@article{KOH2020106,
title = {Automated detection of Alzheimer's disease using bi-directional empirical model decomposition},
journal = {Pattern Recognition Letters},
volume = {135},
pages = {106-113},
year = {2020},
issn = {0167-8655},
doi = {https://doi.org/10.1016/j.patrec.2020.03.014},
url = {https://www.sciencedirect.com/science/article/pii/S0167865520300921},
author = {Joel En Wei Koh and Vicnesh Jahmunah and The-Hanh Pham and Shu Lih Oh and Edward J Ciaccio and U Rajendra Acharya and Chai Hong Yeong and Mohd Kamil Mohd Fabell and Kartini Rahmat and Anushya Vijayananthan and Norlisah Ramli},
abstract = {The build-up of beta-amyloid and rapid spread of tau proteins in the brain cause the death of neurons, leading to Alzheimer's disease (AD). AD is a form of dementia, and the symptoms include memory loss and decision-making difficulties. Current advanced diagnostic modalities are costly or unable to detect the histopathological features of AD. Hence a computational intelligence tool (CIT) for AD diagnosis is proposed in this study. The magnetic resonance images (MRI) of the brain are pre-processed using an adaptive histogram, and decomposed into four IMFS using bidirectional empirical mode decomposition (BEMD). Local binary patterns (LBP) are then computed per IMF, and the histograms are concatenated. Adaptive synthetic sampling (ADASYN) is applied to balance the dataset and Student's t-test is utilized for selection of highly significant features, within each fold for ten-fold validation. Amongst other classifiers, SVM-Poly 1 and random forest(RF) were employed for classification, yielding the highest accuracy of 93.9% each. Our study concludes that the recommended CIT is useful for the automatic classification of AD versus normal MRI imagery in hospitals.}
}
@incollection{GILLAM199523,
title = {Chapter 2 - The Perception of Spatial Layout from Static Optical Information},
editor = {William Epstein and Sheena Rogers},
booktitle = {Perception of Space and Motion},
publisher = {Academic Press},
address = {San Diego},
pages = {23-67},
year = {1995},
series = {Handbook of Perception and Cognition},
isbn = {978-0-12-240530-3},
doi = {https://doi.org/10.1016/B978-012240530-3/50004-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780122405303500043},
author = {Barbara Gillam},
abstract = {Publisher Summary
This chapter reviews the literature on absolute distance, relative distance, surface slant and curvature, and the perception of size and shape within the context of several broad issues that have influenced thinking and experimentation to varying degrees in recent years. One issue that has driven recent research is the way stimulus input is described that carries implicit assumptions about how it is encoded and represented. Euclidian and other conventional frameworks may be restricting and misleading as a basis for visual theory. Another issue raised by computational approaches is the relationship between the processing of different sources of information or cues underlying the perception of spatial layout. Machine vision has tended to treat these cues as separate modules or processing systems, a view that has also received support from psychophysics. Comparison of some seemingly separate processes, specifically perspective and stereopsis, may indicate common mechanisms.}
}
@article{YURKOVICH2018130,
title = {Quantitative -omic data empowers bottom-up systems biology},
journal = {Current Opinion in Biotechnology},
volume = {51},
pages = {130-136},
year = {2018},
note = {Systems biology • Nanobiotechnology},
issn = {0958-1669},
doi = {https://doi.org/10.1016/j.copbio.2018.01.009},
url = {https://www.sciencedirect.com/science/article/pii/S0958166917302276},
author = {James T Yurkovich and Bernhard O Palsson},
abstract = {The large-scale generation of ‘-omic’ data holds the potential to increase and deepen our understanding of biological phenomena, but the ability to synthesize information and extract knowledge from these data sets still represents a significant challenge. Bottom-up systems biology overcomes this hurdle through the integration of disparate -omic data types, and absolutely quantified experimental measurements allow for direct integration into quantitative, mechanistic models. The human red blood cell has served as a starting point for the application of systems biology approaches and has been the focus of a recent burst of generated quantitative metabolomics and proteomics data. Thus, the red blood cell represents the perfect case study through which to examine our ability to glean knowledge from the integration of multiple disparate data types.}
}
@article{CASTRO2023105510,
title = {An experimental and simulation study of the impact of emotional information on analogical reasoning},
journal = {Cognition},
volume = {238},
pages = {105510},
year = {2023},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2023.105510},
url = {https://www.sciencedirect.com/science/article/pii/S0010027723001440},
author = {Ariana A. Castro and John E. Hummel and Howard Berenbaum},
keywords = {Reasoning, Emotion, Computational models, Attention, Analogies},
abstract = {We investigated whether and how emotional information would affect analogical reasoning. We hypothesized that task-irrelevant emotional information would impair performance whereas task-relevant emotional information would enhance it. In Study 1, 233 undergraduates completed a novel version of the People Pieces Task (Emotional Faces People Task), an analogical reasoning task in which the task characters displayed emotional or neutral facial expressions (within-participants). The emotional faces were relevant or irrelevant to the task (between-participants). We simulated the behavioral results using the Learning and Inference with Schemas and Analogies (LISA) model of relational reasoning. LISA is a neurally plausible, symbolic-connectionist computational model of analogical reasoning. In comparison to neutral trials, participants were slower but more accurate on emotion-relevant trials, and were faster but less accurate on emotion-irrelevant trials. Simulations using the LISA model demonstrated that it is possible to account for the effects of emotional information on reasoning in terms of how emotional stimuli attract attention during a reasoning task. In Study 2, 255 undergraduates completed the Emotional Faces People Task at either a high- or low-working memory load. The high working memory load condition of Study 2 replicated the findings of Study 1, showing that participants were more accurate on emotion-relevant trials than on emotion-irrelevant trials; in Study 2, this increased accuracy could not be accounted for by a speed-accuracy tradeoff. The working memory manipulation influenced the manner in which the congruence (with the correct answer) of emotion-irrelevant emotion influenced performance. Simulations using the LISA model showed that manipulating the salience of emotion, the error penalty, as well as vigilance (which determines the likelihood that LISA will notice it has attended to an irrelevant relation), could reasonably reproduce the behavioral results of both low and high working memory load conditions of Study 2.}
}
@article{JOOKEN202336,
title = {Features for the 0-1 knapsack problem based on inclusionwise maximal solutions},
journal = {European Journal of Operational Research},
volume = {311},
number = {1},
pages = {36-55},
year = {2023},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2023.04.023},
url = {https://www.sciencedirect.com/science/article/pii/S0377221723003065},
author = {Jorik Jooken and Pieter Leyman and Patrick {De Causmaecker}},
keywords = {Combinatorial optimization, 0-1 knapsack problem, Packing, Problem instance hardness, Instance space analysis},
abstract = {Decades of research on the 0-1 knapsack problem led to very efficient algorithms that are able to quickly solve large problem instances to optimality. This prompted researchers to also investigate the structure of problem instances that are hard for existing solvers. In the current paper we are interested in investigating which features make 0-1 knapsack problem instances hard to solve to optimality for the state-of-the-art 0-1 knapsack solver. We propose a set of 14 features based on previous work by the authors in which so-called inclusionwise maximal solutions (IMSs) play a central role. Calculating these features is computationally expensive and requires one to solve hard combinatorial problems. Based on new structural results about IMSs, we formulate polynomial and pseudopolynomial time algorithms for calculating these features. These algorithms were executed for two large datasets on a supercomputer in approximately 540 CPU-hours. We show that the proposed features contain important information related to the empirical hardness of a problem instance that was missing in earlier features from the literature by training machine learning models that can accurately predict the empirical hardness of a wide variety of 0-1 knapsack problem instances. Moreover, we show that these features can be cheaply approximated at the cost of less accurate hardness predictions. Using the instance space analysis methodology, we show that hard 0-1 knapsack problem instances are clustered together around a relatively dense region of the instance space and several features behave differently in the easy and hard parts of the instance space.}
}
@article{ARTEMOV20093884,
title = {A tribute to D.B. Spalding and his contributions in science and engineering},
journal = {International Journal of Heat and Mass Transfer},
volume = {52},
number = {17},
pages = {3884-3905},
year = {2009},
note = {Special Issue Honoring Professor D. Brian Spalding},
issn = {0017-9310},
doi = {https://doi.org/10.1016/j.ijheatmasstransfer.2009.03.038},
url = {https://www.sciencedirect.com/science/article/pii/S0017931009002026},
author = {V. Artemov and S.B. Beale and G. {de Vahl Davis} and M.P. Escudier and N. Fueyo and B.E. Launder and E. Leonardi and M.R. Malin and W.J. Minkowycz and S.V. Patankar and A. Pollard and W. Rodi and A. Runchal and S.P. Vanka},
keywords = {D.B. Spalding, Fluid dynamics, Heat transfer, Mass transfer, Combustion},
abstract = {This paper presents a summary of some of the scientific and engineering contributions of Prof. D.B. Spalding up to the present time. Starting from early work on combustion, and his unique work in mass transfer theory, Spalding’s unpublished “unified theory” is described briefly. Subsequent to this, developments in algorithms by the Imperial College group led to the birth of modern computational fluid dynamics, including the well-known SIMPLE algorithm. Developments in combustion, multi-phase flow and turbulence modelling are also described. Finally, a number of academic and industrial applications of computational fluid dynamics and heat transfer applications considered in subsequent years are mentioned.}
}
@article{CHEN199799,
title = {Towards designing sustainable urban wastewater infrastructures: A screening analysis},
journal = {Water Science and Technology},
volume = {35},
number = {9},
pages = {99-112},
year = {1997},
note = {Sustainable Sanitation},
issn = {0273-1223},
doi = {https://doi.org/10.1016/S0273-1223(97)00188-1},
url = {https://www.sciencedirect.com/science/article/pii/S0273122397001881},
author = {J. Chen and M.B. Beck},
keywords = {Sustainability, wastewater treatment technologies, screening analysis, uncertainty, urban drainage system},
abstract = {Whether sustainability can, or should, be defined in a practical operational sense, it is clear that the emergence of such a notion has prompted what seems to be a profound re-thinking of whether our society, economic system, and technology are as we would wish them to be. Sustainable development, clean technology, life-cycle analysis, pollution prevention, and so on, are expressions of a willingness to leave no stone unturned, as it were, in the search for what would be appropriate. With respect to the design and operation of a city's wastewater infrastructure, in particular, this search is characterised by a seeming explosion in the possible combinations of appropriate technologies, gross uncertainty about how novel technologies - only now emerging - might perform in the very long term, and a continuing absence of specific criteria of sustainability for determining the grounds on which any candidate technology might be preferred over another. The paper introduces a simple computational procedure for generating and screening candidate combinations of unit-process technologies for an urban wastewater infrastructure. This is based on the use of Monte Carlo simulation, with the identification of those specific technologies (and combinations thereof) that appear to have the greatest probability of being selected for use under different, possibly evolving, criteria of sustainability. Application of the procedure is illustrated with respect to just a part of this infrastructure, i.e., the wastewater treatment plant.}
}
@article{PLEBE20164,
title = {What is ‘wrong’ in a neural model},
journal = {Cognitive Systems Research},
volume = {39},
pages = {4-14},
year = {2016},
note = {From human to artificial cognition (and back): new perspectives of cognitively inspired AI systems},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2015.12.012},
url = {https://www.sciencedirect.com/science/article/pii/S1389041716000085},
author = {Alessio Plebe},
keywords = {Moral cognition, Neural computation, Orbitofrontal cortex, Amygdala, Self-organization},
abstract = {Neural computation has an influential role in the study of human capacities and behaviors. It has been the dominant approach in the vision science of the last half century, and it is currently one of the fundamental methods of investigation for most higher cognitive functions. Yet, neurocomputational approaches to moral behavior are lacking. Computational modeling in general has been scarcely pursued in morality, and existent non-neural attempts have failed to account for the mental processes involved in morality. In this paper we argue that recently the situation has evolved in a way that subverted the insufficient knowledge on the basic organization of moral cognition in brain circuits, making the project of modeling morality in neurocomputational terms feasible. We will present an original architecture that combines reinforcement learning and Hebbian learning, aimed at simulating forms of moral behavior in a simple artificial context. The relationship between language and morality is controversial. In the analytic tradition of philosophy, morality is essentially the language of morals. On the other side, current cognitive ethology has shown how non human species display behaviors that are surprisingly similar to those prescribed by human ethics. Nevertheless, morality in humans is deeply entrenched with language, and the semantics of words like ‘wrong’ resists consensual explanations. The model here proposed includes an auditory processing pathway, with the purpose of showing how the coding of “wrong”, even if highly simplified with respect to its rich content in natural language, can emerge in the course of moral learning.}
}
@article{BIRD2004337,
title = {Kuhn, naturalism, and the positivist legacy},
journal = {Studies in History and Philosophy of Science Part A},
volume = {35},
number = {2},
pages = {337-356},
year = {2004},
issn = {0039-3681},
doi = {https://doi.org/10.1016/j.shpsa.2004.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S0039368104000184},
author = {Alexander Bird},
keywords = {Kuhn, Naturalism, Positivism, Preston},
abstract = {I defend against criticism the following claims concerning Thomas Kuhn: (i) there is a strong naturalist streak in The structure of scientific revolutions, whereby Kuhn used the results of a posteriori enquiry in addressing philosophical questions; (ii) as Kuhn’s career as a philosopher of science developed he tended to drop the naturalistic elements and to replace them with more traditionally philosophical, a priori approaches; (iii) at the same time there is a significant residue of positivist thought in Kuhn, which Kuhn did not recognise as such; (iv) the naturalistic elements referred to in (i) are the most original and fruitful elements of Kuhn’s thinking; (v) the positivistic elements referred to in (iii) vitiated his thought and acted as factors in preventing Kuhn from developing the naturalistic elements and from following the path taken by much subsequent philosophy of science. Preston presents an alternative reading of Kuhn which emphasizes the Wittgensteinian elements in Kuhn. I argue that this alternative view is, descriptively, poorly supported by the textual evidence and the facts of the history of philosophy of science in the twentieth century. I provide some defence of the naturalistic approach and related themes.}
}
@article{OLIVEIRA202575,
title = {Quantitative and qualitative analysis in urban morphology: systematic legacy and latest developments},
journal = {Proceedings of the Institution of Civil Engineers - Urban Design and Planning},
volume = {178},
number = {2},
pages = {75-87},
year = {2025},
issn = {1755-0793},
doi = {https://doi.org/10.1680/jurdp.24.00047},
url = {https://www.sciencedirect.com/science/article/pii/S1755079325000058},
author = {Vitor Oliveira and Sergio Porta},
keywords = {built environment, quantitative versus qualitative, town & city planning, urban form, urban morphology},
abstract = {Urban morphology studies the physical forms of human settlements and how these change over time by the action of different processes and agents. The field of knowledge has developed several theories, concepts, and methods to describe and explain the phenomena at hands. As in many fields, urban morphology contains a few misconceptions. One of these is the idea that quantitative analysis is a feature of the present and the future, and qualitative analysis of the past. The paper addresses this fallacy. Our discussion of the main schools of thought in urban morphology and their influential researchers suggests that quantitative approaches are well rooted in it since at least the mid-twentieth century and that the dominance of quantitative or qualitative tools is subject to cycles, as it happens in other sciences. Demonstration of both statements leads to a focus on a line of approaches, historico-geographical, configurational, and lately morphometrics, which share a common interest in cross-cases regularities, hence practices of pattern recognition.}
}
@article{WOODWARD2006631,
title = {Does prior mathematics knowledge really lead to variation in elementary statistics performance? Evidence from a developing country},
journal = {International Journal of Educational Development},
volume = {26},
number = {6},
pages = {631-639},
year = {2006},
issn = {0738-0593},
doi = {https://doi.org/10.1016/j.ijedudev.2006.02.005},
url = {https://www.sciencedirect.com/science/article/pii/S0738059306000071},
author = {George Woodward and Don Galagedera},
keywords = {Curriculum, Mathematics, Educational policy, Elementary statistics},
abstract = {A model incorporating prerequisite mathematics performance and other variables deemed to be associated with learning elementary statistics (ES) is developed. The relationship between ES performance and the explanatory variables is well represented by the logistics form. Aptitude, effort and motivation are the only significant explanatory variables of ES performance. Since prerequisite mathematics is not significant, statistical thinking at the tertiary level may be mostly intuitive and non-mathematical. Students with low aptitude experience increasing returns to effort over the first half of the feasible effort interval, while high-aptitude students experience diminishing returns at all levels of effort. The levels of effort required to achieve a minimum pass are interpreted.}
}
@article{ORUN2025105102,
title = {Cognitive behavioural characteristics identification for remote user authentication for cybersecurity},
journal = {Journal of Parallel and Distributed Computing},
pages = {105102},
year = {2025},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2025.105102},
url = {https://www.sciencedirect.com/science/article/pii/S0743731525000693},
author = {Ahmet Orun and Emre Orun and Fatih Kurugollu},
keywords = {Cyber Security, Cognitive Psychology, Artificial Intelligence, Bayesian Networks, Internet},
abstract = {Nowadays cyber-attacks keep threatening global networks and information infrastructures. Day-by-day, the threat is gradually getting more destructive and harder to counter, as the global networks continue to enlarge exponentially with limited security counter-measures. This occurrence urgently demands more sophisticated methods and techniques, such as multi-factor authentication and soft biometrics to respond to evolving threats. This paper is concerned with behavioural soft biometrics and proposes a multidisciplinary remote cognitive observation technique to meet today’s cybersecurity needs. The proposed method introduces a non-traditional “cognitive psychology” and “artificial intelligence” based approach. According to contemporary cognitive psychology research, human cognitive processes can be affected by many different personal factors and emotional states which are specific to an individual. Those factors mainly include personal perception, memory, decision-making, reasoning, learning, etc. In this study we focus on visual (graphical) perception with the support of graphical stimuli environments and investigate how such personal cognitive factors can be exploited within the cybersecurity area for remote user authentication. This technique enables remote access to the cognitive behavioural parameters of an intruder/hacker without any physical contact via online connection, disregarding the distance of the threat. The results show that cognitive stimuli provide crucial information for a behavioural user authentication system to classify the user as “authentic” or “intruder”. The ultimate goal of this work is to develop a supplementary cognitive cyber security tool for “next generation” secure online banking, finance or trade systems.}
}
@article{SOHAIL201947,
title = {A videographic assessment of ferrofluid during magnetic drug targeting: An application of artificial intelligence in nanomedicine},
journal = {Journal of Molecular Liquids},
volume = {285},
pages = {47-57},
year = {2019},
issn = {0167-7322},
doi = {https://doi.org/10.1016/j.molliq.2019.04.022},
url = {https://www.sciencedirect.com/science/article/pii/S0167732219315399},
author = {Ayesha Sohail and Maryam Fatima and Rahamt Ellahi and Khush Bakhat Akram},
keywords = {Ferrofluids, Drug targeting, Artificial intelligence, Videographic footage},
abstract = {Forecasting the thresholds via the computational analysis of magnetic drug targeting, is a useful approach since it can help to design the nanoscale experiments to get the best results and efficiency. In such investigations, an artificial intelligence when interlinked with the computational techniques provide better insight specially for rheological problems. In the proposed model mathematical framework for the magnetic drug targeting is adopted while the flow of the ferrofluid, with different concentrations is taken into account. The flow without any obstruction is compared with the flow having obstruction. The nanoscale dynamics sensitive to such obstructions are documented by videographic footage. Nanaoscale approach and the response of the nanomedicine relative to external agents are used. The pressure gradient, the magnetic susceptibility and the velocity profile of the ferrofluid provides useful thresholds to identify the geometry of the obstacle, and to forecast the resulting dynamics.}
}
@article{ALCANTUD2022118276,
title = {Ranked hesitant fuzzy sets for multi-criteria multi-agent decisions},
journal = {Expert Systems with Applications},
volume = {209},
pages = {118276},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.118276},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422014142},
author = {José Carlos R. Alcantud},
keywords = {Hesitant fuzzy set, Aggregation operator, Score, Ranking, Decision making},
abstract = {This paper introduces and investigates ranked hesitant fuzzy sets, a novel extension of hesitant fuzzy sets that is less demanding than both probabilistic and proportional hesitant fuzzy sets. This new extension incorporates hierarchical knowledge about the various evaluations submitted for each alternative. These evaluations are ranked (for example by their plausibility, acceptability, or credibility), but their position does not necessarily derive from supplementary numerical information (as in probabilistic and proportional hesitant fuzzy sets). In particular, strictly ranked hesitant fuzzy sets arise when no ties exist, i.e., when for any fixed alternative, each submitted evaluation is either strictly more plausible or strictly less plausible than any other submitted evaluation. A detailed comparison with similar models from the literature is performed. Then in order to produce a natural strategy for multi-criteria multi-agent decisions with ranked hesitant fuzzy sets, canonical representations, scores and aggregation operators are designed in the framework of ranked hesitant fuzzy sets. In order to help implementation of this model, Mathematica code is provided for the computation of both scores and aggregators. The decision-making technique that is prescribed is tested with a comparative analysis with four methodologies based on probabilistic hesitant fuzzy information. A conclusion of this numerical exercise is that this methodology is reliable, applicable and robust. All these evidences show that ranked hesitant fuzzy sets are an intuitive extension of the hesitant fuzzy set model designed by V. Torra, that can be implemented in practice with the aid of computationally assisted algorithms.}
}
@article{OITAVEM2011661,
title = {A recursion-theoretic approach to NP},
journal = {Annals of Pure and Applied Logic},
volume = {162},
number = {8},
pages = {661-666},
year = {2011},
issn = {0168-0072},
doi = {https://doi.org/10.1016/j.apal.2011.01.010},
url = {https://www.sciencedirect.com/science/article/pii/S016800721100011X},
author = {I. Oitavem},
keywords = {Computational complexity, Implicit characterization, Recursion schemes, NP},
abstract = {An implicit characterization of the class NP is given, without using any minimization scheme. This is the first purely recursion-theoretic formulation of NP.}
}
@article{MA2019367,
title = {An efficient method to compute different types of generalized inverses based on linear transformation},
journal = {Applied Mathematics and Computation},
volume = {349},
pages = {367-380},
year = {2019},
issn = {0096-3003},
doi = {https://doi.org/10.1016/j.amc.2018.12.064},
url = {https://www.sciencedirect.com/science/article/pii/S0096300318311251},
author = {Jie Ma and Feng Gao and Yongshu Li},
keywords = {Generalized inverse, Linear transformation, Rational matrix, MATHEMATICA},
abstract = {In this paper, we present functional definitions of all types of generalized inverses related to the {1}-inverse, which is a continuation of the work of Campbell and Meyer (2009). According to these functional definitions, we further derive novel representations for all types of generalized inverses related to the {1}-inverse in terms of the bases for R(A*), N(A) and N(A*). Based on these representations, we present the corresponding algorithm for computing various generalized inverses related to the {1}-inverse of a matrix and analyze the computational complexity of our algorithm for a constant matrix. Finally, we implement our algorithm and several known algorithms for symbolic computation of the Moore-–Penrose inverse in the symbolic computational package MATHEMATICA and compare their running times. Numerical experiments show that our algorithm outperforms these known algorithms when applied to compute the Moore–Penrose inverse of one-variable rational matrices, but is not the best choice for two-variable rational matrices in practice.}
}
@incollection{PIOT2006163,
title = {Gross, Maurice (1934–2001)},
editor = {Keith Brown},
booktitle = {Encyclopedia of Language & Linguistics (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {163-164},
year = {2006},
isbn = {978-0-08-044854-1},
doi = {https://doi.org/10.1016/B0-08-044854-2/05135-X},
url = {https://www.sciencedirect.com/science/article/pii/B008044854205135X},
author = {M. Piot},
keywords = {comparative linguistics, computational linguistics, computer dictionaries, French language, linguistic theory, machine translation, mathematical models, syntax-based lexicon},
abstract = {Gross, Maurice (1934–2001) was a pioneer thinker in the field of modern linguistics. Long before computers could facilitate large-scale, lexically-based language study, he built an exhaustive, empirically based inventory of the ‘lexicon-grammar’ of French: the world's first lexical grammar. Since then, researchers in other countries have adopted the Gross model of description, which serves as a computational model for any language.}
}
@article{LATHA2019122052,
title = {Analysing exposure diversity in collaborative recommender systems—Entropy fusion approach},
journal = {Physica A: Statistical Mechanics and its Applications},
volume = {533},
pages = {122052},
year = {2019},
issn = {0378-4371},
doi = {https://doi.org/10.1016/j.physa.2019.122052},
url = {https://www.sciencedirect.com/science/article/pii/S0378437119311963},
author = {R. Latha and R. Nadarajan},
keywords = {Clustering, Quadratic entropy, Exposure diversity, Novelty, Concordance},
abstract = {Recommender Systems are considered as essential business tools to leverage the potential growth of on-line services. Neighbourhood based collaborative filtering, a successful recommendation approach has mainly focused on improving accuracy of predictions. From user point of view, it is more valuable to obtain novel and diverse recommendations rather than monotonic preferences. Ratings given by a user for different categories of items are considered as a tool to access user exposure diversity which signifies his creative and divergent thinking. On the other hand, pair of items is concordant if highly correlated users agree in rating the items. Based on the user exposure diversity and item concordance, the neighbourhood selection process of item based collaborative recommender systems is refined. Rating predictions are made based on the newly selected neighbours. The performance of the proposed approach is investigated for accuracy and diversity of predictions on Movielens data sets. The results demonstrate that the proposed approach outperforms the state of the art recommendation approaches which address accuracy–diversity trade off. Statistical analysis is done to prove the efficiency of the proposed approach.}
}
@article{FINOTTO201385,
title = {Hybrid fuzzy-genetic system for optimising cabled-truss structures},
journal = {Advances in Engineering Software},
volume = {62-63},
pages = {85-96},
year = {2013},
note = {Special Issue dedicated to Professor Zden ek Bittnar on the occasion of his Seventieth Birthday: Part I},
issn = {0965-9978},
doi = {https://doi.org/10.1016/j.advengsoft.2013.04.012},
url = {https://www.sciencedirect.com/science/article/pii/S0965997813000513},
author = {V.C. Finotto and W.R.L. {da Silva} and M. Valášek and P. Štemberk},
keywords = {Hybrid system, Structural optimisation, Cabled-truss, Fuzzy logic, Genetic algorithm, Nonlinear finite element analysis},
abstract = {This paper demonstrates an application of a hybrid fuzzy-genetic system in the optimisation of lightweight cabled-truss structures. These structures are described as a system of cables and triangular bar formations jointed at their ends by hinged connections to form a rigid framework. The optimised lightweight structure is determined through a stochastic discrete topology and sizing optimisation procedure that uses ground structure approach, nonlinear finite element analysis, genetic algorithm, and fuzzy logic. The latter is used to include expertise into the evolutionary search with the aim of filtering individuals with low survival possibility, thereby decreasing the total number of evaluations. This is desired because cables, which are inherently nonlinear elements, demand the use of iterative procedures for computing the structural response. Such procedures are computationally costly since the stiffness matrix is evaluated in each iteration until the structure is in equilibrium. Initially, the proposed system is applied to truss benchmarks. Next, the use of cables is investigated and the system’s performance is compared against genetic algorithms. The results indicate that the hybrid system considerably decreased the number of evaluations over genetic algorithms. Also, cabled-trusses showed a significant improvement in structural mass minimisation when compared with trusses.}
}
@article{PFEIFER199547,
title = {Cognition — perspectives from autonomous agents},
journal = {Robotics and Autonomous Systems},
volume = {15},
number = {1},
pages = {47-70},
year = {1995},
note = {The Biology and Technology of Intelligent Autonomous Agents},
issn = {0921-8890},
doi = {https://doi.org/10.1016/0921-8890(95)00014-7},
url = {https://www.sciencedirect.com/science/article/pii/0921889095000147},
author = {Rolf Pfeifer},
keywords = {Cognition, Autonomous agents, Cheap designs, “New AI”},
abstract = {The predominant paradigm in cognitive science has been the cognitivistic one, exemplified by the “Physical Symbol Systems Hypothesis”. The cognitivistic approach generated hopes that one would soon understand human thinking — hopes that up till now have still not been fulfilled. It is well-known that the cognitivistic approach, in spite of some early successes, has turned out to be fraught with problems. Examples are the frame problem, the symbol grounding problem, and the problems of interacting with a real physical world. In order to come to grips with the problems of cognitivism the study of embodied autonomous systems has been proposed, for example by Rodney Brooks. Brooks' robots can get away with no or very little representation. However, this approach has often been criticized because of the limited abilities of the agents. If they are to perform more intelligent tasks they will need to be equipped with representations or cognition — is an often heard argument. We will illustrate that we are well-advised not to introduce representational or cognitive concepts too quickly. As long as we do not understand the basic relationships between simple architectures and behavior, i.e. as long as we do not understand the dynamics of the system-environment interaction, it is premature to spend our time with speculations about potentially useful architectures for so-called high-level processes. This paper has a tutorial and a review aspect. In addition to presenting our own research, we will review some of the pertinent literature in order to make it usable as an introduction to “New AI”.}
}
@article{XIAO2023103864,
title = {APRS: Automatic pruning ratio search using Siamese network with layer-level rewardsImage 1},
journal = {Digital Signal Processing},
volume = {133},
pages = {103864},
year = {2023},
issn = {1051-2004},
doi = {https://doi.org/10.1016/j.dsp.2022.103864},
url = {https://www.sciencedirect.com/science/article/pii/S105120042200481X},
author = {Huachao Xiao and Yangxin Wang and Jianyi Liu and Jiaxin Huo and Yang Hu and Yu Wang},
keywords = {Structured pruning, Deep reinforcement learning, Pruning ratio search, Siamese network},
abstract = {Structured pruning is still a mainstream model compression technique, for its merit of easy to implement and no reliance on specific hardware supporting library. In most previous works, the layer-wise channel pruning ratios were determined empirically. In this paper, we propose an Automatic Pruning Ratio Search (APRS) algorithm that can find the layer-wise optimal pruning ratio within the deep reinforcement learning framework. To solve the coarse-granularity reward problem existing in some previous works like AMC and CACP, a novel layer-level reward function is designed based on the Siamese network architecture for the fine-granularity agent-environment interaction purpose. We use a computationally efficient way to evaluate the effect of pruning action on each single layer. The incurred “backwardness disadvantage” problem has also been analyzed and addressed. The experiments are performed using the VGG-16, and MobileNet-v1 on the CIFAR10/100 and UC Merced Land-use datasets. The results verified that our method can better reveal the underlying sparse sensitivities of different layers in both high redundancy networks and compact networks, so that resulting a higher network accuracy after pruning compared to the traditional methods.}
}
@article{DAI2025100019,
title = {Why students use or not use generative AI: Student conceptions, concerns, and implications for engineering education},
journal = {Digital Engineering},
volume = {4},
pages = {100019},
year = {2025},
issn = {2950-550X},
doi = {https://doi.org/10.1016/j.dte.2024.100019},
url = {https://www.sciencedirect.com/science/article/pii/S2950550X24000190},
author = {Yun Dai},
keywords = {Artificial intelligence, generative AI, engineering education, student concern, barrier, technology integration, higher education},
abstract = {Generative artificial intelligence (GenAI) technologies are believed to transform engineering education. However, it remains underexamined how engineering students choose to use GenAI or not, along with the reasons behind their choices. To fill this research gap, this study presents a natural experiment that examines student use or non-use of GenAI tools in engineering design tasks in an undergraduate course. In this experiment, the participants (n = 403) were provided with unconstrained access to a GPT 4.0-empowered chatbot and were allowed to use it for their design projects voluntarily. Overall, 59.80 % of the students reported substantial use of GenAI in their design projects, and 40.20 % showed limited or no use. Those adopters used GenAI to aid idea generation and brainstorming, mediate discussions with instructors/TA, overcome non-technical expertise gaps, and optimize their design solutions. Conversely, non-adopters attributed their reluctance and rejection to inherent limitations in GenAI outputs, misalignment between GenAI functionalities and project needs, a lack of adaptation and prompt skills, and unclear benefits of GenAI use for personal growth. This study challenges the popular assumption of naturally active GenAI adoption among university students. It identifies three major factors—task characteristics, decision-maker characteristics, and context characteristics—that shape students' adoption of and interaction with GenAI. The findings highlight the need of establishing a consensus across various stakeholders (i.e., students, instructors, curriculum developers, policymakers, and others), while calling for adaptation and evidence-based decision-making in integrating GenAI tools into engineering education.}
}
@incollection{CELKO2008255,
title = {Chapter 13 - Turning Specifications into Code},
editor = {Joe Celko},
booktitle = {Joe Celko's Thinking in Sets},
publisher = {Morgan Kaufmann},
address = {San Francisco},
pages = {255-271},
year = {2008},
series = {The Morgan Kaufmann Series in Data Management Systems},
isbn = {978-0-12-374137-0},
doi = {https://doi.org/10.1016/B978-012374137-0.50014-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780123741370500146},
author = {Joe Celko},
abstract = {Publisher Summary
This chapter delineates the importance of unlearning the procedural thinking and moves to a pure SQL view. Programmers tend to make the same kinds of errors in their designs and their code over and over. They confuse RDBMS with the file systems and 3GL- or OO-oriented programming environments they first learned. Programmers from the C family of languages tend to put the entire program in lowercase as if they were still using a teletype on a UNIX system. Mainframe programmers tend to put the entire program in uppercase as if they were still using punch cards or a 3270 video monitor for input. Cohesion is how well a module of code does one and only one thing, that it is logically coherent. There are several types of cohesion. The original definitions have been extended from procedural code to include OO and class hierarchies. The symptom in DDL is a table with lots of NULL-able columns. It is probably two or more entities crammed into a single table. The symptom in DML is a query or other statement that tries to do too many things. When the same procedure or query checks inventory and build a personnel report, cohesion problems crop up. The table-valued function shows that the programmer still wants to see procedural coding complete with parameters. An SQL programmer would think in terms of VIEWS and CTES.}
}
@article{GARIRA2023122,
title = {The transmission mechanism theory of disease dynamics: Its aims, assumptions and limitations},
journal = {Infectious Disease Modelling},
volume = {8},
number = {1},
pages = {122-144},
year = {2023},
issn = {2468-0427},
doi = {https://doi.org/10.1016/j.idm.2022.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S2468042722001075},
author = {Winston Garira and Bothwell Maregere},
keywords = {Single scale modelling of infectious disease dynamics, Multiscale modelling of infectious disease dynamics, Scales of organization of infectious disease system, Transmission mechanism theory of disease dynamics, Levels of organization of infectious disease system, The replication-transmission relativity theory of disease dynamics},
abstract = {Most of the progress in the development of single scale mathematical and computational models for the study of infectious disease dynamics which now span over a century is build on a body of knowledge that has been developed to address particular single scale descriptions of infectious disease dynamics based on understanding disease transmission process. Although this single scale understanding of infectious disease dynamics is now founded on a body of knowledge with a long history, dating back to over a century now, that knowledge has not yet been formalized into a scientific theory. In this article, we formalize this accumulated body of knowledge into a scientific theory called the transmission mechanism theory of disease dynamics which states that at every scale of organization of an infectious disease system, disease dynamics is determined by transmission as the main dynamic disease process. Therefore, the transmission mechanism theory of disease dynamics can be seen as formalizing knowledge that has been inherent in the study of infectious disease dynamics using single scale mathematical and computational models for over a century now. The objective of this article is to summarize this existing knowledge about single scale modelling of infectious dynamics by means of a scientific theory called the transmission mechanism theory of disease dynamics and highlight its aims, assumptions and limitations.}
}
@incollection{PASCAL2022231,
title = {10 - A practical guide to paleostress analysis},
editor = {Christophe Pascal},
booktitle = {Paleostress Inversion Techniques},
publisher = {Elsevier},
pages = {231-245},
year = {2022},
isbn = {978-0-12-811910-5},
doi = {https://doi.org/10.1016/B978-0-12-811910-5.00008-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128119105000087},
author = {Christophe Pascal},
keywords = {Fieldwork, Measuring, Processing, Plotting, Interpretation, Reporting},
abstract = {After having presented extensively different theoretical and methodological aspects of paleostress reconstruction methods, the purpose of the final chapter is to introduce recommendations for their practical use in tectonic problems. The discussion focuses on paleostress inversion of fault slip data, the latter being both the most elaborated and the most employed method. Detailed practical advice is given to conduct a paleostress study efficiently, starting with data acquisition in the field, proceeding with subsequent computation of paleostress tensors and ending with the reporting of the results and of their subsequent interpretations.}
}
@article{ABDELHAMID2023101986,
title = {Discovering epistasis interactions in Alzheimer’s disease using integrated framework of ensemble learning and multifactor dimensionality reduction (MDR)},
journal = {Ain Shams Engineering Journal},
volume = {14},
number = {7},
pages = {101986},
year = {2023},
issn = {2090-4479},
doi = {https://doi.org/10.1016/j.asej.2022.101986},
url = {https://www.sciencedirect.com/science/article/pii/S2090447922002970},
author = {Marwa M. {Abd El Hamid} and Mohamed Shaheen and Yasser M.K. Omar and Mai S. Mabrouk},
keywords = {Epistasis Interactions, Alzheimer's disease, Personalized Medicine, Ensemble learning techniques},
abstract = {Alzheimer's disease (AD) is a complex disorder with strong genetic factors. The proposed framework is applied to Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset. We present a novel framework integrating ensemble learning and MDR constructive induction algorithm to discover epistasis interactions associated with AD in a computationally efficient method. Discovering epistasis interactions is a big challenge and significantly impacts personalized medicine (PM). The applied ensemble learning algorithms are random forests (RF) with Gini index and permutation importance, Extreme Gradient Boosting (XGBoost), and classification and regression trees (CART). The classification accuracy of 5-way models varied between (0.8674–0.8758), whereas the accuracy of 2-way, 3-way, and 4-way models varied between (0.6515–0.6649), (0.7071–0.7170), and (0.7811–0.7878) respectively. The promising results of this proposed framework show high-ranked risk genes and up to 5-way epistasis models that contribute to the disease risk efficiently and at higher accuracy.}
}
@article{BEHESHTIANARDEKANI1988183,
title = {An empirical study of the use of business expert systems},
journal = {Information & Management},
volume = {15},
number = {4},
pages = {183-190},
year = {1988},
issn = {0378-7206},
doi = {https://doi.org/10.1016/0378-7206(88)90044-4},
url = {https://www.sciencedirect.com/science/article/pii/0378720688900444},
author = {Mehdi Beheshtian-Ardekani and Linda M. Salchenberger},
keywords = {Business expert systems, Artificial intelligence, Knowledge-based systems, Fifth-generation},
abstract = {The evolution of computers from computational tools to “thinking machines” is causing businesses to evaluate their views of the computer's role. The inevitable availability of smart computers leads to questions of how and when fifth generation hardware and software will be integrated into corporate culture. Here, we present the results of a survey given to information systems managers to determine the extent of expert systems development by data processing departments and expert systems usage in organizations. The attitudes of management toward the future of expert systems are also discussed using the survey data. It was discovered that, while computer managers are receptive toward this new tool, most have no definite plans to develop expert systems in the near future. These results seem to be in conflict with other evidence about the growing numbers of expert systems in business applications. One explanation is that this new technology is part of the continuing “grass roots” movement of end-user computing.}
}
@article{RASHEED201686,
title = {Theoretical accounts to practical models: Grounding phenomenon for abstract words in cognitive robots},
journal = {Cognitive Systems Research},
volume = {40},
pages = {86-98},
year = {2016},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2016.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S1389041715300310},
author = {Nadia Rasheed and Shamsudin H.M. Amin and U. Sultana and Rabia Shakoor and Naila Zareen and Abdul Rauf Bhatti},
keywords = {Grounded cognition, Symbol grounding problem, Cognitive robotics, Connectionist computation},
abstract = {This review concentrates on the issue of acquisition of abstract words in a cognitive robot with the grounding principle, from relevant theories to practical models of agents and robots. Most cognitive robotics models developed for grounding of language take inspiration from the findings of neuroscience and psychology to get the theoretical skeleton of these models. To better understand these modelling approaches, it is indispensable to work from the base (theoretical accounts) to the top (computational models). Therefore in this paper, succinct definition of abstract words is presented first, and then the symbol grounding issue and accounts of grounded cognition for abstract words are given. The next section discusses the computational modelling approaches for abstract words grounding phenomenon. Finally, important cognitive robotics models are reviewed. This paper also points out the strengths and weaknesses of relevant hypotheses and models for the representation of abstract words in the grounded cognition framework and helps the understanding of issues such as where and why modelling efforts stand to address this problem in comparison with theoretical findings.}
}
@article{WANG2022101643,
title = {Model for deep learning-based skill transfer in an assembly process},
journal = {Advanced Engineering Informatics},
volume = {52},
pages = {101643},
year = {2022},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2022.101643},
url = {https://www.sciencedirect.com/science/article/pii/S1474034622001070},
author = {Kung-Jeng Wang and Luh {Juni Asrini} and Lucy Sanjaya and Hong-Phuc Nguyen},
keywords = {Convolutional neural network, Deep learning, Faster region-based convolutional neural network, Human machine interaction, Skill transfer},
abstract = {As the variety of products and manufacturing processes increases, the expansion of flexible training approaches is crucial to support the development of human skills. This study presents a model for skill transfer support that extracts experts’ relevant skills as actions and objects relevant to the action into a computational model for transferring skills. This model engages two modes of deep learning as the groundwork, namely, convolutional neural network (CNN) for action recognition and faster region-based convolutional neural network (R-CNN) for object detection. To evaluate the performance of the proposed model, a case study of the final assembly of a GPU card is conducted. The accuracy of CNN and faster R-CNN are 95.4% and 96.8%, respectively. The goal of this model is to guide junior operators during the assembly by providing step-by-step instructions in performing complex tasks. The present study facilitates flexible training in terms of adapting new skills from skilled operators to naïve operators by deep learning.}
}
@article{ZHU2024102509,
title = {Developing a fast and accurate collision detection strategy for crane-lift path planning in high-rise modular integrated construction},
journal = {Advanced Engineering Informatics},
volume = {61},
pages = {102509},
year = {2024},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.102509},
url = {https://www.sciencedirect.com/science/article/pii/S1474034624001575},
author = {Aimin Zhu and Zhiqian Zhang and Wei Pan},
keywords = {Crane-lift, Path planning, Collision detection, Modular integrated construction},
abstract = {Crane-lift path planning (CLPP) ensures the safe and efficient installation of hefty modules in high-rise modular integrated construction (MiC). The implementation of CLPP requires effective collision detection strategies. However, existing collision detection strategies suffer from limitations in terms of computational intensity or insufficient accuracy. This paper aims to develop a fast and accurate collision detection strategy for CLPP in high-rise MiC projects using a single tower crane, thereby achieving safe and efficient module installation. It is executed with the assumptions that the geometry of the building remains unchanged, the positions and orientations of the lifted module and the tower crane are monitored, and no external loads act on the lifted module. Based on the research scope and assumptions, an octree and bounding box (Oct-Box) integrated strategy is developed. The strategy operates in two stages, the pre-execution and execution stages, supported by two critical technical components: (1) an optimized octree for lifting space division and encoding, and (2) an integrated bounding box algorithm for construction object collision detection. The strategy was evaluated using a real-life MiC project in Hong Kong. The results show that the developed strategy minimized the CLPP time by about 95 %, while ensuring continuous and accurate collision detection. In addition, the strategy was significantly affected by the depth of octree, the encoding method of octree, the bounding box algorithm and the configuration density. The developed Oct-Box strategy for CLPP is novel as it addresses temporal efficiency and spatial tightness in tandem, and marks a breakthrough for collision detection in modular construction.}
}
@article{ALEXOPOULOS20241466,
title = {AJGP Solicits Papers Aimed to Enrich Geriatric Psychiatry},
journal = {The American Journal of Geriatric Psychiatry},
volume = {32},
number = {12},
pages = {1466-1468},
year = {2024},
issn = {1064-7481},
doi = {https://doi.org/10.1016/j.jagp.2024.08.018},
url = {https://www.sciencedirect.com/science/article/pii/S1064748124004482},
author = {George S. Alexopoulos}
}
@article{MARKOVSKY202142,
title = {Behavioral systems theory in data-driven analysis, signal processing, and control},
journal = {Annual Reviews in Control},
volume = {52},
pages = {42-64},
year = {2021},
issn = {1367-5788},
doi = {https://doi.org/10.1016/j.arcontrol.2021.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S1367578821000754},
author = {Ivan Markovsky and Florian Dörfler},
keywords = {Behavioral systems theory, Data-driven control, Missing data estimation, System identification},
abstract = {The behavioral approach to systems theory, put forward 40 years ago by Jan C. Willems, takes a representation-free perspective of a dynamical system as a set of trajectories. Till recently, it was an unorthodox niche of research but has gained renewed interest for the newly emerged data-driven paradigm, for which it is uniquely suited due to the representation-free perspective paired with recently developed computational methods. A result derived in the behavioral setting that became known as the fundamental lemma started a new class of subspace-type data-driven methods. The fundamental lemma gives conditions for a non-parametric representation of a linear time-invariant system by the image of a Hankel matrix constructed from raw time series data. This paper reviews the fundamental lemma, its generalizations, and related data-driven analysis, signal processing, and control methods. A prototypical signal processing problem, reviewed in the paper, is missing data estimation. It includes simulation, state estimation, and output tracking control as special cases. The direct data-driven control methods using the fundamental lemma and the non-parametric representation are loosely classified as implicit and explicit approaches. Representative examples are data-enabled predictive control (an implicit method) and data-driven linear quadratic regulation (an explicit method). These methods are equally amenable to certainty-equivalence as well as to robust control. Emphasis is put on the robustness of the methods under noise. The methods allow for theoretical certification, they are computationally tractable, in comparison with machine learning methods require small amount of data, and are robustly implementable in real-time on complex physical systems.}
}
@incollection{CAPLETTE2017905,
title = {Chapter 36 - The Time Course of Object, Scene, and Face Categorization},
editor = {Henri Cohen and Claire Lefebvre},
booktitle = {Handbook of Categorization in Cognitive Science (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {San Diego},
pages = {905-930},
year = {2017},
isbn = {978-0-08-101107-2},
doi = {https://doi.org/10.1016/B978-0-08-101107-2.00036-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780081011072000361},
author = {Laurent Caplette and Éric McCabe and Caroline Blais and Frédéric Gosselin},
keywords = {Categorization, attention, vision, temporal processing, object recognition, scene recognition, face recognition},
abstract = {We first describe Strategy Length & Internal Practicability (SLIP), a formal model for thinking about categorization, in particular about the time course of categorization. We then discuss an early application of this model to basic-levelness. We then turn to aspects of the time course of categorization that have been neglected in the categorization literature: our limited processing capacities; the necessity of having a flexible categorization apparatus; and the paradox that this inexorably brings about. We propose a twofold resolution of this paradox, attempting, in the process, to bridge work done on categorization in vision, neuropsychology, and physiology.}
}
@article{PURI2023104439,
title = {Automatic detection of Alzheimer’s disease from EEG signals using low-complexity orthogonal wavelet filter banks},
journal = {Biomedical Signal Processing and Control},
volume = {81},
pages = {104439},
year = {2023},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2022.104439},
url = {https://www.sciencedirect.com/science/article/pii/S174680942200893X},
author = {Digambar V. Puri and Sanjay L. Nalbalwar and Anil B. Nandgaonkar and Jayanand P. Gawande and Abhay Wagh},
keywords = {Alzheimer’s disease, Electroencephalogram, Fractal dimension, Orthogonal filter banks, Support vector machine, Wavelets},
abstract = {Background:
Alzheimer’s disease (AD) is one of the most common neurodegenerative disorder. As the incidence of AD is rapidly increasing worldwide, detecting it at an early stage can prevent memory loss and cognitive dysfunctions in patients. Recently, Electroencephalogram (EEG) signals in AD cases show less synchronization and a slowing effect. The abrupt and transient behavior of EEG signals can be detected from specific frequency bands that are cortical rhythms of interest such as delta (0−4Hz), theta (4−8Hz), alpha (8−12Hz), beta1 (12−16Hz), beta2 (16−32Hz), and gamma (32−48Hz).
Method:
This paper proposes novel low-complexity orthogonal wavelet filter banks with vanishing moments (LCOWFBs-v) to decompose the AD and normal controlled (NC) EEG signals into subbands (SBs). A generalized design technique is suggested to reduce the computational complexity of original irrational wavelet filter banks (FBs). The two features, Higuchi’s fractal dimension (HFD) and Katz’s fractal dimension (KFD), were extracted from EEG SBs. The significance of these extracted features has been inspected using Kruskal–Wallis test.
Results:
The present study analyzed the EEG recordings of 23 subjects (AD-12 and NC-11) with the combination of LCOWFBs, HFD, and KFD. The proposed technique achieved a classification accuracy of 98.5% and 98.6% using the LCOWFBs-4 and LCOWFBs-6, respectively with a cubic-support vector machine classifier and 10-fold cross-validation technique.
Conclusion:
The proposed method with newly designed LCOWFBs is efficient compared with the well-known FBs and existing techniques for detecting AD.}
}
@article{BAMU20051794,
title = {Damage, deterioration and the long-term structural performance of cooling-tower shells: A survey of developments over the past 50 years},
journal = {Engineering Structures},
volume = {27},
number = {12},
pages = {1794-1800},
year = {2005},
note = {SEMC 2004 Structural Health Monitoring, Damage Detection and Long-Term Performance},
issn = {0141-0296},
doi = {https://doi.org/10.1016/j.engstruct.2005.04.020},
url = {https://www.sciencedirect.com/science/article/pii/S0141029605002257},
author = {P.C. Bamu and A. Zingoni},
keywords = {Cooling towers, Shell structures, Long-term performance, Damage modelling, Deterioration phenomena, Concrete cracking, Shell imperfections, Durability},
abstract = {The last 50 years have seen a gradual shift in trend in research on concrete hyperbolic cooling-tower shells, from the issues of response to short-term loading and immediate causes of collapse in the early part of this period, to the issues of deterioration phenomena, durability and long-term performance in more recent times. This paper traces these developments. After a revisit of some historical collapses of cooling-tower shells, and a brief consideration of condition surveys and repair programmes instituted in the aftermath of these events, focus shifts to the important question of damage and deterioration, and progress made over the past 30 years in the understanding of these phenomena. In particular, much research has gone into the modelling of cracking and geometric imperfections, which have a considerable effect on the load-carrying capacity of the shell, and are also manifestations of long-term deterioration. While structural monitoring of the progression of deterioration in cooling-tower shells, and the accurate prediction of this through appropriate numerical models, will always be important, the thinking now seems to be shifting towards designing for durability right from the outset.}
}
@article{MULDNER2015127,
title = {Utilizing sensor data to model students’ creativity in a digital environment},
journal = {Computers in Human Behavior},
volume = {42},
pages = {127-137},
year = {2015},
note = {Digital Creativity: New Frontier for Research and Practice},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2013.10.060},
url = {https://www.sciencedirect.com/science/article/pii/S074756321300410X},
author = {Kasia Muldner and Winslow Burleson},
keywords = {Creativity, Student modeling, Eye tracking, EEG, Skin conductance, Intelligent Tutoring Systems},
abstract = {While creativity is essential for developing students’ broad expertise in Science, Technology, Engineering, and Math (STEM) fields, many students struggle with various aspects of being creative. Digital technologies have the unique opportunity to support the creative process by (1) recognizing elements of students’ creativity, such as when creativity is lacking (modeling step), and (2) providing tailored scaffolding based on that information (intervention step). However, to date little work exists on either of these aspects. Here, we focus on the modeling step. Specifically, we explore the utility of various sensing devices, including an eye tracker, a skin conductance bracelet, and an EEG sensor, for modeling creativity during an educational activity, namely geometry proof generation. We found reliable differences in sensor features characterizing low vs. high creativity students. We then applied machine learning to build classifiers that achieved good accuracy in distinguishing these two student groups, providing evidence that sensor features are valuable for modeling creativity.}
}
@article{CHOGA202491,
title = {Rapid dynamic changes of FL.2 variant: A case report of COVID-19 breakthrough infection},
journal = {International Journal of Infectious Diseases},
volume = {138},
pages = {91-96},
year = {2024},
issn = {1201-9712},
doi = {https://doi.org/10.1016/j.ijid.2023.11.011},
url = {https://www.sciencedirect.com/science/article/pii/S1201971223007725},
author = {Wonderful T. Choga and Gobuiwang Khilly {Kurusa (Gasenna)} and James Emmanuel San and Tidimalo Ookame and Irene Gobe and Mohammed Chand and Badisa Phafane and Kedumetse Seru and Patience Matshosi and Boitumelo Zuze and Nokuthula Ndlovu and Teko Matsuru and Dorcas Maruapula and Ontlametse T. Bareng and Kutlo Macheke and Lesego Kuate-Lere and Labapotswe Tlale and Onalethata Lesetedi and Modiri Tau and Mpaphi B. Mbulawa and Pamela Smith-Lawrence and Mogomotsi Matshaba and Roger Shapiro and Joseph Makhema and Darren P. Martin and Tulio {de Oliveira} and Richard J. Lessells and Shahin Lockman and Simani Gaseitsiwe and Sikhulile Moyo},
keywords = {SARS-CoV-2, Evolution, FL.2, Immunocompromised, Botswana},
abstract = {We investigated intra-host genetic evolution using two SARS-CoV-2 isolates from a fully vaccinated (primary schedule x2 doses of AstraZeneca plus a booster of Pfizer), >70-year-old woman with a history of lymphoma and hypertension who presented a SARS-CoV-2 infection for 3 weeks prior to death due to COVID-19. Two full genome sequences were determined from samples taken 13 days apart with both belonging to Pango lineage FL.2: the first detection of this Omicron sub-variant in Botswana. FL.2 is a sub-lineage of XBB.1.9.1. The repertoire of mutations and minority variants in the Spike protein differed between the two time points. Notably, we also observed deletions within the ORF1a and Membrane proteins; both regions are associated with high T-cell epitope density. The internal milieu of immune-suppressed individuals may accelerate SARS-CoV-2 evolution; hence, close monitoring is warranted.}
}
@article{ROBERTS2017225,
title = {Clinical Applications of Stochastic Dynamic Models of the Brain, Part II: A Review},
journal = {Biological Psychiatry: Cognitive Neuroscience and Neuroimaging},
volume = {2},
number = {3},
pages = {225-234},
year = {2017},
issn = {2451-9022},
doi = {https://doi.org/10.1016/j.bpsc.2016.12.009},
url = {https://www.sciencedirect.com/science/article/pii/S2451902217300149},
author = {James A. Roberts and Karl J. Friston and Michael Breakspear},
keywords = {Computational neuroscience, Computational psychiatry, Epilepsy, Mathematical modeling, Melancholia, Stochastic},
abstract = {Brain activity derives from intrinsic dynamics (due to neurophysiology and anatomical connectivity) in concert with stochastic effects that arise from sensory fluctuations, brainstem discharges, and random microscopic states such as thermal noise. The dynamic evolution of systems composed of both dynamic and random fluctuations can be studied with stochastic dynamic models (SDMs). This article, Part II of a two-part series, reviews applications of SDMs to large-scale neural systems in health and disease. Stochastic models have already elucidated a number of pathophysiological phenomena, such as epilepsy and hypoxic ischemic encephalopathy, although their use in biological psychiatry remains rather nascent. Emerging research in this field includes phenomenological models of mood fluctuations in bipolar disorder and biophysical models of functional imaging data in psychotic and affective disorders. Together with deeper theoretical considerations, this work suggests that SDMs will play a unique and influential role in computational psychiatry, unifying empirical observations with models of perception and behavior.}
}
@article{WANG2024121777,
title = {Deep learning-based flatness prediction via multivariate industrial data for steel strip during tandem cold rolling},
journal = {Expert Systems with Applications},
volume = {237},
pages = {121777},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.121777},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423022790},
author = {Qinglong Wang and Jie Sun and Yunjian Hu and Wenqiang Jiang and Xinchun Zhang and Zhangqi Wang},
keywords = {Strip flatness, Tandem cold rolling, Deep learning, Multi-variate prediction, Industrial data},
abstract = {Flatness deviations in the tandem cold-rolling process of steel strips have a direct impact on product quality and shape, leading to strip breakage, reduced working speed, and equipment damage. However, conventional physics-based numerical models are inadequate for accurately predicting flatness in the complex operating conditions and variables of tandem rolling environments. To address this challenge, a novel approach is proposed that utilizes deep convolutional neural networks (DCNNs) based on real industrial data from tandem cold rolling. The multi-input and multi-output architecture of our DCNNs enables them to solve the multi-level nonlinear problem associated with flatness prediction in the tandem cold-rolling process. The flatness profiles are effectively predicted using the proposed method, incorporating multiple variables without requiring additional data pre-processing methods. Additionally, the effects of network width, depth, and topology on flatness prediction performance are thoroughly investigated. The developed Inception-ResNet demonstrates remarkable predictive performance while using fewer model parameters and exhibiting lower computational complexity compared to other network architectures. Specifically, the proposed Inception-ResNet-39 model, consisting of 39 layers of learnable parameters, achieves state-of-the-art predictive performance. Our deep learning-based approach accurately predicts flatness in tandem cold-rolling through end-to-end modeling and provides complete pipelines for model transfer construction to ensure efficient implementation.}
}
@incollection{GEWEKE20013463,
title = {Chapter 56 - Computationally Intensive Methods for Integration in Econometrics**The authors gratefully acknowledge financial support from National Science Foundation grants SBR-9511280, SBR-9731037, SES-9814342, and SES-9819444.},
editor = {James J. Heckman and Edward Leamer},
series = {Handbook of Econometrics},
publisher = {Elsevier},
volume = {5},
pages = {3463-3568},
year = {2001},
issn = {1573-4412},
doi = {https://doi.org/10.1016/S1573-4412(01)05009-7},
url = {https://www.sciencedirect.com/science/article/pii/S1573441201050097},
author = {John Geweke and Michael Keane},
keywords = {Bayesian inference, discrete choice, dynamic optimization, integration, Markov chain Monte Carlo, multinomial probit, normal mixtures, selection models, Primary C15, Secondary C11},
abstract = {Until recently, inference in many interesting models was precluded by the requirement of high dimensional integration. But dramatic increases in computer speed, and the recent development of new algorithms that permit accurate Monte Carlo evaluation of high dimensional integrals, have greatly expanded the range of models that can be considered. This chapter presents the methodology for several of the most important Monte Carlo methods, supplemented by a set of concrete examples that show how the methods are used. Some of the examples are new to the econometrics literature. They include inference in multinomial discrete choice models and selection models in which the standard normality assumption is relaxed in favor of a multivariate mixture of normals assumption. Several Monte Carlo experiments indicate that these methods are successful at identifying departures from normality when they are present. Throughout the chapter the focus is on inference in parametric models that permit rich variation in the distribution of disturbances. The chapter first discusses Monte Carlo methods for the evaluation of high dimensional integrals, including integral simulators like the GHK method, and Markov Chain Monte Carlo methods like Gibbs sampling and the Metropolis–Hastings algorithm. It then turns to methods for approximating solutions to discrete choice dynamic optimization problems, including the methods developed by Keane and Wolpin, and Rust, as well as methods for circumventing the integration problem entirely, such as the approach of Geweke and Keane. The rest of the chapter deals with specific examples: classical simulation estimation for multinomial probit models, both in the cross sectional and panel data contexts; univariate and multivariate latent linear models; and Bayesian inference in dynamic discrete choice models in which the future component of the value function is replaced by a flexible polynomial.}
}
@article{ZHANG2022108760,
title = {Causal discovery and inference-based fault detection and diagnosis method for heating, ventilation and air conditioning systems},
journal = {Building and Environment},
volume = {212},
pages = {108760},
year = {2022},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2022.108760},
url = {https://www.sciencedirect.com/science/article/pii/S0360132322000099},
author = {Chaobo Zhang and Yazhou Zhao and Yang Zhao and Tingting Li and Xuejun Zhang},
keywords = {Fault detection and diagnosis, Heating, Ventilation and air conditioning systems, Building energy conservation, Causal discovery and inference, Individual average causal effect estimation, Backward structural causal model},
abstract = {Data driven-based methods have aroused wide attention in the domain of fault detection and diagnosis of heating, ventilation and air conditioning systems. However, they are good at learning statistical relationships between faults and symptoms rather than their causal relationships, resulting in poor interpretability. This paper proposes a causal discovery and inference-based fault detection and diagnosis method to address this challenge. It applies a do-calculus-based individual average causal effect estimation approach to reveal causal relationships between faults and symptoms. Based on the causal relationships discovered, a backward structural causal model is developed for fault detection and diagnosis. Regression coefficients in the model are visualized using heatmaps to explain the model reasoning processes. The method is validated using the experimental data collected by the ASHARE project RP-1312. The individual average causal effect estimation approach reveals the causal relationships between eleven air handing unit faults and twenty-eight symptoms successfully. The diagnosis accuracy of the backward structural causal model (99.58%) is almost as high as that of k-nearest neighbors, support vector machine, classification and regression trees, deep neural networks and convolutional neural networks. Its hyper-parameter optimization time is reduced by 81.64% and 99.91%, respectively, compared with deep neural networks and convolutional neural networks. And its model training time is reduced by 38.61% and 92.01%, respectively. Based on the heatmap of regression coefficients of the model, it is demonstrated that the decision-making processes of the model are understandable and consistent with the domain knowledge in most cases.}
}
@article{SZUBA2001489,
title = {A formal definition of the phenomenon of collective intelligence and its IQ measure},
journal = {Future Generation Computer Systems},
volume = {17},
number = {4},
pages = {489-500},
year = {2001},
note = {Workshop on Bio-inspired Solutions to Parallel Computing problems},
issn = {0167-739X},
doi = {https://doi.org/10.1016/S0167-739X(99)00136-3},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X99001363},
author = {Tadeusz Szuba},
keywords = {Collective intelligence, Quasi-chaotic model of computations, Synergy, IQ, PROLOG},
abstract = {This paper presents a formalization of collective intelligence (CI). A molecular, quasi-chaotic model of computations allows us to model CI in social structures, and to define its measure (IQS). This methodology works for bacterial colonies and social insects as well as for human social structures. With the CI theory some patterns of human behavior receive formal justification, others can be explained as IQS optimization. The CI formalization assumes that it is a property of a social structure, initializing when individuals interact, and as a result, acquiring the ability to solve new or more complex problems. CI amplifies if the structure improves synergy, which further increases the spectrum and complexity of the problems, which can be solved together. The formalization covers areas where CI results in physical synergy and mental/logical cooperation.}
}
@article{ABRUSCI2025100401,
title = {AI4Design: A generative AI-based system to improve creativity in design–A field evaluation},
journal = {Computers and Education: Artificial Intelligence},
volume = {8},
pages = {100401},
year = {2025},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2025.100401},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X25000414},
author = {Luca Abrusci and Karma Dabaghi and Stefano D'Urso and Filippo Sciarrone},
keywords = {Creativity, Design, Generative artificial intelligence},
abstract = {Chatbots serve as valuable instruments for enhancing students' educational experience and aiding them in their day-to-day academic tasks. Advances in Generative AI (GAI) have ushered in increasingly sophisticated and adaptive chatbots, with ChatGPT and DALL⋅E being prime examples. ChatGPT excels at generating text-based answers across diverse areas of knowledge, while DALL⋅E is adept at converting text-based concepts into visual imagery. These technologies are increasingly used by students across various levels of education. In this study, we introduce AI4Design, a web-based system designed to assist design students with their course projects by acting as an intelligent chatbot. The field of design is propitious for such work because of the increasing use of technology and the necessity of introducing its critical use during study. Comprising two integrated modules, the system is based on a two-step workflow. The first step is anchored on ChatGPT, enabling students to prompt questions and receive answers. The second step allows for the generation of one or more images based on the system's answer to the initial question. Our research assesses whether our system can offer valuable insights and inspiration to students in their design work. We conducted an exploratory study in the Design domain involving 31 students from the Lebanese American University. Over a two- to three-day period, participants used the AI4Design system to enhance their projects. A subsequent evaluation of their work indicated improvements in conceptual clarity and visual outputs that highlighted a measurable increase in creativity, supporting the efficacy of both the system and its foundational learning model, which will be confirmed in the future through a large-scale experimental study. Meanwhile, our study suggests that in the iterative design process, GAI can assist students in making better decisions by giving them just-in-time access to a broader palette of possibilities.}
}
@article{HARTLEY1997169,
title = {Semantic networks: visualizations of knowledge},
journal = {Trends in Cognitive Sciences},
volume = {1},
number = {5},
pages = {169-175},
year = {1997},
issn = {1364-6613},
doi = {https://doi.org/10.1016/S1364-6613(97)01057-7},
url = {https://www.sciencedirect.com/science/article/pii/S1364661397010577},
author = {Roger T. Hartley and John A. Barnden},
abstract = {The history of semantic networks is almost as long as that of their parent discipline, artificial intelligence. They have formed the basis of many fascinating, yet controversial, discussions in conferences and in the literature, ranging from metaphysics through to complexity theory in computer science. Many excellent surveys of the field have been written, and yet it is our belief that none of them has examined the important link between their use as a formal scheme for knowledge representation and their more heuristic use as an informal tool for thinking. In our consideration of semantic networks as computerized tools, we will discuss three levels of abstraction that we believe can help us understand how semantic networks are used. I}
}
@article{HOGAN200855,
title = {Advancing the dialogue between inner and outer empiricism: A comment on O’Nualláin},
journal = {New Ideas in Psychology},
volume = {26},
number = {1},
pages = {55-68},
year = {2008},
issn = {0732-118X},
doi = {https://doi.org/10.1016/j.newideapsych.2007.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S0732118X07000293},
author = {Michael J. Hogan},
keywords = {Consciousness, Inner empiricism, Outer empiricism, Evolution, No-mind, Mythos, Logos},
abstract = {In a recent contribution to New Ideas in Psychology, Seán O’Nualláin draws out a distinction between inner and outer empiricism, and suggests that consciousness research can benefit from analysis in both directions, that is, via the exploration of facts and relations that facilitate a third-person understanding of consciousness (by reference to an analysis of the structures, processes, and functions of the brain) and via the direct exploration of conscious experience itself, both in terms of its computational (content filled) and non-computational (content empty) aspects. In positing a substrate of subjectivity independent of the contents of consciousness (and, more specifically, a state of “nothingness”), Ò’Nualláin follows a long tradition deeply rooted in mythical, religious, and esoteric schools of belief and practice. Although there is considerable debate amongst philosophers, psychologists, and neuroscientists as to whether or not a non-computational view of consciousness is viable, O’Nualláin accepts that such a possibility does exist. Further, he suggests that a dialogue between the inner and outer empiricists will be fruitful. In this comment I, critique Ò’Nualláin's initial thoughts on the subject and draw out a series of useful distinctions that will help to advance the dialogue between inner and outer empiricism. Critical amongst these distinctions is explicit reference to (1) ontological and epistemological interdependencies in consciousness research, and (2) states of consciousness that describe the transition from “mindfulness” through “nothingness” to “no-mind”.}
}
@article{TARAPOULOUZI2022123410,
title = {Heavy metals detection at chemometrics-powered electrochemical (bio)sensors},
journal = {Talanta},
volume = {244},
pages = {123410},
year = {2022},
issn = {0039-9140},
doi = {https://doi.org/10.1016/j.talanta.2022.123410},
url = {https://www.sciencedirect.com/science/article/pii/S0039914022002065},
author = {Maria Tarapoulouzi and Vincenzo Ortone and Stefano Cinti},
keywords = {Multivariate analysis, Design of experiment, Artificial intelligence, Electroanalysis, Sensors, Heavy metals},
abstract = {Heavy metals represent a serious issue regarding both environmental and health status. Their monitoring is necessary and it is necessary the development of decentralized approaches that are able to enforce the risk assessment. Electrochemical sensors and biosensors, with the various architectures, represent a solid reality often involved for this type of analytical determination. Although these approaches offer easy-to-use and portable tools, some limitations are often highlighted in presence of multi-targets and/or real matrices. However, chemometrics- and artificial intelligence-based tools, both for designing and for data analyzing, display the capability in producing novel functionality towards the management of complex matrices which often contain more information than those that are visualized with sensor detection. Design of experiment, exploratory, predictive and regression analysis can push the world of electrochemical (bio)sensors beyond the state of the art, because is still too large the number of analytical chemists that do not deal with multivariate thinking. In this paper, the use of multivariate methods applied to electrochemical sensing of heavy metals is showed, and each approach is described in terms of efficacy and outputs.}
}
@article{YANG2022849,
title = {Creative problem solving in knowledge-rich contexts},
journal = {Trends in Cognitive Sciences},
volume = {26},
number = {10},
pages = {849-859},
year = {2022},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2022.06.012},
url = {https://www.sciencedirect.com/science/article/pii/S1364661322001565},
author = {Wenjing Yang and Adam E. Green and Qunlin Chen and Yoed N. Kenett and Jiangzhou Sun and Dongtao Wei and Jiang Qiu},
keywords = {creativity, creative problem solving, knowledge, analogy, transfer},
abstract = {Creative problem solving (CPS) in real-world contexts often relies on reorganization of existing knowledge to serve new, problem-relevant functions. However, classic creativity paradigms that minimize knowledge content are generally used to investigate creativity, including CPS. We argue that CPS research should expand consideration of knowledge-rich problem contexts, both in novices and experts within specific domains. In particular, paradigms focusing on creative analogical transfer of knowledge may reflect CPS skills that are applicable to real-world problem solving. Such paradigms have begun to provide process-level insights into cognitive and neural characteristics of knowledge-rich CPS and point to multiple avenues for fruitfully expanding inquiry into the role of crystalized knowledge in creativity.}
}
@article{MIKITEN1995141,
title = {Intuition-based computing: A new kind of ‘virtual reality’},
journal = {Mathematics and Computers in Simulation},
volume = {40},
number = {1},
pages = {141-147},
year = {1995},
issn = {0378-4754},
doi = {https://doi.org/10.1016/0378-4754(95)00023-1},
url = {https://www.sciencedirect.com/science/article/pii/0378475495000231},
author = {Terry M. Mikiten},
keywords = {Intuition, Mind, Problem-solving, Creativity, Cognition, Computing, Grand challenge},
abstract = {It is helpful to consider the mind and the computer as two separate information domains. Each has a separate system of rules that guide behavior. The interaction between the two is characterized as an interplay between rule systems. In this view, there should be interactions which are optimal and others which are not. To understand this, the first task is to identify the rules that operate in each domain. The next is to see how they interact. It is concluded that rules of the mind which give rise to what is generally termed ‘intuition’ is altogether compatible with rules of computation. This, in turn, suggests computational systems capable of independent ‘intuitive’ processing on the one hand, and other computational systems which can serve to augment human intuition.}
}
@article{RAJPAL2022119624,
title = {Psychedelics and schizophrenia: Distinct alterations to Bayesian inference},
journal = {NeuroImage},
volume = {263},
pages = {119624},
year = {2022},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2022.119624},
url = {https://www.sciencedirect.com/science/article/pii/S105381192200739X},
author = {Hardik Rajpal and Pedro A.M. Mediano and Fernando E. Rosas and Christopher B. Timmermann and Stefan Brugger and Suresh Muthukumaraswamy and Anil K. Seth and Daniel Bor and Robin L. Carhart-Harris and Henrik J. Jensen},
keywords = {Psychedelics, Schizophrenia, Information theory, Predictive processing},
abstract = {Schizophrenia and states induced by certain psychotomimetic drugs may share some physiological and phenomenological properties, but they differ in fundamental ways: one is a crippling chronic mental disease, while the others are temporary, pharmacologically-induced states presently being explored as treatments for mental illnesses. Building towards a deeper understanding of these different alterations of normal consciousness, here we compare the changes in neural dynamics induced by LSD and ketamine (in healthy volunteers) against those associated with schizophrenia, as observed in resting-state M/EEG recordings. While both conditions exhibit increased neural signal diversity, our findings reveal that this is accompanied by an increased transfer entropy from the front to the back of the brain in schizophrenia, versus an overall reduction under the two drugs. Furthermore, we show that these effects can be reproduced via different alterations of standard Bayesian inference applied on a computational model based on the predictive processing framework. In particular, the effects observed under the drugs are modelled as a reduction of the precision of the priors, while the effects of schizophrenia correspond to an increased precision of sensory information. These findings shed new light on the similarities and differences between schizophrenia and two psychotomimetic drug states, and have potential implications for the study of consciousness and future mental health treatments.}
}
@article{LANG2017298,
title = {Mesoscopic Simulation Models for Logistics Planning Tasks in the Automotive Industry},
journal = {Procedia Engineering},
volume = {178},
pages = {298-307},
year = {2017},
note = {RelStat-2016: Proceedings of the 16th International Scientific Conference Reliability and Statistics in Transportation and Communication October 19-22, 2016. Transport and Telecommunication Institute, Riga, Latvia},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2017.01.118},
url = {https://www.sciencedirect.com/science/article/pii/S1877705817301182},
author = {Sebastian Lang and Tobias Reggelin and Toralf Wunder},
keywords = {automotive industry, logistics planning, production planning, mesoscopic simulation, discrete-rate simulation},
abstract = {The paper evaluates mesoscopic simulation models applied to logistics planning tasks in the automotive industry. In terms of level of detail, mesoscopic simulation models fall between object based discrete-event simulation models and flow based continuous simulation models. Mesoscopic models represent logistics flow processes on an aggregated level through piecewise constant flow rates instead of modeling individual flow objects. The results are not obtained by counting individual objects but by using mathematical formulas to calculate the results as continuous quantities in every modeling time step. This leads to a fast model creation and computation. The authors expect that mesoscopic simulation models can help to support decisions on the operational, tactical and strategic level of planning. The paper describes a mesoscopic simulation model of the goods receiving of an assembly plant and compares the simulation results and computation time with a discrete-event model.}
}
@incollection{SCHONER202471,
title = {Chapter 4 - Toward a neural theory of goal-directed reaching movements},
editor = {Mindy F. Levin and Maurizio Petrarca and Daniele Piscitelli and Susanna Summa},
booktitle = {Progress in Motor Control},
publisher = {Academic Press},
pages = {71-102},
year = {2024},
isbn = {978-0-443-23987-8},
doi = {https://doi.org/10.1016/B978-0-443-23987-8.00008-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780443239878000080},
author = {Gregor Schöner and Lukas Bildheim and Lei Zhang},
keywords = {Dynamic field theory, Neural dynamics, Neural timers, Target selection, Movement initiation, Mouse-tracking paradigm, Degrees of freedom problem, Posture movement problem},
abstract = {How do we bring about goal-directed motor acts? Reaching for an object that offers a useful exemplary case around which the processes underlying human movement behavior can be studied. Such reaching entails processes from scene and object perception, target selection, and movement initiation, to timing and control. These processes are typically studied in different subdisciplines, using different methods based on different theoretical concepts. Yet they are continuously coupled online and evolve in a closed loop. Understanding how they work together thus requires an integrative theoretical framework. While abstract computational ideas are often invoked for such integration, we argue for a theoretical account that is grounded in neural principles. We review the key concepts of a neural theory of goal-directed reaching movements that draw on neural dynamic models of population activation in which recurrent connectivity provides stability. For each component process, we discuss the key issues and empirical constraints for a neural dynamic account. Although a complete neural architecture of goal-directed movement behavior is still under development, the outline we provide interfaces with a large set of empirical findings.}
}
@article{ROBERTSON2008436,
title = {New frontiers in space propulsion sciences},
journal = {Energy Conversion and Management},
volume = {49},
number = {3},
pages = {436-452},
year = {2008},
note = {Space Nuclear Power and Propulsion},
issn = {0196-8904},
doi = {https://doi.org/10.1016/j.enconman.2007.10.013},
url = {https://www.sciencedirect.com/science/article/pii/S019689040700369X},
author = {Glen A. Robertson and P.A. Murad and Eric Davis},
keywords = {Space propulsion, Warp drive, Worm Holes, EM propulsion},
abstract = {Mankind’s destiny points toward a quest for the stars. Realistically, it is difficult to achieve this using current space propulsion science and develop the prerequisite technologies, which for the most part requires the use of massive amounts of propellant to be expelled from the system. Therefore, creative approaches are needed to reduce or eliminate the need for a propellant. Many researchers have identified several unusual approaches that represent immature theories based upon highly advanced concepts. These theories and concepts could lead to creating the enabling technologies and forward thinking necessary to eventually result in developing new directions in space propulsion science. In this paper, some of these theoretical and technological concepts are examined – approaches based upon Einstein’s General Theory of Relativity, spacetime curvature, superconductivity, and newer ideas where questions are raised regarding conservation theorems and if some of the governing laws of physics, as we know them, could be violated or are even valid. These conceptual ideas vary from traversable wormholes, Krasnikov tubes and Alcubierre’s warpdrive to Electromagnetic (EM) field propulsion with possible hybrid systems that incorporate our current limited understanding of zero point fields and quantum mechanics.}
}
@incollection{VALLERO2025503,
title = {Chapter 19 - The future},
editor = {Daniel A. Vallero},
booktitle = {Fundamentals of Water Pollution},
publisher = {Elsevier},
pages = {503-504},
year = {2025},
isbn = {978-0-443-28987-3},
doi = {https://doi.org/10.1016/B978-0-443-28987-3.00014-X},
url = {https://www.sciencedirect.com/science/article/pii/B978044328987300014X},
author = {Daniel A. Vallero},
keywords = {Emerging water pollutants, Emerging treatment technologies, Environmental communications},
abstract = {Those chapter concludes the book with a discussion of some of the successes of water pollution controls and water supply, along with remaining challenges.}
}
@article{ALAYANDE2020e00436,
title = {Estimating effective rates of protection in Nigeria's protected cement industry},
journal = {Scientific African},
volume = {8},
pages = {e00436},
year = {2020},
issn = {2468-2276},
doi = {https://doi.org/10.1016/j.sciaf.2020.e00436},
url = {https://www.sciencedirect.com/science/article/pii/S2468227620301745},
author = {Folarin Alayande},
keywords = {Trade protection, Effective rate of protection, Trade policy, Nigerian cement},
abstract = {Trade protection for selected products is a key element of smart industrial policy in many developing countries. Understanding quantitative measures of trade protection for industrial and consumer commodities is therefore a key requisite to determining the effectiveness of export-led growth in particular, and overall industrial policy. However, accurate estimates for trade policy incentives provided to industrial products are few and comparable datasets are sparse, with the most recent published country datasets dated as far back as 2012, yet with limited sector indices. This study estimates the effective rate of protection (ERP), a key index of trade protection and industrial policy, for the cement industry, one of the largest manufacturing industries in Nigeria. Time series data for 16 years, on the actual cost of trade protection, including tariff barriers and import prohibition bans, from 2000 to 2015 is used. With the ERP, the true cost of protection of domestic manufactures from imported goods, is computed using input shares and tariff data from the United Nations COMTRADE database. The data show the basis for computation and provides a re-useable template for central planners in the computation of effective rate of protection for similar manufacturing industries and other African countries. The computed ERP for the cement industry in Nigeria show a relatively high protection rate, and the overwhelming impact of trade prohibition on the ERP after the implementation of the Federal Government's incentive-led Backward Integration Programme. The evidence is compared with earlier data on Africa. Preliminary findings and trend analysis indicate a high correlation between the ERP and value added to gross domestic product (GDP).}
}
@article{MACHADO2021103322,
title = {Contributions of modularity to the circular economy: A systematic review of literature},
journal = {Journal of Building Engineering},
volume = {44},
pages = {103322},
year = {2021},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2021.103322},
url = {https://www.sciencedirect.com/science/article/pii/S2352710221011803},
author = {Natália Machado and Sandra Naomi Morioka},
keywords = {Modularity, Circular economy, Product modular design, Module optimization, Product lifecycle},
abstract = {In the creation of practices to foster the thinking of the circular economy (CE), modularity can be a facilitator. There are many studies that address modularity and the circular economy individualized, but the integration between the two is still little addressed. This study conducts a systematic review of the literature and aims to identify how modularity can contribute to the circular economy. The data analysis begins with the identification of the main characteristics of the literature that address modularity and circular economy, bringing the evolution of studies over the years, main journals, co-citation of references, co-occurrence of keywords and shows four research clusters linked to modularity and CE, being: conceptualization of modularization, modular design of products, module optimization and product lifecycle. In addition, identifies fifteen benefits of modularity that can contribute to the implementation of strategies for the circular economy and five barriers from the perspective of the circular economy that can inhibit the contribution process. So, it was possible to create an integrative conceptual framework that shows how modularity can contribute to the circular economy. From the results, future research is identified in order to contribute to the transition from a linear economy to a circular economy.}
}
@article{WANG2024130178,
title = {A measurement-device-independent quantum secure digital payment},
journal = {Physica A: Statistical Mechanics and its Applications},
volume = {655},
pages = {130178},
year = {2024},
issn = {0378-4371},
doi = {https://doi.org/10.1016/j.physa.2024.130178},
url = {https://www.sciencedirect.com/science/article/pii/S0378437124006873},
author = {Qingle Wang and Jiacheng Liu and Guodong Li and Yunguang Han and Yuqian Zhou and Long Cheng},
keywords = {Quantum digital payment, Measurement-device-independent, Quantum secure communication, Authentication},
abstract = {In contemporary society, digital payment systems are crucial, yet vulnerable to security breaches. Based on the principles of quantum physics, quantum digital payment (QDP) protocols offer a theoretically superior security paradigm compared to those reliant on computational complexity. Nevertheless, those QDP protocols in practice are frequently compromised by imperfections in measurement devices, facilitating valuable information interception by malicious entities. Addressing this vulnerability, we propose a measurement-device-independent quantum secure digital payment (MDI-QSDP) protocol, designed to enhance security in digital payment systems by eliminating side-channel attacks on measurement devices. This protocol extends the framework of a novelly developed measurement-device-independent quantum secure communication (MDI-QSC) protocol, which supports secure dialogic exchanges without prior key sharing. Utilizing the proposed MDI-QSC protocol, participants can not only engage in secure direct communication but also establish a private key for subsequent encrypted interactions. Our MDI-QSDP protocol incorporates a robust authentication mechanism, ensuring that only legitimate participants can initiate transactions, thereby bolstering security. A comprehensive security analysis of the proposed protocol demonstrates its resilience against identity theft, information leakage, and other potential security breaches. Furthermore, simulations employing practical experimental parameters validate the protocol’s applicability and effectiveness in real-world scenarios, thereby confirming its potential to significantly enhance the security of future quantum digital payments.}
}
@incollection{SUNDARARAJAN2025326,
title = {Bioinformatics for Clinical Diagnostics},
editor = {Shoba Ranganathan and Mario Cannataro and Asif M. Khan},
booktitle = {Encyclopedia of Bioinformatics and Computational Biology (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {326-332},
year = {2025},
isbn = {978-0-323-95503-4},
doi = {https://doi.org/10.1016/B978-0-323-95502-7.00278-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780323955027002785},
author = {Vijayaraghava Seshadri Sundararajan and Prashanth N. Suravajhala},
keywords = {Algorithms, Artificial intelligence, ChatGPT, Clinical diagnostics, Disease predictions, Microbiology, Pathology, Radiology},
abstract = {Clinical diagnostics is the process of identifying diseases or medical conditions in patients through various tests, including laboratory analyses, imaging, and genetic testing. It helps guide treatment, monitor disease progression, and evaluate overall health. Accurate diagnostics are crucial for early detection, personalized medicine, and improving patient outcomes. Bioinformatics plays a significant role in clinical diagnostics by analyzing vast amounts of biological data, especially genetic information. Bioinformatics tools can help interpret genomic sequences, identify disease-associated mutations, and uncover biomarkers for specific conditions. This accelerates diagnosis, allows for personalized treatments, and supports research into novel diagnostics, ultimately making healthcare more precise and efficient. Bioinformatics tools for image analysis have hastened the analysis of pathological images and scans, providing accurate diagnosis. This chapter provides an overview of the latest bioinformatics developments in clinical diagnostics.}
}
@article{CAI201253,
title = {On fast and accurate block-based motion estimation algorithms using particle swarm optimization},
journal = {Information Sciences},
volume = {197},
pages = {53-64},
year = {2012},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2012.02.014},
url = {https://www.sciencedirect.com/science/article/pii/S002002551200117X},
author = {Jing Cai and W. {David Pan}},
keywords = {Particle swarm optimization, Motion estimation, Fast block-matching methods, Video sequences, Computational complexity},
abstract = {Both fast and accurate block-matching algorithms are critical to efficient compression of video frames using motion estimation and compensation. While the particle swarm optimization approach holds the promise of alleviating the local optima problem suffered typically by existing very fast block matching methods, motion estimation algorithms based on particle swarm optimization in the literature appear to be either much slower than some leading fast block-matching methods for a given accuracy of motion estimation, or less accurate for a given computational complexity. In this paper, we show that the conventional particle swarm optimization approach, which was originally designed to solve general optimization problems where fast convergence of the algorithm might not be a primary concern, could be modified appropriately so that it could provide accurate motion estimation with very low computational cost in the specific context of video motion estimation. To this end, we proposed a new block matching algorithm based on a set of strategies adapted from the standard particle swarm optimization approach. Extensive simulations showed that the proposed method could achieve significant improvements over leading fast block matching methods including the diamond search and the cross-diamond search methods, in terms of both estimation accuracy and computational cost. In particular, the proposed method based on particle swarm optimization is not only much faster, but also remarkably more accurate (about 2dB higher in terms of the Peak Signal-to-Noise-Ratio) than the competing methods on video sequences with large motion.}
}
@article{KRIVY2023100057,
title = {Digital ecosystem: The journey of a metaphor},
journal = {Digital Geography and Society},
volume = {5},
pages = {100057},
year = {2023},
issn = {2666-3783},
doi = {https://doi.org/10.1016/j.diggeo.2023.100057},
url = {https://www.sciencedirect.com/science/article/pii/S2666378323000090},
author = {Maroš Krivý},
keywords = {Nature, Ecosystem, Digital capitalism, Platform capitalism, Metaphor, Imaginaries},
abstract = {The term “digital ecosystem” has become ubiquitous through a seemingly endless stream of scholarship, punditry and hyperbole around digitalization, to the point that the metaphor is becoming dead. Considering “ecosystem” as a traveling concept straddling natural, social and technical systems, this article traces the extension of “digital ecosystem,” along with the adjacent “business ecosystem” and “entrepreneurial ecosystem,” in the fields of computer science, economy, governance and environmental policy. The origins of the concept as a form of circuitry applied to nature are outlined as a background against which to trace its role as a socio-technical metaphor for digital capitalism. Since the 1990s, various formulations of “ecosystem” have offered a naturalistic interpretation to phenomena ranging from economic interactions to digital infrastructure and the urban everyday. I conclude that by representing the internet and the market as complex, self-organizing processes, the metaphor prioritizes the imperative of adapting to—and downplays the possibility of challenging—our erratic digital capitalism. The article contributes by illuminating the ideological work of naturalistic models in the digital political economy. Evidence on using digital ecosystems in environmental policy is still emerging but points to a form of legitimacy exchange that reduces environmental problems to technical issues.}
}
@article{CHOI2025102535,
title = {Comparison study of PR curriculum and PR job posts},
journal = {Public Relations Review},
volume = {51},
number = {1},
pages = {102535},
year = {2025},
issn = {0363-8111},
doi = {https://doi.org/10.1016/j.pubrev.2024.102535},
url = {https://www.sciencedirect.com/science/article/pii/S0363811124001140},
author = {Minhee Choi and Baobao Song and Yani Zhao and Lauren Tortella},
keywords = {Public relations industry skills, Public relations education, KSAO, Natural language processing},
abstract = {This study undertakes a comprehensive examination by comparing requirements outlined in entry-level PR job postings with the curricula of 83 undergraduate programs accredited by ACEJMC. Although the comparative analysis underscores the consistent alignment of PR education with industry expectations, some discrepancies in degree requirements and required skills and abilities are found. By employing a natural language processing approach, this study not only investigates current industry needs in terms of human resources but also provides practical implications for PR education.}
}
@article{AHMAN201351,
title = {Normalization by Evaluation and Algebraic Effects},
journal = {Electronic Notes in Theoretical Computer Science},
volume = {298},
pages = {51-69},
year = {2013},
note = {Proceedings of the Twenty-ninth Conference on the Mathematical Foundations of Programming Semantics, MFPS XXIX},
issn = {1571-0661},
doi = {https://doi.org/10.1016/j.entcs.2013.09.007},
url = {https://www.sciencedirect.com/science/article/pii/S1571066113000534},
author = {Danel Ahman and Sam Staton},
keywords = {Algebraic effects, Type theory, Normalization by evaluation, Presheaves, Monads},
abstract = {We examine the interplay between computational effects and higher types. We do this by presenting a normalization by evaluation algorithm for a language with function types as well as computational effects. We use algebraic theories to treat the computational effects in the normalization algorithm in a modular way. Our algorithm is presented in terms of an interpretation in a category of presheaves equipped with partial equivalence relations. The normalization algorithm and its correctness proofs are formalized in dependent type theory (Agda).}
}
@incollection{VALYAN202025,
title = {Chapter 4 - Decision-making deficits in substance use disorders: cognitive functions, assessment paradigms, and levels of evidence},
editor = {Antonio Verdejo-Garcia},
booktitle = {Cognition and Addiction},
publisher = {Academic Press},
pages = {25-61},
year = {2020},
isbn = {978-0-12-815298-0},
doi = {https://doi.org/10.1016/B978-0-12-815298-0.00004-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128152980000046},
author = {Alireza Valyan and Hamed Ekhtiari and Ryan Smith and Martin P. Paulus},
keywords = {Assessment, Behavioral tasks, Computational models, Decision-making dysfunction, fMRI, Intervention, Substance use disorder},
abstract = {Aberrant decision-making plays an important role in both the onset and maintenance of substance use disorders (SUDs). The current state of research within the field of SUDs can be usefully summarized within three broad dimensions: (1) the goal of characterizing the affected cognitive components that contribute to aberrant decision-making (i.e., value, probability, time, and learning functions), (2) the instruments/methods used to accomplish that goal (i.e., self-reports, behavioral tasks, computational modeling, and brain mapping), and (3) the levels of evidence afforded by those instruments/methods. In this chapter, we review and organize the most recent findings based on this three-dimensional framework. Our aim is to (1) provide a comprehensive synthesis of current research on decision-making in SUDs that can serve as a useful resource to guide future research, (2) highlight current limitations in the field and promising future research directions, and (3) illustrate ways in which the framework that we provide may inform the design and implementation of interventional strategies that can advance the field of addiction medicine.}
}
@article{SUTOYO2015435,
title = {Dynamic Difficulty Adjustment in Tower Defence},
journal = {Procedia Computer Science},
volume = {59},
pages = {435-444},
year = {2015},
note = {International Conference on Computer Science and Computational Intelligence (ICCSCI 2015)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.07.563},
url = {https://www.sciencedirect.com/science/article/pii/S187705091502092X},
author = {Rhio Sutoyo and Davies Winata and Katherine Oliviani and Dedy Martadinata Supriyadi},
keywords = {dynamic game balancing, tower defence, dynamic difficulty adjustment, computational intelligence},
abstract = {When we play tower defence game, generally we repeat the same stages several times with the same enemies. Moreover, when the players play a stage that is ridiculously hard or way too easy, they would probably quit the game because it ismoderately frustrating or boring. The purpose of this research is to createa game that can adapt to the players’ ability so the difficulty of the game becomes dynamic. In other words, the game will have different difficultiesof levels according to the players’ ability. High difficulty levels will be set if the players use good strategy and low difficulty levels will be set if the players use bad strategy. In this work, we determine the difficulties based on players’ lives, enemies’ health, and passive skills (skill points) that are chosen by the player. With three of these factors, players will have varies experience of playing tower defence because different combination will give different results to the system and difficulties of the games will be different for each gameplay. The result of this research is a dynamic difficulty tower defence game, dynamic difficulty adjustment (DDA) document, and gameplay outputs for best, average, and worst strategy cases.}
}
@article{HAID2024,
title = {Exploring AI: Transforming medical practice, education and research},
journal = {Journal of Pediatric Urology},
year = {2024},
issn = {1477-5131},
doi = {https://doi.org/10.1016/j.jpurol.2024.12.013},
url = {https://www.sciencedirect.com/science/article/pii/S1477513124006740},
author = {Bernhard Haid and Caleb Nelson and M. İrfan Dönmez and Salvatore Cascio and Massimo Garriboli and Anka Nieuwhof-Leppink and Christina Ching and Luis H. Braga and Ilina Rosklija and Luke Harper},
keywords = {Artificial intelligence, Large language models, History, Education}
}
@article{PARK2025101119,
title = {Code suggestions and explanations in programming learning: Use of ChatGPT and performance},
journal = {The International Journal of Management Education},
volume = {23},
number = {2},
pages = {101119},
year = {2025},
issn = {1472-8117},
doi = {https://doi.org/10.1016/j.ijme.2024.101119},
url = {https://www.sciencedirect.com/science/article/pii/S1472811724001903},
author = {Arum Park and Taekyung Kim},
keywords = {Future of education, Education, OpenAI, ChatGPT, Management education, Programming skills},
abstract = {This study investigates the role of generative artificial intelligence (AI) chatbots, particularly ChatGPT, in enhancing programming education for university students, specifically in big data analytics. The research addresses the growing need for innovative educational practices, especially in developed East Asian countries like South Korea, where declining university enrollment presents new challenges. Using a sample size of N = 343 students, this mixed-methods research employed controlled experiments and surveys to compare student performance in programming tasks across three groups: those using ChatGPT, those using Stack Overflow, and a control group without external assistance. Results showed that students using ChatGPT significantly outperformed those relying on Stack Overflow or no assistance, particularly in hands-on coding tasks. This research contributes to the ongoing discourse on AI in education by providing empirical evidence of generative AI's effectiveness in improving learning outcomes and engagement, while also highlighting the challenges associated with integrating AI into educational settings. The findings emphasize the potential of ChatGPT to personalize learning experiences, improve performance, and offer real-time support, underscoring the need for a balanced curriculum design that incorporates AI while maintaining academic integrity and human oversight.}
}
@article{KAPUSTINA2024100072,
title = {User-friendly and industry-integrated AI for medicinal chemists and pharmaceuticals},
journal = {Artificial Intelligence Chemistry},
volume = {2},
number = {2},
pages = {100072},
year = {2024},
issn = {2949-7477},
doi = {https://doi.org/10.1016/j.aichem.2024.100072},
url = {https://www.sciencedirect.com/science/article/pii/S2949747724000307},
author = {Olga Kapustina and Polina Burmakina and Nina Gubina and Nikita Serov and Vladimir Vinogradov},
keywords = {Machine Learning, Medicinal Chemistry, Pharmaceutics, Data-Driven Drug Discovery},
abstract = {Artificial intelligence has brought crucial changes to the whole field of natural sciences. Myriads of machine learning algorithms have been developed to facilitate the work of experimental scientists. Molecular property prediction and drug synthesis planning become routine tasks. Moreover, inverse design of compounds with tunable properties as well as on-the-fly autonomous process optimization and chemical space exploration became possible in silico. Affordable robotic platforms exist able to perform thousands of experiments every day, analyzing the results and tuning the protocols. Despite this, most of these developments get trapped at the stage of code or overlooked, limiting their use by experimental scientists. Meanwhile, visibility and the number of user-friendly tools and technologies available to date is too low to compensate for this fact, rendering the development of novel therapeutic compounds inefficient. In this Review, we set the goal to bridge the gap between modern technologies and experimental scientists to improve drug development efficacy. Here we survey advanced and easy-to-use technologies able to help medical chemists at every stage of their research, including those integrated in technological processes during COVID-19 pandemic motivated by the need for fast yet precise solutions. Moreover, we review how these technologies are integrated by industry and clinics to streamline drug development and production. These technologies already transform the current paradigm of scientific thinking and revolutionize not only medicinal chemistry, but the whole field of natural sciences.}
}
@incollection{BRUDER2017101,
title = {Chapter 5 - Infrastructural intelligence: Contemporary entanglements between neuroscience and AI},
editor = {Tara Mahfoud and Sam McLean and Nikolas Rose},
series = {Progress in Brain Research},
publisher = {Elsevier},
volume = {233},
pages = {101-128},
year = {2017},
booktitle = {Vital Models},
issn = {0079-6123},
doi = {https://doi.org/10.1016/bs.pbr.2017.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S0079612317300547},
author = {Johannes Bruder},
keywords = {Artificial intelligence, Computational neuroscience, Brain imaging, Google DeepMind technologies, Default mode network},
abstract = {In this chapter, I reflect on contemporary entanglements between artificial intelligence and the neurosciences by tracing the development of Google's recent DeepMind algorithms back to their roots in neuroscientific studies of episodic memory and imagination. Google promotes a new form of “infrastructural intelligence,” which excels by constantly reassessing its cognitive architecture in exchange with a cloud of data that surrounds it, and exhibits putatively human capacities such as intuition. I argue that such (re)alignments of biological and artificial intelligence have been enabled by a paradigmatic infrastructuralization of the brain in contemporary neuroscience. This infrastructuralization is based in methodologies that epistemically liken the brain to complex systems of an entirely different scale (i.e., global logistics) and has given rise to diverse research efforts that target the neuronal infrastructures of higher cognitive functions such as empathy and creativity. What is at stake in this process is no less than the shape of brains to come and a revised understanding of the intelligent and creative social subject.}
}
@article{MARTIN2018105,
title = {On an inferential model construction using generalized associations},
journal = {Journal of Statistical Planning and Inference},
volume = {195},
pages = {105-115},
year = {2018},
note = {Confidence distributions},
issn = {0378-3758},
doi = {https://doi.org/10.1016/j.jspi.2016.11.006},
url = {https://www.sciencedirect.com/science/article/pii/S0378375816301537},
author = {Ryan Martin},
keywords = {Likelihood, Marginalization, Monte Carlo, Plausibility function, Random set, Validity},
abstract = {The inferential model (IM) approach, like fiducial and its generalizations, depends on a representation of the data-generating process. Here, a particular variation on the IM construction is considered, one based on generalized associations. The resulting generalized IM is more flexible in that it does not require a complete specification of the data-generating process and is provably valid under mild conditions. Computation and marginalization strategies are discussed, and two applications of this generalized IM approach are presented.}
}
@article{BENZEKRY201553,
title = {Metronomic reloaded: Theoretical models bringing chemotherapy into the era of precision medicine},
journal = {Seminars in Cancer Biology},
volume = {35},
pages = {53-61},
year = {2015},
note = {Complexity in Cancer Biology},
issn = {1044-579X},
doi = {https://doi.org/10.1016/j.semcancer.2015.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S1044579X15000759},
author = {Sébastien Benzekry and Eddy Pasquier and Dominique Barbolosi and Bruno Lacarelle and Fabrice Barlési and Nicolas André and Joseph Ciccolini},
keywords = {Metronomic chemotherapy, Mathematical modeling, PK/PD, Precision medicine},
abstract = {Oncology has benefited from an increasingly growing number of groundbreaking innovations over the last decade. Targeted therapies, biotherapies, and the most recent immunotherapies all contribute to increase the number of therapeutic options for cancer patients. Consequently, substantial improvements in clinical outcomes for some disease with dismal prognosis such as lung carcinoma or melanoma have been achieved. Of note, the latest innovations in targeted therapies or biotherapies do not preclude the use of standard cytotoxic agents, mostly used in combination. Importantly, and despite the rise of bioguided (a.k.a. precision) medicine, the administration of chemotherapeutic agents still relies on the maximum tolerated drug (MTD) paradigm, a concept inherited from theories conceptualized nearly half a century ago. Alternative dosing schedules such as metronomic regimens, based upon the repeated and regular administration of low doses of chemotherapeutic drugs, and adaptive therapy (i.e. modulating the dose and frequency of cytotoxics administration to control disease progression rather than eradicate it at all cost) have emerged as possible strategies to improve response rates while reducing toxicities. The recent changes in paradigm in the way we theorize cancer biology and evolution, metastatic spreading and tumor ecology, alongside the recent advances in the field of immunotherapy, have considerably strengthened the interest for these alternative approaches. This paper aims at reviewing the recent evolutions in the field of theoretical biology of cancer and computational oncology, with a focus on the consequences these changes have on the way we administer chemotherapy. Here, we advocate for the development of model-guided strategies to refine doses and schedules of chemotherapy administration in order to achieve precision medicine in oncology.}
}
@incollection{ZEYER2015235,
title = {11 - For the mutual benefit: Health information provision in the science classroom},
editor = {Catherine {Arnott Smith} and Alla Keselman},
booktitle = {Meeting Health Information Needs Outside Of Healthcare},
publisher = {Chandos Publishing},
pages = {235-261},
year = {2015},
isbn = {978-0-08-100248-3},
doi = {https://doi.org/10.1016/B978-0-08-100248-3.00011-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780081002483000111},
author = {Albert Zeyer and Daniel M. Levin and Alla Keselman},
keywords = {Health education, Health literacy, Information, Knowledge, Science education},
abstract = {In this chapter, the authors argue that the school science classroom should help students deal with complex real-life information about health and disease. They also discuss means by which curriculum and instruction in science education can be tied to these issues. The chapter reviews opportunities and challenges presented to individuals by the expectations of participatory health care, focusing on models of health literacy that can help understand and address the challenges. The authors argue that the problem of ensuring effective information use often lies in a transmission approach to health information provision. Transmitted knowledge is often not understood nor applied, as demonstrated in studies of human papillomavirus vaccination education. An alternative to knowledge transmission is the approach that aims to foster critical literacy, which is grounded in critical thinking essential to the practice of science. The chapter reviews a number of interdisciplinary science education activities that introduce health issues in the context of biology, physics, and chemistry education, ensuring deep understanding needed for developing critical literacy. It also discusses science education approaches and theories that encourage the development of deep, culturally meaningful science knowledge. Finally, the chapter reviews professional development and the role of various professionals, including teachers and librarians, in the collaborative endeavor of effective health information provision.}
}
@article{BOLAND2019195,
title = {The price of discretizing time: a study in service network design},
journal = {EURO Journal on Transportation and Logistics},
volume = {8},
number = {2},
pages = {195-216},
year = {2019},
note = {Special Issue: Advances in vehicle routing and logistics optimization: exact methods},
issn = {2192-4376},
doi = {https://doi.org/10.1007/s13676-018-0119-x},
url = {https://www.sciencedirect.com/science/article/pii/S2192437620300327},
author = {Natashia Boland and Mike Hewitt and Luke Marshall and Martin Savelsbergh},
keywords = {Service network design, Time-space network, Time expanded network, Approximation},
abstract = {Researchers and practitioners have long recognized that many transportation problems can be naturally and conveniently modeled using time-expanded networks. In such models, nodes represent locations at distinct points in time and arcs represent possible actions, e.g., moving from one location to another at a particular point of time, or staying in the same location for a period of time. To use a time-expanded network, time must be discretized, i.e., the planning horizon is partitioned into discrete time intervals. The length of these intervals, therefore, must be chosen, and the parameters involving time, e.g., travel duration and due times, need to be mapped to these discrete intervals. Short intervals yield a high-quality approximation to the continuous-time problem, but typically induce a computationally intractable model; whereas long intervals can yield a computationally tractable, but low-quality model. The loss of quality is due to the approximation introduced by the mapping of parameters involving time. To guide researchers and practitioners in their use of time-expanded networks, we explore the choice of time discretization and its impact, by means of an extensive computational study on the service network design problem. The empirical results show that in some cases the loss of quality, i.e., the relative gap between the discretized and continuous-time optimal values, can be greater than 20%. We also investigate metrics that characterize and help identify instances that are likely to be sensitive to discretization and could incur a large loss of solution quality.}
}
@incollection{NOVOTNY1996149,
title = {Computational Biochemistry of Antibodies and T-Cell Receptors},
editor = {Frederic M. Richards and David E. Eisenberg and Peter S. Kim},
series = {Advances in Protein Chemistry},
publisher = {Academic Press},
volume = {49},
pages = {149-260},
year = {1996},
booktitle = {Antigen Binding Molecules: Antibodies and T-cell Receptors},
issn = {0065-3233},
doi = {https://doi.org/10.1016/S0065-3233(08)60490-8},
url = {https://www.sciencedirect.com/science/article/pii/S0065323308604908},
author = {Jiri Novotny and Jürgen Bajorath}
}
@article{ZHOU2025112831,
title = {Modeling of milling force in multi-axis machining process for thin-walled sculptured surface},
journal = {Thin-Walled Structures},
volume = {208},
pages = {112831},
year = {2025},
issn = {0263-8231},
doi = {https://doi.org/10.1016/j.tws.2024.112831},
url = {https://www.sciencedirect.com/science/article/pii/S0263823124012709},
author = {Tianxiang Zhou and Caixu Yue and Xianli Liu and Shaocong Sun and Shiliang Wei and Anshan Zhang},
keywords = {Thin-walled sculptured surface, Multi-axis machining, Ball-end milling, Cutting force, Cutter workpiece engagement, Instantaneous undeformed chip thickness},
abstract = {Ball-end milling is the preferred machining method for machining thin-walled sculptured surfaces as a method with higher efficiency and accuracy. However, with the change of tool attitude during the machining process, the cutter workpiece engagement (CWE) area and the instantaneous undeformed chip thickness (IUCT) will change at the same time, which makes it more difficult to accurately predict the milling force. In this paper, in order to accurately predict the milling force for multi-axis machining, the real CWE area is determined using the upper and lower boundary ideas and based on the optimization criterion. Subsequently, the IUCT model based on the infinitesimal point outer normal vector is proposed by considering the factors of tool attitude change and tool run-out, which avoids complicated iterative calculations. Finally, relying on the thin-walled sculptured surface machining experiments, a series of validation experiments were carried out under different cutting conditions, and the results show that the experimental data and the simulated data have good consistency in shape and size, which proves the validity and accuracy of the model and achieves a better result in terms of applicability, and provides a new way of thinking for the machining of sculptured surface thin-walled parts in real industrial scenarios.}
}
@incollection{IRISH2024,
title = {Interactions between episodic and semantic memory},
booktitle = {Reference Module in Neuroscience and Biobehavioral Psychology},
publisher = {Elsevier},
year = {2024},
isbn = {978-0-12-809324-5},
doi = {https://doi.org/10.1016/B978-0-443-15754-7.00009-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780443157547000092},
author = {Muireann Irish and Matthew D. Grilli},
keywords = {Aging, Alzheimer's disease, Amnesia, Autobiographical memory, Episodic memory, Gradients, Hippocampus, Prospection, Semantic dementia, Semantic memory},
abstract = {In the present chapter, we challenge the idea that episodic and semantic memory are distinct memory systems supported by dissociable brain networks. Drawing on converging findings from cognitive neuroscience and neuropsychology, we show how these forms of declarative memory share remarkably similar neural networks and interact to support an array of cognitive endeavors. We contend that these points of overlap and apparent dissociations can be reconciled by situating the representational content of declarative memory along an episodic-semantic gradient or continuum. This continuum perspective can account for several bodies of research showing that episodic and semantic memory are highly interdependent in natural states and functional contexts, from the way memories are reorganized over time to how humans mentally construct future scenarios. We discuss these points of synergy and highlight unresolved questions, with a view to orienting the field towards a more integrated perspective.}
}
@article{GUTIERREZORTIZ2022100164,
title = {Biofuel production from supercritical water gasification of sustainable biomass},
journal = {Energy Conversion and Management: X},
volume = {14},
pages = {100164},
year = {2022},
issn = {2590-1745},
doi = {https://doi.org/10.1016/j.ecmx.2021.100164},
url = {https://www.sciencedirect.com/science/article/pii/S2590174521000891},
author = {F.J. {Gutiérrez Ortiz}},
keywords = {Supercritical water, Gasification, Biofuel, Hydrogen, Sustainability, Process simulation},
abstract = {A review of biofuel production from supercritical water gasification (SCWG) of sustainable biomass has been performed, mainly organic waste, following a critical thinking in this field of knowledge. Thus, sub- and super- critical water properties and hydrothermal processing are briefly commented on. Then, the feedstocks usable in SCWG are fully reviewed and a brief description of the studies on the kinetics and mechanisms of reactions is carried out. Next, thermodynamic and process simulation are discussed, aimed at producing liquid and gas biofuels. After that, a brief comment on the viability of SCWG processes to produce biofuels is provided based on techno-economic and lifecycle assessments. Finally, some remarks on where we are and where we should go are given in order to advance this technology towards its maturity. This review explains some misleading concepts applied to SCWG processes, provides a brief but comprehensive overview of the technology focused on producing biofuels in a sustainable way, allows a better understanding of the SCWG of biomass for biofuel production, and proposes a series of improvements to be made and examined in the future research.}
}
@incollection{PINTARIC20162367,
title = {Towards Outcomes-Based Education of Computer-Aided Chemical Engineering},
editor = {Zdravko Kravanja and Miloš Bogataj},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {38},
pages = {2367-2372},
year = {2016},
booktitle = {26th European Symposium on Computer Aided Process Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-444-63428-3.50399-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780444634283503994},
author = {Zorka Novak Pintarič and Zdravko Kravanja},
keywords = {Computer-Aided, Chemical Engineering, Education, Bologna process, Learning Outcomes},
abstract = {Chemical engineering education is nowadays increasingly supported by the use of various computational tools as the employers’ requirements for computing skills of graduates are growing too. However, students often acquire computational skills in an unsystematic manner due to a lack of defining and applying computer-based outcomes within the syllabuses suitable for the particular level of the Bologna three-cycle system. This paper bridges this gap by providing the review of the essential learning outcomes in the computer-aided chemical engineering education during all three cycles. The identified outcomes gradually progress from application-based competencies up to more advanced process modeling ones based on knowledge synthesis and creation. Accordingly, the educational strategies and curricula can be redesigned in order to integrate courses more efficiently both horizontally and vertically, and upgrade the use of computational tools.}
}
@article{YURKOVICH2017431,
title = {A Padawan Programmer’s Guide to Developing Software Libraries},
journal = {Cell Systems},
volume = {5},
number = {5},
pages = {431-437},
year = {2017},
issn = {2405-4712},
doi = {https://doi.org/10.1016/j.cels.2017.08.003},
url = {https://www.sciencedirect.com/science/article/pii/S2405471217303368},
author = {James T. Yurkovich and Benjamin J. Yurkovich and Andreas Dräger and Bernhard O. Palsson and Zachary A. King},
abstract = {With the rapid adoption of computational tools in the life sciences, scientists are taking on the challenge of developing their own software libraries and releasing them for public use. This trend is being accelerated by popular technologies and platforms, such as GitHub, Jupyter, R/Shiny, that make it easier to develop scientific software and by open-source licenses that make it easier to release software. But how do you build a software library that people will use? And what characteristics do the best libraries have that make them enduringly popular? Here, we provide a reference guide, based on our own experiences, for developing software libraries along with real-world examples to help provide context for scientists who are learning about these concepts for the first time. While we can only scratch the surface of these topics, we hope that this article will act as a guide for scientists who want to write great software that is built to last.}
}
@article{JIANG2024100795,
title = {Generative urban design: A systematic review on problem formulation, design generation, and decision-making},
journal = {Progress in Planning},
volume = {180},
pages = {100795},
year = {2024},
note = {Generative urban design: A systematic review on problem formulation, design generation, and decision-making},
issn = {0305-9006},
doi = {https://doi.org/10.1016/j.progress.2023.100795},
url = {https://www.sciencedirect.com/science/article/pii/S0305900623000569},
author = {Feifeng Jiang and Jun Ma and Christopher John Webster and Alain J.F. Chiaradia and Yulun Zhou and Zhan Zhao and Xiaohu Zhang},
keywords = {Generative urban design, Urban form generation, Generative method, AI-generated content (AIGC), Generative AI, Human-machine collaboration},
abstract = {Urban design is the process of designing and shaping the physical forms of cities, towns, and suburbs. It involves the arrangement and design of street systems, groups of buildings, public spaces, and landscapes, to make the urban environment performative and sustainable. The typical design process, reliant on manual work and expert experience has unavoidable low efficiency in generating high-performing design solutions due to the involvement of complex social, institutional, and economic contexts and the trade-off between conflicting preferences of different stakeholder groups. Taking advantage of artificial intelligence (AI) and computational capacity, generative urban design (GUD) has been developed as a trending technical direction to narrow the gaps and produce design solutions with high efficiency at early design stages. It uses computer-aided generative methods, such as evolutionary optimization and deep generative models, to efficiently explore complex solution spaces and automatically generate design options that satisfy conflicting objectives and various constraints. GUD experiments have attracted much attention from academia, practitioners, and public authorities in recent years. However, a systematic review of the current stage of GUD research is lacking. This study, therefore, reports on a systematic investigation of the existing literature according to the three key stages in the GUD process: (1) design problem formulation, (2) design option generation, and (3) decision-making. For each stage, current trends, findings, and limitations from GUD studies are examined. Future directions and potential challenges are discussed and presented. The review is highly interdisciplinary and involves articles from urban study, computer science, social science, management, and other fields. It reports what scholars have found in GUD experiments and organizes a diverse and complicated technical agenda into something accessible to all stakeholders. The results and discoveries will serve as a holistic reference for GUD developers and users in both academia and industry and form a baseline for the field of GUD development in the coming years.}
}
@incollection{SNOWDEN2015572,
title = {Semantic Memory},
editor = {James D. Wright},
booktitle = {International Encyclopedia of the Social & Behavioral Sciences (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {572-578},
year = {2015},
isbn = {978-0-08-097087-5},
doi = {https://doi.org/10.1016/B978-0-08-097086-8.51059-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780080970868510599},
author = {Julie S. Snowden},
keywords = {Amodal, Anterior temporal lobe, Brain networks, Conceptual knowledge, Distributed representations, Multimodal, Object knowledge, Schema, Semantic dementia, Semantic features, Semantic memory, Word knowledge},
abstract = {Semantic memory refers to our conceptual knowledge of the world. Understanding of semantic memory has come from several sources: cognitive studies of healthy individuals, computational modeling, patients with disordered semantic memory due to brain disease, and brain imaging and stimulation. The converging evidence indicates that semantic memory involves distributed brain networks, which, at least in part, are linked to the sensory processes involved in perception, action, and language. Whether there is also representation in amodal format remains an area of contention. Knowledge of the world, beyond word and object meanings, is a challenge for future studies of semantic memory.}
}
@article{HART2023182,
title = {Riders on a storm – The evaluation and control of creative processes A comment on: “A systematic framework of creative metacognition” by Izabela Lebuda and Mathias Benedek},
journal = {Physics of Life Reviews},
volume = {47},
pages = {182-183},
year = {2023},
issn = {1571-0645},
doi = {https://doi.org/10.1016/j.plrev.2023.10.029},
url = {https://www.sciencedirect.com/science/article/pii/S1571064523001689},
author = {Yuval Hart}
}
@article{CAI2024133949,
title = {A state-of-the-art review of solar-induced ventilation technology for built environment regulation: Classification, modeling, evaluation, potential and challenges},
journal = {Energy},
volume = {313},
pages = {133949},
year = {2024},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2024.133949},
url = {https://www.sciencedirect.com/science/article/pii/S0360544224037277},
author = {Yang Cai and Zheng-Yu Shu and Jian-Wei He and Yong-Cai Li and Yuan-Da Cheng and Kai-Liang Huang and Fu-Yun Zhao},
keywords = {Solar-induced ventilation technology, Built environment regulation, Numerical model, Thermal characteristic, Performance evaluation},
abstract = {In the face of escalating environmental challenges and dwindling fossil fuel reserves, the transition to renewable and sustainable energy sources has become a paramount global objective, which has led to a surge in research and application of renewable energy sources. Among them, solar energy utilization has been placed at the forefront of energy conservation revolution owing to its significant advantages in terms of sustainability and environmentally-friendliness. Solar-induced ventilation technology (SVT) is a typical way to integrate clean energy with buildings, considerably enhancing solar energy utilization efficiency while achieving building energy conservation and indoor thermal environment regulation. However, summaries as comprehensive as possible for SVT's application in envelopes are ambiguous in the current academia. Different analytical models, parameters and evaluation indicators need to be reviewed to describe the energy flow transfer and the impact on indoor thermal environment, which makes it indispensable to carry out an comprehensive overview for the latest investigation progress. This article endeavors to carry out an elaborate review of the theoretical analysis and constructive application of SVT from an energy utilization and building thermal environment perspective. Firstly, various types of SVT envelopes are classified simultaneously according to development and innovation in solar energy utilization. Furthermore, four different analytical models, namely, heat transfer model, thermal resistance network model, pressure balance model as well as computational fluid dynamics model, have been summarized, which would be helpful to analyze the thermal performance. Through literature review, this article discusses the impact of numerous parameters on system performance, especially the ventilation effect and thermal environment in buildings, from aspects of geometry, material properties and environmental conditions. In addition, a comprehensive collection of the important evaluation indicators based on the energy, thermal comfort and economic evaluations has been introduced to evaluate the thermal performance and indoor environment regulation capability of SVT envelopes, which provided a clear reference on developing and application SVT for high energy efficiency design towards carbon-neutral building envelopes. Finally, the challenges and potential are pointed out in terms of performance enhancement and the expansion of application scenarios. The results of the survey indicated that due to the development of novel technologies and materials, SVT holds great advantages in mitigating building energy consumption and regulating thermal environment, which shows a diversified development trend and promotes the process of global sustainable development. The review of the current SVT building envelope not only clarified the high feasibility of SVT in promoting passive building ventilation, energy saving and enhancing the level of indoor thermal environments, but also provided guidance and identifies the direction of optimization for cutting-edge research.}
}
@article{WITTKUHN2021367,
title = {Replay in minds and machines},
journal = {Neuroscience & Biobehavioral Reviews},
volume = {129},
pages = {367-388},
year = {2021},
issn = {0149-7634},
doi = {https://doi.org/10.1016/j.neubiorev.2021.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S0149763421003444},
author = {Lennart Wittkuhn and Samson Chien and Sam Hall-McMaster and Nicolas W. Schuck},
keywords = {Replay, Reinforcement learning, Machine learning, Representation learning, Decision-making},
abstract = {Experience-related brain activity patterns reactivate during sleep, wakeful rest, and brief pauses from active behavior. In parallel, machine learning research has found that experience replay can lead to substantial performance improvements in artificial agents. Together, these lines of research suggest that replay has a variety of computational benefits for decision-making and learning. Here, we provide an overview of putative computational functions of replay as suggested by machine learning and neuroscientific research. We show that replay can lead to faster learning, less forgetting, reorganization or augmentation of experiences, and support planning and generalization. In addition, we highlight the benefits of reactivating abstracted internal representations rather than veridical memories, and discuss how replay could provide a mechanism to build internal representations that improve learning and decision-making.}
}
@article{PECIS2025100573,
title = {In blockchain we trust: Ideologies and discourses sustaining trust in bitcoin},
journal = {Information and Organization},
volume = {35},
number = {2},
pages = {100573},
year = {2025},
issn = {1471-7727},
doi = {https://doi.org/10.1016/j.infoandorg.2025.100573},
url = {https://www.sciencedirect.com/science/article/pii/S1471772725000193},
author = {Lara Pecis and Lucia Cervi and Lucas Introna},
keywords = {Bitcoin, Blockchain, Trust, Ideology, Discourse, Critical discourse analysis},
abstract = {In this paper, we examine the discourses and ideologies that underpin trust in Bitcoin (BTC) as an algorithm-driven socio-technical system, raising critical questions about how trust is established and sustained in complex socio-technical assemblages. Through a Critical Discourse Analysis (CDA) of three significant events in the cryptocurrency, we identify two interconnected, yet sometimes contradictory, ideologies enacted through four discourses that construct specific subject positions to produce and maintain trust in Bitcoin. The first, technical sovereignty, reflects adherence to notions of technical utopianism. The second, which we term peer-to-peer neoliberalism, frames BTC as a political experiment rooted in the individualization of responsibility and risk. Our paper contributes to the existing literature by arguing that algorithm-driven technologies like BTC neither establish trust solely through their apparent technical neutrality and security nor simply replace traditional institutional mechanisms of governance, control, and interaction. Instead, they are enacted through discourses and material arrangements that require continuous maintenance. This maintenance relies on power relations enabled by these ideologies yet remains contingent upon the ongoing reinforcement of the ideologies themselves—rendering trust inherently precarious and always at risk. This insight shifts the analytical focus from the dominant emphasis in the literature on technical features, social arrangements, and user perceptions to the underlying ideological frameworks that shape these elements, as such.}
}