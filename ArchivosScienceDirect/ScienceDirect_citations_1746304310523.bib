@article{WILLS2019104027,
title = {Reflexivity, coding and quantum biology},
journal = {Biosystems},
volume = {185},
pages = {104027},
year = {2019},
issn = {0303-2647},
doi = {https://doi.org/10.1016/j.biosystems.2019.104027},
url = {https://www.sciencedirect.com/science/article/pii/S0303264719302394},
author = {Peter R Wills},
keywords = {Genetic coding, Reflexivity, Quantum biology, Information, Autocatalysis, Aminoacyl-tRRNA synthetase (aaRS)},
abstract = {Biological systems are fundamentally computational in that they process information in an apparently purposeful fashion rather than just transferring bits of it in a purely syntactical manner. Biological information, such has genetic information stored in DNA sequences, has semantic content. It carries meaning that is defined by the molecular context of its cellular environment. Information processing in biological systems displays an inherent reflexivity, a tendency for the computational information-processing to be “about” the behaviour of the molecules that participate in the computational process. This is most evident in the operation of the genetic code, where the specificity of the reactions catalysed by the aminoacyl-tRNA synthetase (aaRS) enzymes is required to be self-sustaining. A cell’s suite of aaRS enzymes completes a reflexively autocatalytic set of molecular components capable of making themselves through the operation of the code. This set requires the existence of a body of reflexive information to be stored in an organism’s genome. The genetic code is a reflexively self-organised mapping of the chemical properties of amino acid sidechains onto codon “tokens”. It is a highly evolved symbolic system of chemical self-description. Although molecular biological coding is generally portrayed in terms of classical bit-transfer events, various biochemical events explicitly require quantum coherence for their occurrence. Whether the implicit transfer of quantum information, qbits, is indicative of wide-ranging quantum computation in living systems is currently the subject of extensive investigation and speculation in the field of Quantum Biology.}
}
@article{TANG2019101065,
title = {Addressing cascading effects of earthquakes in urban areas from network perspective to improve disaster mitigation},
journal = {International Journal of Disaster Risk Reduction},
volume = {35},
pages = {101065},
year = {2019},
issn = {2212-4209},
doi = {https://doi.org/10.1016/j.ijdrr.2019.101065},
url = {https://www.sciencedirect.com/science/article/pii/S2212420918308720},
author = {Pan Tang and Qi Xia and Yueyao Wang},
keywords = {Earthquakes, Urban areas, Cascading effects, Disaster mitigation, Disaster chains, Social network analysis},
abstract = {Given the rising size and complexity of urban areas, the city governments are faced to the challenges of cascading effects triggered by devastating earthquakes, in which the disastrous consequences are amplified significantly by combined effects of the occurred secondary events with interrelationships on the elements at risks. As a low-probability and high impact natural disaster, the escalation of secondary events are guided by the vulnerability paths, as well as their interconnections should be considered from system perspectives during the preparedness and mitigation process. This research aims to develop, model and analyze cascading effects scenario of earthquakes in urban areas for supporting decision making in disaster risk reduction. A framework for addressing cascading effects of earthquakes in urban area is presented. The procedure for developing cascading effects scenario of such highly complex and uncertain disasters by identifying the triggered disaster chains is introduced. A directed network was built to model and visualize the secondary events with interrelationships involving in the cascading effects scenario. In particular, a range of network metrics are developed to examine the relational patterns of hazardous events based on Social Network Analysis. Together with, how to design disaster mitigation strategies according to network analysis results is introduced, such as disaster chains with priorities to be blocked, hazardous events to be mitigated firstly, and essential collaborative relationships among the responsible organizations. Furthermore, a case study in an urban area in Shenzhen City, China was conducted to highlight the application of the proposed framework. This research presents an innovative approach to address cascading effects in urban areas of earthquakes by developing the triggered worst case scenario, as well as understanding secondary events with interrelationships using network analysis method for providing insights to design disaster mitigation strategies from system thinking perspectives.}
}
@article{CHEN2019398,
title = {Form-finding with robotics: Rapid and flexible fabrication of glass fiber reinforced concrete panels using thermoformed molds},
journal = {Journal of Computational Design and Engineering},
volume = {6},
number = {3},
pages = {398-403},
year = {2019},
issn = {2288-4300},
doi = {https://doi.org/10.1016/j.jcde.2018.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S2288430018300642},
author = {Yutong Chen and Jing Lin Koh and Xia Tian and Yi Qian Goh and Stylianos Dritsas},
keywords = {Form-finding, Digital fabrication, Parametric design},
abstract = {We present a process that revisits form-finding within digital media, namely parametric design and robotic fabrication. It is inspired by classical architectural and engineering experiments producing minimal surfaces and tensile structures by physical simulation of materials and natural forces. Fabrication is based on thermoforming, where thin sheets of amorphous PET are heat-treated and while in malleable state, where the material behaves like stretchable membrane, an industrial robot imprints a shape and sheets are rapidly cooled down assuming their final form. Key aspects of the approach include: (a) Speed: as each sheet is formed within seconds; (b) Flexibility, as a wide-range of shapes are produced without fabrication of unique dies; and (c) Resilience, as unlike traditional form-finding processes where the derived forms are ephemeral, the objects produced here are robust, they may be used directly or employed in subsequent fabrication processes. The produced sheets are used here as molds for glass-reinforced concrete casting offering excellent surface quality and the ability to create geometry unlike any conventional fabrication techniques. We present the design and development of the process and a proof-of-concept artwork produced.}
}
@article{HONG2023116066,
title = {Portfolio allocation strategy for active learning Kriging-based structural reliability analysis},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {412},
pages = {116066},
year = {2023},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2023.116066},
url = {https://www.sciencedirect.com/science/article/pii/S0045782523001901},
author = {Linxiong Hong and Bin Shang and Shizheng Li and Huacong Li and Jiaming Cheng},
keywords = {Structural reliability analysis, Portfolio allocation, Kriging, Active learning, Failure probability},
abstract = {Recently, numerous studies have focused on structural reliability analysis, with the Kriging-based active learning method being particularly popular. A variety of Kriging-based learning functions have been proposed, and shown to perform well in various tasks. However, no single learning function has been demonstrated to consistently outperformed the others in all tasks, and selecting the most appropriate learning function for a given task remains a challenge in engineering applications. In this paper, inspired by the multi-armed bandit strategy, a portfolio allocation of different learning functions is proposed to resolve the issue of selecting a single one, where the better learning functions are selected online according to their past performance. Finally, three classical numerical examples and two engineering applications are adopted to validate the effectiveness of the proposed method.}
}
@article{ZHANG2022846,
title = {A novel resilience modeling method for community system considering natural gas leakage evolution},
journal = {Process Safety and Environmental Protection},
volume = {168},
pages = {846-857},
year = {2022},
issn = {0957-5820},
doi = {https://doi.org/10.1016/j.psep.2022.10.030},
url = {https://www.sciencedirect.com/science/article/pii/S0957582022008953},
author = {Xinqi Zhang and Guoming Chen and Dongdong Yang and Rui He and Jingyu Zhu and Shengyu Jiang and Jiawei Huang},
keywords = {Resilience modeling, Natural gas pipeline leakage, Community system, Computational Fluid Dynamics (CFD), Dynamic Bayesian network (DBN)},
abstract = {With rising natural gas demand, the issue of emergency management of gas leaks in communities is becoming more prominent. Resilience engineering is a successful approach to improving the system's ability in dealing with emergencies. In contrast to natural disasters such as floods and tornadoes, the consequences of a gas leak are closely related to the accident's evolutionary path. However, few reported works have taken the evolution of disasters into account in the assessment of resilience. Conventional methods fail to quantify the public safety performance of a disturbance caused by a natural gas leak. This work presents a novel dynamic approach to assessing resilience that incorporates the evolution of an incident and its interaction with emergency measures. A network structuring model of accident evolution is developed by the Functional Resonance Analysis Method (FRAM) to analyze the potential accident propagation. The explosion consequence of gas leakage escalation is simulated based on Computational Fluid Dynamics (CFD), and the personnel injury criterion is adopted to quantify the temporal and spatial variation characteristics of the system degradation. Then, the dynamic Bayesian network (DBN) is then used to track the interactions between accidents and emergency measures. Taking a real accident case (Shiyan underground gas explosion) as an example, the case study indicates that the proposed method can identify and prioritize emergency measures, as well as provide strong support for decision-making and arrangements in emergency management.}
}
@article{KABOSOVA2022109668,
title = {Shape optimization during design for improving outdoor wind comfort and solar radiation in cities},
journal = {Building and Environment},
volume = {226},
pages = {109668},
year = {2022},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2022.109668},
url = {https://www.sciencedirect.com/science/article/pii/S0360132322008988},
author = {Lenka Kabošová and Angelos Chronis and Theodoros Galanos and Stanislav Kmeť and Dušan Katunský},
keywords = {Parametric architecture, Real-time wind analysis, Real-time sun analysis, Performative design, InFraRed, CFD, Computational fluid dynamics},
abstract = {This paper delivers an idea of weather-based optimization as a sustainable design strategy addressing the changing climate. An environment-driven design technique is introduced and tested at the urban and architectural scale. Utilizing the interplay between the architectural intention and weather influences (specifically solar radiation and wind effects), the optimal design solution for the urban configuration and architectural shape emerges. An exploratory case study near the amphitheater in Kosice, Slovakia, demonstrates the proposed approach. Through the real-time iterative analysis of the environmental performance of multiple design variants, an urban concept of offices/apartment blocks, reacting to the local wind and sun situation, is formed. The wind flow situation and comfort are investigated in a design loop, blending the newly developed AI-driven simulation prediction models of InFraRed11InFraRed: AI-based real-time analysis of wind, sun, and thermal comfort in the streets developed by the CIL of the AIT in Vienna, Austria. and the sun hours analysis in Ladybug with the Galapagos optimization within Grasshopper. The final step of this method is the design and subsequent wind and sun analysis of fluid-shaped lamellae in three variants acting as wind catchers/shading systems. Improved pedestrian wind comfort for outdoor sitting (more than 30% increase in areas suitable for short and prolonged sitting) and optimum sunlight hours (25% gain in sunlight during winter solstice) is achieved using the proposed technique.}
}
@article{CHONG2024103352,
title = {Integrable approximations of dispersive shock waves of the granular chain},
journal = {Wave Motion},
volume = {130},
pages = {103352},
year = {2024},
issn = {0165-2125},
doi = {https://doi.org/10.1016/j.wavemoti.2024.103352},
url = {https://www.sciencedirect.com/science/article/pii/S0165212524000829},
author = {Christopher Chong and Ari Geisler and Panayotis G. Kevrekidis and Gino Biondini},
abstract = {In the present work we revisit the shock wave dynamics in a granular chain with precompression. By approximating the model by an α-Fermi–Pasta–Ulam–Tsingou chain, we leverage the connection of the latter in the strain variable formulation to two separate integrable models, one continuum, namely the KdV equation, and one discrete, namely the Toda lattice. We bring to bear the Whitham modulation theory analysis of such integrable systems and the analytical approximation of their dispersive shock waves in order to provide, through the lens of the reductive connection to the granular crystal, an approximation to the shock wave of the granular problem. A detailed numerical comparison of the original granular chain and its approximate integrable-system-based dispersive shocks proves very favorable in a wide parametric range. The gradual deviations between (approximate) theory and numerical computation, as amplitude parameters of the solution increase are quantified and discussed.}
}
@article{HE2025,
title = {Strategic Conflicts in the Aviation Industry: Evolution and Impact Analysis using Graph Model with Application to Airspace Conflict between European Countries and Russia},
journal = {Transport Economics and Management},
year = {2025},
issn = {2949-8996},
doi = {https://doi.org/10.1016/j.team.2025.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S2949899625000139},
author = {Shawei He and Changmin Jiang},
keywords = {airspace closure, nationalization of leased aircraft, graph model for Conflict Resolution (GMCR), decision support},
abstract = {In this paper, a general framework of analyzing strategic conflicts in aviation industry is proposed based on Graph Model for Conflict Resolution (GMCR). The ongoing conflict regarding the mutual closure of airspace between Russia and the European countries (EUC) and the nationalization of leased aircrafts by Russia are investigated using GMCR for the first time. Outcomes at five possible scenarios are analyzed by calculating the equilibria in which decision makers (DM) behave with different preferences, or with new choice of actions. The evolution analysis is further carried out at each scenario to demonstrate how the conflict could evolve from the starting state to the calculated equilibria. Amidst the ongoing military conflict, it can be suggested whether the conflicts regarding the airspace closure and the nationalization of aircraft can be solved at various scenarios and under what conditions, through convenient modeling and rigorous computation. The calculation result indicates that negotiation could help solve the conflict, while sanctions along could not force Russia to reopen its airspace. This research can provide an enhanced understanding of the conflict regarding airspace sanctions between Russia and the EUC and the guidance of actions for concerned decision makers.}
}
@article{JANG2024132519,
title = {Comparative study on gradient-free optimization methods for inverse source-term estimation of radioactive dispersion from nuclear accidents},
journal = {Journal of Hazardous Materials},
volume = {461},
pages = {132519},
year = {2024},
issn = {0304-3894},
doi = {https://doi.org/10.1016/j.jhazmat.2023.132519},
url = {https://www.sciencedirect.com/science/article/pii/S0304389423018022},
author = {Siho Jang and Juryong Park and Hyun-Ha Lee and Chun-Sil Jin and Eung Soo Kim},
keywords = {Gradient-free optimization, Multi-units & multiple radionuclides release scenario, Improving source-term estimation, Environmental radioactivity monitoring, GPU parallelization},
abstract = {In this study, we rigorously assess the performance of three gradient-free optimization algorithms—Ensemble Kalman Inversion (EKI), Particle Swarm Optimization (PSO), and Genetic Algorithm (GA)—for estimating source terms in diverse radionuclide release scenarios. Our analysis encompasses both single and multiple sources with varying radionuclide compositions, delving into the influence of decay constants and radioactivity on source estimation accuracy. Although estimating a single radionuclide from a single source exhibits outstanding results, estimating multiple radionuclides from a single source proves more arduous due to the limited information available for discerning gamma dose rates. Contrary to expectations, increasing the number of observation stations does not consistently improve the likelihood of finding accurate solutions in ill-posed inverse problems. Impressively, under our simulation settings, EKI demonstrates competitive performance in terms of convergence, accuracy, and runtime compared to PSO and GA, with GPU parallelization further bolstering computational efficiency. We explore strategies for enhancing source term estimation, including incorporating prior information, applying uncertainty removal techniques, and optimizing observation placement. Additionally, this study underscores the intricate role of relative error in determining multi-radionuclide estimation accuracy from gamma dose measurements. By employing the Gaussian plume model under steady-state conditions, our research lays the groundwork for future applications of Lagrangian dispersion models with real-time data integration. The insights gleaned from our study promise to advance environmental radioactivity monitoring and catalyze the development of cutting-edge, real-time source estimation technologies in full-scale systems.}
}
@article{PINCETL2012S32,
title = {Nature, urban development and sustainability – What new elements are needed for a more comprehensive understanding?},
journal = {Cities},
volume = {29},
pages = {S32-S37},
year = {2012},
note = {Current Research on Cities},
issn = {0264-2751},
doi = {https://doi.org/10.1016/j.cities.2012.06.009},
url = {https://www.sciencedirect.com/science/article/pii/S0264275112001059},
author = {Stephanie Pincetl},
keywords = {Urban metabolism, Sustainability, Political ecology, Urban ecosystem services},
abstract = {With the rise of interest in urban sustainability, the question of nature is front and center. This review suggests bridging between three distinct research paths concerned with urban areas and nature: urban ecosystem services, urban metabolism and urban political ecology to forge new thinking to transition from the sanitary city of the twentieth century to the sustainable city of the twenty-first. Cities are anthropogenic creations, sourcing their materials from nearby and far-off places, transforming those materials into products, goods and the physical infrastructure of cities. Tracking that flow of nature into the built environment, and the other flows such as water, needs to be accounted for as part of nature in the city. Cities – having entirely transformed the place they are located through building – have a unique nature, a nature planted by people, and made up of plants and animals that are often different than what had existed in the first place. The services of this new assemblage of species in the city, need to be studied critically. But ultimately, cities are the product of human volition, driven by economics, culture, politics and history. Understanding those drivers – the political ecology of place – provides an interpretive framework for reconsidering the nature of cities and its place in moving from a modernist sanitary city to a gray/green sustainable city.}
}
@article{ZHENG2018266,
title = {When algorithms meet journalism: The user perception to automated news in a cross-cultural context},
journal = {Computers in Human Behavior},
volume = {86},
pages = {266-275},
year = {2018},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2018.04.046},
url = {https://www.sciencedirect.com/science/article/pii/S0747563218302103},
author = {Yue Zheng and Bu Zhong and Fan Yang},
keywords = {Automated news, Algorithm, Computational journalism, Robot journalism, News user, Cultural difference},
abstract = {Automated journalism – the use of algorithms in writing news reports – underscores the new direction of media transformation in the 21st century as it may reshape how the news is produced and consumed. Such writing algorithms have been increasingly adopted in U.S. and Chinese newsroom, but how well they are accepted by news users deserves more research. A comparative study was thus conducted to examine how U.S. and Chinese news users perceive the quality of algorithm-generated news reports, how much they like and trust such reports. Results show that U.S. and Chinese users demonstrated more shared, rather than different, perceptions to automated news. The users did not perceive automated content in a linear way, but viewed them by considering the interaction of the authors (i.e., journalists or algorithms), the media outlets (i.e., traditional or online media) and cultural background (i.e., U.S. or Chinese users).}
}
@article{WEST2023300,
title = {Agent-based methods facilitate integrative science in cancer},
journal = {Trends in Cell Biology},
volume = {33},
number = {4},
pages = {300-311},
year = {2023},
issn = {0962-8924},
doi = {https://doi.org/10.1016/j.tcb.2022.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S0962892422002409},
author = {Jeffrey West and Mark Robertson-Tessi and Alexander R.A. Anderson},
keywords = {agent-based mathematical models, integrative science, tissue homeostasis, cancer metabolism, immune–tumor interactions},
abstract = {In this opinion, we highlight agent-based modeling as a key tool for exploration of cell–cell and cell–environment interactions that drive cancer progression, therapeutic resistance, and metastasis. These biological phenomena are particularly suited to be captured at the cell-scale resolution possible only within agent-based or individual-based mathematical models. These modeling approaches complement experimental work (in vitro and in vivo systems) through parameterization and data extrapolation but also feed forward to drive new experiments that test model-generated predictions.}
}
@article{PENG2024100093,
title = {Memristor-based spiking neural networks: cooperative development of neural network architecture/algorithms and memristors},
journal = {Chip},
volume = {3},
number = {2},
pages = {100093},
year = {2024},
issn = {2709-4723},
doi = {https://doi.org/10.1016/j.chip.2024.100093},
url = {https://www.sciencedirect.com/science/article/pii/S270947232400011X},
author = {Huihui Peng and Lin Gan and Xin Guo},
keywords = {Spike neural networks, Hardware, Memristor, Algorithm, Cooperative development},
abstract = {Inspired by the structure and principles of the human brain, spike neural networks (SNNs) appear as the latest generation of artificial neural networks, attracting significant and universal attention due to their remarkable low-energy transmission by pulse and powerful capability for large-scale parallel computation. Current research on artificial neural networks gradually change from software simulation into hardware implementation. However, such a process is fraught with challenges. In particular, memristors are highly anticipated hardware candidates owing to their fast-programming speed, low power consumption, and compatibility with the complementary metal–oxide semiconductor (CMOS) technology. In this review, we start from the basic principles of SNNs, and then introduced memristor-based technologies for hardware implementation of SNNs, and further discuss the feasibility of integrating customized algorithm optimization to promote efficient and energy-saving SNN hardware systems. Finally, based on the existing memristor technology, we summarize the current problems and challenges in this field.}
}
@article{HEIRDSFIELD2004443,
title = {Factors affecting the process of proficient mental addition and subtraction: case studies of flexible and inflexible computers},
journal = {The Journal of Mathematical Behavior},
volume = {23},
number = {4},
pages = {443-463},
year = {2004},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2004.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S0732312304000495},
author = {Ann M. Heirdsfield and Tom J. Cooper},
keywords = {Mental computation, Addition, Subtraction, Accuracy},
abstract = {The relationship between mental computation and number sense is complex: mental computation can facilitate number sense when students are encouraged to be flexible, but flexibility and number sense is neither sufficient nor necessary for accuracy in mental computation. It is possible for familiarity with a strategy to compensate for a lack of number sense and inefficient processes. This study reports on six case studies exploring Year 3 students’ procedures for and understanding of mental addition and subtraction, and understanding of number sense and other cognitive, metacognitive, and affective factors associated with mental computation. The case studies indicate that the mental computation process is composed of four stages in which cognitive, metacognitive and affective factors operate differently for flexible and inflexible computers. The authors propose a model in which the differences between computer types are seen in terms of the application of different knowledges in number facts, numeration, effect of operation on number, and beliefs and metacognition on strategy choice and strategy implementation.}
}
@article{DRAGO2011361,
title = {Cyclic alternating pattern in sleep and its relationship to creativity},
journal = {Sleep Medicine},
volume = {12},
number = {4},
pages = {361-366},
year = {2011},
issn = {1389-9457},
doi = {https://doi.org/10.1016/j.sleep.2010.11.009},
url = {https://www.sciencedirect.com/science/article/pii/S1389945711000578},
author = {Valeria Drago and Paul S. Foster and Kenneth M. Heilman and Debora Aricò and John Williamson and Pasquale Montagna and Raffaele Ferri},
keywords = {Sleep, Creativity, Cyclic alternating pattern, Torrance, Frontal lobe functions, Arousal},
abstract = {Background/objectives
Sleep has been shown to enhance creativity, but the reason for this enhancement is not entirely known. There are several different physiologic states associated with sleep. In addition to rapid (REM) and non-rapid eye movement (NREM) sleep, NREM sleep can be broken down into Stages (1–4) that are characterized by the degree of EEG slow-wave activity. In addition, during NREM sleep the cyclic alternating pattern (CAPs) of EEG activity has been described which can also be divided into three subtypes (A1–A3) according to the frequency of the EEG waves. Differences in CAP subtype ratios have been previously linked to cognitive performances. The purpose of this study was to asses the relationship between CAP activity during sleep and creativity.
Methods
The participants were eight healthy young adults (four women) who underwent three consecutive nights of polysomnographic recording and took the Abbreviated Torrance Test for Adults (ATTA) on the second and third mornings after the recordings.
Results
There were positive correlations between Stage 1 of NREM sleep and some measures of creativity such as fluency (R=.797; p=.029) and flexibility (R=.43; p=.002), between Stage 4 of NREM sleep and originality (R=.779; p=.034) and a global measure of figural creativity (R=.758; p=.040). There was also a negative correlation between REM sleep and originality (R=−.827; p=.042). During NREM sleep the CAP rate, which in young people reflects primarily the A1 subtype, also correlated with originality (R=.765; p=.038).
Conclusions
NREM sleep is associated with low levels of cortical arousal, and low cortical arousal may enhance the ability of people to access to the remote associations that are critical for creative innovations. In addition, A1 CAP subtypes reflect frontal activity, and the frontal lobes are important for divergent thinking, also a critical aspect of creativity.}
}
@incollection{CHIB20013569,
title = {Chapter 57 - Markov Chain Monte Carlo Methods: Computation and Inference},
editor = {James J. Heckman and Edward Leamer},
series = {Handbook of Econometrics},
publisher = {Elsevier},
volume = {5},
pages = {3569-3649},
year = {2001},
issn = {1573-4412},
doi = {https://doi.org/10.1016/S1573-4412(01)05010-3},
url = {https://www.sciencedirect.com/science/article/pii/S1573441201050103},
author = {Siddhartha Chib},
keywords = {:, Cl, C4},
abstract = {This chapter reviews the recent developments in Markov chain Monte Carlo simulation methods. These methods, which are concerned with the simulation of high dimensional probability distributions, have gained enormous prominence and revolutionized Bayesian statistics. The chapter, provides background on the relevant Markov chain theory and provides detailed information on the theory and practice of Markov chain sampling based on the Metropolis–Hastings and Gibbs sampling algorithms. Convergence diagnostics and strategies for implementation are also discussed. A number of examples drawn from Bayesian statistics are used to illustrate the ideas. The chapter also covers in detail the application of MCMC methods to the problems of prediction and model choice.}
}
@article{CERKA2015376,
title = {Liability for damages caused by artificial intelligence},
journal = {Computer Law & Security Review},
volume = {31},
number = {3},
pages = {376-389},
year = {2015},
issn = {2212-473X},
doi = {https://doi.org/10.1016/j.clsr.2015.03.008},
url = {https://www.sciencedirect.com/science/article/pii/S026736491500062X},
author = {Paulius Čerka and Jurgita Grigienė and Gintarė Sirbikytė},
keywords = {Artificial intelligence, Liability for damages, Legal regulation, AI-as-Tool, Risks by AI, Respondeat (respondent) superior, Vicarious liability, Strict liability},
abstract = {The emerging discipline of Artificial Intelligence (AI) has changed attitudes towards the intellect, which was long considered to be a feature exclusively belonging to biological beings, i.e. homo sapiens. In 1956, when the concept of Artificial Intelligence emerged, discussions began about whether the intellect may be more than an inherent feature of a biological being, i.e. whether it can be artificially created. AI can be defined on the basis of the factor of a thinking human being and in terms of a rational behavior: (i) systems that think and act like a human being; (ii) systems that think and act rationally. These factors demonstrate that AI is different from conventional computer algorithms. These are systems that are able to train themselves (store their personal experience). This unique feature enables AI to act differently in the same situations, depending on the actions previously performed. The ability to accumulate experience and learn from it, as well as the ability to act independently and make individual decisions, creates preconditions for damage. Factors leading to the occurrence of damage identified in the article confirm that the operation of AI is based on the pursuit of goals. This means that with its actions AI may cause damage for one reason or another; and thus issues of compensation will have to be addressed in accordance with the existing legal provisions. The main issue is that neither national nor international law recognizes AI as a subject of law, which means that AI cannot be held personally liable for the damage it causes. In view of the foregoing, a question naturally arises: who is responsible for the damage caused by the actions of Artificial Intelligence? In the absence of direct legal regulation of AI, we can apply article 12 of United Nations Convention on the Use of Electronic Communications in International Contracts, which states that a person (whether a natural person or a legal entity) on whose behalf a computer was programmed should ultimately be responsible for any message generated by the machine. Such an interpretation complies with a general rule that the principal of a tool is responsible for the results obtained by the use of that tool since the tool has no independent volition of its own. So the concept of AI-as-Tool arises in the context of AI liability issues, which means that in some cases vicarious and strict liability is applicable for AI actions.}
}
@article{CLINDANIEL2024105890,
title = {Digital formation processes: A high-frequency, large-scale investigation},
journal = {Journal of Archaeological Science},
volume = {161},
pages = {105890},
year = {2024},
issn = {0305-4403},
doi = {https://doi.org/10.1016/j.jas.2023.105890},
url = {https://www.sciencedirect.com/science/article/pii/S030544032300170X},
author = {Jon Clindaniel and Matthew Magnani},
keywords = {Formation processes, Palimpsests, Big data, Digital materiality, Craigslist, Free stuff, Social stratification},
abstract = {Large sources of digital trace data (i.e. “Big Data”) have become increasingly important in the study of material culture. However, akin to the offline material culture traditionally studied by archaeologists, digital trace data is rarely a passive reflection of human behavior – it is a complex palimpsest produced through a variety of erasure and accretion formation processes. To better understand how digital trace palimpsests are formed and how digital formation processes influence and inform our ability to interpret the offline material processes they index, we introduce a computational method – high-frequency archaeological survey – which allows us to observe digital formation processes at a high temporal resolution, as well as a large spatial scale. Using this method every hour for one month, we surveyed posts from across the United States in Craigslist's “Free Stuff” category (popularly called “Curb Alert”), a user-generated source of big digital trace data, indexing material things that have been placed on users' curbs for removal by scavengers or trash collectors. For each post, we observed its time-to-erasure and any edits that were made during the study period – finding that the posts that survive represent a biased sample of those that were posted over the course of the month, conditioned by how recently and on what day the post is posted, the material characteristics of things that are posted about, as well as regional variation. Far from only being evidence of biased end-of-month data, however, we show that further analysis of identified digital formation processes can be an important object of study in its own right – in this case, shedding new light on social scientific questions linking the exchange of “free stuff” with the process of social stratification and urban inequality in the United States. Overall, our findings suggest the importance of accounting for and explicitly analyzing digital formation processes in studies that utilize digital trace data.}
}
@article{ARCK201954,
title = {When 3 Rs meet a forth R: Replacement, reduction and refinement of animals in research on reproduction},
journal = {Journal of Reproductive Immunology},
volume = {132},
pages = {54-59},
year = {2019},
issn = {0165-0378},
doi = {https://doi.org/10.1016/j.jri.2019.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S0165037819300385},
author = {Petra Clara Arck},
keywords = {Reproduction, Mouse models, 3R principle, Immunology},
abstract = {Research endeavors aiming to understand the maternal immune adaptation to pregnancy significantly rely on the use of animal models, such as mice and rats. These models have provided important insights into the pathophysiology of a number of pregnancy disorders in humans. However, the use of animal models in scientific research is a vividly debated and emotive topic. The 3R principles – replacement, reduction and refinement of research animals – have been propagated a few decades ago. The present review advocates a forward-thinking consciousness to address the 3R principles in research projects in the field of reproductive biology and immunology. Specific measures and alternative methods are being proposed to replace research animals by using e.g. tissue engineering approaches, biobank-derived tissue, ‘placenta-on-a-chip’ devices or in silico methods. The latter may involve data queries from repositories now available to provide single cell sequencing information on reproductive tissues. Reduction of research animals by gestational imaging and a wealth of suggestions for refinement are proposed. Taken together, the measures and guidelines introduced in this review are expected to spark a reconsideration of experimental designs in the area of reproductive biology and immunology in order to implement 3R principle where applicable.}
}
@article{FORRY2013634,
title = {Ready or not: Associations between participation in subsidized child care arrangements, pre-kindergarten, and Head Start and children’s school readiness},
journal = {Early Childhood Research Quarterly},
volume = {28},
number = {3},
pages = {634-644},
year = {2013},
issn = {0885-2006},
doi = {https://doi.org/10.1016/j.ecresq.2013.03.009},
url = {https://www.sciencedirect.com/science/article/pii/S0885200613000367},
author = {Nicole D. Forry and Elizabeth E. Davis and Kate Welti},
keywords = {Low-income, School readiness, Pre-kindergarten, Head Start, Child care subsidies},
abstract = {Research has found disparities in young children’s development across income groups. A positive association between high-quality early care and education and the school readiness of children in low-income families has also been demonstrated. This study uses linked administrative data from Maryland to examine the variations in school readiness associated with different types of subsidized child care, and with dual enrollment in subsidized child care and state pre-kindergarten or Head Start. Using multivariate methods, we analyze linked subsidy administrative data and portfolio-based kindergarten school readiness assessment data to estimate the probability of children’s school readiness in three domains: personal and social development, language and literacy, and mathematical thinking. Compared to children in subsidized family child care or informal care, those in subsidized center care are more likely to be rated as fully ready to learn on the two pre-academic domains. Regardless of type of subsidized care used, enrollment in pre-kindergarten, but not Head Start, during the year prior to kindergarten is strongly associated with being academically ready for kindergarten. No statistically significant associations are found between type of subsidized care, pre-kindergarten enrollment, or Head Start and assessments of children’s personal/social development.}
}
@article{ALOIMONOS201542,
title = {The Cognitive Dialogue: A new model for vision implementing common sense reasoning},
journal = {Image and Vision Computing},
volume = {34},
pages = {42-44},
year = {2015},
issn = {0262-8856},
doi = {https://doi.org/10.1016/j.imavis.2014.10.010},
url = {https://www.sciencedirect.com/science/article/pii/S0262885614001656},
author = {Yiannis Aloimonos and Cornelia Fermüller},
keywords = {Vision and language, Integration of perception, action, and cognition, Cognition modulating image processing},
abstract = {We propose a new model for vision, where vision is part of an intelligent system that reasons. To achieve this we need to integrate perceptual processing with computational reasoning and linguistics. In this paper we present the basics of this formalism.}
}
@article{SHAWKY2023103476,
title = {Blockchain-based secret key extraction for efficient and secure authentication in VANETs},
journal = {Journal of Information Security and Applications},
volume = {74},
pages = {103476},
year = {2023},
issn = {2214-2126},
doi = {https://doi.org/10.1016/j.jisa.2023.103476},
url = {https://www.sciencedirect.com/science/article/pii/S2214212623000601},
author = {Mahmoud A. Shawky and Muhammad Usman and David Flynn and Muhammad Ali Imran and Qammer H. Abbasi and Shuja Ansari and Ahmad Taha},
keywords = {AVISPA simulation, BAN-logic, Key reconciliation, Public key infrastructure, Secret key extraction, Smart contracts-based blockchain},
abstract = {Intelligent transportation systems are an emerging technology that facilitates real-time vehicle-to-everything communication. Hence, securing and authenticating data packets for intra- and inter-vehicle communication are fundamental security services in vehicular ad-hoc networks (VANETs). However, public-key cryptography (PKC) is commonly used in signature-based authentication, which consumes significant computation resources and communication bandwidth for signatures generation and verification, and key distribution. Therefore, physical layer-based secret key extraction has emerged as an effective candidate for key agreement, exploiting the randomness and reciprocity features of wireless channels. However, the imperfect channel reciprocity generates discrepancies in the extracted key, and existing reconciliation algorithms suffer from significant communication costs and security issues. In this paper, PKC-based authentication is used for initial legitimacy detection and exchanging authenticated probing packets. Accordingly, we propose a blockchain-based reconciliation technique that allows the trusted third party (TTP) to publish the correction sequence of the mismatched bits through a transaction using a smart contract. The smart contract functions enable the TTP to map the transaction address to vehicle-related information and allow vehicles to obtain the transaction contents securely. The obtained shared key is then used for symmetric key cryptography (SKC)-based authentication for subsequent transmissions, saving significant computation and communication costs. The correctness and security robustness of the scheme are proved using Burrows–Abadi–Needham (BAN)-logic and Automated Validation of Internet Security Protocols and Applications (AVISPA) simulator. We also discussed the scheme’s resistance to typical attacks. The scheme’s performance in terms of packet delay and loss ratio is evaluated using the network simulator (OMNeT++). Finally, the computation analysis shows that the scheme saves ∼99% of the time required to verify 1000 messages compared to existing PKC-based schemes.}
}
@article{DEMURO20241,
title = {Artificial intelligence and the ethnographic encounter: Transhuman language ontologies, or what it means “to write like a human, think like a machine”},
journal = {Language & Communication},
volume = {96},
pages = {1-12},
year = {2024},
issn = {0271-5309},
doi = {https://doi.org/10.1016/j.langcom.2024.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S0271530924000119},
author = {Eugenia Demuro and Laura Gurney},
keywords = {Artificial intelligence, Language ontologies, Ethnographic encounter, Transhumanism, Posthumanism},
abstract = {In this paper, we employ the language ontologies framework to artificial intelligence (specifically, OpenAI's ChatGPT) to investigate the ‘ethnographic encounter’ between human and non-human language users. Our focus is on the exchange and interplay between human language users and non-human artificial language generators in the production of written text. We analyse how such programs transform our understanding of what language is or might be; their practices to create language are unfamiliar, and yet they make sense to human interlocutors. Drawing from, and building on, the language ontologies framework, we discuss the practices involved in such encounters and suggest the need for an updated ‘toolkit’ in our understanding of language to account for transhuman interactions.}
}
@article{SETO2022102891,
title = {Connected in health: Place-to-place commuting networks and COVID-19 spillovers},
journal = {Health & Place},
volume = {77},
pages = {102891},
year = {2022},
issn = {1353-8292},
doi = {https://doi.org/10.1016/j.healthplace.2022.102891},
url = {https://www.sciencedirect.com/science/article/pii/S1353829222001526},
author = {Christopher H. Seto and Corina Graif and Aria Khademi and Vasant G. Honavar and Claire E. Kelling},
keywords = {Commuting networks, COVID-19, Fixed-effects, Spatial models, Computational statistics, Mobility data},
abstract = {Biweekly county COVID-19 data were linked with Longitudinal Employer-Household Dynamics data to analyze population risk exposures enabled by pre-pandemic, country-wide commuter networks. Results from fixed-effects, spatial, and computational statistical approaches showed that commuting network exposure to COVID-19 predicted an area's COVID-19 cases and deaths, indicating spillovers. Commuting spillovers between counties were independent from geographic contiguity, pandemic-time mobility, or social media ties. Results suggest that commuting connections form enduring social linkages with effects on health that can withstand mobility disruptions. Findings contribute to a growing relational view of health and place, with implications for neighborhood effects research and place-based policies.}
}
@article{GTOTH202138,
title = {Revascularization decisions in patients with chronic coronary syndromes: Results of the second International Survey on Interventional Strategy (ISIS-2)},
journal = {International Journal of Cardiology},
volume = {336},
pages = {38-44},
year = {2021},
issn = {0167-5273},
doi = {https://doi.org/10.1016/j.ijcard.2021.05.005},
url = {https://www.sciencedirect.com/science/article/pii/S0167527321008172},
author = {Gabor {G. Toth} and Nils P. Johnson and William Wijns and Balint Toth and Alexandru Achim and Stephane Fournier and Emanuele Barbato},
keywords = {Chronic coronary syndrome, Coronary ischemia, Coronary revascularization},
abstract = {Background
In chronic coronary syndromes, guidelines mandate invasive functional guidance of revascularization whenever non-invasive proof of ischemia is missing. ISIS-2 survey aimed to evaluate how the adoption of guideline recommendation on ischemia-guided revascularization has evolved over the last 5–7 years.
Methods
In ISIS-2 participants assessed five complete angiograms, presenting only intermediate stenoses without information on non-invasive pre-testing. Fractional flow reserve was known for each stenosis, but remained undisclosed. Participants could determine stenosis significance either by angiography or by requesting an adjunctive invasive diagnostic method (intravascular imaging or functional tests). Primary endpoint was the rate of requesting adjunctive functional assessment. Secondary endpoints were the rate of concordance between angiography-based decisions and know functional severity. ISIS-2 utilized the same web-based platform as ISIS-1 in 2013. (NCT04001452).
Results
334 participants performed 2059 lesion evaluations: 1202 (59%) decisions were based solely on angiography without expressed need for further evaluation. These decisions were discordant with known functional significance in 39%, mainly with potential of overtreatment. Participants requested invasive functional assessment in 643 (31%) and intravascular imaging in 214 (10%) cases. Compared to ISIS-1 the rate of purely angiography-based decisions has decreased (59% vs 66%; p < 0.001), while invasive functional tests were more frequently requested (31% vs 25%; p < 0.001).
Conclusions
ISIS-2 suggests an evolving pattern in the intention to integrate invasive coronary physiology into the revascularization decisions. However, the disconnect between recommendations and current thinking is still dominant.}
}
@article{RANTANEN20153612,
title = {The Future of Pharmaceutical Manufacturing Sciences},
journal = {Journal of Pharmaceutical Sciences},
volume = {104},
number = {11},
pages = {3612-3638},
year = {2015},
issn = {0022-3549},
doi = {https://doi.org/10.1002/jps.24594},
url = {https://www.sciencedirect.com/science/article/pii/S0022354916301514},
author = {Jukka Rantanen and Johannes Khinast},
keywords = {quality by design (QBD), process analytical technology (PAT), mathematical model, materials science,  modeling},
abstract = {ABSTRACT
The entire pharmaceutical sector is in an urgent need of both innovative technological solutions and fundamental scientific work, enabling the production of highly engineered drug products. Commercial-scale manufacturing of complex drug delivery systems (DDSs) using the existing technologies is challenging. This review covers important elements of manufacturing sciences, beginning with risk management strategies and design of experiments (DoE) techniques. Experimental techniques should, where possible, be supported by computational approaches. With that regard, state-of-art mechanistic process modeling techniques are described in detail. Implementation of materials science tools paves the way to molecular-based processing of future DDSs. A snapshot of some of the existing tools is presented. Additionally, general engineering principles are discussed covering process measurement and process control solutions. Last part of the review addresses future manufacturing solutions, covering continuous processing and, specifically, hot-melt processing and printing-based technologies. Finally, challenges related to implementing these technologies as a part of future health care systems are discussed.}
}
@article{HOLZER20163,
title = {Design exploration supported by digital tool ecologies},
journal = {Automation in Construction},
volume = {72},
pages = {3-8},
year = {2016},
note = {Computational and generative design for digital fabrication: Computer-Aided Architectural Design Research in Asia (CAADRIA)},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2016.07.003},
url = {https://www.sciencedirect.com/science/article/pii/S0926580516301467},
author = {Dominik Holzer},
keywords = {Parametric design, Environmental performance optimization, Interoperability, multidisciplinary design, Convergence, Optioneering, Multi-criteria},
abstract = {Designers take advantage of tool ecologies in order to find the most purposeful way of connecting often distinct processes that inform morphological design and associated building performance feedback. The ability to set up logical connections of design parameters across different digital applications becomes ever more relevant in a time where the proliferation of computational tools has led to a fundamental transformation in architectural education. Morphological exploration and form-finding get increasingly enriched by environmental performance feedback. This paper points out a major step forward in software interoperability and the alignment of digital design applications, allowing users to engage with morphological form-finding enriched by real-time physical building performance feedback. The key innovation presented here relates to tools available to designers who neither possess in-depth programming skills, nor need to rely on custom-developed scripts in order to advance their concepts. A recent architectural design studio serves as a testbed to interrogate the level of convergence among tools for morphological design and performance optimization.}
}
@article{KHADE20214955,
title = {hdANM: a new comprehensive dynamics model for protein hinges},
journal = {Biophysical Journal},
volume = {120},
number = {22},
pages = {4955-4965},
year = {2021},
issn = {0006-3495},
doi = {https://doi.org/10.1016/j.bpj.2021.10.017},
url = {https://www.sciencedirect.com/science/article/pii/S0006349521008766},
author = {Pranav M. Khade and Domenico Scaramozzino and Ambuj Kumar and Giuseppe Lacidogna and Alberto Carpinteri and Robert L. Jernigan},
abstract = {Hinge motions are essential for many protein functions, and their dynamics are important to understand underlying biological mechanisms. The ways that these motions are represented by various computational methods differ significantly. By focusing on a specific class of motion, we have developed a new hinge-domain anisotropic network model (hdANM) that is based on the prior identification of flexible hinges and rigid domains in the protein structure and the subsequent generation of global hinge motions. This yields a set of motions in which the relative translations and rotations of the rigid domains are modulated and controlled by the deformation of the flexible hinges, leading to a more restricted, specific view of these motions. hdANM is the first model, to our knowledge, that combines information about protein hinges and domains to model the characteristic hinge motions of a protein. The motions predicted with this new elastic network model provide important conceptual advantages for understanding the underlying biological mechanisms. As a matter of fact, the generated hinge movements are found to resemble the expected mechanisms required for the biological functions of diverse proteins. Another advantage of this model is that the domain-level coarse graining makes it significantly more computationally efficient, enabling the generation of hinge motions within even the largest molecular assemblies, such as those from cryo-electron microscopy. hdANM is also comprehensive as it can perform in the same way as the well-known protein dynamics models (anisotropic network model, rotations-translations of blocks, and nonlinear rigid block normal mode analysis), depending on the definition of flexible and rigid parts in the protein structure and on whether the motions are extrapolated in a linear or nonlinear fashion. Furthermore, our results indicate that hdANM produces more realistic motions as compared to the anisotropic network model. hdANM is an open-source software, freely available, and hosted on a user-friendly website.}
}
@incollection{SEN201693,
title = {5 - Conclusions},
editor = {Syamal K. Sen and Ravi P. Agarwal},
booktitle = {Zero},
publisher = {Academic Press},
pages = {93-142},
year = {2016},
isbn = {978-0-08-100774-7},
doi = {https://doi.org/10.1016/B978-0-08-100774-7.00005-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780081007747000053},
author = {Syamal K. Sen and Ravi P. Agarwal},
keywords = {Black holes, computational zero versus absolute zero, dark matter, Epoch, error in error-free computation, existence of year zero, fast computation, fundamental particle, glory of zero, intense concentration, irrational number without zero, numerical zero consciousness, place-value system, Pythagoras theorem, quantum universe, quantum zero, storage capacity and computational power, Vedic-Hindu-Buddhist legacy, zero in natural mathematics, zero space},
abstract = {We have stressed, among others, on the globally accepted Indian zero along with its place-value system. Pointed out are (i) the impossibility of a zero-free irrational number, (ii) the requirement of a minimum two symbols to represent any information, (iii) the reason for the survival of radix-10 number system, (iv) the distinction of a numerical zero and the absolute zero and the consequent eternal nonremovable error, (v) existence/nonexistence of a building block of matter beyond a fundamental particle, (vi) the importance of intense concentration for innovation, (vii) the nonexactness of zero in physics, (viii) our continuing scientific quest for the origin of the universe, (ix) the existence of pure consciousness eternally everywhere as well as the distinction between artificial and natural consciousness, (x) the pervasiveness of computational mathematics/science in all sciences, engineering, and technologies, (xi) the distinction between living and nonliving computers in terms of storage and computational power, (xii) the extraordinary living computers among human beings, and (xiii) the solution of Y2K problem.}
}
@article{HUSSAIN2023135948,
title = {Efficient synthesis of nicotinaldehyde-based crystalline organic derivatives: Comparative analysis between experimental and DFT study},
journal = {Journal of Molecular Structure},
volume = {1290},
pages = {135948},
year = {2023},
issn = {0022-2860},
doi = {https://doi.org/10.1016/j.molstruc.2023.135948},
url = {https://www.sciencedirect.com/science/article/pii/S0022286023010426},
author = {Shahid Hussain and Muhammad Adeel and Muhammad Khalid and Ume Aiman and Alexander Villinger and Ataualpa A.C. Braga and Saad M. Alshehri and Muhammad {Adnan Asghar}},
keywords = {Pyridine, Nicotinaldehyde, NBO analysis, Density functional theory, MEP},
abstract = {The analogues of pyridine ring structures demonstrate various physiological as well as biological activities. The current research is based on experimental along with computational investigations of two new phenyl substituted nicotinaldehyde derivatives; 2-(2,4-difluorophenyl)pyridine-3-carbaldehyde (DFPPC) and 2-(2,5-dichlorophenyl)pyridine-3-carbaldehyde (DCPPC) . For structural optimization of DFPPC as well as DCPPC and to explore nonlinear optical properties, computational quantum chemical analysis was executed via density functional theory (DFT) calculations by employing M06 level with 6–311G(d,p) basis set. A consensus among theoretical (DFT) and experimental (SC-XRD) results was observed by the calculation of geometric parameters. Molecular electrostatic potential (MEP), natural bond orbital (NBO) analysis, natural population analysis (NPA), nonlinear optical (NLO), global reactivity parameters (GRPs), and frontier molecular orbital (FMO) exploration were carried out at M06/6–311G(d,p), to comprehend hyper-conjugative interactions, electron density, electronic communications and oscillation strength. The HOMO/LUMO energy gap of DCPPC (5.108 eV) was observed to be lower than DFPPC i.e., 5.170 eV, which resulted in its higher value of global softness (0.196 Eh) along with lower global hardness (2.554 Eh) value than DFPPC. The NLO attributes of DFPPC as well as DCPPC was calculated by evaluating the total dipole moment (μtot), average linear polarizability ⟨α⟩ and second hyperpolarizability (γtot) at aforementioned level. From the NLO results, it was observed that DCPPC exhibits a higher average linear polarizability value such as 3.0772 × 10−23 esu than DFPPC i.e., 2.6116 × 10−23 esu . Whereas, higher results of γtot were observed for DFPPC i.e., 3.2455 × 10−35 than DCPPC (3.0708 × 10−35 esu). The distinguished NLO characteristics revealed that, both the chromophores (DFPPC and DCPPC) can be recognized as highly efficient NLO materials for future applications.}
}
@article{PASMAN201880,
title = {How can we improve process hazard identification? What can accident investigation methods contribute and what other recent developments? A brief historical survey and a sketch of how to advance},
journal = {Journal of Loss Prevention in the Process Industries},
volume = {55},
pages = {80-106},
year = {2018},
issn = {0950-4230},
doi = {https://doi.org/10.1016/j.jlp.2018.05.018},
url = {https://www.sciencedirect.com/science/article/pii/S0950423018300329},
author = {Hans J. Pasman and William J. Rogers and M. Sam Mannan},
keywords = {Accident-incident investigation, Hazard identification, Causation, System approach},
abstract = {Risk assessment is essential for various purposes such as facility siting, safeguarding, and licensing. Hazard identification (HAZID), which suffers greatly from incompleteness, is still the weakest link in risk assessment. Of course, this recognition is not new and many efforts have been spent to improve the situation, of which some have been rather successful. To find out what can go wrong, creative divergent thinking is required. Hazard identification should result in scenario definition. In that respect, applying the present tools as HAZOP and FMEA there is still a great emphasis on the material and equipment aspects. In contrast, underlying management and leadership failure in its many forms reflecting in organizational and human failure, due to complexity, attracts much less attention. Unlike in HAZID, in accident investigation the occurrence of an event with nasty consequences is no doubt a fact, so there must be one or more causes and the traces will lead to them. Over the years, methods for accident and incident investigation have gone through a significant evolution. From the early-on simplistic domino stone model and the human operator always at fault, via models of latent failure due to failing management involvement and via extensive root cause analysis (RCA) to a system approach. Hence, in accident investigation, management failure appearing in the many possible forms of human and organizational factors, obtained already 30 years ago with the RCA technique much attention, while it nowadays culminates in the socio-technical system approach. So, the question arises whether for improved HAZID we can learn from the accident investigation experience. In addition, safer design and advances from static risk assessment towards more accurate predictive operational dynamic risk assessment and management, will also be enabled by possibilities offered by big data and analytics. Digitization, automation and simulation, hence computerization, will be of great help in improving the identification of hazards and tracing the corresponding scenarios. The paper reviews the developmental history of both accident investigation and hazard identification methodology; incidentally it will identify commonality and differences. On the basis of the comparison and of recent advances in computerization, the paper will investigate to what extent beneficial modifications and additions can be made to obtain a higher degree of completeness in HAZID.}
}
@article{MENEGHETTI2021101614,
title = {Learning from navigation, and tasks assessing its accuracy: The role of visuospatial abilities and wayfinding inclinations},
journal = {Journal of Environmental Psychology},
volume = {75},
pages = {101614},
year = {2021},
issn = {0272-4944},
doi = {https://doi.org/10.1016/j.jenvp.2021.101614},
url = {https://www.sciencedirect.com/science/article/pii/S0272494421000670},
author = {Chiara Meneghetti and Laura Miola and Enrico Toffalini and Massimiliano Pastore and Francesca Pazzaglia},
keywords = {Navigation, Route retracing, Shortcut, Landmark locating, Visuospatial abilities, Wayfinding inclinations, Informed priors},
abstract = {How individual differences in visuospatial thinking relate to environment learning from navigation is of growing interest and needs to be approached systematically. Here, a sample of 292 undergraduates learnt a virtual path (desktop-based), and their learning accuracy was assessed with recall tasks, i.e. route retracing, shortcut finding and landmark locating tasks. Several individual visuospatial measures, tasks and questionnaires, were administered. Relations between individual measures and recall tasks were estimated with regression models taking quantitative evidence available in the literature into account, and treated as Bayesian informed priors established by a meta-analysis. The results provide robust evidence of visuospatial abilities and wayfinding inclinations (composing two distinct factors) both affecting recall task performance, particularly the former. A different contribution of individual measures as a function of recall task is envisaged. This study offers new insight on the role of individual visuospatial measures in environment learning (navigation-like) and how they are related.}
}
@article{GALDO2022101508,
title = {The quest for simplicity in human learning: Identifying the constraints on attention},
journal = {Cognitive Psychology},
volume = {138},
pages = {101508},
year = {2022},
issn = {0010-0285},
doi = {https://doi.org/10.1016/j.cogpsych.2022.101508},
url = {https://www.sciencedirect.com/science/article/pii/S0010028522000445},
author = {Matthew Galdo and Emily R. Weichart and Vladimir M. Sloutsky and Brandon M. Turner},
keywords = {Attention, Category learning, Eye-tracking, Cognitive bias, Capacity, Model comparison},
abstract = {For better or worse, humans live a resource-constrained existence; only a fraction of physical sensations ever reach conscious awareness, and we store a shockingly small subset of these experiences in memory for later use. Here, we examined the effects of attention constraints on learning. Among models that frame selective attention as an optimization problem, attention orients toward information that will reduce errors. Using this framing as a basis, we developed a suite of models with a range of constraints on the attention available during each learning event. We fit these models to both choice and eye-fixation data from four benchmark category-learning data sets, and choice data from another dynamic categorization data set. We found consistent evidence for computations we refer to as “simplicity”, where attention is deployed to as few dimensions of information as possible during learning, and “competition”, where dimensions compete for selective attention via lateral inhibition.}
}
@article{HERTZ2025105993,
title = {Beyond the matrix: Experimental approaches to studying cognitive agents in social-ecological systems},
journal = {Cognition},
volume = {254},
pages = {105993},
year = {2025},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2024.105993},
url = {https://www.sciencedirect.com/science/article/pii/S0010027724002798},
author = {Uri Hertz and Raphael Köster and Marco A. Janssen and Joel Z. Leibo},
abstract = {Studying social-ecological systems, in which agents interact with each other and their environment are important both for sustainability applications and for understanding how human cognition functions in context. In such systems, the environment shapes the agents' experience and actions, and in turn collective action of agents changes social and physical aspects of the environment. Here we review current investigation approaches, which rely on a lean design, with discrete actions and outcomes and little scope for varying environmental parameters and cognitive demands. We then introduce a multiagent reinforcement learning (MARL) approach, which builds on modern artificial intelligence techniques, and provides new avenues to model complex social worlds, while preserving more of their characteristics, and allowing them to capture a variety of social phenomena. These techniques can be fed back to the laboratory where they make it easier to design experiments in complex social situations without compromising their tractability for computational modeling. We showcase the potential MARL by discussing several recent studies that have used it, detailing the way environmental settings and cognitive constraints can lead to the emergence of complex cooperation strategies. This novel approach can help researchers bring together insights from human cognition, sustainability, and AI, to tackle real world problems of social-ecological systems.}
}
@article{VANBAVEL2015167,
title = {The neuroscience of moral cognition: from dual processes to dynamic systems},
journal = {Current Opinion in Psychology},
volume = {6},
pages = {167-172},
year = {2015},
note = {Morality and ethics},
issn = {2352-250X},
doi = {https://doi.org/10.1016/j.copsyc.2015.08.009},
url = {https://www.sciencedirect.com/science/article/pii/S2352250X15002031},
author = {Jay J {Van Bavel} and Oriel FeldmanHall and Peter Mende-Siedlecki},
abstract = {Prominent theories of morality have integrated philosophy with psychology and biology. Although this approach has been highly generative, we argue that it does not fully capture the rich and dynamic nature of moral cognition. We review research from the dual-process tradition, in which moral intuitions are automatically elicited and reasoning is subsequently deployed to correct these initial intuitions. We then describe how the computations underlying moral cognition are diverse and widely distributed throughout the brain. Finally, we illustrate how social context modulates these computations, recruiting different systems for real (vs. hypothetical) moral judgments, examining the dynamic process by which moral judgments are updated. In sum, we advocate for a shift from dual-process to dynamic system models of moral cognition.}
}
@article{KEARNS2025256,
title = {Biomimetic Digital Twins and Multiomics: Applications to Rheumatoid Arthritis and the Potential Reclassification of Variants of Unknown Clinical Significance},
journal = {The Journal of Molecular Diagnostics},
volume = {27},
number = {4},
pages = {256-269},
year = {2025},
issn = {1525-1578},
doi = {https://doi.org/10.1016/j.jmoldx.2024.12.012},
url = {https://www.sciencedirect.com/science/article/pii/S1525157825000376},
author = {William G. Kearns and Joe Glick and Lawrence Baisch and Andrew Benner and Dalton Brough and Luke Du and Chandra Germain and Laura Kearns and Georgios Stamoulis},
abstract = {The National Academies of Sciences, Engineering, and Medicine issued a report on December 15, 2023, “Foundational Research Gaps and Future Directions for Digital Twins.” This described the importance of using biomimetic digital twins and multiomics in research. These were incorporated in the current analysis of patients with rheumatoid arthritis (RA). Exome sequencing, genotype-phenotype ranking, and biomimetic digital twin analysis were used to identify five pathogenic and one likely pathogenic DNA variants in patient samples analyzed, which were absent from controls. The variants identified in these genes, P2RX7, HTRA2, PTPN22, FLG, CD46, and EIF4G1, play a role in the development of RA. Additionally, 3172 variants of unknown clinical significance (VUSs) were identified in patient samples, which were absent from controls. All VUSs appeared to be associated with RA. Hidden or dark data were identified from six genes. These genes, often found in patient samples, included HIF1A, HLA-DOA, PTGER3, HIPK3, TGFBR3, and HIF1A-AS3. VUSs identified in genes HIF1A, HLA-DOA, PTGER3, and HIPK3 were directly related to the pathogenesis of RA, whereas VUSs identified in genes TGFBR3 and HIF1A-AS3 were indirectly related. The current results suggest that biomimetic digital twins and multiomics can provide further insight into the development of RA. This may also potentially help with the process of reclassifying VUSs. The reclassification of VUSs will play a critical role in complex molecular diagnostics and drug development.}
}
@article{TRANVAN2025101353,
title = {What is the biggest motivator for 10 th-graders when learning programming? Case study in Vietnam},
journal = {Social Sciences & Humanities Open},
volume = {11},
pages = {101353},
year = {2025},
issn = {2590-2911},
doi = {https://doi.org/10.1016/j.ssaho.2025.101353},
url = {https://www.sciencedirect.com/science/article/pii/S2590291125000804},
author = {Hung {Tran Van} and Hoa Nguyen Thi and Kiet Tran {Ly Anh} and Kiet Nguyen {Ba Tuan}},
keywords = {Learning motivation, Impact, Learning programming, 10th grade students, Learning environment},
abstract = {Motivation is one of the important elements in the learning process, particularly for complex subjects like programming, which demand consistent engagement and strong problem-solving abilities. Grasping the nuances of student motivation is essential for educational researchers and practitioners dedicated to crafting more effective and engaging learning environments. This study examines four pivotal factors influencing 10th-grade students' motivation to learn programming in Da Nang, Vietnam: family influence, teacher influence, learning environment, and self-influence. By exploring these dimensions, we aim to propose actionable strategies that educators and parents can implement to foster a supportive and motivating educational atmosphere. Our analysis, conducted using SPSS24 and Structural Equation Modeling (SEM), highlighted that self-influence, family influence, learning environment, and teaching influence significantly drive student motivation. Key metrics include a Kaiser-Meyer-Olkin (KMO) value of .803, Cronbach's Alpha values exceeding 0.6, a Comparative Fit Index (CFI) of 0.946, Tucker-Lewis Index (TLI) of 0.932, and a Root Mean Square Error of Approximation (RMSEA) of 0.056. Additionally, ANOVA results with a significant value of .000 < .005 confirms that the independent variables positively impact students' motivation. These findings are valuable for educators, especially those involved in programming instruction at both high school and university levels.}
}
@article{BOETTKE202344,
title = {On the feasibility of technosocialism},
journal = {Journal of Economic Behavior & Organization},
volume = {205},
pages = {44-54},
year = {2023},
issn = {0167-2681},
doi = {https://doi.org/10.1016/j.jebo.2022.10.046},
url = {https://www.sciencedirect.com/science/article/pii/S0167268122004048},
author = {Peter J. Boettke and Rosolino A. Candela},
keywords = {Economic calculation, F.A. Hayek, Ludwig von Mises, Technosocialism},
abstract = {Technological advances associated with computing power and the prospect of artificial intelligence have renewed interest on the economic feasibility of socialism. The question of such feasibility turns on whether the problem of economic calculation has fundamentally changed. In spite of the prospect of what King and Petty (2021) refer to as “technosocialism,” we argue that technological advances in computation cannot replace the competitive discovery process that takes place within the context of the market. We do so by situating the case for technosocialism in the context of the socialist calculation debate. Understood in these terms, technosocialism represents a restatement of the case for market socialism, which incorrectly framed the “solution” to economic calculation under socialism as one of computing data, rather than the discovery of context-specific knowledge that only emerges through the exchange of property rights. Therefore, the arguments put forth by Ludwig von Mises and F.A. Hayek, and later Israel Kirzner and Don Lavoie, regarding the impossibility of economic calculation under socialism remains just as relevant today.}
}
@article{KHOURY2024102470,
title = {Compassion questionnaire for animals: Scale development and validation},
journal = {Journal of Environmental Psychology},
volume = {100},
pages = {102470},
year = {2024},
issn = {0272-4944},
doi = {https://doi.org/10.1016/j.jenvp.2024.102470},
url = {https://www.sciencedirect.com/science/article/pii/S0272494424002433},
author = {Bassam Khoury and Rodrigo C. Vergara},
keywords = {Compassion, Animal, Nature, Scale, Questionnaire},
abstract = {Objectives
No measure of compassion for animals exists. Previous scales measured empathy or attitudes towards animals. In line with previous compassion questionnaires for self (CQS) and others (CQO), the proposed Compassion Questionnaire for Animals (CQA) aims to operationalize compassion for animals by grounding it in affective, cognitive, behavioral, and interrelatedness dimensions, each representing a set of skills that can be cultivated through training and practice.
Methods
Based on the proposed theoretical approach, the CQA items were developed through consultations with a panel of eight graduate students. A large study was conducted to validate the CQA, investigate the relationship between empathy/compassion for other human beings and compassion for animals, and test the role of gender and age in compassion for animals.
Results
Results suggested the presence of three dimensions along with a global latent variable. Psychometric characteristics of the CQA and its subscales were robust. These findings were additionally supported by convergent and discriminate evidence; as such, the CQA presented strong associations with measures of empathy for animals and nature relatedness. In addition, empathy and compassion for other human beings and for animals were found to be moderately associated. Gender and age were found to be related to compassion for animals, with women and older individuals displaying higher levels of compassion.
Conclusions
The CQA is the first scale that operationalizes compassion for animals as a set of affective, cognitive, behavioral, and interrelatedness skills/abilities with important theoretical and practical implications. Limitations as well as theoretical and practical implications of the CQA are thoroughly discussed.}
}
@article{VAZIRIZADE2017230,
title = {Seismic reliability assessment of structures using artificial neural network},
journal = {Journal of Building Engineering},
volume = {11},
pages = {230-235},
year = {2017},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2017.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S2352710216303163},
author = {Sayyed Mohsen Vazirizade and Saeed Nozhati and Mostafa Allameh Zadeh},
keywords = {Seismic reliability, Artificial neural network, Monte Carlo Simulation, Failure probability},
abstract = {Localization and quantification of structural damage and estimating the failure probability are key outputs in the reliability assessment of structures. In this study, an Artificial Neural Network (ANN) is used to reduce the computational effort required for reliability analysis and damage detection. Toward this end, one demonstrative structure is modeled and then several damage scenarios are defined. These scenarios are considered as training data sets for establishing an ANN model. In this regard, the relationship between structural response (input) and structural stiffness (output) is established using ANN models. The established ANN is more economical and achieves reasonable accuracy in detection of structural damage under a set of ground motions. Furthermore, in order to assess the reliability of a structure, five random variables are considered. These are columns’ area of the first, second, and third floor, elasticity modulus, and gravity loads. The ANN is trained by suing the Monte Carlo Simulation (MCS) technique. Finally, the trained neural network specifies the failure probability of the proposed structure. Although MCS can predict the failure probability for a given structure, the ANN model helps simulation techniques to receive an acceptable accuracy and reduce computational effort.}
}
@article{SUN2012227,
title = {Memory systems within a cognitive architecture},
journal = {New Ideas in Psychology},
volume = {30},
number = {2},
pages = {227-240},
year = {2012},
issn = {0732-118X},
doi = {https://doi.org/10.1016/j.newideapsych.2011.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S0732118X11000729},
author = {Ron Sun},
keywords = {Memory, Cognitive architecture, CLARION},
abstract = {This article addresses the division of memory systems in relation to an overall cognitive architecture. As understanding the architecture is essential to understanding the mind, developing computational cognitive architectures is an important enterprise in computational psychology (computational cognitive modeling). The article proposes a set of hypotheses concerning memory systems from the standpoint of a cognitive architecture, in particular, the four-way division of memory (including explicit and implicit procedural memory and explicit and implicit declarative memory). It then discusses in detail how these hypotheses may be validated through examining qualitatively the literature on memory. A quick review follows of computational simulations of a variety of quantitative data (which are not limited to narrowly conceived “memory tasks”). Results of accounting for both qualitative and quantitative data point to the promise of this approach.}
}
@article{BANERJEE2025100511,
title = {Challenges and opportunities in the industrial usage controller synthesis tools: A review of LTL-based opensource tools for automated control design},
journal = {Results in Control and Optimization},
volume = {18},
pages = {100511},
year = {2025},
issn = {2666-7207},
doi = {https://doi.org/10.1016/j.rico.2024.100511},
url = {https://www.sciencedirect.com/science/article/pii/S2666720724001401},
author = {Amar Banerjee and Venkatesh Choppella},
keywords = {Controller synthesis, Industrial control, LTL, Tool study},
abstract = {Controller synthesis is pivotal in automating control system design from formal specifications and enhancing industrial system verification and optimization processes. This paper critically evaluates LTL-based controller synthesis, highlighting significant gaps in tool support that hinder its widespread adoption in the industry. Despite substantial theoretical progress, an apparent disparity persists between academic research outcomes and the robust, practical tools demanded by industry. Through a comprehensive evaluation, this study reveals mismatches between industrial requirements and the capabilities of current open-source tools. The findings emphasize underexplored challenges and propose future research directions and strategies for practical integration. This work aims to bridge the gap by advocating for enhanced tool support, enabling solutions that align with industrial standards and fostering the broader application of controller synthesis across various sectors.}
}
@incollection{SANCHEZ1995865,
title = {Interfaces for learning},
editor = {Yuichiro Anzai and Katsuhiko Ogawa and Hirohiko Mori},
series = {Advances in Human Factors/Ergonomics},
publisher = {Elsevier},
volume = {20},
pages = {865-870},
year = {1995},
booktitle = {Symbiosis of Human and Artifact},
issn = {0921-2647},
doi = {https://doi.org/10.1016/S0921-2647(06)80136-X},
url = {https://www.sciencedirect.com/science/article/pii/S092126470680136X},
author = {J. Sanchez and M. Lumbreras},
abstract = {The current literature emphasizes critical aspects of learning and cognition involved in human-computer interaction. We present a conceptualization for designing interfaces for learning and thinking through the use of modern ideas for building educational software. We address the construction of Hyperstories as a metaphor for enhancing thought and reasoning skills. The advantages of using multimedia for building this type of software, as well as the complexities involved are analyzed and discussed.}
}
@article{ADLER20258,
title = {Cancer cell populations},
journal = {Seminars in Cancer Biology},
volume = {109},
pages = {8-9},
year = {2025},
issn = {1044-579X},
doi = {https://doi.org/10.1016/j.semcancer.2024.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S1044579X24000981},
author = {Frederick R. Adler and Herbert Levine and Amy Brock}
}
@article{JETTER201445,
title = {Fuzzy Cognitive Maps for futures studies—A methodological assessment of concepts and methods},
journal = {Futures},
volume = {61},
pages = {45-57},
year = {2014},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2014.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S0016328714000809},
author = {Antonie J. Jetter and Kasper Kok},
keywords = {Fuzzy Cognitive Maps, Future studies, Scenarios, Mental model, System thinking},
abstract = {Fuzzy Cognitive Map (FCM) modelling is highly suitable for the demands of future studies: it uses a mix of qualitative and quantitative approaches, it enables the inclusion of multiple and diverse sources to overcome the limitations of expert opinions, it considers multivariate interactions that lead to nonlinearities, and it aims to make implicit assumptions (or mental models) explicit. Despite these properties, the field of future studies is slow to adopt FCM and to apply the increasingly solid theoretical foundations and rigorous practices for FCM applications that are evolving in other fields. This paper therefore discusses theoretical and practical aspects of constructing and applying FCMs within the context of future studies: based on an extensive literature review and the authors’ experience with FCM projects, it provides an introduction of fundamental concepts of FCM modelling, a step-wise description and discussion of practical methods and their pitfalls, and an overview over future research directions for FCM in future studies.}
}
@article{SHAHAB2025121507,
title = {Droplet sorting computer: Design, optimization and device dynamics},
journal = {Chemical Engineering Science},
volume = {311},
pages = {121507},
year = {2025},
issn = {0009-2509},
doi = {https://doi.org/10.1016/j.ces.2025.121507},
url = {https://www.sciencedirect.com/science/article/pii/S0009250925003306},
author = {Mohammad Shahab and Raghunathan Rengaswamy},
keywords = {Droplet sorting, Microfluidics, Reinforcement learning, Multi-agent systems, Optimization, Scalability},
abstract = {Droplet sorting is a crucial component for integrated lab-on-a-chip applications, but conventional methods require active control and a sorter component. This study presents a state-of-the-art droplet sorting computer that incorporates a quality droplet sorter into a microfluidic device without the need for an additional mechanism for active detection and control. The device is computationally optimized using a novel combined design framework based on multi-agent reinforcement learning. The beauty of this sorting computer is that a device optimized for fewer drops can be used for thousands of drops, based on optimal hydrodynamic interactions between drops. The framework for developing a sorting computer is described, and the dynamics behind passive routing of droplets inside the device is explained. The proposed device can achieve droplet movement similar to that of an automated microfluidic device for droplet sorting or any given objective, making it a pioneer in passive droplet routing when implemented with appropriate process conditions.}
}
@article{DIGLIO2023233,
title = {Approximation schemes for districting problems with probabilistic constraints},
journal = {European Journal of Operational Research},
volume = {307},
number = {1},
pages = {233-248},
year = {2023},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2022.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S0377221722007172},
author = {Antonio Diglio and Juanjo Peiró and Carmela Piccolo and Francisco Saldanha-da-Gama},
keywords = {Location, Districting, Stochastic demand, Chance-constraint balancing, Heuristics},
abstract = {In this work a districting problem with stochastic demand is investigated. Chance-constraints are used to model the balancing requirements. Explicit contiguity constraints are also considered. After motivating the problem and discussing several modeling aspects, an approximate deterministic counterpart is proposed which is the core of new solution algorithms devised. The latter are based upon a location-allocation scheme, whose first step consists of considering either a problem with a sample of scenarios or a sample of single-scenario problems. This leads to two variants of a new heuristic. The second version calls for the use of a so-called attractiveness function as a means to find a good trade-off between the (approximate) solutions obtained for the single-scenario problems. Different definitions of such functions are discussed. Extensive computational tests were performed whose results are reported.}
}
@article{MATTILA2025237,
title = {Closing the Gap: Advancing service management in the hospitality and tourism industry amidst the AI revolution},
journal = {Journal of Hospitality and Tourism Management},
volume = {62},
pages = {237-245},
year = {2025},
issn = {1447-6770},
doi = {https://doi.org/10.1016/j.jhtm.2025.01.017},
url = {https://www.sciencedirect.com/science/article/pii/S1447677025000178},
author = {Anna S. Mattila and Laurie Wu and Peihao Wang},
keywords = {Artificial intelligence, Service management, Gaps model, Service automation, Service experience, Service design},
abstract = {The Gaps Model of service quality has long been the keystone of the service management literature. This classic theoretical model offers valuable guidance for understanding the opportunities and challenges posed by the increasing adoption of artificial intelligence (AI) in service management. Drawing on recent research and practical insights, this research examines the impact of AI on each one of the service quality gaps identified in the Gaps Model. In addition, we discuss the dual potential of AI to either bridge or widen these gaps, cautioning for digitally responsible implementations of AI. Towards the end of the paper, we discuss future research directions to advance service management with AI integrations in the hospitality and tourism industry.}
}
@article{IM2009193,
title = {Diagnosing skills of statistical hypothesis testing using the Rule Space Method},
journal = {Studies in Educational Evaluation},
volume = {35},
number = {4},
pages = {193-199},
year = {2009},
issn = {0191-491X},
doi = {https://doi.org/10.1016/j.stueduc.2009.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S0191491X0900042X},
author = {Seongah Im and Yue Yin},
keywords = {Cognitive diagnostic assessment, Rule Space Method, Statistical hypothesis testing, Educational evaluation},
abstract = {This study illustrated the use of the Rule Space Method to diagnose students’ proficiencies in, skills and knowledge of statistical hypothesis testing. Participants included 96 undergraduate and, graduate students, of whom 94 were classified into one or more of the knowledge states identified by, the rule space analysis. Analysis at the level of proficiency groups showed that the critical difference, between low and medium proficiency groups was the understanding of statistical concepts and, knowledge while the critical skill discriminating the medium proficiency group from the high, proficiency group was to mange complex computational procedures. In addition, attribute profiles of, two students showed how students with the same total score can possess different strengths and, weaknesses.}
}
@incollection{YU2018177,
title = {Chapter 11 - Geospatial Data Discovery, Management, and Analysis at National Aeronautics and Space Administration (NASA)},
editor = {Feras A. Batarseh and Ruixin Yang},
booktitle = {Federal Data Science},
publisher = {Academic Press},
pages = {177-191},
year = {2018},
isbn = {978-0-12-812443-7},
doi = {https://doi.org/10.1016/B978-0-12-812443-7.00011-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128124437000119},
author = {Manzhu Yu and Min Sun},
keywords = {Big data, Big data management, Climate, Data discovery, Large-scale scientific simulation, Natural phenomena, Spatiotemporal},
abstract = {The 21st century is experiencing simultaneous changes in global population, urbanization, and climate. These changes, along with the rapid growth of geospatial data and increasing popularity of data discovery, access, and analytics techniques, lead to the promising future of innovation and achievements in geospatial and spatiotemporal thinking, computing, and application. However, big geospatial data bring forth challenges that require the cutting-edge science and technology to address. In this chapter, we highlight some of the characteristics and challenges in geospatial and spatiotemporal data discovery, management, processing, and analytics, and provide solutions based on recent research achievements as case studies. These study cases provide concrete examples of challenges faced when handling geospatial and spatiotemporal big data and the potential solutions in high level of accuracy, interoperability, and scalability.}
}
@article{KRAVCHENKO2020310,
title = {Multi-faceted approach to solving issue of ensuring “zero mortality” on Russian roads},
journal = {Transportation Research Procedia},
volume = {50},
pages = {310-320},
year = {2020},
note = {XIV International Conference on Organization and Traffic Safety Management in Large Cities (OTS-2020)},
issn = {2352-1465},
doi = {https://doi.org/10.1016/j.trpro.2020.10.037},
url = {https://www.sciencedirect.com/science/article/pii/S2352146520307833},
author = {Pavel Kravchenko and Sultan Zhankaziev and Elena Oleshchenko},
keywords = {system analysis, efficiency, automated monitoring system, vehicle, stochastic approach, traffic safety},
abstract = {“The fundamental principle of systems thinking is the ability to look at events, objects and phenomena from various perspectives considered as an aggregate” (O’Connor and McDermott 2006). The article continues studying the issue of preventing the occurrence of causes of death on Russian roads, i.e. “zero” mortality. The study results are presented in a series of articles published in the “Transport of the Russian Federation” journal. It provides a rationale for the feasibility and relevance of applying a methodological approach, which is new for Russian science and practice, to solving the issue of ensuring road traffic safety (RTS) in the interpretation of its meaning, comparable with the meaning of the term “zero mortality”, namely, a multi-faceted approach that considers a multilateral assessment of the quality of management decisions adopted using the knowledge of a full set of different opinions on the objects of any complexity being studied. They include RTS assurance systems, for which a set of different opinions (aspects) regarding all their possible facets, sides, properties, features, etc. — on behalf of the state, authorities, operating structures, and society — can be transformed into a set of factors affecting the level of provided RTS, having a subset of “dangerous” factors that can become causes of death in road traffic accidents (RTAs) in the road traffic environment. The article contains: a digest of the above subset of factors, which are essential for the issue being studied and can serve as the basis for expanding the possibilities of forming a set of death causes, adjusting the functions and corresponding types of required activities functionally bound by a common goal; substantiation of a functional and structural mathematical model for the state RTS system, an algorithmic model for the mechanisms forming its main functional properties and subsets of non-accidental causes of death, which can be understood and addressed preventing the moment of a possible serious RTA, and accidental ones, which cannot be understood and addressed; a “starting” sample of literary sources, capable of expanding (when referring to the publications) and ensuring an increase in the progress of solving the issue of “zero mortality” on Russian roads by 2030 as established by the state strategy for RTS.}
}
@article{KARTHIK2022243,
title = {Prognostic Kalman Filter Based Bayesian Learning Model for Data Accuracy Prediction},
journal = {Computers, Materials and Continua},
volume = {72},
number = {1},
pages = {243-259},
year = {2022},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2022.023864},
url = {https://www.sciencedirect.com/science/article/pii/S1546221822010840},
author = {S. Karthik and Robin Singh Bhadoria and Jeong Gon Lee and Arun Kumar Sivaraman and Sovan Samanta and A. Balasundaram and Brijesh Kumar Chaurasia and S. Ashokkumar},
keywords = {Bayesian learning model, kalman filter, machine learning, data accuracy prediction},
abstract = {Data is always a crucial issue of concern especially during its prediction and computation in digital revolution. This paper exactly helps in providing efficient learning mechanism for accurate predictability and reducing redundant data communication. It also discusses the Bayesian analysis that finds the conditional probability of at least two parametric based predictions for the data. The paper presents a method for improving the performance of Bayesian classification using the combination of Kalman Filter and K-means. The method is applied on a small dataset just for establishing the fact that the proposed algorithm can reduce the time for computing the clusters from data. The proposed Bayesian learning probabilistic model is used to check the statistical noise and other inaccuracies using unknown variables. This scenario is being implemented using efficient machine learning algorithm to perpetuate the Bayesian probabilistic approach. It also demonstrates the generative function for Kalman-filer based prediction model and its observations. This paper implements the algorithm using open source platform of Python and efficiently integrates all different modules to piece of code via Common Platform Enumeration (CPE) for Python.}
}
@incollection{YASH2025495,
title = {Chapter 17 - In silico techniques, artificial intelligence, and machine learning for enhanced efficacy of extracellular vesicle–based diagnosis and therapeutics},
editor = {Krishnan Anand and Chithravel Vadivalagan and Prakash Gangadaran and Sathish Muthu and Ben Peacock},
booktitle = {Extracellular Vesicles for Therapeutic and Diagnostic Applications},
publisher = {Elsevier},
pages = {495-521},
year = {2025},
series = {Nanotechnology in Biomedicine},
isbn = {978-0-443-23891-8},
doi = {https://doi.org/10.1016/B978-0-443-23891-8.00013-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780443238918000135},
author = {Jain Yash and Jayawant Maharsh and Shashank Shivaji Kamble and Abhishek Guldhe and Virupaksha Bastikar},
keywords = {Extracellular vesicles, omics data analysis, machine learning algorithms, artificial intelligence, diagnostics, and potential therapeutics},
abstract = {The nanosized extracellular vesicles are the secretions from cells, either prokaryotic or eukaryotic; these are the cluster of various biomolecules, including but not limited to RNA, proteins, and polysaccharides, which function in cell signaling and act as carriers. The recent advancement in their isolation and long storage has enabled them to be used for various diagnostic and therapeutic applications. Using these vesicles, novel drug delivery systems are designed; moreover, they can help diagnose many lethal diseases in their very early stages (e.g., cancer), which gives us an edge to act against them. Bioinformatics tools like molecular docking and dynamic simulation studies can help design a better binding site for the targeted extracellular vesicles for diagnostic purposes. By contrast, simulation studies can help us design novel extracellular vesicles for therapeutic purposes by studying the behavior and interactions of the vesicles with their surroundings inside the body. Additionally, the omics data analysis of these extracellular vesicles helps characterize genomic, metabolomic, and proteomic levels, which tells us about their functions and potential application in various fields. Furthermore, these studies can be validated using advanced computational techniques such as machine learning and artificial intelligence, as the machine learning approach creates algorithms based on the data to build models that can perform complex analyses. By contrast, an artificial intelligence would predict how a human would perform complex tasks such as learning, analyzing, and reasoning. The parametric data received through extensive bioinformatical analysis can be analyzed by many patterns’ recognition, reinforcement, or generative machine learning models. These models help devise physical diagnostic tests for various diseases, predict newer and better versions of existing extracellular vesicles, and strengthen our knowledge of the drug delivery mechanisms affiliated with these extracellular vesicles. More extensive research in integrating artificial intelligence into these machine learning models can enhance the model’s prediction rate and output quality.}
}
@article{TRESADERN20171478,
title = {Industrial medicinal chemistry insights: neuroscience hit generation at Janssen},
journal = {Drug Discovery Today},
volume = {22},
number = {10},
pages = {1478-1488},
year = {2017},
issn = {1359-6446},
doi = {https://doi.org/10.1016/j.drudis.2017.05.013},
url = {https://www.sciencedirect.com/science/article/pii/S1359644617301216},
author = {Gary Tresadern and Frederik J.R. Rombouts and Daniel Oehlrich and Gregor Macdonald and Andres A. Trabanco},
abstract = {The role of medicinal chemistry has changed over the past 10 years. Chemistry had become one step in a process; funneling the output of high-throughput screening (HTS) on to the next stage. The goal to identify the ideal clinical compound remains, but the means to achieve this have changed. Modern medicinal chemistry is responsible for integrating innovation throughout early drug discovery, including new screening paradigms, computational approaches, novel synthetic chemistry, gene-family screening, investigating routes of delivery, and so on. In this Foundation Review, we show how a successful medicinal chemistry team has a broad impact and requires multidisciplinary expertise in these areas.}
}
@article{ANDERSON2015283,
title = {Sparse factors for the positive and negative syndrome scale: Which symptoms and stage of illness?},
journal = {Psychiatry Research},
volume = {225},
number = {3},
pages = {283-290},
year = {2015},
issn = {0165-1781},
doi = {https://doi.org/10.1016/j.psychres.2014.12.025},
url = {https://www.sciencedirect.com/science/article/pii/S016517811401018X},
author = {Ariana Anderson and Marsha Wilcox and Adam Savitz and Hearee Chung and Qingqin Li and Giacomo Salvadore and Dai Wang and Isaac Nuamah and Steven P. Riese and Robert M. Bilder},
keywords = {PANSS, Confirmatory factor analysis, Exploratory factor analysis, Schizophrenia, RDoC, Dimensional Measures},
abstract = {The Positive and Negative Syndrome Scale (PANSS) is frequently described with five latent factors, yet published factor models consistently fail to replicate across samples and related disorders. We hypothesize that (1) a subset of the PANSS, instead of the entire PANSS scale, would produce the most replicable five-factor models across samples, and that (2) the PANSS factor structure may be different depending on the treatment phase, influenced by the responsiveness of the positive symptoms to treatment. Using exploratory factor analysis, confirmatory factor analysis and cross validation on baseline and post-treatment observations from 3647 schizophrenia patients, we show that five-factor models fit best across samples when substantial subsets of the PANSS items are removed. The optimal model at baseline (five factors) omits 12 items: Motor Retardation, Grandiosity, Somatic Concern, Lack of Judgment and Insight, Difficulty in Abstract Thinking, Mannerisms and Posturing, Disturbance of Volition, Preoccupation, Disorientation, Excitement, Guilt Feelings and Depression. The PANSS factor models fit differently before and after patients have been treated. Patients with larger treatment response in positive symptoms have larger variations in factor structure across treatment stage than the less responsive patients. Negative symptom scores better predict the positive symptoms scores after treatment than before treatment. We conclude that sparse factor models replicate better on new samples, and the underlying disease structure of Schizophrenia changes upon treatment.}
}
@incollection{TURKLE20017035,
title = {Human–Computer Interface},
editor = {Neil J. Smelser and Paul B. Baltes},
booktitle = {International Encyclopedia of the Social & Behavioral Sciences},
publisher = {Pergamon},
address = {Oxford},
pages = {7035-7038},
year = {2001},
isbn = {978-0-08-043076-8},
doi = {https://doi.org/10.1016/B0-08-043076-7/04333-3},
url = {https://www.sciencedirect.com/science/article/pii/B0080430767043333},
author = {S. Turkle},
abstract = {The computer/human interface refers to the modalities through which people interact with computational technologies. Looked at over the last half century, the trend has been from a style of interaction in which the computer is approached as a mechanism to a style of interaction in which the computer is approached as a behaving and aware organism. This first trend has been related to another, from people acting directly on the machine to acting indirectly on its increasingly elaborate self presentation in which many layers of programming stand between underlying processes and what is presented to the user. This movement is more than technical. Computational technologies have served as objects to think with, that is, as carrier objects for ideas. Through their changing interfaces, the computer has moved from being the carrier object of a culture of calculation to that of a culture of simulation. The culture of simulation promotes a way of understanding in which users are encouraged to take computers ‘at interface value.’}
}
@article{RAMFUL2014119,
title = {Reversible reasoning in fractional situations: Theorems-in-action and constraints},
journal = {The Journal of Mathematical Behavior},
volume = {33},
pages = {119-130},
year = {2014},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2013.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S0732312313000990},
author = {Ajay Ramful},
keywords = {Division, Fraction, Multiplicative reasoning, Reversibility, Units, Vergnaud's theory},
abstract = {The aim of this study was to investigate, at a fine-grained level of detail, the theorems-in-action deployed and the constraints encountered by middle-school students in reasoning reversibly in the multiplicative domain of fraction. A theorem-in-action (Vergnaud, 1988) is a conceptual construct to trace students’ reasoning in a problem solving situation. Two seventh grade students were interviewed in a rural middle-school in the southern part of the United States. The students’ strategies were examined with respect to the numerical features of the problem situations and the ways they viewed and operated on fractional units. The results show that reversible reasoning is sensitive to the numeric feature of problem parameters. Relatively prime numbers and fractional quantities acted as inhibitors preventing the cueing of the multiplication–division invariant, thereby constraining students from reasoning reversibly. Among others, two key resources were identified as being essential for reasoning reversibly in fractional contexts: firstly, interpreting fractions in terms of units, which enabled the students to access their whole number knowledge and secondly, the unit-rate theorem-in-action. Failure to conceptualize multiplicative relations in reverse constrained the students to use more primitive strategies, leading them to solve problems non-deterministically and at higher computational costs.}
}
@article{1987755,
title = {Computation of signatures of linear airgun arrays: Vaage, S. and B. Ursin, 1987. Geophys. Prospect., 35(3):281–287. SERES A/S, P.O. Box 1965, Moholtan, 7001 Trondheim, Norway},
journal = {Deep Sea Research Part B. Oceanographic Literature Review},
volume = {34},
number = {9},
pages = {755},
year = {1987},
issn = {0198-0254},
doi = {https://doi.org/10.1016/0198-0254(87)90164-6},
url = {https://www.sciencedirect.com/science/article/pii/0198025487901646}
}
@article{DAI2025200249,
title = {Practical exploration of English translation activity courses in universities under the background of artificial intelligence},
journal = {Systems and Soft Computing},
volume = {7},
pages = {200249},
year = {2025},
issn = {2772-9419},
doi = {https://doi.org/10.1016/j.sasc.2025.200249},
url = {https://www.sciencedirect.com/science/article/pii/S2772941925000675},
author = {Jiexia Dai},
keywords = {Artificial intelligence, English translation activity class, Recurrent neural network, Translation level},
abstract = {In recent years, along with the increasing international exchange, cultivating English translation talents has become particularly important. This study aims to explore the application effect of artificial intelligence technology in college English translation activity courses and compare the differences between traditional teaching methods and AI-based methods. Through comparative analysis of the improvement of the translation ability of the two groups of students, the practical value and feasibility of AI technology in translation teaching is evaluated to provide reference and guidance for future translation education. This paper used the neural network machine translation model in artificial intelligence to solve the practical problems in implementing of college English translation activities. Intelligent translation through Recurrent Neural Network (RNN) can better leverage the role of artificial intelligence, which provides new methods for the sustainable development of translation activity classes in universities. Group A adopts the traditional English translation teaching method, mainly relying on teachers' explanations and students' practice. Course content includes interpretation of translation theory, text translation practice, and discussion and analysis based on translation cases. It mainly uses paper textbooks and reference books, as well as some online translation resources, but does not involve the application of artificial intelligence technology. Group B integrates artificial intelligence technology into English translation teaching method, using neural network translation model (such as RNN) to assist teaching. In addition to traditional translation theory and practice, the course also includes training and practice on the use of AI translation tools. Using AI translation software, an online translation platform and a specially developed AI-based translation practice platform, the platform is an artificial intelligence translation model integrated with RNN, designed to assist teaching and improve students' translation skills. The experimental results of this article indicate that the average translation scores of Group A and Group B students in the first 6 months of the experiment were 65.77 and 65.71, respectively. The scores of Group A and Group B students after the experiment for 6 months were 68.57 and 82.69, respectively. The results show that translation teaching in the context of artificial intelligence significantly improves the efficiency and accuracy of students' translation, and enhances the interaction and interest of learning. This finding shows that the AI-based technology in translation teaching is not only feasible and effective, but also can provide new ideas for the modernization and intelligent development of translation education, and improve students' practical translation ability and professional competitiveness.}
}
@article{MATT2013420,
title = {Implementation of Lean Production in Small Sized Enterprises},
journal = {Procedia CIRP},
volume = {12},
pages = {420-425},
year = {2013},
note = {Eighth CIRP Conference on Intelligent Computation in Manufacturing Engineering},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2013.09.072},
url = {https://www.sciencedirect.com/science/article/pii/S2212827113007130},
author = {D.T. Matt and E. Rauch},
keywords = {Manufacturing, Productivity, Small Enterprises},
abstract = {The introduction and implementation of Lean Production Principles over the last twenty years has had a notable impact on many manufacturing enterprises. The practice shows that lean production methods and instruments are not equally applicable to large and small companies. After the implementation in large enterprises belonging to the automotive sector the concept of lean thinking was introduced successfully in medium sized enterprises. Small enterprises have been ignored for a long time and special investigations about this topic are rarely. Considering statistical data and analysis about the economic importance of small enterprises we can see, that they are numerous and create a notable part of the total value added in the non-financial business economy. This paper analysis in a first step the role and potential of small enterprises – especially in Italy – and shows then a preliminary study of the suitability of existing lean methods for the application in this type of organization. The research was combined with an industrial case study in a small enterprise to analyse the difficulties in the implementation stage and to identify the critical success factors. The results of this preliminary study should illustrate the existing hidden potential in small enterprises as well as a selection of suitable methods for productivity improvements. This research will be the base for a further and more detailed research project.}
}
@article{GOLTSOS2022397,
title = {Inventory – forecasting: Mind the gap},
journal = {European Journal of Operational Research},
volume = {299},
number = {2},
pages = {397-419},
year = {2022},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2021.07.040},
url = {https://www.sciencedirect.com/science/article/pii/S0377221721006500},
author = {Thanos E. Goltsos and Aris A. Syntetos and Christoph H. Glock and George Ioannou},
keywords = {Forecasting, Inventory control, Inventory forecasting, Literature review},
abstract = {We are concerned with the interaction and integration between demand forecasting and inventory control, in the context of supply chain operations. The majority of the literature is fragmented. Forecasting research more often than not assumes forecasting to be an end in itself, disregarding any subsequent stages of computation that are needed to transform forecasts into replenishment decisions. Conversely, most contributions in inventory theory assume that demand (and its parameters) are known, in effect disregarding any preceding stages of computation. Explicit recognition of these shortcomings is an important step towards more realistic theoretical developments, but still not particularly helpful unless they are somehow addressed. Even then, forecasts often constitute exogenous variables that serially feed into a stock control model. Finally, there is a small but growing stream of research that is explicitly built around jointly tackling the inventory forecasting question. We introduce a framework to define four levels of integration: from disregarding, to acknowledging, to partly addressing, to fully understanding the interactions. Focusing on the last two, we conduct a structured review of relevant (integrated) academic contributions in the area of forecasting and inventory control and argue for their classification with regard to integration. We show that the development from one level to another is in many cases chronological in order, but also associated with specific schools of thought. We also argue that although movement from one level to another adds realism, it also adds complexity in terms of actual implementations, and thus a trade-off exists. The article makes a contribution into an area that has always been fragmented despite the importance of bringing the forecasting and inventory communities together to solve problems of common interest. We close with an indicative agenda for further research and a call for more theoretical contributions, but also more work that would help to expand the empirical knowledge base in this area.}
}
@article{DOBREW2025104941,
title = {Make-up strategies with incomplete markets and bounded rationality},
journal = {European Economic Review},
volume = {173},
pages = {104941},
year = {2025},
issn = {0014-2921},
doi = {https://doi.org/10.1016/j.euroecorev.2024.104941},
url = {https://www.sciencedirect.com/science/article/pii/S0014292124002708},
author = {Michael Dobrew and Rafael Gerke and Sebastian Giesen and Joost Röttger},
keywords = {Make-up strategies, Incomplete markets, Bounded rationality, Effective lower bound, Average inflation targeting},
abstract = {We study the impact of market incompleteness and bounded rationality on the effectiveness of make-up strategies. Using a heterogeneous-agent New Keynesian model with reflective expectations, we show that make-up strategies can mitigate the negative consequences of an occasionally-binding effective lower bound. However, the benefits are small when cognitive ability is in line with micro-evidence. These findings are independent of market (in)completeness, emphasising the importance of rational expectations. While market incompleteness and bounded rationality complement each other in attenuating the effects of forward guidance, we do not observe such a complementarity for the effectiveness of make-up strategies.}
}
@article{BOHLENDER2018428,
title = {Compositional Verification of PLC Software using Horn Clauses and Mode Abstraction},
journal = {IFAC-PapersOnLine},
volume = {51},
number = {7},
pages = {428-433},
year = {2018},
note = {14th IFAC Workshop on Discrete Event Systems WODES 2018},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2018.06.336},
url = {https://www.sciencedirect.com/science/article/pii/S2405896318306669},
author = {Dimitri Bohlender and Stefan Kowalewski},
keywords = {Formal verification, Programmable logic controllers, Industry automation, Software tools, Software safety},
abstract = {Real-world PLC software is modular and composed of many different function blocks. Due to its cyclic execution, an engineer’s mental model of a single function block often exhibits state machine semantics – partitioning a block’s behaviours into different modes of operation. We propose a technique called mode abstraction for automatic computation of such high-level models from program semantics and investigate its impact on the overall model checking performance. To this end, we use constrained horn clauses to characterise the program semantics, mode transitions and safety goals in a compositional way. The resulting verification conditions are discharged using an SMT solver. Evaluation of our prototypical implementation on examples from the PLCopen Safety library shows the effectiveness of both the chosen formalism and mode abstraction.}
}
@incollection{HASKELL200195,
title = {Chapter 6 - Knowledge Base and Transfer: On the Usefulness of Useless Knowledge},
editor = {Robert E. Haskell},
booktitle = {Transfer of Learning},
publisher = {Academic Press},
address = {San Diego},
pages = {95-113},
year = {2001},
series = {Educational Psychology},
issn = {18716148},
doi = {https://doi.org/10.1016/B978-012330595-4/50007-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012330595450007X},
author = {Robert E. Haskell},
abstract = {Publisher Summary
Knowledge base broadly includes knowledge acquired by reading, personal experience, careful listening, and astute observing. It also includes thinking and it is the absolute requirement not only for transfer but for thinking and reasoning. This chapter presents the general knowledge-base conditions necessary for optimal transfer to occur and explain its importance. Transfer of learning requires more than quick-fix strategies and learners must have a knowledge base in a subject in order to even know enough to ask questions about it. Without a sufficient knowledge base, novices do not have a framework within which to formulate adequate questions. Possessing a large knowledge base enables a learner to think about the subject in depth. To achieve general transfer, one often requires much more than immediately useful knowledge. It requires learning that may be considered useless knowledge. There are several examples of knowledge that appeared to have absolutely no use, but years later, these "useless" knowledge was applied to other areas, which later had major applied importance.}
}
@article{KOSTERHALE2013836,
title = {Theory of Mind: A Neural Prediction Problem},
journal = {Neuron},
volume = {79},
number = {5},
pages = {836-848},
year = {2013},
issn = {0896-6273},
doi = {https://doi.org/10.1016/j.neuron.2013.08.020},
url = {https://www.sciencedirect.com/science/article/pii/S089662731300754X},
author = {Jorie Koster-Hale and Rebecca Saxe},
abstract = {Predictive coding posits that neural systems make forward-looking predictions about incoming information. Neural signals contain information not about the currently perceived stimulus, but about the difference between the observed and the predicted stimulus. We propose to extend the predictive coding framework from high-level sensory processing to the more abstract domain of theory of mind; that is, to inferences about others’ goals, thoughts, and personalities. We review evidence that, across brain regions, neural responses to depictions of human behavior, from biological motion to trait descriptions, exhibit a key signature of predictive coding: reduced activity to predictable stimuli. We discuss how future experiments could distinguish predictive coding from alternative explanations of this response profile. This framework may provide an important new window on the neural computations underlying theory of mind.}
}
@article{GRAMELSBERGER2011296,
title = {What do numerical (climate) models really represent?},
journal = {Studies in History and Philosophy of Science Part A},
volume = {42},
number = {2},
pages = {296-302},
year = {2011},
note = {Model-Based Representation in Scientific Practice},
issn = {0039-3681},
doi = {https://doi.org/10.1016/j.shpsa.2010.11.037},
url = {https://www.sciencedirect.com/science/article/pii/S0039368110001160},
author = {Gabriele Gramelsberger},
keywords = {Climate models, Computing, Symbolic forms, Scientific modelling},
abstract = {The translation of a mathematical model into a numerical one employs various modifications in order to make the model accessible for computation. Such modifications include discretizations, approximations, heuristic assumptions, and other methods. The paper investigates the divergent styles of mathematical and numerical models in the case of a specific piece of code in a current atmospheric model. Cognizance of these modifications means that the question of the role and function of scientific models has to be reworked. Neither are numerical models pure intermediaries between theory and data, nor are they autonomous tools of inquiry. Instead, theory and data are transformed into a new symbolic form of research due to the fact that computation has become an essential requirement for every scientific practice. Therefore the question is posed: What do numerical (climate) models really represent?}
}
@article{FAIRHURST1980447,
title = {Towards a rationale for neural stability: A model of neural computation and network architecture},
journal = {International Journal of Bio-Medical Computing},
volume = {11},
number = {6},
pages = {447-459},
year = {1980},
issn = {0020-7101},
doi = {https://doi.org/10.1016/0020-7101(80)90012-4},
url = {https://www.sciencedirect.com/science/article/pii/0020710180900124},
author = {M.C. Fairhurst and G.P. Goutos},
abstract = {Networks of Boolean processing cells with low connectivity are known to be inherently stable, in the sense that they exhibit only limited reverberatory activity among their possible state transitions. This paper discusses the value of such a network as a functional model of a neural system and, in the light of the observed decrease in stability with increasing cell connectivity, seeks to identify the features of network architecture and cell computation which act to protect network stability, thereby providing a framework for an understanding of neural stability.}
}
@article{MENENDEZHERRERO2024210,
title = {Persistence of atoms in molecules: there is room beyond electron densities},
journal = {IUCrJ},
volume = {11},
number = {2},
pages = {210-223},
year = {2024},
issn = {2052-2525},
doi = {https://doi.org/10.1107/S2052252524000915},
url = {https://www.sciencedirect.com/science/article/pii/S2052252524000198},
author = {María Menéndez-Herrero and Ángel {Martín Pendás} and X. Zhang},
keywords = {computational modeling, density functional theory, molecular simulations, energy minimization, electron densities, Born maxima},
abstract = {The 3N-dimensional maxima of the square of the wavefunction, the so-called Born maxima, show beyond doubt that the electronic structure of atoms persists in molecules, either in their original ground state or some low-lying excited state. The electron density is only a low-dimensional projection of this much richer landscape.
Evidence that the electronic structure of atoms persists in molecules to a much greater extent than has been usually admitted is presented. This is achieved by resorting to N-electron real-space descriptors instead of one- or at most two-particle projections like the electron or exchange-correlation densities. Here, the 3N-dimensional maxima of the square of the wavefunction, the so-called Born maxima, are used. Since this technique is relatively unknown to the crystallographic community, a case-based approach is taken, revisiting first the Born maxima of atoms in their ground state and then some of their excited states. It is shown how they survive in molecules and that, beyond any doubt, the distribution of electrons around an atom in a molecule can be recognized as that of its isolated, in many cases excited, counterpart, relating this fact with the concept of energetic promotion. Several other cases that exemplify the applicability of the technique to solve chemical bonding conflicts and to introduce predictability in real-space analyses are also examined.}
}
@article{LI2025100338,
title = {RICE AlgebraBot: Lessons learned from designing and developing responsible conversational AI using induction, concretization, and exemplification to support algebra learning},
journal = {Computers and Education: Artificial Intelligence},
volume = {8},
pages = {100338},
year = {2025},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2024.100338},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X24001413},
author = {Chenglu Li and Wanli Xing and Yukyeong Song and Bailing Lyu},
keywords = {Conversational AI, Large language models, FAccT AI in education, Technology design and development, Math learning},
abstract = {The importance and challenge of Algebra learning is widely recognized, with students across the U.S. facing difficulties due to the subject's complexity. While extensive research has focused on enhancing Algebra learning in K-12 education, the reusability, scalability, and effectiveness of the strategies employed (e.g., manual interventions and digital tutoring platforms) remain limited. Conversational AI (ConvAI), enabled by the advancement of large language models (LLMs), emerges as a potential tool for automatic, personalized, and effective student support. However, ethical concerns surrounding diversity, safety, sentiment, and stereotype associated with ConvAI are prominent, and empirical studies examining its application in education are scarce. The purpose of this study is to develop a ConvAI system that mitigates the potential ethical concerns and empirically evaluate the effect of such a system for math learning. Specifically, we first examined computational strategies to mitigate the ethical concerns of ConvAI in educational setting with educational big data (npretraining = 2,097,139) and found that researchers could effectively enhance ConvAI responsibility through the investigated algorithmic strategies. Then, a ConvAI system was constructed using these strategies, guided by learning sciences principles. Lastly, we examined students' eye-tracking patterns, acceptance, and learning processes when using this ConvAI system to learn Algebra through a random experiment (nparticipant = 151). Participants using the developed ConvAI demonstrated generally increased visual attention levels as compared to the control group. Moreover, participants expressed a positive acceptance towards the ConvAI technology. Finally, participants' interaction patterns with the ConvAI technology influenced their Algebra learning. These results provide insights for both educational researchers and practitioners to integrate ConvAI in learning environments.}
}
@article{LOCHAB202116,
title = {An improved flux limiter using fuzzy modifiers for Hyperbolic Conservation Laws},
journal = {Mathematics and Computers in Simulation},
volume = {181},
pages = {16-37},
year = {2021},
issn = {0378-4754},
doi = {https://doi.org/10.1016/j.matcom.2020.09.012},
url = {https://www.sciencedirect.com/science/article/pii/S0378475420303207},
author = {Ruchika Lochab and Vivek Kumar},
keywords = {Flux limiters, Hyperbolic equations, High resolution methods, Fuzzy logic},
abstract = {The objective of the work in this paper is to computationally tackle a range of problems in hyperbolic conservation laws, which is an interesting branch of computational fluid dynamics. For the simulation of issues in hyperbolic conservation laws, this work explores the concept of fuzzy logic-based operators. This research presents a unique mixture of fuzzy sets and logic with a new branch of conservation laws from fluid dynamics. The approach considers a computational procedure based on the reconstruction of several high-order numerical methods termed as flux-limited methods using some fuzzy logic operators. With the aid of fuzzy modifiers, these flux limiters are further modified. This approach results in improved convergence of approximations and maintains the problem’s basic properties to be solved. Additionally, to ensure improved results, modified flux-limited methods are imposed on some famous test problems. The application results are provided wherever required. The work has demonstrated that it is possible to use such technique and apply it to complex areas of computational fluid dynamics to produce a more straightforward approach to studying other topics like flux-limited methods and hence opens up an exciting gateway for future work.}
}
@article{GRAY2000401,
title = {Objects, Actions, and Images: A Perspective on Early Number Development},
journal = {The Journal of Mathematical Behavior},
volume = {18},
number = {4},
pages = {401-413},
year = {2000},
issn = {0732-3123},
doi = {https://doi.org/10.1016/S0732-3123(00)00025-0},
url = {https://www.sciencedirect.com/science/article/pii/S0732312300000250},
author = {Eddie Gray and Demetra Pitta and David Tall},
abstract = {It is the purpose of this article to present a review of research evidence that indicates the existence of qualitatively different thinking in elementary number development. In doing so, the article summarizes empirical evidence obtained over a period of 10 years. This evidence first signaled qualitative differences in numerical processing, and was seminal in the development of the notion of procept. More recently, it examines the role of imagery in elementary number processing. Its conclusions indicate that in the abstraction of numerical concepts from numerical processes qualitatively different outcomes may arise because children concentrate on different objects or different aspects of the objects, which are components of numerical processing.}
}
@article{HUANG2025135787,
title = {Integrated economic and environmental optimization for industrial consumers: A dual-objective approach with multi-carrier energy systems and fuzzy decision-making},
journal = {Energy},
volume = {324},
pages = {135787},
year = {2025},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2025.135787},
url = {https://www.sciencedirect.com/science/article/pii/S036054422501429X},
author = {Anzhong Huang and Qiuxiang Bi and Luote Dai},
keywords = {Multi-carrier energy hub, Bi-objective optimization, Peak load management, Fuzzy-decision making, Stability of industrial consumers},
abstract = {Industrial energy systems require innovative optimization strategies to simultaneously minimize costs and reduce environmental impact. This study presents a dual-objective optimization model that integrates economic and environmental considerations within a multi-carrier energy hub framework. The proposed approach incorporates a peak load management strategy to optimize energy consumption patterns, a fuzzy decision-making method to handle operational uncertainties, and an enhanced non-dominated sorting genetic algorithm II to improve multi-objective optimization efficiency. The model employs Pareto-optimal solutions, offering decision-makers a structured method to balance cost reduction and emissions minimization. The effectiveness of the proposed framework is validated through case studies, demonstrating that peak load management significantly flattens load profiles, optimizes distributed energy resources, and reduces reliance on grid electricity during peak hours. The enhanced non-dominated sorting genetic algorithm II ensures better convergence toward optimal trade-offs, improving computational performance. Results indicate that the integration of peak load management leads to a 0.93 % reduction in operational costs and a 0.79 % decrease in carbon emissions compared to conventional energy management approaches. These findings underscore the potential of the developed model in enhancing industrial energy efficiency and sustainability, providing a robust and adaptable solution for modern industrial consumers.}
}
@article{LI2022296,
title = {The role of information structures in game-theoretic multi-agent learning},
journal = {Annual Reviews in Control},
volume = {53},
pages = {296-314},
year = {2022},
issn = {1367-5788},
doi = {https://doi.org/10.1016/j.arcontrol.2022.03.003},
url = {https://www.sciencedirect.com/science/article/pii/S1367578822000086},
author = {Tao Li and Yuhan Zhao and Quanyan Zhu},
keywords = {Multi-agent learning, Information structures, Reinforcement learning, Belief generation, Game theory, Value of Information},
abstract = {Multi-agent learning (MAL) studies how agents learn to behave optimally and adaptively from their experience when interacting with other agents in dynamic environments. The outcome of a MAL process is jointly determined by all agents’ decision-making. Hence, each agent needs to think strategically about others’ sequential moves, when planning future actions. The strategic interactions among agents makes MAL go beyond the direct extension of single-agent learning to multiple agents. With the strategic thinking, each agent aims to build a subjective model of others decision-making using its observations. Such modeling is directly influenced by agents’ perception during the learning process, which is called the information structure of the agent’s learning. As it determines the input to MAL processes, information structures play a significant role in the learning mechanisms of the agents. This review creates a taxonomy of MAL and establishes a unified and systematic way to understand MAL from the perspective of information structures. We define three fundamental components of MAL: the information structure (i.e., what the agent can observe), the belief generation (i.e., how the agent forms a belief about others based on the observations), as well as the policy generation (i.e., how the agent generates its policy based on its belief). In addition, this taxonomy enables the classification of a wide range of state-of-the-art algorithms into four categories based on the belief-generation mechanisms of the opponents, including stationary, conjectured, calibrated, and sophisticated opponents. We introduce Value of Information (VoI) as a metric to quantify the impact of different information structures on MAL. Finally, we discuss the strengths and limitations of algorithms from different categories and point to promising avenues of future research.}
}
@article{20244431,
title = {The wide-reaching power of technology},
journal = {Cell},
volume = {187},
number = {17},
pages = {4431-4432},
year = {2024},
issn = {0092-8674},
doi = {https://doi.org/10.1016/j.cell.2024.07.046},
url = {https://www.sciencedirect.com/science/article/pii/S0092867424008407}
}
@article{BOWLER2021104535,
title = {Children perform extensive information gathering when it is not costly},
journal = {Cognition},
volume = {208},
pages = {104535},
year = {2021},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2020.104535},
url = {https://www.sciencedirect.com/science/article/pii/S0010027720303541},
author = {Aislinn Bowler and Johanna Habicht and Madeleine E. Moses-Payne and Niko Steinbeis and Michael Moutoussis and Tobias U. Hauser},
keywords = {Cognitive development, Information gathering, Computational modelling},
abstract = {Humans often face decisions where little is known about the choice options. Gathering information prior to making a choice is an important strategy to improve decision making under uncertainty. This is of particular importance during childhood and adolescence, when knowledge about the world is still limited. To examine how much information youths gather, we asked 107 children (8–9 years, N = 30), early (12–13 years, N = 41) and late adolescents (16–17 years, N = 36) to perform an information sampling task. We find that children gather significantly more information before making a decision compared to adolescents, but only if it does not come with explicit costs. Using computational modelling, we find that this is because children have reduced subjective costs for gathering information. Our findings thus demonstrate how children overcome their limited knowledge and neurocognitive constraints by deploying excessive information gathering, a developmental feature that could inform aberrant information gathering in psychiatric disorders.}
}
@article{ZHANG201313,
title = {A semantic representation model for design rationale of products},
journal = {Advanced Engineering Informatics},
volume = {27},
number = {1},
pages = {13-26},
year = {2013},
note = {Modeling, Extraction, and Transformation of Semantics in Computer Aided Engineering Systems},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2012.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S1474034612000985},
author = {Yingzhong Zhang and Xiaofang Luo and Jian Li and Jennifer J. Buis},
keywords = {Design rationale, Design semantics, Representation model, Ontology},
abstract = {Design rationale (DR) is crucial information in product design decision support, design analysis and design reuse. In this paper, based on the Issue-based Information System (IBIS) model, a new ontology-based semantic representation model for DR information; the integrated issue, solution, artifact and argument (ISAA) model; is proposed. The ISAA model introduces the ontology-based semantic representation mode to the DR representation and expands the concept elements of IBIS. The class of concept elements and the semantic relationships among them are defined by Web Ontology Language (OWL). The axioms and rules which are used to reason and analyze DR are defined and encoded with Semantic Web Rule Language (SWRL), which enrich the semantic relations defined by OWL. The ISAA model represents the directed graph of IBIS to the Resource Description Framework (RDF) graph and serializes to an RDF/XML document which lays the foundation for retrieving and reasoning semantic information of DR. A semantic annotator integrating with the visual product design model was developed, by which the discrete information of thinking is captured and abstracted to a conceptual representation of the ISAA model. Finally, an example of DR representation for the spring operating mechanism of a high-voltage circuit breaker product is given.}
}
@article{ATKINSON20137060,
title = {Evolutionary optimization for ranking how-to questions based on user-generated contents},
journal = {Expert Systems with Applications},
volume = {40},
number = {17},
pages = {7060-7068},
year = {2013},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2013.06.017},
url = {https://www.sciencedirect.com/science/article/pii/S0957417413003977},
author = {John Atkinson and Alejandro Figueroa and Christian Andrade},
keywords = {Community question-answering, Question-answering systems, Concept clustering, Evolutionary computation, HPSG parsing},
abstract = {In this work, a new evolutionary model is proposed for ranking answers to non-factoid (how-to) questions in community question-answering platforms. The approach combines evolutionary computation techniques and clustering methods to effectively rate best answers from web-based user-generated contents, so as to generate new rankings of answers. Discovered clusters contain semantically related triplets representing question–answers pairs in terms of subject-verb-object, which is hypothesized to improve the ranking of candidate answers. Experiments were conducted using our evolutionary model and concept clustering operating on large-scale data extracted from Yahoo! Answers. Results show the promise of the approach to effectively discovering semantically similar questions and improving the ranking as compared to state-of-the-art methods.}
}
@article{MEJIA201899,
title = {The Power of Writing, a Pebble Hierarchy and a Narrative for the Teaching of Automata Theory},
journal = {Electronic Notes in Theoretical Computer Science},
volume = {339},
pages = {99-110},
year = {2018},
note = {The XLII Latin American Computing Conference},
issn = {1571-0661},
doi = {https://doi.org/10.1016/j.entcs.2018.06.007},
url = {https://www.sciencedirect.com/science/article/pii/S1571066118300513},
author = {Carolina Mejía and J. {Andres Montoya} and Christian Nolasco},
keywords = {Pebble automata, finite state automata},
abstract = {In this work we study pebble automata. Those automata constitute an infinite hierarchy of discrete models of computation. The hierarchy begins at the level of finite state automata (0-pebble automata) and approaches the model of one-tape Turing machines. Thus, it can be argued that it is a complete hierarchy that covers, in a continuous way, all the models of automata that are important in the theory of computation. We investigate the use of this hierarchy as a narrative for the teaching of automata theory. We also investigate some fundamental questions concerning the power of pebble automata.}
}
@article{VUONG2025,
title = {Critical remarks on current practices of data article publishing: Issues, challenges, and recommendations},
journal = {Data Science and Informetrics},
year = {2025},
issn = {2694-6106},
doi = {https://doi.org/10.1016/j.dsim.2025.01.009},
url = {https://www.sciencedirect.com/science/article/pii/S2694610625000098},
author = {Quan-Hoang Vuong and Viet-Phuong La and Minh-Hoang Nguyen},
keywords = {Data paper, FAIR Principles, Editing and reviewing processes, Quality control, Open science, Mindsponge Theory},
abstract = {ABSTRACT
The contribution of the data paper publishing paradigm to the knowledge generation and validation processes is becoming substantial and pivotal. In this paper, through the information-processing perspective of Mindsponge Theory, we discuss how the data article publishing system serves as a filtering mechanism for quality control of the increasingly chaotic datasphere. The overemphasis on machine-actionality and technical standards presents some shortcomings and limitations of the data article publishing system, such as the lack of consideration of humanistic values, radical race for big data, and inadequate use of expertise in data evaluation. Without addressing the shortcomings and limitations, the reusability of data will be hindered, and scientific investment to facilitate data sharing will be wasted. Thus, we suggest that the current data paper publishing paradigm needs to be updated with a new philosophy of data.}
}
@article{PADILLARIVERA201979,
title = {A systematic review of the sustainability assessment of bioenergy: The case of gaseous biofuels},
journal = {Biomass and Bioenergy},
volume = {125},
pages = {79-94},
year = {2019},
issn = {0961-9534},
doi = {https://doi.org/10.1016/j.biombioe.2019.03.014},
url = {https://www.sciencedirect.com/science/article/pii/S0961953419301102},
author = {Alejandro Padilla-Rivera and María Guadalupe Paredes and Leonor Patricia Güereca},
keywords = {Sustainability assessment, Systematic review, Bioenergy, Gaseous biofuels, Renewable energy, Sustainability of bioenergy},
abstract = {In recent years, achieving sustainability in renewable energy systems has become important for achieving future economic prosperity and energy security all over the world; therefore, multiple attempts have been made to assess their sustainability. This means that in addition to considering the technological and economic factors, environmental and social aspects should also be considered. However, the wide-ranging concept of sustainability and the various methodological frameworks presented make their interpretation and correct implementation difficult. In this research, through a systematic literature review, we summarize and analyze the current research on the sustainability assessment of bioenergy production/use (also referred as gaseous biofuels) for electricity and heat generation. Sustainability approaches and their underlying factors from the three dimensions of sustainability were consolidated and structured in this systematic review. In addition, a set of indicators (environmental, social and economic) is provided based on the literature analyzed that decision makers can use to evaluate the sustainability performance of bioenergy systems. The main finding indicates that although there are various international efforts on measuring sustainability, only 32 of studies of the 8542 works initially screened (less than 1%) have an integrated approach that considers all three aspects of sustainability, i.e., environmental, economic and social aspects. In most cases, the focus is on one of the three aspects. Additionally, 50% of the studies evaluated included another dimension, i.e., a cultural, institutional or technical dimension. These results support the idea that a multidimensional sustainability assessment is feasible and facilitates decision-making processes towards a sustainable energy future.}
}
@article{WANG2022477,
title = {Methodology of network pharmacology for research on Chinese herbal medicine against COVID-19: A review},
journal = {Journal of Integrative Medicine},
volume = {20},
number = {6},
pages = {477-487},
year = {2022},
issn = {2095-4964},
doi = {https://doi.org/10.1016/j.joim.2022.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S2095496422000966},
author = {Yi-xuan Wang and Zhen Yang and Wen-xiao Wang and Yu-xi Huang and Qiao Zhang and Jia-jia Li and Yu-ping Tang and Shi-jun Yue},
keywords = {Chinese traditional medicine, Herbal medicine, Network pharmacology, Compound identification, COVID-19},
abstract = {Traditional Chinese medicine, as a complementary and alternative medicine, has been practiced for thousands of years in China and possesses remarkable clinical efficacy. Thus, systematic analysis and examination of the mechanistic links between Chinese herbal medicine (CHM) and the complex human body can benefit contemporary understandings by carrying out qualitative and quantitative analysis. With increasing attention, the approach of network pharmacology has begun to unveil the mystery of CHM by constructing the heterogeneous network relationship of “herb-compound-target-pathway,” which corresponds to the holistic mechanisms of CHM. By integrating computational techniques into network pharmacology, the efficiency and accuracy of active compound screening and target fishing have been improved at an unprecedented pace. This review dissects the core innovations to the network pharmacology approach that were developed in the years since 2015 and highlights how this tool has been applied to understanding the coronavirus disease 2019 and refining the clinical use of CHM to combat it.}
}
@article{PINTOCARVALHO2022107255,
title = {An efficient multiscale strategy to predict the evolution of the real contact area between rough surfaces},
journal = {Tribology International},
volume = {165},
pages = {107255},
year = {2022},
issn = {0301-679X},
doi = {https://doi.org/10.1016/j.triboint.2021.107255},
url = {https://www.sciencedirect.com/science/article/pii/S0301679X21004035},
author = {R. {Pinto Carvalho} and A.M. {Couto Carneiro} and F.M. {Andrade Pires} and T. Doca},
keywords = {Contact area, Roughness, Contact homogenisation, Multiscale modelling},
abstract = {An efficient and general multiscale strategy to model rough contact and determine the evolution of the real contact area is proposed, based on the splitting of the surface power spectrum. A multiplicative homogenisation scheme is developed to evaluate the contact area fraction effectively, by incorporating statistical information of the contact pressure field in the scale transitions. A strategy for separating the roughness frequencies and for generating the topography at each scale is proposed. The comparison between direct numerical simulation and the proposed multiscale strategy demonstrates the capability to predict the contact area evolution of multiscale rough topographies. The significant reduction of the computational cost endorses the applicability of the proposed multiscale framework to practical circumstances.}
}
@article{HAHN2020363,
title = {Argument Quality in Real World Argumentation},
journal = {Trends in Cognitive Sciences},
volume = {24},
number = {5},
pages = {363-374},
year = {2020},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2020.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S1364661320300206},
author = {Ulrike Hahn},
keywords = {argument quality, logic, probability, rationality},
abstract = {The idea of resolving dispute through the exchange of arguments and reasons has been central to society for millennia. We exchange arguments as a way of getting at the truth in contexts as diverse as science, the court room, and our everyday lives. In democracies, political decisions should be negotiated through argument, not deception, or even worse, brute force. If argument is to lead to the truth or to good decisions, then some arguments must be better than others and ‘argument strength’ must have some meaningful connection with truth. Can argument strength be measured in a way that tracks an objective relationship with truth and not just mere persuasiveness? This article describes recent developments in providing such measures.}
}
@article{KOU2023109788,
title = {Infrared small target segmentation networks: A survey},
journal = {Pattern Recognition},
volume = {143},
pages = {109788},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109788},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004867},
author = {Renke Kou and Chunping Wang and Zhenming Peng and Zhihe Zhao and Yaohong Chen and Jinhui Han and Fuyu Huang and Ying Yu and Qiang Fu},
keywords = {Infrared small target, Characteristic analysis, Segmentation network, Deep learning, Collaborative technology, Data-driven, False alarm, Missed detection},
abstract = {Fast and robust small target detection is one of the key technologies in the infrared (IR) search and tracking systems. With the development of deep learning, there are many data-driven IR small target segmentation algorithms, but they have not been extensively surveyed; we believe our proposed survey is the first to systematically survey them. Focusing on IR small target segmentation tasks, we summarized 7 characteristics of IR small targets, 3 feature extraction methods, 8 design strategies, 30 segmentation networks, 8 loss functions, and 13 evaluation indexes. Then, the accuracy, robustness, and computational complexities of 18 segmentation networks on 5 public datasets were compared and analyzed. Finally, we have discussed the existing problems and future trends in the field of IR small target detection. The proposed survey is a valuable reference for both beginners adapting to current trends in IR small target detection and researchers already experienced in this field.}
}
@article{IGAMBERDIEV201715,
title = {The quantum basis of spatiotemporality in perception and consciousness},
journal = {Progress in Biophysics and Molecular Biology},
volume = {130},
pages = {15-25},
year = {2017},
note = {Quantum information models in biology: from molecular biology to cognition},
issn = {0079-6107},
doi = {https://doi.org/10.1016/j.pbiomolbio.2017.02.008},
url = {https://www.sciencedirect.com/science/article/pii/S0079610716301687},
author = {Abir U. Igamberdiev and Nikita E. Shklovskiy-Kordi},
keywords = {Cytoskeleton, Molecular computation, Neuron, Quantum measurement, Spatiotemporality},
abstract = {Living systems inhabit the area of the world which is shaped by the predictable space-time of physical objects and forces that can be incorporated into their perception pattern. The process of selecting a “habitable” space-time is the internal quantum measurement in which living systems become embedded into the environment that supports their living state. This means that living organisms choose a coordinate system in which the influence of measurement is minimal. We discuss specific roles of biological macromolecules, in particular of the cytoskeleton, in shaping perception patterns formed in the internal measurement process. Operation of neuron is based on the transmission of signals via cytoskeleton where the digital output is generated that can be decoded through a reflective action of the perceiving agent. It is concluded that the principle of optimality in biology as formulated by Liberman et al. (BioSystems 22, 135–154, 1989) is related to the establishment of spatiotemporal patterns that are maximally predictable and can hold the living state for a prolonged time. This is achieved by the selection of a habitable space approximated to the conditions described by classical physics.}
}
@article{KADERAVEK201527,
title = {SCIIENCE: The creation and pilot implementation of an NGSS-based instrument to evaluate early childhood science teaching},
journal = {Studies in Educational Evaluation},
volume = {45},
pages = {27-36},
year = {2015},
issn = {0191-491X},
doi = {https://doi.org/10.1016/j.stueduc.2015.03.003},
url = {https://www.sciencedirect.com/science/article/pii/S0191491X15000218},
author = {Joan N. Kaderavek and Tamala North and Regina Rotshtein and Hoangha Dao and Nicholas Liber and Geoff Milewski and Scott C. Molitor and Charlene M. Czerniak},
keywords = {Discourse analysis, Teacher assessment, Language of science in classrooms, Validity/reliability},
abstract = {This paper describes the development, testing and implementation of the Systematic Characterization of Inquiry Instruction in Early LearNing Classroom Environments (SCIIENCE). The SCIIENCE instrument was designed to capture best practices outlined in the National Research Council's Framework for K-12 Science Education as they occur within a science lesson. The goals of the SCIIENCE instrument are to (a) assess the quality of science instruction in PK-3 classrooms, (b) capture teacher behaviors and instructional practices that engage students in the lesson, promote scientific studies, encourage higher-level thinking, and (c) provide a feedback mechanism for guiding professional development of PK-3 teachers. Science educators can apply this instrument to teacher behaviors and use the data to improve classroom inquiry instructional methodology.}
}
@article{COOPER200672,
title = {Definability as hypercomputational effect},
journal = {Applied Mathematics and Computation},
volume = {178},
number = {1},
pages = {72-82},
year = {2006},
note = {Special Issue on Hypercomputation},
issn = {0096-3003},
doi = {https://doi.org/10.1016/j.amc.2005.09.072},
url = {https://www.sciencedirect.com/science/article/pii/S0096300305008350},
author = {S. Barry Cooper},
keywords = {Computability, Definability, Hypercomputation},
abstract = {The classical simulation of physical processes using standard models of computation is fraught with problems. On the other hand, attempts at modelling real-world computation with the aim of isolating its hypercomputational content have struggled to convince. We argue that a better basic understanding can be achieved through computability theoretic deconstruction of those physical phenomena most resistant to classical simulation. From this we may be able to better assess whether the hypercomputational enterprise is proleptic computer science, or of mainly philosophical interest.}
}
@article{ALEMANY2018429,
title = {Effects of binary variables in mixed integer linear programming based unit commitment in large-scale electricity markets},
journal = {Electric Power Systems Research},
volume = {160},
pages = {429-438},
year = {2018},
issn = {0378-7796},
doi = {https://doi.org/10.1016/j.epsr.2018.03.019},
url = {https://www.sciencedirect.com/science/article/pii/S0378779618300919},
author = {Juan Alemany and Leszek Kasprzyk and Fernando Magnago},
keywords = {Binary variables relaxation, Branch and cut algorithm, Day-ahead electricity market clearing, Mixed integer linear programming, Unit commitment},
abstract = {Mixed integer linear programming is one of the main approaches used to solve unit commitment problems. Due to the computational complexity of unit commitment problems, several researches remark the benefits of using less binary variables or relaxing them for the branch-and-cut algorithm. However, integrality constraints relaxation seems to be case dependent because there are many instances where applying it may not improve the computational burden. In addition, there is a lack of extensive numerical experiments evaluating the effects of the relaxation of binary variables in mixed integer linear programming based unit commitment. Therefore, the primary purpose of this work is to analyze the effects of binary variables and compare different relaxations, supported by extensive computational experiments. To accomplish this objective, two power systems are used for the numerical tests: the IEEE118 test system and a very large scale real system. The results suggest that a direct link between the relaxation of binary variables and computational burden cannot be easily assured in the general case. Therefore, relaxing binary variables should not be used as a general rule-of-practice to improve computational burden, at least, until each particular model is tested under different load scenarios and formulations to quantify the final effects of binary variables on the specific UC implementation. The secondary aim of this work is to give some preliminary insight into the reasons that could be supporting the binary relaxation in some UC instances.}
}
@article{CAKIROGLU2023101279,
title = {A model to develop activities for teaching programming through metacognitive strategies},
journal = {Thinking Skills and Creativity},
volume = {48},
pages = {101279},
year = {2023},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2023.101279},
url = {https://www.sciencedirect.com/science/article/pii/S1871187123000494},
author = {Ünal ÇAKIROĞLU and Betül ER},
keywords = {Programming, Teaching programming, Metacognition, Problem solving},
abstract = {Among many endeavors to explore ways to effective teaching programming in classrooms, focusing on metacognition is somewhat neglected. This study suggests a framework for integrating metacognitive strategies to the programming teaching process. In order to achieve this, the knowledge and skills required for writing quality programs and the nature of the metacognition is interrelated with each other encompassing problem solving strategies with programming. The framework has two dimensions; one is metacognitive knowledge including conditional, procedure and declarative knowledge and the other is metacognitive regulation including planning, monitoring and evaluating activities embodied in a specialized worksheet. The activities were designed to be implemented in three phases of programming: at the beginning of the programming, during the programming and after finishing the programming. The suggested framework with the worksheet is pilot tested and validated to be used in the undergraduate introductory programming classrooms. We hope that the suggested roadmap may contribute to the instructional designers and researchers in the field of programming teaching and evaluation.}
}
@article{BOUVRY2025105081,
title = {The European master for HPC curriculum},
journal = {Journal of Parallel and Distributed Computing},
volume = {201},
pages = {105081},
year = {2025},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2025.105081},
url = {https://www.sciencedirect.com/science/article/pii/S0743731525000486},
author = {Pascal Bouvry and Mats Brorsson and Ramon Canal and Aryan Eftekhari and Siegfried Höfinger and Didier Smets and Harald Köstler and Tomáš Kozubek and Ezhilmathi Krishnasamy and Josep Llosa and Alexandra Lukas-Rother and Xavier Martorell and Dirk Pleiter and Ana Proykova and Maria-Ribera Sancho and Olaf Schenk and Cristina Silvano},
keywords = {Computing education, High-performance computing, Master in HPC, Model curricula},
abstract = {The use of High-Performance Computing (HPC) is crucial for addressing various grand challenges. While significant investments are made in digital infrastructures that comprise HPC resources, its realisation, operation, and, in particular, its use critically depends on suitably trained experts. In this paper, we present the results of an effort to design and implement a pan-European reference curriculum for a master's degree in HPC.}
}
@article{GRABOWSKI201921,
title = {A Primer on Data Analytics in Functional Genomics: How to Move from Data to Insight?},
journal = {Trends in Biochemical Sciences},
volume = {44},
number = {1},
pages = {21-32},
year = {2019},
issn = {0968-0004},
doi = {https://doi.org/10.1016/j.tibs.2018.10.010},
url = {https://www.sciencedirect.com/science/article/pii/S0968000418302263},
author = {Piotr Grabowski and Juri Rappsilber},
keywords = {data integration, data science, functional genomics, machine learning, systems biology},
abstract = {High-throughput methodologies and machine learning have been central in developing systems-level perspectives in molecular biology. Unfortunately, performing such integrative analyses has traditionally been reserved for bioinformaticians. This is now changing with the appearance of resources to help bench-side biologists become skilled at computational data analysis and handling large omics data sets. Here, we show an entry route into the field of omics data analytics. We provide information about easily accessible data sources and suggest some first steps for aspiring computational data analysts. Moreover, we highlight how machine learning is transforming the field and how it can help make sense of biological data. Finally, we suggest good starting points for self-learning and hope to convince readers that computational data analysis and programming are not intimidating.}
}
@article{MANNUCCI2023103265,
title = {Exploring potential futures: Evaluating the influence of deep uncertainties in urban planning through scenario planning: A case study in Rome, Italy},
journal = {Futures},
volume = {154},
pages = {103265},
year = {2023},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2023.103265},
url = {https://www.sciencedirect.com/science/article/pii/S0016328723001696},
author = {Simona Mannucci and Jan H. Kwakkel and Michele Morganti and Marco Ferrero},
keywords = {Scenario planning, Deep uncertainties, Urban planning, Decision-making, Climate adaptation},
abstract = {Cities play a critical role in developing adaptable strategies to address the challenges posed by climate change. However, the inherent complexity of urban environments and their uncertain future conditions necessitate exploring innovative approaches and tools to assist the current planning practices. This paper presents a workflow rooted in model-based scenario planning for long-term adaptation planning given uncertain futures. To demonstrate the workflow’s effectiveness, a pertinent case study was conducted in a flood-prone area of Rome. The study employed a land-use change model to examine potential urban growth patterns, considering the uncertain implementation of new poles of attraction. This interdisciplinary study constitutes an initial stride toward implementing uncertainty within urban planning frameworks. Future prospects encompass the integration of multiple models for cross-scale analysis, embracing further critical environmental and social aspects. This research contributes to advancing urban resilience strategies. It enhances the understanding of adapting to an uncertain future in the face of climate change, as urban areas must embrace comprehensive planning to ensure flexible adaptation when faced with climate-driven uncertainties in long-term planning. In conclusion, the study underscores that embracing uncertainty is a challenge and a pivotal opportunity to shape resilient and adaptable urban futures.}
}
@article{PAVLOVA2024103631,
title = {A dual process model of spontaneous conscious thought},
journal = {Consciousness and Cognition},
volume = {118},
pages = {103631},
year = {2024},
issn = {1053-8100},
doi = {https://doi.org/10.1016/j.concog.2023.103631},
url = {https://www.sciencedirect.com/science/article/pii/S105381002300168X},
author = {Maria K. Pavlova},
keywords = {Automatic processing, Cognitive control, Executive failure, Involuntary attention, Mental effort, Mind wandering, Meta-awareness, Modality and interference in working memory, Process–occurrence framework, Spontaneous thought},
abstract = {In the present article, I review theory and evidence on the psychological mechanisms of mind wandering, paying special attention to its relation with executive control. I then suggest applying a dual-process framework (i.e., automatic vs. controlled processing) to mind wandering and goal-directed thought. I present theoretical arguments and empirical evidence in favor of the view that mind wandering is based on automatic processing, also considering its relation to the concept of working memory. After that, I outline three scenarios for an interplay between mind wandering and goal-directed thought during task performance (parallel automatic processing, off-task thought substituting on-task thought, and non-disruptive mind wandering during controlled processing) and address the ways in which the mind-wandering and focused-attention spells can terminate. Throughout the article, I formulate empirical predictions. In conclusion, I discuss how automatic and controlled processing may be balanced in human conscious cognition.}
}
@article{GOMOLLONBEL20245056,
title = {Connecting chemical worlds for a sustainable future},
journal = {Chemical Science},
volume = {15},
number = {14},
pages = {5056-5060},
year = {2024},
issn = {2041-6520},
doi = {https://doi.org/10.1039/d3sc06815c},
url = {https://www.sciencedirect.com/science/article/pii/S2041652024004267},
author = {Fernando Gomollón-Bel and Javier García-Martínez},
abstract = {Chemistry plays a central role in science and is the basis of one of the major, more impactful, and diverse industries. However, to address the most pressing global challenges, we must learn to create connections in an effective and meaningful way, with other disciplines, industries, and society at large. Here, we present the IUPAC Top Ten Emerging Technologies in Chemistry as an example of an initiative that highlights the value of the most promising advances in chemistry and contributes to creating connections to accelerate sustainable solutions for our society and our planet.}
}
@article{KIM2021188548,
title = {A primer on applying AI synergistically with domain expertise to oncology},
journal = {Biochimica et Biophysica Acta (BBA) - Reviews on Cancer},
volume = {1876},
number = {1},
pages = {188548},
year = {2021},
issn = {0304-419X},
doi = {https://doi.org/10.1016/j.bbcan.2021.188548},
url = {https://www.sciencedirect.com/science/article/pii/S0304419X21000457},
author = {Jason Kim and Rebecca Kusko and Benjamin Zeskind and Jenny Zhang and Renan Escalante-Chong},
abstract = {Background
The concurrent growth of large-scale oncology data alongside the computational methods with which to analyze and model it has created a promising environment for revolutionizing cancer diagnosis, treatment, prevention, and drug discovery. Computational methods applied to large datasets have accelerated the drug discovery process by reducing bottlenecks and widening the search space beyond what is experimentally tractable. As the research community gains understanding of the myriad genetic underpinnings of cancer via sequencing, imaging, screens, and more that are ingested, transformed, and modeled by top open-source machine learning and artificial intelligence tools readily available, the next big drug candidate might seem merely an “Enter” key away. Of course, the reality is more convoluted, but still promising.
Scope of review
We present methods to approach the process of building an AI model, with strong emphasis on the aspects of model development we believe to be crucial to success but that are not commonly discussed: diligence in posing questions, identifying suitable datasets and curating them, and collaborating closely with biology and oncology experts while designing and evaluating the model. Digital pathology, Electronic Health Records, and other data types outside of high-throughput molecular data are reviewed well by others and outside of the scope of this review. This review emphasizes the importance of considering the limitations of the datasets, computational methods, and our minds when designing AI models. For example, datasets can be biased towards areas of research interest, funding, and particular patient populations. Neural networks may learn representations and correlations within the data that are grounded not in biological phenomena, but statistical anomalies erroneously extracted from the training data. Researchers may mis-interpret or over-interpret the output, or design and evaluate the training process such that the resultant model generalizes poorly. Fortunately, awareness of the strengths and limitations of applying data analytics and AI to drug discovery enables us to leverage them carefully and insightfully while maximizing their utility. These applications when performed in close collaboration with domain experts, together with continuous critical evaluation, generation of new data to minimize known blind spots as they are found, and rigorous experimental validation, increases the success rate of the study. We will discuss applications including AI-assisted target identification, drug repurposing, patient stratification, and gene prioritization.
Major conclusions
Data analytics and AI have demonstrated capabilities to revolutionize cancer research, prevention, and treatment by maximizing our understanding and use of the expanding panoply of experimental data. However, to separate promise from true utility, computational tools must be carefully designed, critically evaluated, and constantly improved. Once that is achieved, a human-computer hybrid discovery process will outperform one driven by each alone.
General significance
This review highlights the challenges and promise of synergizing predictive AI models with human expertise towards greater understanding of cancer.}
}
@article{GAL2014246,
title = {Novel approaches to address challenges in modelling aquatic ecosystems},
journal = {Environmental Modelling & Software},
volume = {61},
pages = {246-248},
year = {2014},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2014.08.008},
url = {https://www.sciencedirect.com/science/article/pii/S1364815214002321},
author = {Gideon Gal and Matthew Hipsey and Karsten Rinke and Barbara Robson},
keywords = {Ecological modelling, Freshwater, Marine, Biogeochemistry, Lake, Estuary},
abstract = {Aquatic ecosystems are under increasing stress due to direct and indirect human activities. In response to this increased stress, aquatic ecosystems models are increasingly used to simulate water quality responses to changes. The increasing use of these models has not come without challenges. This thematic issue brings together examples of the latest thinking and novel approaches addressing key areas across a range of aquatic ecosystems, from lakes to rivers to marine waters. Topics include approaches applied to cover the full range of activities from methodological and technical developments of model-driven research of aquatic ecosystem functioning to model applications in lake management and decision-making. This thematic issue will provide additional momentum towards the ongoing development and improvement of aquatic models and their application.}
}
@article{GACS19811,
title = {Causal nets or what is a deterministic computation?},
journal = {Information and Control},
volume = {51},
number = {1},
pages = {1-19},
year = {1981},
issn = {0019-9958},
doi = {https://doi.org/10.1016/S0019-9958(81)90058-9},
url = {https://www.sciencedirect.com/science/article/pii/S0019995881900589},
author = {Péter Gács and Leonid A. Levin},
abstract = {The network approach to computation is more direct and “physical” than the one based on some specific computing devices (like Turing machines). However, the size of a usual—e.g., Boolean—network does not reflect the complexity of computing the corresponding function, since a small network may be very hard to find even if it exists. A history of the work of a particular computing device can be described as a network satisfying some restrictions. The size of this network reflects the complexity of the problem, but the restrictions are usually somewhat arbitrary and even awkward. Causal nets are restricted only by determinism (causality) and locality of interaction. Their geometrical characteristics do reflect computational complexities. And various imaginary computer devices are easy to express in their terms. The elementarity of this concept may help bringing geometrical and algebraic (and maybe even physical) methods into the theory of computations. This hope is supported by the group-theoretical criterion given in this paper for computability from symmetrical initial configurations.}
}
@article{DEMPSTER1990261,
title = {Causality and statistics},
journal = {Journal of Statistical Planning and Inference},
volume = {25},
number = {3},
pages = {261-278},
year = {1990},
issn = {0378-3758},
doi = {https://doi.org/10.1016/0378-3758(90)90076-7},
url = {https://www.sciencedirect.com/science/article/pii/0378375890900767},
author = {A.P. Dempster},
keywords = {Causal inference, causal effects, subjectivity, objectivity, randomization, experimentation.},
abstract = {Many aspects of statistical design, modelling, and inference have close and important connections with causal thinking. These are analyzed in the paper against a philosophical background that regards formal mathematical models as having dual interpretations, reflecting both objectivist reality and subjectivist rationality. The latter aspect weakens the need for an objective theory of probabilistic causation, and suggests that a traditional image of causes as deterministic mechanisms should remain primary. It is argued that such causes should guide much preformal thinking about what to include in formal statistical models, especially of dynamic phenomena. The statistical measurement of causal effects is facilitated by good statistical design, including randomization where feasible, and requires other methodologies for controlling and assessing uncertainties, for example in model construction and inference. Illustrative examples include case studies where the problem is to assess retrospectively the causes of observed events and where the task is to assess future risks from controllable factors.}
}
@article{LI20241035,
title = {Current status and construction scheme of smart geothermal field technology},
journal = {Petroleum Exploration and Development},
volume = {51},
number = {4},
pages = {1035-1048},
year = {2024},
issn = {1876-3804},
doi = {https://doi.org/10.1016/S1876-3804(24)60523-9},
url = {https://www.sciencedirect.com/science/article/pii/S1876380424605239},
author = {Gensheng LI and Xianzhi SONG and Yu SHI and Gaosheng WANG and Zhongwei HUANG},
keywords = {smart geothermal field, intelligent development of geothermal reservoirs, application scenario, intelligent characterization, intelligent simulation, intelligent optimization control, smart management},
abstract = {To address the key problems in the application of intelligent technology in geothermal development, smart application scenarios for geothermal development are constructed. The research status and existing challenges of intelligent technology in each scenario are analyzed, and the construction scheme of smart geothermal field system is proposed. The smart geothermal field is an organic integration of geothermal development engineering and advanced technologies such as the artificial intelligence. At present, the technology of smart geothermal field is still in the exploratory stage. It has been tested for application in scenarios such as intelligent characterization of geothermal reservoirs, dynamic intelligent simulation of geothermal reservoirs, intelligent optimization of development schemes and smart management of geothermal development. However, it still faces many problems, including the high computational cost, difficult real-time response, multiple solutions and strong model dependence, difficult real-time optimization of dynamic multi-constraints, and deep integration of multi-source data. The construction scheme of smart geothermal field system is proposed, which consists of modules including the full database, intelligent characterization, intelligent simulation and intelligent optimization control. The connection between modules is established through the data transmission and the model interaction. In the next stage, it is necessary to focus on the basic theories and key technologies in each module of the smart geothermal field system, to accelerate the lifecycle intelligent transformation of the geothermal development and utilization, and to promote the intelligent, stable, long-term, optimal and safe production of geothermal resources.}
}
@article{LI20103574,
title = {The process model to aid innovation of products conceptual design},
journal = {Expert Systems with Applications},
volume = {37},
number = {5},
pages = {3574-3587},
year = {2010},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2009.10.034},
url = {https://www.sciencedirect.com/science/article/pii/S0957417409009002},
author = {Wenqiang Li and Yan Li and Jian Wang and Xiaoying Liu},
keywords = {Conceptual design, Innovative strategies, Process mapping, Extension reasoning, Mathematical model},
abstract = {Currently, designers often pay little attention to integrated innovation during the design process of products. In addition, the product assistance design systems mainly focus on the detailed design phrase and the construction function of mathematics models are often been neglected. In order to solve these problems, this paper proposes a conceptual design process model to aid multi-stage innovation of product design based on the integration of the essential rules of the Axiomatic Design (AD) model, Function–Behaviour–Structure (FBS) model, and the guideline of functional creative thinking logics. By utilising the function tree and functional structure tree as the mediums to express the design information and by applying the conflict solving strategies of Extensic theory, the conceptual design process is defined as an integrated system with five stages and four mappings. The integrated logical processes of this model are described with mathematical language. Thus, the whole transformation from design experiences to design principles and to mathematical model finally to aided design system is realized perfectly in the proposed process model. The meaningful exploration on the nature and practical processes of product conceptual design is carried out in this research.}
}