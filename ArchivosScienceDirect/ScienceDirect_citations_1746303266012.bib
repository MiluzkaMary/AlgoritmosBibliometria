@article{SUPPES2004457,
title = {Semantic computations of truth based on associations already learned},
journal = {Journal of Applied Logic},
volume = {2},
number = {4},
pages = {457-467},
year = {2004},
note = {CMSRA},
issn = {1570-8683},
doi = {https://doi.org/10.1016/j.jal.2004.07.006},
url = {https://www.sciencedirect.com/science/article/pii/S1570868304000461},
author = {Patrick Suppes and Jean-Yves Béziau},
keywords = {Truth, Computaton, Empirical statements, Associative networks, Spreading activation},
abstract = {This article sets forth a detailed theoretical proposal of how the truth of ordinary empirical statements, often atomic in form, is computed. The method of computation draws on psychological concepts such as those of associative networks and spreading activation, rather that the concepts of philosophical or logical theories of truth. Axioms for a restricted class of cases are given, as well as some detailed examples.}
}
@article{WANG20098093,
title = {A computational narrative construction method with applications in organizational learning of social service organizations},
journal = {Expert Systems with Applications},
volume = {36},
number = {4},
pages = {8093-8102},
year = {2009},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2008.10.035},
url = {https://www.sciencedirect.com/science/article/pii/S0957417408007495},
author = {W.M. Wang and C.F. Cheung and W.B. Lee and S.K. Kwok},
keywords = {Narrative construction, Knowledge management, Concept mapping, Knowledge-based systems, Computational, Reflective learning, Narrative simulation},
abstract = {Acquisition of knowledge must be interwoven with the process of applying it. However, traditional training methods which provide abstract knowledge have shown ineffective for gaining experience of the work. In order to solve this problem, more and more researchers have included narrative in simulation, which is known as narrative simulation. By providing the narratives, participants recognize the choices, decisions, and experience that lead to the consequences of those decisions. It has been proven that narrative simulation is very useful in facilitating in-depth learning and reflective learning. However, conventional methods of data collection and narrative construction for narrative simulation are labor intensive and time consuming. They make use of previous narratives manually and directly. They are inadequate to cope with the fast moving world where knowledge is changing rapidly. In order to provide a way for facilitating the construction of narrative simulation, a novel computational narrative construction method is proposed. By incorporating technologies of knowledge-based system (KBS), computational linguistics, and artificial intelligence (AI), the proposed method provides an efficient and effective way for collecting narratives and automating the construction of narratives. The method converts the unstructured narratives into a structural representation for abstraction and facilitating computing processing. Moreover, it constructs the narratives that combine multiple narratives into a single narrative by applying a forecasting algorithm. The proposed method was successfully implemented in early intervention in mental health care of a social service company in Hong Kong since the case records in that process have structural similarities to narrative. The accuracies of data conversion and predictive function were measured based on recall and precision and encouraging results were obtained. High recall and precision are achieved in the data conversion function, and high recall for the predictive function when new concepts are excluded. The results show that it is possible for converting multiple narratives into a single narrative automatically. Based on the approach, it helps to stimulate knowledge workers to explore new problem solving methods so as to increase the quality of their solutions.}
}
@article{SKOWRON20115939,
title = {Information systems in modeling interactive computations on granules},
journal = {Theoretical Computer Science},
volume = {412},
number = {42},
pages = {5939-5959},
year = {2011},
note = {Rough Sets and Fuzzy Sets in Natural Computing},
issn = {0304-3975},
doi = {https://doi.org/10.1016/j.tcs.2011.05.045},
url = {https://www.sciencedirect.com/science/article/pii/S0304397511004634},
author = {Andrzej Skowron and Piotr Wasilewski},
keywords = {Interactive computing, Interactive systems, Multi-agent systems, Rough sets, Granular computing, Wisdom technology},
abstract = {In this paper, we discuss the importance of information systems in modeling interactive computations performed on (complex) granules and we propose a formal approach to interactive computations based on generalized information systems and rough sets which can be combined with other soft computing paradigms such as fuzzy sets or evolutionary computing, but also with machine learning and data mining techniques. Information systems are treated as dynamic granules used for representing the results of the interaction of attributes with the environment. Two kinds of attributes are distinguished, namely, the perception attributes, including sensory attributes, and the action attributes. Sensory attributes are the basic perception attributes, other perception attributes are constructed on the basis of the sensory ones. Actions are activated when their guards, being often complex and vague concepts, are satisfied to a satisfactory degree. The guards can be approximated on the basis of measurements performed by sensory attributes rather than defined exactly. Satisfiability degrees for guards are results of reasoning called the adaptive judgment. The approximations are induced using hierarchical modeling. We show that information systems can be used for modeling more advanced forms of interactions in hierarchical modeling. The role of hierarchical interactions is emphasized in the modeling of interactive computations. Some illustrative examples of interactions used in the ACT-R 6.0 system are reported. ACT-R 6.0 is based on a cognitive architecture and can be treated as an example of a highly interactive complex granule which can be involved in hierarchical interactions. For modeling of interactive computations, we propose much more general information systems than the studied dynamic information systems (see, e.g., Ciucci (2010) [8] and Pałasiński and Pancerz (2010) [32]). For example, the dynamic information systems are making it possible to consider incremental changes in information systems. However, they do not contain the perception and action attributes necessary for modeling interactive computations, in particular for modeling intrastep interactions.}
}
@incollection{HOLCOMBE2005407,
title = {30 Computational modelling of creativity in abstract art},
editor = {Grant Malcolm},
series = {Studies in Multidisciplinarity},
publisher = {Elsevier},
volume = {2},
pages = {407-424},
year = {2005},
booktitle = {Multidisciplinary Approaches to Visual Representations and Interpretations},
issn = {1571-0831},
doi = {https://doi.org/10.1016/S1571-0831(04)80058-3},
url = {https://www.sciencedirect.com/science/article/pii/S1571083104800583},
author = {Mike Holcombe and Samantha Smith and Rowan Merewood and Andy Swingeford},
abstract = {Artistic creativity is studied through the construction of computational models of a number of well-known modern artists. In particular, the work of Piet Mondrian, M.C. Escher and Paul Klee are suitable vehicles for investigation since their work is accompanied by extensive writings describing the ideas and motivation behind their compositions. In particular, we have tried to abstract from their theories, rules that describe the construction process or the properties that their finished artefacts posses in order to create software programs that can articulate these rules. In this way, we are able to simulate either automatically or with user interaction, the process of creating works of art of a similar genre and satisfying the properties desired by the artist. Since the rules are bound to be considerably more complex than those currently exposed, we are looking to use machine-learning techniques to develop more sophisticated agents, which may behave more closely like the actual artist.}
}
@article{HADIMOGAVI2024100027,
title = {ChatGPT in education: A blessing or a curse? A qualitative study exploring early adopters’ utilization and perceptions},
journal = {Computers in Human Behavior: Artificial Humans},
volume = {2},
number = {1},
pages = {100027},
year = {2024},
issn = {2949-8821},
doi = {https://doi.org/10.1016/j.chbah.2023.100027},
url = {https://www.sciencedirect.com/science/article/pii/S2949882123000270},
author = {Reza {Hadi Mogavi} and Chao Deng and Justin {Juho Kim} and Pengyuan Zhou and Young {D. Kwon} and Ahmed {Hosny Saleh Metwally} and Ahmed Tlili and Simone Bassanelli and Antonio Bucchiarone and Sujit Gujar and Lennart E. Nacke and Pan Hui},
keywords = {Artificial intelligence (AI), Generative AI, ChatGPT, Education, Human-computer interaction (HCI),, Early adopters, Social media, Qualitative research},
abstract = {To foster the development of pedagogically potent and ethically sound AI-integrated learning landscapes, it is pivotal to critically explore the perceptions and experiences of the users immersed in these contexts. In this study, we perform a thorough qualitative content analysis across four key social media platforms. Our goal is to understand the user experience (UX) and views of early adopters of ChatGPT across different educational sectors. The results of our research show that ChatGPT is most commonly used in the domains of higher education, K-12 education, and practical skills training. In social media dialogues, the topics most frequently associated with ChatGPT are productivity, efficiency, and ethics. Early adopters' attitudes towards ChatGPT are multifaceted. On one hand, some users view it as a transformative tool capable of amplifying student self-efficacy and learning motivation. On the other hand, there is a degree of apprehension among concerned users. They worry about a potential overdependence on the AI system, which they fear might encourage superficial learning habits and erode students’ social and critical thinking skills. This dichotomy of opinions underscores the complexity of Human-AI Interaction in educational contexts. Our investigation adds depth to this ongoing discourse, providing crowd-sourced insights for educators and learners who are considering incorporating ChatGPT or similar generative AI tools into their pedagogical strategies.}
}
@article{PAL20133944,
title = {Title Paper: Natural computing: A problem solving paradigm with granular information processing},
journal = {Applied Soft Computing},
volume = {13},
number = {9},
pages = {3944-3955},
year = {2013},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2013.06.026},
url = {https://www.sciencedirect.com/science/article/pii/S1568494613002159},
author = {Sankar K. Pal and Saroj K. Meher},
keywords = {Natural computing, Granular computing, Soft computing, Hybrid model, Decision systems},
abstract = {Natural computing, inspired by biological course of action, is an interdisciplinary field that formalizes processes observed in living organisms to design computational methods for solving complex problems, or designing artificial systems with more natural behaviour. Based on the tasks abstracted from natural phenomena, such as brain modelling, self-organization, self-repetition, self evaluation, Darwinian survival, granulation and perception, nature serves as a source of inspiration for the development of computational tools or systems that are used for solving complex problems. Nature inspired main computing paradigms used for such development include artificial neural networks, fuzzy logic, rough sets, evolutionary algorithms, fractal geometry, DNA computing, artificial life and granular or perception-based computing. Information granulation in granular computing is an inherent characteristic of human thinking and reasoning process performed in everyday life. The present article provides an overview of the significance of natural computing with respect to the granulation-based information processing models, such as neural networks, fuzzy sets and rough sets, and their hybridization. We emphasize on the biological motivation, design principles, application areas, open research problems and challenging issues of these models.}
}
@incollection{LUCHINI2023195,
title = {Chapter 13 - Brain networks of creative cognition},
editor = {Roni Reiter-Palmon and Sam Hunter},
booktitle = {Handbook of Organizational Creativity (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
pages = {195-207},
year = {2023},
isbn = {978-0-323-91840-4},
doi = {https://doi.org/10.1016/B978-0-323-91840-4.00021-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780323918404000219},
author = {Simone Luchini and Roger E. Beaty},
keywords = {Creativity, Default network, Divergent thinking, Executive control network, Functional connectivity, Network neuroscience},
abstract = {In recent years there has been an increasing interest in the role of brain networks supporting creative thinking. This chapter provides a summary of the literature on the network neuroscience of creativity, providing a twofold argument by separately detailing research in domain-general and domain-specific creativity. The first section will concern two main lines of research on domain-general creativity: (1) the neurocognitive mechanisms of creative cognition (how brain networks map onto specific cognitive processes involved in creative thinking), and (2) the individual differences in brain network connectivity and creative ability (how brain networks relate to differences in creative abilities). The second section, on domain-specific creativity, will then consider three domains of artistic creativity: (1) music improvisation, (2) figural creativity, and (3) literary creativity. Throughout this chapter we discuss common themes and shared findings between domain-general and domain-specific creativity. We will then conclude by outlining some of the limitations in the literature and by providing some directions for future research.}
}
@article{SAVIN2021106878,
title = {Free associations of citizens and scientists with economic and green growth: A computational-linguistics analysis},
journal = {Ecological Economics},
volume = {180},
pages = {106878},
year = {2021},
issn = {0921-8009},
doi = {https://doi.org/10.1016/j.ecolecon.2020.106878},
url = {https://www.sciencedirect.com/science/article/pii/S0921800920309484},
author = {Ivan Savin and Stefan Drews and Jeroen {van den Bergh}},
keywords = {Structural topic modelling, Growth-vs-environment debate, Public opinion, Scientific opinion, Green growth},
abstract = {The debate about the relationship between economic growth and environmental sustainability triggers a range of associations. Here we analyze open-ended textual responses of citizens and scientists concerning their associations with the terms “economic growth” and “green growth”. We derive from the responses a number of topics and examine how associations differ across distinct opinion segments of people, namely supporters of Green growth, Agrowth and Degrowth. The results indicate that the general public is more critical of the notion of economic growth than academic researchers. Citizens stress problems of corruption, social inequality, unemployment and poverty, with less variation among the three opinion segments compared to scientists. The latter more strongly emphasize the environmental consequences of economic growth. Concerning associations of scientists with the term “green growth”, we find topics questioning its feasibility to be more likely expressed by Degrowth supporters, while topics stressing the possibility of sustainable economic growth by Green growth supporters. We find that topic polarization is stronger for scientists than citizens. Our results provide further validation for opinion clusters identified in previous studies and uncover additional insights about related views on growth and sustainability.}
}
@article{AUSTIN2006544,
title = {Matrix and finite element stack machines for structural engineering computations with units},
journal = {Advances in Engineering Software},
volume = {37},
number = {8},
pages = {544-559},
year = {2006},
issn = {0965-9978},
doi = {https://doi.org/10.1016/j.advengsoft.2005.10.004},
url = {https://www.sciencedirect.com/science/article/pii/S0965997805001833},
author = {Mark A. Austin},
keywords = {Stack machine, Matrix computations, Physical units, Scripting language design, Finite element analysis},
abstract = {Despite the well known benefits of physical units, matrices, and matrix algebra in engineering computations, most engineering analysis packages are essentially dimensionless. This paper describes the design and implementation of matrix and finite element stack machines for Aladdin, a new computational environment that embeds units inside matrix and finite element calculations. Functionality of the Aladdin stack machine is illustrated by working step by step through the setup and execution of three examples: (1) Parsing and stack machine execution for x=2in; (2) Deflection analysis of a cantilever beam, and (3) Rollup maneuver for a long cantilever beam.}
}
@article{OTOOLE2024100080,
title = {Extending human creativity with AI},
journal = {Journal of Creativity},
volume = {34},
number = {2},
pages = {100080},
year = {2024},
issn = {2713-3745},
doi = {https://doi.org/10.1016/j.yjoc.2024.100080},
url = {https://www.sciencedirect.com/science/article/pii/S2713374524000062},
author = {Katherine O'Toole and Emőke-Ágnes Horvát},
keywords = {Computational creativity, Generative AI, HCI},
abstract = {The development of generative AI has led to novel ways that technology can be integrated into creative activities. However, this has also raised concerns about how human creators will be affected, and what impact it may have on creative industries. As a result, there has been research into how we can design AI tools that work with human creators, rather than replacing them. In this paper we review approaches utilized to build AI tools that facilitate human creativity and allow users to engage fully and authentically in the creative process. These include leveraging AI models to help us shed light on elements of the creative process, building interfaces that encourage exploration of ideas, and designing technological affordances that can support the development of new creative practices.}
}
@article{MATENCIO2021129639,
title = {A physicochemical, thermodynamical, structural and computational evaluation of kynurenic acid/cyclodextrin complexes},
journal = {Food Chemistry},
volume = {356},
pages = {129639},
year = {2021},
issn = {0308-8146},
doi = {https://doi.org/10.1016/j.foodchem.2021.129639},
url = {https://www.sciencedirect.com/science/article/pii/S0308814621006452},
author = {Adrián Matencio and Fabrizio Caldera and Alberto {Rubin Pedrazzo} and Yousef {Khazaei Monfared} and Nilesh {K. Dhakar} and Francesco Trotta},
keywords = {Kynurenic acid, Cyclodextrin, Inclusion complex, Physicochemical, Stability},
abstract = {In this work, the interaction between Kynurenic acid (KYNA) and several natural and modified cyclodextrins (CDs) is carried out. Among all the CD tested, HPβ-CD showed the strongest complexation constant (KF), with a value of 270.94 ± 29.80 M−1. Between natural (α- and β-) CDs, the complex of KYNA with β-CD was the most efficient. The inclusion complex of KYNA with CDs showed a strong influence of pH and temperature. The KF value decreased at high pH values, when the pKa was passed. Moreover, an increase of the temperature caused a decrease in the KF values. The thermodynamic parameters of the complexation (ΔH°, ΔS° and ΔG°) were studied with negative entropy, enthalpy and spontaneity of the process at 25 °C. Moreover, the inclusion complex was also characterized using FTIR and TGA. Finally, molecular docking calculations provided different interactions and their influence in the complexation constant.}
}
@article{KUMAR20224712,
title = {Efficient computational stochastic framework for performance optimization of E-waste management plant},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {34},
number = {8, Part A},
pages = {4712-4728},
year = {2022},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2022.05.018},
url = {https://www.sciencedirect.com/science/article/pii/S1319157822001677},
author = {Naveen Kumar and Deepak Sinwar and Monika Saini and Dinesh Kumar Saini and Ashish Kumar and Manjit Kaur and Dilbag Singh and Heung-No Lee},
keywords = {E-waste management plant, Availability, Maintainability, Genetic Algorithm, Differential Evolution, Particle Swarm Optimization, Markov Birth-Death Process},
abstract = {Purpose
Reliability and maintainability are the key system effectiveness measures in process and manufacturing industries, and treatment plants, especially in E-waste management plants. The present work is proposed with a motto to develop a stochastic framework for the e-waste management plant to optimize its availability integrated with reliability, availability, maintainability, and dependability (RAMD) measures and Markovian analysis to estimate the steady-state availability of the E-waste management plant. In the analysis an effort is also made to identify the best performing algorithm for availability optimization of the e-waste plant.
Methodology
A stochastic model for a particular plant is developed and its availability is optimized using various metaheuristic approaches like a genetic algorithm (GA), particle swarm optimization (PSO), and differential evolutions (DE). The most sensitive component is identified using RAMD methodology while the effect of deviation in various failure and repair rates are observed by the proposed model. The failure and repair rates follow an exponential distribution. All time-dependent random variables are statistically independent.
Originality/Novelties
A novel stochastic model is presented for an e-waste management plant and optimum availability is obtained using metaheuristic approaches. The proposed methodology is not so far discussed in the reliability analysis of process industries.
Findings
The numerical results of the proposed model compared to identify the most efficient algorithm. It is observed that genetic algorithm provides the maximum value (0.92330969) of availability at a population size 2500 after 500 iterations. PSO algorithm attained the maximum value (0.99996744) of availability just after 50 iterations and 100 population size. So, its rate of convergence is faster than GA. The optimum value of availability is 0.99997 using differential evolution after 500 iterations and population size of more than 1000. These findings are very beneficial for system designers.
Practical Implications
The proposed methodology can be utilized to find the reliability measures of other process industries.}
}
@article{SAND2022100955,
title = {Three cases that demonstrate how students connect the domains of mathematics and computing},
journal = {The Journal of Mathematical Behavior},
volume = {67},
pages = {100955},
year = {2022},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2022.100955},
url = {https://www.sciencedirect.com/science/article/pii/S0732312322000232},
author = {Odd Petter Sand and Elise Lockwood and Marcos D. Caballero and Knut Mørken},
keywords = {Computing, Modeling, Programming, Thinking and learning, Connections, Undergraduate students},
abstract = {This study uses actor-oriented transfer perspective to investigate different ways in which students make connections across the domains of mathematics and computing. We interview first-year students at the University of Oslo as they work with a set of tutorials that we designed to integrate knowledge from both domains. The cases we present here demonstrate four different types of cross-domain connections: (a) mathematically reproducing the work of a computer program, (b) cyclically improving a program to produce better output, (c) coupling math to output to justify program improvements and (d) coupling math to code to justify program design. We provide rich examples of the ways in which students make these connections and discuss affordances for mathematical learning in this context.}
}
@article{MOSTAFA20118782,
title = {A neuro-computational intelligence analysis of the global consumer software piracy rates},
journal = {Expert Systems with Applications},
volume = {38},
number = {7},
pages = {8782-8803},
year = {2011},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2011.01.090},
url = {https://www.sciencedirect.com/science/article/pii/S0957417411001102},
author = {Mohamed M. Mostafa},
keywords = {Global software piracy, Ethical behavior, Neural networks, Bayesian regression, Evolutionary computation models},
abstract = {Software piracy represents a major damage to the moral fabric associated with the respect of intellectual property. The rate of software piracy appears to be increasing globally, suggesting that additional research that uses new approaches is necessary to evaluate the problem. The study remedies previous econometric and methodological shortcomings by applying Bayesian, robust and evolutionary computation robust regression algorithms to formally test empirical literature on software piracy. To gain further insights into software piracy at the global level, the study also uses five neuro-computational intelligence methodologies: multi-layer perceptron neural network (MLP), probabilistic neural network (PNN), radial basis function neural network (RBF), generalized regression neural network (GRNN) and Kohonen’s self-organizing maps (SOM) to classify, predict and cluster software piracy rates among 102 nations. At the empirical level, this research shows that software piracy is significantly affected by the wealth of nation as measured by gross domestic product (GDP), the nation’s expenditure on research and development and the nation’s judicial efficiency. At the methodological level, this research shows that neuro-computational models outperform traditional statistical techniques such as regression analysis, discriminant analysis and cluster analysis in predicting, classifying and clustering software piracy rates due to their robustness and flexibility of modeling algorithms.}
}
@article{MUKTI2024117407,
title = {Computer aided sketching in the early-stage design of complex vessels},
journal = {Ocean Engineering},
volume = {305},
pages = {117407},
year = {2024},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2024.117407},
url = {https://www.sciencedirect.com/science/article/pii/S0029801824007443},
author = {M.H. Mukti and R.J. Pawling and D.J. Andrews},
abstract = {Various methods have been developed for automated and semi-automated architecture generation in the computer aided ship design processes. The question remains as to how this can speed up the design process without losing the requirement elucidation intent for concept phase. This paper presents a novel approach with a software toolset to develop design and analysis approaches to early stage ship design and provide a sketching tool. This was done by enhancing the user interface and experience of the UCL Network Block Approach to achieve a “thinking sketch” in a way that is “quick” and “fluid” enough to promote inventive and creative sketching comparable to hand sketching. The UCL Network Block Approach draws on the UCL Design Building Block (DBB) approach and uses network methods applied to the synthesis of distributed ship service systems (DS3) and Computer Aided Ship Design (CASD) to expand DS3 definition in early stage ship design. The UCL originated inside-out/DBB approach to sketch driven synthesis has been made translatable to both DBB ship descriptions and ensuring early stage naval architectural “balance”. The proposed approach has been used for the first time successfully to not only carry out a rapid sketching exercise for a naval ship design but also enable quick preliminary analysis of a set of DS3 networks.}
}
@article{KLEIN2014437,
title = {Computation and Visualization of Patch Geometries for the Design of Carbon Fiber Reinforced Parts at Early Design Stages},
journal = {Procedia CIRP},
volume = {21},
pages = {437-442},
year = {2014},
note = {24th CIRP Design Conference},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2014.03.133},
url = {https://www.sciencedirect.com/science/article/pii/S2212827114006738},
author = {Daniel Klein and Kaja Scheler and Sandro Wartzack},
keywords = {Lightweight design, Early design stages, Endless fibre reinforced composites},
abstract = {The market for carbon fibers is forecast to experience a double-digit growth over the next years. The reason for this development can be found in the special characteristics of Carbon Fiber Reinforced Plastics (CFRP) like high stiffness and strength at very low weight which make this composite an ideal material for lightweight design. However, the design of parts made of CFRP is a tightrope walk between costs, mechanical characteristics and manufacturability for product developers. On the one hand, the mechanical properties are highly dependent on the ideal fiber orientation within the part and the unique material characteristics can only be exploited with a suitable fiber orientation, but on the other hand, the ideal fiber orientation is often not manufacturable or the required manufacturing technique is too expensive. Therefore, a novel algorithm to support product developers in finding a manufacturable fiber orientation or patch layout which is as close as possible to the ideal fiber orientation is introduced. This algorithm computes and highlights areas with constant fiber orientation (=cluster) based upon the ideal fiber alignment from the CAIO method. With the help of the visualization of the clusters, product developers can be supported in the decision for the best patch placement and geometry as well as in choosing the best manufacturing technique. It is important to point out that the algorithm is intended for endless fiber reinforced parts only.}
}
@article{LIU2019678,
title = {A review of the smart world},
journal = {Future Generation Computer Systems},
volume = {96},
pages = {678-691},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2017.09.010},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17319532},
author = {Hong Liu and Huansheng Ning and Qitao Mu and Yumei Zheng and Jing Zeng and Laurence T. Yang and Runhe Huang and Jianhua Ma},
keywords = {Smart world, Ubiquitous computing, Ambient intelligence, Cyber–physical–social-thinking, Internet of Things},
abstract = {Smart world is an attractive prospect with comprehensive development of ubiquitous computing involving penetrative intelligence into ubiquitous things, including physical objects (e.g., wearable devices), cyber entities (e.g., cloud services), social people (e.g., social networking) and human thinking (e.g., brain cognition). This work systematically overviews related works in the field of the smart world, and explains prospects in emerging areas. The smart world evolutions are discussed through four progressive phases, and the representative projects are accordingly introduced. Meanwhile, smart world elements and the smart world driven applications are respectively analyzed in the contexts of cyber–physical–social-thinking hyperspace. Moreover, enabling technologies including ubiquitous intelligence, web intelligence, brain informatics, social computing, big data, and security and privacy are respectively discussed. Finally, perspectives referring to ubiquitous sensing, ubiquitous object modeling, smart services, and philosophical, ethical and legal issues, are presented for identifying trends and challenges in the smart world.}
}
@article{TESCH2001633,
title = {Applying optimal control theory for elements of quantum computation in molecular systems},
journal = {Chemical Physics Letters},
volume = {343},
number = {5},
pages = {633-641},
year = {2001},
issn = {0009-2614},
doi = {https://doi.org/10.1016/S0009-2614(01)00748-5},
url = {https://www.sciencedirect.com/science/article/pii/S0009261401007485},
author = {Carmen M. Tesch and Lukas Kurtz and Regina {de Vivie-Riedle}},
abstract = {Elements of quantum computation are implemented in a vibrationally excited molecule applying optimal control theory. The two different IR-active modes of acetylene are taken as a two-qubit-system. Optimal control theory is used to design laser pulses that allow transitions within each qubit separately. Calculations for initial state preparation and basic quantum gates are presented.}
}
@article{OLTETEANU201581,
title = {comRAT-C: A computational compound Remote Associates Test solver based on language data and its comparison to human performance},
journal = {Pattern Recognition Letters},
volume = {67},
pages = {81-90},
year = {2015},
note = {Cognitive Systems for Knowledge Discovery},
issn = {0167-8655},
doi = {https://doi.org/10.1016/j.patrec.2015.05.015},
url = {https://www.sciencedirect.com/science/article/pii/S0167865515001609},
author = {Ana-Maria Olteţeanu and Zoe Falomir},
keywords = {Computational creativity, Remote Associates Test, Cognitive systems, Knowledge base, Language corpus, Cognitive modeling},
abstract = {Discovering the processes and types of knowledge organization which are involved in the creative process is a challenge up to this date. Human creativity is usually measured by psychological tests, such as the Remote Associates Test (RAT). In this paper, an approach based on a specific type of knowledge organization and processes which enables automatic solving of RAT queries is implemented (comRAT) as a part of a more general cognitive theoretical framework for creative problem-solving (CreaCogs). This aims to study: (a) whether a convergence process can be used to solve such queries and (b) if frequency of appearance of the test items in language data may influence knowledge association or discovery in solving such problems. The comRAT uses a knowledge base of language data extracted from the Corpus of Contemporary American English. The results obtained are compared to results obtained in empirical tests with humans. In order to explain why some answers might be preferred over others, frequencies of appearance of the queries and solutions are analyzed. The difficulty encountered by humans when solving RAT queries is expressed in response times and percentage of participants solving the query, and a significant moderate correlation between human data on query difficulty and the data provided by this approach is obtained.}
}
@article{PROSPERETTI20031089,
title = {Appendix 3: Report of study group on computational physics},
journal = {International Journal of Multiphase Flow},
volume = {29},
number = {7},
pages = {1089-1099},
year = {2003},
issn = {0301-9322},
doi = {https://doi.org/10.1016/S0301-9322(03)00081-8},
url = {https://www.sciencedirect.com/science/article/pii/S0301932203000818},
author = {Andrea Prosperetti and Grétar Tryggvason},
keywords = {Computational multiphase flow, Direct numerical simulations, Numerical methods},
abstract = {The great improvement of algorithms and computing hardware in the last few years must be ranked as one of the most important turning points in the history of multiphase flow research. After a brief review of some of this recent progress, it is pointed out that, besides its application to solving actual problems, computational physics plays other key roles: (1) As a tool to develop and understand basic physics and as a guide toward asking more penetrating questions; (2) As an aid in closing the averaged equations; (3) As a means to learn to compute better. Roadblocks toward greater effectiveness are the huge complexity of many of the necessary computational tasks but also, at a more practical level, the transmission of “computational knowledge” from one researcher to another, much in the same way as experimentalists can rely on readily available equipment (e.g., lasers, etc.), without having to build each item themselves. The solution to this problem will require a cultural shift––from a “cottage industry” to a “big science” mentality––which can be aided by a different attitude on the part of the funding agencies. Great synergism can be achieved by a closer integration of the multiphase computational physics enterprise with both Applied Mathematics and Computer Science.}
}
@article{DAUCE20101,
title = {Computational neuroscience, from multiple levels to multi-level},
journal = {Journal of Physiology-Paris},
volume = {104},
number = {1},
pages = {1-4},
year = {2010},
note = {Computational Neuroscience, from Multiple Levels to Multi-level},
issn = {0928-4257},
doi = {https://doi.org/10.1016/j.jphysparis.2009.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S0928425709000837},
author = {Emmanuel Daucé and Laurent Perrinet}
}
@article{DANOS200773,
title = {Distributed Measurement-based Quantum Computation},
journal = {Electronic Notes in Theoretical Computer Science},
volume = {170},
pages = {73-94},
year = {2007},
note = {Proceedings of the 3rd International Workshop on Quantum Programming Languages (QPL 2005)},
issn = {1571-0661},
doi = {https://doi.org/10.1016/j.entcs.2006.12.012},
url = {https://www.sciencedirect.com/science/article/pii/S1571066107000564},
author = {Vincent Danos and Ellie D'Hondt and Elham Kashefi and Prakash Panangaden},
keywords = {Formal language, quantum communication, quantum computing, semantics},
abstract = {We develop a formal model for distributed measurement-based quantum computations, adopting an agent-based view, such that computations are described locally where possible. Because the network quantum state is in general entangled, we need to model it as a global structure, reminiscent of global memory in classical agent systems. Local quantum computations are described as measurement patterns. Since measurement-based quantum computation is inherently distributed, this allows us to extend naturally several concepts of the measurement calculus [V. Danos, E. Kashefi and P. Panangaden, The measurement calculus (2004), arXiv:quant-ph/0412135], a formal model for such computations. Our goal is to define an assembly language, i.e. we assume that computations are well-defined and we do not concern ourselves with verification techniques. The operational semantics for systems of agents is given by a probabilistic transition system, and we define operational equivalence in a way that it corresponds to the notion of bisimilarity. With this in place, we prove that teleportation is bisimilar to a direct quantum channel, and this also within the context of larger networks.}
}
@article{CORDASCO201152,
title = {Efficient on-line algorithms for Euler diagram region computation},
journal = {Computational Geometry},
volume = {44},
number = {1},
pages = {52-68},
year = {2011},
issn = {0925-7721},
doi = {https://doi.org/10.1016/j.comgeo.2010.07.003},
url = {https://www.sciencedirect.com/science/article/pii/S0925772110000581},
author = {Gennaro Cordasco and Rosario {De Chiara} and Andrew Fish},
keywords = {Euler diagrams, Region computation, Diagram generation},
abstract = {Euler diagrams are an accessible and effective visualisation of data involving simple set-theoretic relationships. Sets are represented by closed curves in the plane and often have wellformedness conditions placed on them in order to enhance comprehensibility. The theoretical underpinning for tool support has usually focussed on the problem of generating an Euler diagram from an abstract model. However, the problem of efficient computation of the abstract model from the concrete diagram has not been addressed before, despite this computation being a necessity for computer interpretations of user drawn diagrams. This may be used, together with automated manipulations of the abstract model, for purposes such as semantic information presentation or diagrammatic theorem proving. Furthermore, in interactive settings, the user may update diagrams “on-line” by adding and removing curves, for example, in which case a system requirement is the update of the abstract model (without the necessity of recomputation of the entire abstract model). We define the notion of marked Euler diagrams, together with a method for associating marked points on the diagram with regions in the plane. Utilising these, we provide on-line algorithms which quickly compute the abstract model of a weakly reducible wellformed Euler diagram (constructible as a sequence of additions or removals of curves, keeping a wellformed diagram at each step), and quickly updates both the set of curves in the plane as well as the abstract model according to the on-line operations. Efficiency is demonstrated by comparison with a common, naive algorithm. Furthermore, the methodology enables a straightforward implementation which has subsequently been realised as an application for the user classification domain.}
}
@article{CONTI2021272,
title = {Harnessing Visual Imagery and Oculomotor Behaviour to Understand Prospection},
journal = {Trends in Cognitive Sciences},
volume = {25},
number = {4},
pages = {272-283},
year = {2021},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2021.01.009},
url = {https://www.sciencedirect.com/science/article/pii/S1364661321000115},
author = {Federica Conti and Muireann Irish},
keywords = {future thinking, visual mental imagery, episodic memory, imagination, hippocampus, default mode network},
abstract = {Much of the rich internal world constructed by humans is derived from, and experienced through, visual mental imagery. Despite growing appreciation of visual exploration in guiding episodic memory processes, extant theories of prospection have yet to accommodate the precise role of visual mental imagery in the service of future-oriented thinking. We propose that the construction of future events relies on the assimilation of perceptual details originally experienced, and subsequently reinstantiated, predominantly in the visual domain. Individual differences in the capacity to summon discrete aspects of visual imagery can therefore account for the diversity of content generated by humans during future simulation. Our integrative framework provides a novel testbed to query alterations in future thinking in health and disease.}
}
@article{PAPAIOANNOU2021e07984,
title = {Complexity analysis of the brain activity in Autism Spectrum Disorder (ASD) and Attention Deficit Hyperactivity Disorder (ADHD) due to cognitive loads/demands induced by Aristotle's type of syllogism/reasoning. A Power Spectral Density and multiscale entropy (MSE) analysis},
journal = {Heliyon},
volume = {7},
number = {9},
pages = {e07984},
year = {2021},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2021.e07984},
url = {https://www.sciencedirect.com/science/article/pii/S2405844021020879},
author = {Anastasia G. Papaioannou and Eva Kalantzi and Christos C. Papageorgiou and Kalliopi Korombili and Anastasia Βokou and Artemios Pehlivanidis and Charalabos C. Papageorgiou and George Papaioannou},
keywords = {Multiscale entropy, Power Spectral Density, Aristotle's syllogism, ASD-ADHD, Systems of thinking I&II, Cognitive load},
abstract = {Objective
We aim to investigate whether EEG dynamics differ in adults with ASD (Autism Spectrum Disorders), ADHD (attention-deficit/hyperactivity disorder), compared with healthy subjects during the performance of an innovative cognitive task: Aristotle's valid and invalid syllogisms. We follow the Neuroanatomical differences type of criterion in assessing the results of our study in supporting or not the dual-process theory of Kahneman, 2011) (Systems I & II of thinking).
Method
We recorded EEGs from 14 scalp electrodes in 30 adults with ADHD, 30 with ASD and 24 healthy, normal subjects. The subjects were exposed in a set of innovative cognitive tasks (inducing varying cognitive loads), the Aristotle's four types of syllogism mentioned above. The multiscale entropy (MSE), a nonlinear information-theoretic measure or tool was computed to extract features that quantify the complexity of the EEG.
Results
The dynamics of the curves of the grand average of MSE values of the ADHD and ASD participants was significantly in higher levels for the majority of time scales, than the healthy subjects over a number of brain regions (electrodes locations), during the performance of both valid and invalid types of syllogism. This result is seemingly not in accordance of the broadly accepted ‘theory’ of complexity loss in ‘pathological’ subjects, but actually this is not the case as explained in the text. ADHD subjects are engaged in System II of thinking, for both Valid and Invalid syllogism, ASD and Control in System I for valid and invalid syllogism, respectively. A surprising and ‘provocative’ result of this paper, as shown in the next sections, is that the Complexity-variability of ASD and ADHD subjects, when they face Aristotle's types of syllogisms, is higher than that of the control subjects. An explanation is suggested as described in the text. Also, in the case of invalid type of Aristotelian syllogisms, the linguistic and visuo-spatial systems are both engaged ONLY in the temporal and occipital regions of the brain, respectively, of ADHD subjects. In the case of valid type, both above systems are engaged in the temporal and occipital regions of the brain, respectively, of both ASD and ADHD subjects, while in the control subjects only the visuo-spatial type is engaged (Goel et al., 2000; Knauff, 2007).
Conclusion
Based on the results of the analysis described in this work, the differences in the EEG complexity between the three groups of participants lead to the conclusion that cortical information processing is changed in ASD and ADHD adults, therefore their level of cortical activation may be insufficient to meet the peculiar cognitive demand of Aristotle's reasoning.
Significance
The present paper suggest that MSE, is a powerful and efficient nonlinear measure in detecting neural dysfunctions in adults with ASD and ADHD characteristics, when they are called on to perform in a very demanding as well as innovative set of cognitive tasks, that can be considered as a new diagnostic ‘benchmark’ in helping detecting more effectively such type of disorders. A linear measure alone, as the typical PSD, is not capable in making such a distinction. The work contributes in shedding light on the neural mechanisms of syllogism/reasoning of Aristotelian type, as well as toward understanding how humans reason logically and why ‘pathological’ subjects deviate from the norms of formal logic.}
}
@article{ROBERTS201648,
title = {Mathematical and computational models of the retina in health, development and disease},
journal = {Progress in Retinal and Eye Research},
volume = {53},
pages = {48-69},
year = {2016},
issn = {1350-9462},
doi = {https://doi.org/10.1016/j.preteyeres.2016.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S1350946216300106},
author = {Paul A. Roberts and Eamonn A. Gaffney and Philip J. Luthert and Alexander J.E. Foss and Helen M. Byrne},
keywords = {Oxygen, Neuroglobin, Photoreceptors, Angiogenesis, Retinitis pigmentosa, Choroidal neovascularisation},
abstract = {The retina confers upon us the gift of vision, enabling us to perceive the world in a manner unparalleled by any other tissue. Experimental and clinical studies have provided great insight into the physiology and biochemistry of the retina; however, there are questions which cannot be answered using these methods alone. Mathematical and computational techniques can provide complementary insight into this inherently complex and nonlinear system. They allow us to characterise and predict the behaviour of the retina, as well as to test hypotheses which are experimentally intractable. In this review, we survey some of the key theoretical models of the retina in the healthy, developmental and diseased states. The main insights derived from each of these modelling studies are highlighted, as are model predictions which have yet to be tested, and data which need to be gathered to inform future modelling work. Possible directions for future research are also discussed. Whilst the present modelling studies have achieved great success in unravelling the workings of the retina, they have yet to achieve their full potential. For this to happen, greater involvement with the modelling community is required, and stronger collaborations forged between experimentalists, clinicians and theoreticians. It is hoped that, in addition to bringing the fruits of current modelling studies to the attention of the ophthalmological community, this review will encourage many such future collaborations.}
}
@article{YAMAGUCHI2023427,
title = {Equitable STEM+CS learning experiences for girls of color: nurturing an independent learning approach via a learning ecosystem},
journal = {Journal for Multicultural Education},
volume = {17},
number = {4},
pages = {427-442},
year = {2023},
issn = {2053-535X},
doi = {https://doi.org/10.1108/JME-01-2023-0004},
url = {https://www.sciencedirect.com/science/article/pii/S2053535X23000368},
author = {Ryoko Yamaguchi and Veronica {Hankerson Madrigal} and Cyntrica N. Eaton and Jamika D. Burge},
keywords = {Black girls, Computer science, Computational thinking, Dependent learning, Equity, Independent learning, Learning behaviors, Learning ecosystem, Middle school girls, STEM, STEM+CS},
abstract = {Purpose
There is a critical need to understand how to attract Black girls and other girls of color to the science, technology, engineering, math, and computer science (STEM+CS) field. This study aims to look at the design and implementation of a CS learning ecosystem that supports girls of color in acquiring critical CS skills starting in middle school.
Design/methodology/approach
This mixed-method case study included 53 girls, between the ages of 11 and 13, in four US middle schools. Study methods included the analysis of a pre-program student survey, longitudinal interviews and focus groups, weekly observations and computing artifacts.
Findings
Program participants were interested in CS, were confident in their ability to learn CS, had prior coding and CS experience and had parents and teachers who encouraged them to learn CS. But some students showed dependent learning behaviors while engaging in CS activities. These included relying on instructors and being reticent to make mistakes–behaviors that limit learning. The CS learning ecosystem supported students as they shifted from applying dependent learning approaches to applying independent learning approaches. Instructors sustained a growth mindset and supported productive struggle as students learned CS skills.
Originality/value
A CS learning system supported equitable learning experiences and helped students develop independent learning behaviors that led to deeper engagement in CS.}
}
@article{MINGERS201767,
title = {Back to the future: A critique of Demetis and Lee's “Crafting theory to satisfy the requirements of systems science”},
journal = {Information and Organization},
volume = {27},
number = {1},
pages = {67-71},
year = {2017},
issn = {1471-7727},
doi = {https://doi.org/10.1016/j.infoandorg.2017.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S1471772717300118},
author = {John Mingers},
abstract = {Demetis and Lee's paper outlines criteria for constructing theory in accordance with systems science. This is a laudable aim but in this comment I suggest that their view of systems thinking is both narrow and somewhat dated. Demetis and Lee equate systems science with only one aspect of it – General Systems Thinking (GST) – and they discuss in detail only one theorist – Niklas Luhmann. I draw attention to a range of other systems approaches including system dynamics, soft systems methodology, complexity theory, critical systems thinking, critical realism and multimethodology. I conclude with tentative guidelines of my own.}
}
@article{SCHULTEMECKLENBECK2013242,
title = {A lack of appetite for information and computation. Simple heuristics in food choice},
journal = {Appetite},
volume = {71},
pages = {242-251},
year = {2013},
issn = {0195-6663},
doi = {https://doi.org/10.1016/j.appet.2013.08.008},
url = {https://www.sciencedirect.com/science/article/pii/S0195666313003668},
author = {Michael Schulte-Mecklenbeck and Matthias Sohn and Emanuel {de Bellis} and Nathalie Martin and Ralph Hertwig},
keywords = {Food choice, Heuristics, Process tracing, Rational choice, MouselabWeb},
abstract = {The predominant, but largely untested, assumption in research on food choice is that people obey the classic commandments of rational behavior: they carefully look up every piece of relevant information, weight each piece according to subjective importance, and then combine them into a judgment or choice. In real world situations, however, the available time, motivation, and computational resources may simply not suffice to keep these commandments. Indeed, there is a large body of research suggesting that human choice is often better accommodated by heuristics—simple rules that enable decision making on the basis of a few, but important, pieces of information. We investigated the prevalence of such heuristics in a computerized experiment that engaged participants in a series of choices between two lunch dishes. Employing MouselabWeb, a process-tracing technique, we found that simple heuristics described an overwhelmingly large proportion of choices, whereas strategies traditionally deemed rational were barely apparent in our data. Replicating previous findings, we also observed that visual stimulus segments received a much larger proportion of attention than any nutritional values did. Our results suggest that, consistent with human behavior in other domains, people make their food choices on the basis of simple and informationally frugal heuristics.}
}
@article{RICHARDSON2022100935,
title = {Extending the two-component model of delusion to substance use disorder etiology and recovery},
journal = {New Ideas in Psychology},
volume = {66},
pages = {100935},
year = {2022},
issn = {0732-118X},
doi = {https://doi.org/10.1016/j.newideapsych.2022.100935},
url = {https://www.sciencedirect.com/science/article/pii/S0732118X22000058},
author = {George B. Richardson and Nathan McGee},
keywords = {Brain disease model of addiction, Two-component model of delusion, Bayes Theorem, Belief, Substance use disorder},
abstract = {The brain disease model (BMDA) and psychosocial models of addiction attend to phenomena at different levels of biological organization, and evidence suggests neither is sufficient to explain substance use disorder (SUD). Here, we extend a Bayesian model of the emergence and persistence of delusions to SUD etiology and recovery, building upon efforts to link lower-level impacts of psychoactive compounds to higher-level phenomena such as attitudes, beliefs, and self-control. According to the resulting two-component model of SUD, psychoactive substances interact with genetic and environmental factors to produce delusions about the biological importance of substance use and its contexts by perturbating basic human affective systems. These delusions are most often revised or rejected based on individuals’ existing belief systems. But in some individuals, factors explaining the persistence of an array of delusions (e.g., lower levels of executive functioning) prevent the evaluation and revision system from rejecting or revising beliefs that attribute high salience to substance-related stimuli. This theory provides novel hypotheses regarding the potential roles of factors such as dichotomous thinking, positive illusions and self-deception, and denial or lack of awareness in SUD etiology and recovery. Furthermore, it provides an account of SUD that may result in less stigma than the BDMA.}
}
@article{MOORE202542,
title = {Considerations for using participatory systems modeling as a tool for implementation mapping in chronic disease prevention},
journal = {Annals of Epidemiology},
volume = {101},
pages = {42-51},
year = {2025},
issn = {1047-2797},
doi = {https://doi.org/10.1016/j.annepidem.2024.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S1047279724002758},
author = {Travis R. Moore and Erin Hennessy and Yuilyn Chang Chusan and Laura Ellen Ashcraft and Christina D. Economos},
keywords = {Implementation science, Implementation mapping, Community-engaged research, Systems thinking, Epidemiology, Systems science},
abstract = {Effective chronic disease prevention requires a systems approach to the design, implementation, and refinement of interventions that account for the complexity and interdependence of factors influencing health outcomes. This paper proposes the Participatory Implementation Systems Mapping (PISM) process, which combines participatory systems modeling with implementation strategy development to enhance intervention design and implementation planning. PISM leverages the collaborative efforts of researchers and community partners to analyze complex health systems, identify key determinants, and develop tailored interventions and strategies that are both adaptive and contextually relevant. The phases of the PISM process include strategize, innovate, operationalize, and assess. We describe and demonstrate how each phase contributes to the overall goal of effective and sustainable intervention implementation. We also address the challenges of data availability, model complexity, and resource constraints. We offer solutions such as innovative data collection methods and participatory model development to enhance the robustness and applicability of systems models. Through a case study on the development of a chronic disease prevention intervention, the paper illustrates the practical application of PISM and highlights its potential to guide epidemiologists and implementation scientists in developing interventions that are responsive to the complexities of real-world health systems. The conclusion calls for further research to refine participatory systems modeling techniques, overcome existing challenges in data availability, and expand the use of PISM in diverse public health contexts.}
}
@article{ASCENZI2020115295,
title = {Theoretical mathematics, polarized light microscopy and computational models in healthy and pathological bone},
journal = {Bone},
volume = {134},
pages = {115295},
year = {2020},
issn = {8756-3282},
doi = {https://doi.org/10.1016/j.bone.2020.115295},
url = {https://www.sciencedirect.com/science/article/pii/S8756328220300752},
author = {Maria-Grazia Ascenzi},
keywords = {Biomechanics, Lamella, Low-trauma fracture, Mathematics, Osteon, Pathology},
abstract = {The needs of everyday life, such as counting and measuring, are roots of theoretical mathematics. I believe these roots are why mathematical ideas ground research so amazingly well within many scientific fields. Initially trained as a theoretical mathematician and having collaborated with non-mathematicians in the field of bone research, I address the advantages and challenges of collaborations across fields of research among investigators trained in different disciplines. I report on the mathematical ideas that have guided my research on the mechanics of bone tissue. I explain how the mathematical ideas of local vs. global properties influence my research. Polarized light microscopy (PLM) is a tool that I use consistently, in association with other microscopy techniques, to investigate bone in its healthy state and in the presence of bone disease, in humans and in animal models. I review the results that I and investigators around the world have obtained with PLM. Applied to thin bone sections, PLM yields extinct (black) and bright (white) signals that are interpreted in terms of the orientation of collagen type I, by means of other microscopy techniques. Collagen type I is an elementary component of bone tissue. Its orientation is important for the mechanical function of bone. Images obtained by PLM at a specific bone site yield big data sets regarding collagen orientation. Multiple data sets in respect of multiple sites are often needed for research because the bone tissue differs by location in response to the distinct forces acting on it. Mathematics, defined by philosophers as the theory of patterns, offers the backdrop for pattern identification in the big data sets regarding collagen orientation. I also discuss the computational aspect of the research, pursuant to which the patterns identified are incorporated in simulations of mechanical behaviors of bone. These mathematical ideas serve to understand the role of collagen orientation in bone fracture risk.}
}
@article{MARCIALROMERO2008171,
title = {Sequential Real Number Computation and Recursive Relations},
journal = {Electronic Notes in Theoretical Computer Science},
volume = {202},
pages = {171-189},
year = {2008},
note = {Proceedings of the Fourth International Conference on Computability and Complexity in Analysis (CCA 2007)},
issn = {1571-0661},
doi = {https://doi.org/10.1016/j.entcs.2008.03.014},
url = {https://www.sciencedirect.com/science/article/pii/S1571066108001199},
author = {J. Raymundo Marcial-Romero and M. Andrew Moshier},
keywords = {exact real-number computation, sequential computation, recursive relations, semantics, non-determinism, PCF},
abstract = {In the first author's thesis [Marcial-Romero, J. R., “Semantics of a sequential language for exact real-number computation”, PhD thesis at the University of Birmingham, 2004)], a sequential language, LRT, for real number computation is investigated. The thesis includes a proof that all polynomials are programmable, but that work comes short of giving a complete characterization of the expressive power of the language even for first-order functions. The technical problem is that LRT is non-deterministic. So a natural characterization of its expressive power should be in terms of relations rather than functions. In [Brattka, V., Recursive characterization of computable real-valued functions and relations, Theoretical Computer Science 162 (1) (1996) 45–77], Brattka investigates a formalization of recursive relations in the style of Kleene's recursive functions on the natural numbers. This paper establishes the expressive power of LRTp, a variant of LRT, in terms of Brattka's recursive relations. Because Brattka already did the work of establishing the precise connection between his recursive relations and Type 2 Theory of Effectivity, we thus obtain a complete characterization of first-order definability in LRTp.}
}
@article{FLOWERS2025119061,
title = {Context matters: Modeling thermochronologic data in geologic frameworks using the Great Unconformity as a case study},
journal = {Earth and Planetary Science Letters},
volume = {650},
pages = {119061},
year = {2025},
issn = {0012-821X},
doi = {https://doi.org/10.1016/j.epsl.2024.119061},
url = {https://www.sciencedirect.com/science/article/pii/S0012821X2400493X},
author = {R.M. Flowers and B.A. Peak},
keywords = {Geologic context approach, (U-Th)/He, Thermal history, Great Unconformity, Pikes Peak, Tava},
abstract = {The critical importance of sample context and geologic information for interpreting geochronologic data has long been fundamental to the Earth sciences. However, the lack of quantitative uncertainties associated with contextual, observational information means that much geologic data cannot be statistically treated in computational models using the same approaches as quantitative datasets. This challenge is showcased by the current debate over whether and how geologic data should be used when modeling thermochronologic results, which has important implications for deriving time-temperature (tT) paths from which burial and exhumation histories are interpreted. Holistically leveraging observational data to test hypotheses and determine the set of geologically reasonable thermal histories that can explain thermochronologic results has a longstanding history, but some recent studies have criticized this approach as one that arbitrarily limits tT solutions. Here, a geologic context approach to thermal history modeling, in which observational and thermochronologic datasets are combined to design geologically valid models and reach the most geologically likely interpretation, is illustrated using an example of constraining Great Unconformity exhumation in Colorado where this modeling philosophy has been questioned. Although the quality of geologic data and their applicability to modeled samples can vary and be debated, this does not mean that all geologic data are inherently unreliable and therefore discardable. Exploring models with varying or minimal constraints can be useful to test different hypotheses and determine the resolving power of the data, but using an endmember context-blind approach to interpret thermochronologic results can produce outcomes that violate fundamental aspects of the geology. The strategy outlined here is not the only valid approach to modeling thermochronologic data, but if the purpose of the modeling is to derive meaningful interpretations about sample tT paths in order to better illuminate the geologic history, then critical thinking about the sample context, first order geologic observations, and primary relationships should be integral components of the modeling process.}
}
@article{JONES2015e38,
title = {Complexity and forensic pathology},
journal = {Forensic Science International},
volume = {257},
pages = {e38-e43},
year = {2015},
issn = {0379-0738},
doi = {https://doi.org/10.1016/j.forsciint.2015.08.026},
url = {https://www.sciencedirect.com/science/article/pii/S0379073815003709},
author = {Richard Martin Jones},
keywords = {Complexity, Chaos, Nonlinear, Pathophysiology, Forensic pathology, Forensic medicine},
abstract = {It has become increasingly apparent that nonlinearity and complexity are the norm in human physiological systems, the relevance of which is informing an enhanced understanding of basic pathological processes such as inflammation, the host response to severe trauma, and critical illness. This article will explore how an understanding of nonlinear systems and complexity might inform the study of the pathophysiology of deaths of medicolegal interest, and how ‘complexity thinking’ might usefully be incorporated into modern forensic medicine and forensic pathology research, education and practice.}
}
@article{DALLAGO2016150,
title = {Computation by interaction for space-bounded functional programming},
journal = {Information and Computation},
volume = {248},
pages = {150-194},
year = {2016},
note = {Development on Implicit Computational Complexity (DICE 2013)},
issn = {0890-5401},
doi = {https://doi.org/10.1016/j.ic.2015.04.006},
url = {https://www.sciencedirect.com/science/article/pii/S089054011500142X},
author = {Ugo {Dal Lago} and Ulrich Schöpp},
keywords = {Implicit computational complexity, Logarithmic space, Type system, Geometry of interaction, Functional programming},
abstract = {When programming with sublinear space constraints one often needs to use special implementation techniques even for simple tasks, such as function composition. In this paper, we study how such implementation techniques can be supported in a functional programming language. Our approach is based on modelling computation by interaction using the Int construction of Joyal, Street & Verity. We apply this construction to a term model of a first-order programming language and use the resulting structure to derive the functional programming language intml. Intml can be understood as a programming language simplification of Stratified Bounded Affine Logic. We formulate intml by means of a type system inspired by Baillot & Terui's Dual Light Affine Logic. We show that it captures the complexity classes flogspace and nflogspace. We illustrate its expressiveness by showing how typical graph algorithms, such a test for acyclicity in undirected graphs, can be represented.}
}
@article{MANALU2023641,
title = {Developing Nusantara Mobile Application to Support Local Tourism in Indonesia},
journal = {Procedia Computer Science},
volume = {227},
pages = {641-650},
year = {2023},
note = {8th International Conference on Computer Science and Computational Intelligence (ICCSCI 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.10.568},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923017350},
author = {Daniella Oktalina Manalu and Yudhistya Ayu Kusumawati and Cuk Tho},
keywords = {mobile application, tourism, local tourism},
abstract = {Tourism is a very important sector and has a major influence on development and national income. Moreover, Indonesia has thousands of tourist destinations that are very beautiful and interesting to visit, both for Indonesians and foreigners. It's just that, there are still many local tours, such as tourist villages, which are still not well known by most people. In fact, there are many cultures, customs, places of recreation, or characteristics of an area that need to be seen and introduced to outsiders, even to Indonesians themselves. Therefore, this study aims to explain the problems that occur in the field of tourism, as well as provide solutions in the form of tourism applications that aim to help promote local Indonesian tourism, as well as make it easy for travel enthusiasts to organize their travel plans. The process of making this travel application is also carried out through various research and interviews with potential users and IT people in order to produce an attractive and effective application. This study uses the design thinking method. Researchers collected data sources from literature studies and surveys through questionnaires, where the results of the data obtained from the questionnaires were numerical or quantitative. The aim is to determine the level of public interest in tourism, as well as determine the level of potential users of this tourism application. That way, the goals of this application will be achieved and effective in helping solve tourism problems.}
}
@article{MENGOV20061636,
title = {Fast computation of a gated dipole field},
journal = {Neural Networks},
volume = {19},
number = {10},
pages = {1636-1647},
year = {2006},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2006.05.031},
url = {https://www.sciencedirect.com/science/article/pii/S0893608006001316},
author = {George Mengov and Kalin Georgiev and Stefan Pulov and Trifon Trifonov and Krassimir Atanassov},
keywords = {Gated dipole field, Adaptive resonance theory, Generalized net},
abstract = {We address the need to develop efficient algorithms for numerical simulation of models, based in part or entirely on adaptive resonance theory. We introduce modifications that speed up the computation of the gated dipole field (GDF) in the Exact ART neural network. The speed increase of our solution amounts to at least an order of magnitude for fields with more than 100 gated dipoles. We adopt a ‘divide and rule’ approach towards the original GDF differential equations by grouping them into three categories, and modify each category in a separate way. We decouple the slow-dynamics part — the neurotransmitters from the rest of system, solve their equations analytically, and adapt the solution to the remaining fast-dynamics processes. Part of the node activations are integrated by an unsophisticated numerical procedure switched on and off according to rules. The remaining activations are calculated at equilibrium. We implement this logic in a Generalized Net (GN) — a tool for parallel processes simulation which enables a fresh look at developing efficient models. Our software implementation of generalized nets appears to add little computational overhead.}
}
@article{BARA2001839,
title = {Model theory of deduction: a unified computational approach},
journal = {Cognitive Science},
volume = {25},
number = {6},
pages = {839-901},
year = {2001},
issn = {0364-0213},
url = {https://www.sciencedirect.com/science/article/pii/S0364021301000568},
author = {Bruno G. Bara and Monica Bucciarelli and Vincenzo Lombardo},
keywords = {Mental models, Deduction, Computational model, Development},
abstract = {One of the most debated questions in psychology and cognitive science is the nature and the functioning of the mental processes involved in deductive reasoning. However, all existing theories refer to a specific deductive domain, like syllogistic, propositional or relational reasoning. Our goal is to unify the main types of deductive reasoning into a single set of basic procedures. In particular, we bring together the microtheories developed from a mental models perspective in a single theory, for which we provide a formal foundation. We validate the theory through a computational model (UNICORE) which allows fine-grained predictions of subjects’ performance in different reasoning domains. The performance of the model is tested against the performance of experimental subjects—as reported in the relevant literature—in the three areas of syllogistic, relational and propositional reasoning. The computational model proves to be a satisfactory artificial subject, reproducing both correct and erroneous performance of the human subjects. Moreover, we introduce a developmental trend in the program, in order to simulate the performance of subjects of different ages, ranging from children (3–6) to adolescents (8–12) to adults (>21). The simulation model performs similarly to the subjects of different ages. Our conclusion is that the validity of the mental model approach is confirmed for the deductive reasoning domain, and that it is possible to devise a unique mechanism able to deal with the specific subareas. The proposed computational model (UNICORE) represents such a unifying structure.}
}
@article{MARINHO2021e06079,
title = {Quantum computational investigations and molecular docking studies on amentoflavone},
journal = {Heliyon},
volume = {7},
number = {1},
pages = {e06079},
year = {2021},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2021.e06079},
url = {https://www.sciencedirect.com/science/article/pii/S2405844021001845},
author = {Márcia M. Marinho and Francisco Wagner Q. Almeida-Neto and Emanuelle M. Marinho and Leonardo P. {da Silva} and Ramon R.P.P.B. Menezes and Ricardo P. {dos Santos} and Emmanuel S. Marinho and Pedro {de Lima-Neto} and Alice M.C. Martins},
keywords = {Antichagasic agent, Biflavonoid, DFT, Fukui analysis, NLO},
abstract = {Chagas disease is a neglected tropical disease caused by the protozoan parasite Trypanosoma cruzi, with approximately 6–7 million people infected worldwide, becoming a public health problem in tropical countries, thus generating an increasing demand for the development of more effective drugs, due to the low efficiency of the existing drugs. Aiming at the development of a new antichagasic pharmacological tool, the density functional theory was used to calculate the reactivity descriptors of amentoflavone, a biflavonoid with proven anti-trypanosomal activity in vitro, as well as to perform a study of interactions with the enzyme cruzain, an enzyme key in the evolutionary process of T-cruzi. Structural properties (in solvents with different values of dielectric constant), the infrared spectrum, the frontier orbitals, Fukui analysis, thermodynamic properties were the parameters calculated from DFT method with the monomeric structure of the apigenin used for comparison. Furthermore, molecular docking studies were performed to assess the potential use of this biflavonoid as a pharmacological antichagasic tool. The frontier orbitals (HOMO-LUMO) study to find the band gap of compound has been extended to calculate electron affinity, ionization energy, electronegativity electrophilicity index, chemical potential, global chemical hardness and global chemical softness to study the chemical behaviour of compound. The optimized structure was subjected to molecular Docking to characterize the interaction between amentoflavone and cruzain enzyme, a classic pharmacological target for substances with anti-gas activity, where significant interactions were observed with amino acid residues from each one's catalytic sites enzyme. These results suggest that amentoflavone has the potential to interfere with the enzymatic activity of cruzain, thus being an indicative of being a promising antichagasic agent.}
}
@article{MADHJA2020107068,
title = {Energy-aware tree network formation among computationally weak nodes},
journal = {Computer Networks},
volume = {168},
pages = {107068},
year = {2020},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2019.107068},
url = {https://www.sciencedirect.com/science/article/pii/S1389128619309533},
author = {Adelina Madhja and Sotiris Nikoletseas and Alexandros A. Voudouris},
keywords = {Wireless power transfer, Tree network formation, Energy balance},
abstract = {We study the fundamental problem of distributed network formation among mobile agents of limited computational power that aim to achieve energy balance by wirelessly transmitting and receiving energy in a peer-to-peer manner. Specifically, we design simple distributed protocols consisting of a small number of states and interaction rules for the formation of arbitrary and k-ary tree networks. Furthermore, we evaluate (theoretically and also using computer simulations) a plethora of energy redistribution protocols that exploit different levels of knowledge in order to achieve desired energy distributions among the agents which require that every agent has exactly or at least twice the energy of the agents of higher depth, according to the structure of the network. Our study shows that without using any knowledge about the network structure, such energy distributions cannot be achieved in a timely manner, meaning that there might be high energy loss during the redistribution process. On the other hand, only a few extra bits of information seem to be enough to guarantee quick convergence to energy distributions that satisfy particular properties, yielding low energy loss.}
}
@article{SHIVHARE2016243,
title = {On the Cognitive Process of Abstraction},
journal = {Procedia Computer Science},
volume = {89},
pages = {243-252},
year = {2016},
note = {Twelfth International Conference on Communication Networks, ICCN 2016, August 19– 21, 2016, Bangalore, India Twelfth International Conference on Data Mining and Warehousing, ICDMW 2016, August 19-21, 2016, Bangalore, India Twelfth International Conference on Image and Signal Processing, ICISP 2016, August 19-21, 2016, Bangalore, India},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2016.06.051},
url = {https://www.sciencedirect.com/science/article/pii/S1877050916311164},
author = {Radhika Shivhare and Ch. Aswani Kumar},
keywords = {Abstraction, Cognitive Informatics, Concept Algebra, Formal Concept Analysis.},
abstract = {Concepts are the basic elements of propositions. Concepts can be best understood as constituted by its subset of objects (Extent) and subset of attributes (Intent). Psychological capacities of human mind for example, learning, thinking, memorizing can be performed by concepts and their association. In this paper, we will explain how human will be able to generalize concrete concepts of Formal Concept Analysis into abstract concepts. In particular, we model the functionalities of concept algebra by making use of Formal Concept Analysis; we illustrate the proposed model with experiments on sample context. This model simulates the thinking process of human mind.}
}
@incollection{PRIETOMARTINEZ201919,
title = {Chapter 2 - Computational Drug Design Methods—Current and Future Perspectives},
editor = {Kunal Roy},
booktitle = {In Silico Drug Design},
publisher = {Academic Press},
pages = {19-44},
year = {2019},
isbn = {978-0-12-816125-8},
doi = {https://doi.org/10.1016/B978-0-12-816125-8.00002-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012816125800002X},
author = {Fernando D. Prieto-Martínez and Edgar López-López and K. {Eurídice Juárez-Mercado} and José L. Medina-Franco},
keywords = {Artificial intelligence, Big data, Chemical space, Chemoinformatics, Deep learning, Molecular modeling, Polypharmacology, SmART, Target fishing, Virtual screening},
abstract = {Computer-aided drug design (CADD) comprises a broad range of theoretical and computational approaches that are part of modern drug discovery. CADD methods have made key contributions to the development of drugs that are in clinical use or in clinical trials. Such methods have emerged and evolved along with experimental approaches used in drug design. In this chapter we discuss the major CADD methods and examples of recent applications to drugs that have advanced in clinical trials or that have been approved for clinical use. We also comment on representative trends in current drug discovery that are shaping the development of novel methods, such as computer-aided drug repurposing. Similarly we present emerging concepts and technologies in molecular modeling and chemoinformatics. Furthermore, this chapter discusses the authors’ point of view of the challenges of traditional and novel CADD methods to increase their positive impact in drug discovery.}
}
@incollection{MEDINAFRANCO2015455,
title = {Chapter 21 - Discovery and Development of Lead Compounds from Natural Sources Using Computational Approaches},
editor = {Pulok K. Mukherjee},
booktitle = {Evidence-Based Validation of Herbal Medicine},
publisher = {Elsevier},
address = {Boston},
pages = {455-475},
year = {2015},
isbn = {978-0-12-800874-4},
doi = {https://doi.org/10.1016/B978-0-12-800874-4.00021-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128008744000210},
author = {José L. Medina-Franco},
keywords = {Chemical space, Chemoinformatics, Computer-aided drug design, Dietary components, Drug discovery, Molecular diversity, Pharmacological profiling, Structure–activity relationships, Target fishing, Virtual screening},
abstract = {This chapter discusses the synergy between natural product-based drug discovery and methods used in computer-aided drug design. For centuries, Nature has been the source of compounds that are currently in the clinic or that have been used as molecular probes to identify therapeutic targets. In addition, Nature has inspired the development of a significant number of pharmaceutical agents. In contrast, computational approaches applied to drug discovery date back to only a few decades. Nonetheless, computational methods are evolving at an impressive speed and are making significant contributions to identifying and developing bioactive compounds of therapeutic relevance. Computational methods have a broad range of applications in natural product research including the organization and comprehensive analysis of molecular databases, systematic screening of natural products libraries, computer-aided optimization of lead compounds, and identification of biological activities for natural products of dietary origin.}
}
@article{LIEFGREEN2020101332,
title = {Strategies for selecting and evaluating information},
journal = {Cognitive Psychology},
volume = {123},
pages = {101332},
year = {2020},
issn = {0010-0285},
doi = {https://doi.org/10.1016/j.cogpsych.2020.101332},
url = {https://www.sciencedirect.com/science/article/pii/S001002852030061X},
author = {Alice Liefgreen and Toby Pilditch and David Lagnado},
keywords = {Information search, OED framework, Utility functions, Inquiry, Question asking, Strategies, Probabilistic reasoning, Bayesian Networks},
abstract = {Within the domain of psychology, Optimal Experimental Design (OED) principles have been used to model how people seek and evaluate information. Despite proving valuable as computational-level methods to account for people’s behaviour, their descriptive and explanatory powers remain largely unexplored. In a series of experiments, we used a naturalistic crime investigation scenario to examine how people evaluate queries, as well as outcomes, in probabilistic contexts. We aimed to uncover the psychological strategies that people use, not just to assess whether they deviated from OED principles. In addition, we explored the adaptiveness of the identified strategies across both one-shot and stepwise information search tasks. We found that people do not always evaluate queries strictly in OED terms and use distinct strategies, such as by identifying a leading contender at the outset. Moreover, we identified aspects of zero-sum thinking and risk aversion that interact with people’s information search strategies. Our findings have implications for building a descriptive account of information seeking and evaluation, accounting for factors that currently lie outside the realm of information-theoretic OED measures, such as context and the learner’s own preferences.}
}
@article{PEREZ2008755,
title = {A computational evaluation of the effect of intramedullary nail material properties on the stabilization of simulated femoral shaft fractures},
journal = {Medical Engineering & Physics},
volume = {30},
number = {6},
pages = {755-760},
year = {2008},
issn = {1350-4533},
doi = {https://doi.org/10.1016/j.medengphy.2007.08.004},
url = {https://www.sciencedirect.com/science/article/pii/S1350453307001567},
author = {Angel Perez and Andrew Mahar and Charles Negus and Peter Newton and Tom Impelluso},
keywords = {Finite element method, Simulated pediatric femur fractures, Intramedullary nails, Biomechanical stability},
abstract = {Titanium flexible intramedullary nails have become far more prevalent for stabilization of pediatric femur fractures in recent years. While steel may be expected to have superior fracture stability due to its higher elastic modulus; titanium alloy has experimentally demonstrated improved biomechanical stability, as measured by gap closure and nail slippage. The purpose of this study was to verify these observations computationally, and thus, explain why titanium alloy may be better suited for surgical fixation of fractured femurs. A finite element model of a femur with complete mid-diaphyseal fracture and having two 3.5mm nails in a retrograde “C” pattern was created. Static analyses were run in which the nail material properties were titanium alloy or stainless steel, respectively. Gap closure for the stainless steel nails was 1.03mm; while the titanium alloy nails had 0.69mm of closure. Titanium alloy nails slipped slightly less at each loading increment than stainless steel nails. The titanium alloy nails distributed stress more evenly along the nail axis, resulting in lower peak magnitudes. These results agree with previously published clinical and biomechanical studies that reported increased gap closure and nail slippage with stainless steel nails. The increased deformation of the titanium alloy nail likely increases the contact area with the intramedullary canal wall, thus, increasing stability. Additionally, stainless steel nails had higher curve apex von Mises stresses, potentially inducing a stress-shielding effect which could hamper remodeling and consequently increase risk of re-fracture.}
}
@article{PAUSELLI201874,
title = {Computational linguistic analysis applied to a semantic fluency task to measure derailment and tangentiality in schizophrenia},
journal = {Psychiatry Research},
volume = {263},
pages = {74-79},
year = {2018},
issn = {0165-1781},
doi = {https://doi.org/10.1016/j.psychres.2018.02.037},
url = {https://www.sciencedirect.com/science/article/pii/S0165178117309824},
author = {Luca Pauselli and Brooke Halpern and Sean D. Cleary and Benson S. Ku and Michael A. Covington and Michael T. Compton},
keywords = {Automatic Data Processing, Formal Thought Disorder, Psychosis, Schizophrenia, Semantics, Semantic Fluency Tasks},
abstract = {Although rating scales to assess formal thought disorder exist, there are no objective, high-reliability instruments that can quantify and track it. This proof-of-concept study shows that CoVec, a new automated tool, is able to differentiate between controls and patients with schizophrenia with derailment and tangentiality. According to ratings from the derailment and tangentiality items of the Scale for the Assessment of Positive Symptoms, we divided the sample into three groups: controls, patients without formal thought disorder, and patients with derailment/tangentiality. Their lists of animals produced during a one-minute semantic fluency task were processed using CoVec, a newly developed software that measures the semantic similarity of words based on vector semantic analysis. CoVec outputs were Mean Similarity, Coherence, Coherence-5, and Coherence-10. Patients with schizophrenia produced fewer words than controls. Patients with derailment had a significantly lower mean number of words and lower Coherence-5 than controls and patients without derailment. Patients with tangentiality had significantly lower Coherence-5 and Coherence-10 than controls and patients without tangentiality. Despite the small samples of patients with clinically apparent thought disorder, CoVec was able to detect subtle differences between controls and patients with either or both of the two forms of disorganization.}
}
@article{HAN2020106264,
title = {A new computational model based on Archimedean copula for probabilistic unbalanced linguistic term set and its application to multiple attribute group decision making},
journal = {Computers & Industrial Engineering},
volume = {140},
pages = {106264},
year = {2020},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2019.106264},
url = {https://www.sciencedirect.com/science/article/pii/S0360835219307338},
author = {Bing Han and Zhifu Tao and Huayou Chen and Ligang Zhou and Jinpei Liu},
keywords = {Multiple attribute group decision making, Probabilistic unbalanced linguistic term set, Archimedean copula, Weighted average aggregation operator},
abstract = {This paper proposes the concept of the probabilistic unbalanced linguistic term set which considers not only the probability of linguistic variables but also the non-uniform and non-symmetric distribution of linguistic labels. A new computational model on basis of Archimedean copula and corresponding co-copula is developed to deal with probabilistic unbalanced linguistic information. The most advantage of the model is that it can keep the closure of the operation. Some operational properties and particular cases are further investigated. We present the concepts of Archimedean copula weighted probabilistic unbalanced linguistic arithmetic average aggregation operator and Archimedean copula weighted probabilistic unbalanced linguistic geometric average aggregation operator, some properties are also discussed. Finally, the effectiveness and universality of the developed approach are illustrated by a hospital selection and comparison analysis. A sensitivity analysis is also performed to test the robustness of proposed methods.}
}
@article{CHARPENTIER20163365,
title = {Sensitivity computations in higher order continuation methods},
journal = {Applied Mathematical Modelling},
volume = {40},
number = {4},
pages = {3365-3380},
year = {2016},
issn = {0307-904X},
doi = {https://doi.org/10.1016/j.apm.2015.10.033},
url = {https://www.sciencedirect.com/science/article/pii/S0307904X15006952},
author = {Isabelle Charpentier and Komlanvi Lampoh},
keywords = {Continuation, Homotopy, Sensitivity, Automatic differentiation, Diamant, Complex nonlinear eigenvalue problem,},
abstract = {Sensitivity analysis is a key tool in the study of the relationships between the input parameters of a model and the output solution. Although sensitivity analysis is extensively addressed in the literature, little attention has been brought to the methodological aspects of the sensitivity of nonlinear parametric solutions computed through a continuation technique. This paper proposes four combinations of sensitivity analysis with continuation and homotopy methods, including sensitivity analysis along solution branches or at a particular point. Theoretical aspects are discussed in the higher order continuation framework Diamant. The sensitivity methods are applied to a thermal ignition problem and some free vibration problems. Remarkable eigenvalue maps are produced for the complex nonlinear eigenvalue problems.}
}
@article{QUAN20196515,
title = {Smart Design for Sustainable Neighborhood Development},
journal = {Energy Procedia},
volume = {158},
pages = {6515-6520},
year = {2019},
note = {Innovative Solutions for Energy Transitions},
issn = {1876-6102},
doi = {https://doi.org/10.1016/j.egypro.2019.01.108},
url = {https://www.sciencedirect.com/science/article/pii/S1876610219301183},
author = {Steven Jige Quan},
keywords = {Smart Design, Sustainable Neighborhood Development, Design Decision Making, Multi-objective Optimization, Genetic Algorithms, Pareto optimal},
abstract = {This study proposes the Smart Design method to support the design decision making in the sustainable neighborhood development with multiple objectives. Instead of the “creative design” approach in the scenario making in traditional PSS and recent Geodesign frameworks, the Smart Design method applies the optimization algorithms to search for optimal design solutions in the design space. It integrates the design thinking, computational performance modeling and optimization techniques to efficiently and effectively approximate optimal designs. This method is applied to a hypothetical residential neighborhood design case study with three sustainability objectives: to maximize FAR, to minimize building energy use, and to minimize outdoor human discomfort. Based on the form parameterization, the Nondominated Sorting Genetic Algorithm II (NSGA-II) algorithm is utilized to guide the evolution of the neighborhood design throughout 80 generations, with neighborhood performance modeling tools. The Smart Design method is able to identify 38 representative design solutions as Pareto optimal which are equally optimal. Those solutions set a basis for discussions and negotiations among stake holders to make design decisions with the three objectives. Further research will be focused on addressing the challenges such as recursive objective definitions, parametrization of complex forms, quantification of performances and optimization uncertainties, from simple cases to more realistic and complex designs for sustainable neighborhood development.}
}
@article{XIA20025,
title = {Applications of computational fluid dynamics (cfd) in the food industry: a review},
journal = {Computers and Electronics in Agriculture},
volume = {34},
number = {1},
pages = {5-24},
year = {2002},
issn = {0168-1699},
doi = {https://doi.org/10.1016/S0168-1699(01)00177-6},
url = {https://www.sciencedirect.com/science/article/pii/S0168169901001776},
author = {Bin Xia and Da-Wen Sun},
keywords = {Computational fluid dynamics, , Food, Refrigeration, Cooling, Drying, Sterilisation, Mixing, Chilling, Modelling, Simulation},
abstract = {Computational fluid dynamics (cfd) is a simulation tool, which uses powerful computer and applied mathematics to model fluid flow situations for the prediction of heat, mass and momentum transfer and optimal design in industrial processes. It is only in recent years that cfd has been applied in the food processing industry. This paper reviews the application of cfd in food processing industries including drying, sterilisation, refrigeration and mixing. The advantages of using cfd are discussed and the future of cfd applications is also outlined.}
}
@article{ROTH2023101278,
title = {Reset and restoration. The looming conservative turn of management theory: An extension of Foss et al.},
journal = {Scandinavian Journal of Management},
volume = {39},
number = {3},
pages = {101278},
year = {2023},
issn = {0956-5221},
doi = {https://doi.org/10.1016/j.scaman.2023.101278},
url = {https://www.sciencedirect.com/science/article/pii/S0956522123000192},
author = {Steffen Roth},
keywords = {The Great Reset, Management theory, Cronyism, Stratification, Conservatism, Restorism},
abstract = {This article is a reply to Foss et al.’s (2022) contribution to the special issue of the Scandinavian Journal of Management on The Great Reset of management and organization theory. In their article, the authors make a strong case that “reset thinking” geared towards a more “sustainable” redesign of the global economy promotes extensive state interventionism and cronyism capitalism, and therefore reject the idea of a need for “a fundamental rethink of existing management theory”. Whereas I do agree with the authors on most points, I am less convinced that “existing management theory” will suffice to address the problem of “reset thinking”. In this article, I demonstrate that the economy-bias of existing theories is a gateway for “reset thinking” geared towards an allegedly necessary re-/socialisation of management and organisation. A research agenda on cronyism must therefore be complemented by one on privilege and hierarchy not only as undesirable side-effects of cronyism, but also as desired outcomes of advocacy for specific minorities or missions. As self-identifications with group interests or calls for missions have become popular in management theory, I conclude that this new appetite for privilege might undermine not only the higher ideals of many management theorists, but also the foundations of modern society.}
}
@article{SCHMID2022,
title = {Mendelian or Multifactorial? Current Undergraduate Genetics Assessments Focus on Genes and Rarely Include the Environment},
journal = {Journal of Microbiology & Biology Education},
volume = {23},
number = {3},
year = {2022},
issn = {1935-7877},
doi = {https://doi.org/10.1128/jmbe.00093-22},
url = {https://www.sciencedirect.com/science/article/pii/S1935787722000302},
author = {Kelly M. Schmid and Dennis Lee and Monica Weindling and Awais Syed and Stephanie-Louise Yacoba Agyemang and Brian Donovan and Gregory Radick and Michelle K. Smith and L. Kate Wright},
keywords = {assessment, curriculum, environment, genes, genetics, undergraduate},
abstract = {Undergraduate genetics courses have historically focused on simple genetic models, rather than taking a more multifactorial approach where students explore how traits are influenced by a combination of genes, the environment, and gene-by-environment interactions. While a focus on simple genetic models can provide straightforward examples to promote student learning, they do not match the current scientific understanding and can result in deterministic thinking among students.
ABSTRACT
Undergraduate genetics courses have historically focused on simple genetic models, rather than taking a more multifactorial approach where students explore how traits are influenced by a combination of genes, the environment, and gene-by-environment interactions. While a focus on simple genetic models can provide straightforward examples to promote student learning, they do not match the current scientific understanding and can result in deterministic thinking among students. In addition, undergraduates are often interested in complex human traits that are influenced by the environment, and national curriculum standards include learning objectives that focus on multifactorial concepts. This research aims to discover to what extent multifactorial genetics is currently being assessed in undergraduate genetics courses. To address this, we analyzed over 1,000 assessment questions from a commonly used undergraduate genetics textbook; published concept assessments; and open-source, peer-reviewed curriculum materials. Our findings show that current genetics assessment questions overwhelmingly emphasize the impact of genes on phenotypes and that the effect of the environment is rarely addressed. These results indicate a need for the inclusion of more multifactorial genetics concepts, and we suggest ways to introduce them into undergraduate courses.}
}
@article{SWANSON201854,
title = {How failure is productive in the creative process: Refining student explanations through theory-building discussion},
journal = {Thinking Skills and Creativity},
volume = {30},
pages = {54-63},
year = {2018},
note = {The Role of Failure in Promoting Thinking Skills and Creativity},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2018.03.005},
url = {https://www.sciencedirect.com/science/article/pii/S1871187117301785},
author = {Hillary Swanson and Allan Collins},
keywords = {Knowledge in pieces, Microgenetic learning analysis, Knowledge construction, Constructivist instruction, Science learning, Creative thinking, Critical thinking, Creative problem solving},
abstract = {We argue that failure can play a productive role in students’ creative knowledge-construction process. As evidence, we present a fine-grained analysis of a whole-class theory-building discussion with 8th grade students. The goal of the discussion was to construct a theoretical account for why a glass of cold milk warmed quickly at first and then more slowly as it approached room temperature. Though they initially produced scientifically non-normative explanations, by the end of the discussion the class had refined their ideas into an explanation of difference drives rate – a relationship at the heart of Newton’s law of heating and other equilibration phenomena. The students’ flawed initial explanations were productive in the knowledge-construction process, as the raw material they ultimately refined into a more scientific explanation. We argue that the theory-building discussion supported both creative and critical thinking and that this pedagogical approach has the power, more generally, to leverage failure productively for science learning.}
}
@article{ROSEN20211,
title = {A word is worth a thousand pictures: A 20-year comparative analysis of aberrant abstraction in schizophrenia, affective psychosis, and non-psychotic depression},
journal = {Schizophrenia Research},
volume = {238},
pages = {1-9},
year = {2021},
issn = {0920-9964},
doi = {https://doi.org/10.1016/j.schres.2021.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S0920996421003674},
author = {Cherise Rosen and Martin Harrow and Liping Tong and Thomas H. Jobe and Helen Harrow},
keywords = {Abstraction, Concretism, Aberrant abstraction, Schizophrenia, Affective psychosis, Unipolar depression non-psychotic},
abstract = {Abstract thinking is a cognitive process that involves the assimilation of concepts reduced from diffuse sensory input, organized, and interpreted in a manner beyond the obvious. There are multiple facets by which abstraction is measured that include semantic, visual-spatial and social comprehension. This study examined the prevalence and course of abstract and concrete responses to semantic proverbs and aberrant abstraction (composite score of semantic, visual-spatial, and social comprehension) over 20 years in 352 participants diagnosed with schizophrenia, affective psychosis, and unipolar non-psychotic depression. We utilized linear models, two-way ANOVA and contrasts to compare groups and change over time. Linear models with Generalized Estimation Equation (GEE) to determine association. Our findings show that regardless of diagnosis, semantic proverb interpretation improves over time. Participants with schizophrenia give more concrete responses to proverbs when compared to affective psychosis and unipolar depressed without psychosis. We also show that the underlying structure of concretism encompasses increased conceptual overinclusion at index hospitalization and idiosyncratic associations at follow-up; whereas, abstract thinking overtime encompasses increased visual-spatial abstraction at index and rich associations with increased social comprehension scores at follow-up. Regardless of diagnosis, premorbid functioning, descriptive characteristics, and IQ were not associated with aberrant abstraction. Delusions are highly and positively related to aberrant abstraction scores, while hallucinations are mildly and positively related to this score. Lastly, our data point to the importance of examining the underlying interconnected structures of ‘established’ constructs vis-à-vis mixed methods to provide a description of the rich interior world that may not always map onto current quantitative measures.}
}
@article{KERBER2012239,
title = {A worst-case bound for topology computation of algebraic curves},
journal = {Journal of Symbolic Computation},
volume = {47},
number = {3},
pages = {239-258},
year = {2012},
issn = {0747-7171},
doi = {https://doi.org/10.1016/j.jsc.2011.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S0747717111001775},
author = {Michael Kerber and Michael Sagraloff},
keywords = {Topology computation, Algebraic curve, Amortized analysis, Complexity analysis},
abstract = {Computing the topology of an algebraic plane curve C means computing a combinatorial graph that is isotopic to C and thus represents its topology in R2. We prove that, for a polynomial of degree n with integer coefficients bounded by 2ρ, the topology of the induced curve can be computed with Õ(n8ρ(n+ρ)) bit operations (Õ indicates that we omit logarithmic factors). Our analysis improves the previous best known complexity bounds by a factor of n2. The improvement is based on new techniques to compute and refine isolating intervals for the real roots of polynomials, and on the consequent amortized analysis of the critical fibers of the algebraic curve.}
}
@article{CORBIN2023100645,
title = {A comparison of linguistic patterns between individuals with current major depressive disorder, past major depressive disorder, and controls in a virtual, psychiatric research interview},
journal = {Journal of Affective Disorders Reports},
volume = {14},
pages = {100645},
year = {2023},
issn = {2666-9153},
doi = {https://doi.org/10.1016/j.jadr.2023.100645},
url = {https://www.sciencedirect.com/science/article/pii/S266691532300183X},
author = {Lisette Corbin and Emily Griner and Salman Seyedi and Zifan Jiang and Kailey Roberts and Mina Boazak and Ali {Bahrami Rad} and Gari D. Clifford and Robert O. Cotes},
keywords = {Depression, LIWC, Psychiatric interview, Computational linguistics},
abstract = {Major Depressive Disorder (MDD) is a leading health burden worldwide. Previous research has demonstrated that linguistic analysis of depressed individuals’ written and oral speech has potential as a diagnostic and monitoring biomarker. We sought to determine if the semantic content of speech differs between individuals with current MDD, past MDD, and controls. We recruited 53 volunteers for a simulated telehealth psychiatric intake interview. The sample included 14 individuals with current MDD, 21 with past MDD, and 18controls, all confirmed using a semi-structured diagnostic interview. The manually-transcribed interview transcripts were analyzed utilizing the LIWC-22 dictionary and statistical tests were applied to identify differences in the linguistic patterns between each clinical categorization. When comparing depressed subjects (either current or past) versus controls, significant differences were found for emotional tone, total function words, auxiliary verbs, negative tone, negative emotion, anxiety, sadness, attention, and visual. Individuals with past MDD only differed from those with current MDD in use of analytical thinking and auxiliary verbs. These results indicate that LIWC categories could differentiate current or past depressed subjects from controls, but fewer differences emerged when comparing current and past MDD. Further prospective studies with larger sample sizes are needed to confirm these findings.}
}
@article{METHLING2022100013,
title = {Heuristics in multi-criteria decision-making: The cost of fast and frugal decisions},
journal = {EURO Journal on Decision Processes},
volume = {10},
pages = {100013},
year = {2022},
issn = {2193-9438},
doi = {https://doi.org/10.1016/j.ejdp.2022.100013},
url = {https://www.sciencedirect.com/science/article/pii/S2193943822000024},
author = {Florian Methling and Sara J.M. Abdeen and Rüdiger {von Nitzsch}},
keywords = {MCDM, Decision support, Heuristics, Utility theory, Value-focused thinking},
abstract = {There has been an ongoing debate in research regarding the use of heuristics in decision-making. Advocators have succeeded in showing that applying heuristics not only reduces effort but can even be more accurate than analytical approaches under certain conditions. Others point out the biases and cognitive distortions inherent in disregarding information. Researchers have used both simulations and experiments to study how the use of heuristics affects the decision's outcome. However, a good decision is determined by the process and not a lucky outcome. It is a conscious reflection on the decision-maker's information and preferences. Therefore, a heuristic must be assessed by its ability to match a structured decision processing all available information. Thus, the question remains: how often does the reduction of information considered in heuristic decisions lead to a different recommended alternative? We applied different heuristics to a dataset of 945 real, personal decisions. We have found that by using heuristics instead of a fully developed decision structure, in 60.34% of cases, a different alternative would have been recommended to the decision-maker leading to a mean relative utility loss for the deviating decisions of 34.58%. This shows that a continuous effort to reflect on the weighing of objectives and alternatives leads to better decisions.}
}
@article{FOLLI2022102458,
title = {Biases in belief reports},
journal = {Journal of Economic Psychology},
volume = {88},
pages = {102458},
year = {2022},
issn = {0167-4870},
doi = {https://doi.org/10.1016/j.joep.2021.102458},
url = {https://www.sciencedirect.com/science/article/pii/S016748702100088X},
author = {Dominik Folli and Irenaeus Wolff},
keywords = {Belief elicitation, Belief formation, Belief-action consistency, Framing effects, Projection, Consensus effect, Wishful thinking,  rationalization},
abstract = {Belief elicitation is important in many different fields of economic research. We show that how a researcher elicits such beliefs – in particular, whether the belief is about the participant’s opponent, an unrelated other, or the population of others – strongly affects the belief reports. We study the underlying processes and find a clear consensus effect. Yet, when matching the opponent’s action would lead to a low payoff and the researcher asks for the belief about this opponent, ex-post rationalization kicks in and beliefs are re-adjusted again. Hence, we recommend to ask about unrelated others or about the population in such cases, as ‘opponent beliefs’ are even more detached from the beliefs participants had when deciding about their actions in the corresponding game. We find no evidence of wishful thinking in any of the treatments.}
}
@article{HAMALAINEN2024100050,
title = {Generating policy alternatives for decision making: A process model, behavioural issues, and an experiment},
journal = {EURO Journal on Decision Processes},
volume = {12},
pages = {100050},
year = {2024},
issn = {2193-9438},
doi = {https://doi.org/10.1016/j.ejdp.2024.100050},
url = {https://www.sciencedirect.com/science/article/pii/S2193943824000062},
author = {Raimo P. Hämäläinen and Tuomas J. Lahtinen and Kai Virtanen},
keywords = {Policy decision, Generation of policy alternatives, Portfolio decision analysis, Path dependence, Cognitive biases and heuristics},
abstract = {The generation of alternative policies is essential in complex decision tasks with multiple interests and stakeholders. A diverse set of policies is typically desirable to cover the range of options and objectives. Decision modelling literature has often assumed that clearly defined decision alternatives are readily available. This is not a realistic assumption in practice. We present a structured process model for the generation of policy alternatives in settings that include non-quantifiable elements and where portfolio optimisation approaches are not applicable. Behavioural issues and path dependence as well as heuristics and biases which can occur during the process are discussed. The behavioural experiment compares policy alternatives obtained by using two different portfolio generation techniques. The results of the experiment demonstrate that path dependence can occur in policy generation. We report thinking patterns of subjects which relate to biases and heuristics.}
}
@article{PALMERI2004378,
title = {Computational approaches to the development of perceptual expertise},
journal = {Trends in Cognitive Sciences},
volume = {8},
number = {8},
pages = {378-386},
year = {2004},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2004.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S1364661304001603},
author = {Thomas J. Palmeri and Alan C-N. Wong and Isabel Gauthier},
abstract = {Dog experts, ornithologists, radiologists and other specialists are noted for their remarkable abilities at categorizing, identifying and recognizing objects within their domain of expertise. A complete understanding of the development of perceptual expertise requires a combination of thorough empirical research and carefully articulated computational theories that formalize specific hypotheses about the acquisition of expertise. A comprehensive computational theory of the development of perceptual expertise remains elusive, but we can look to existing computational models from the object-recognition, perceptual-categorization, automaticity and related literatures for possible starting points. Arguably, hypotheses about the development of perceptual expertise should first be explored within the context of existing computational models of visual object understanding before considering the creation of highly modularized adaptations for particular domains of perceptual expertise.}
}
@article{NUNES2020117761,
title = {Thinking the future of membranes: Perspectives for advanced and new membrane materials and manufacturing processes},
journal = {Journal of Membrane Science},
volume = {598},
pages = {117761},
year = {2020},
issn = {0376-7388},
doi = {https://doi.org/10.1016/j.memsci.2019.117761},
url = {https://www.sciencedirect.com/science/article/pii/S0376738819333113},
author = {Suzana P. Nunes and P. Zeynep Culfaz-Emecen and Guy Z. Ramon and Tymen Visser and Geert Henk Koops and Wanqin Jin and Mathias Ulbricht},
abstract = {The state-of-the-art of membrane technology is characterized by a number of mature applications such as sterile filtration, hemodialysis, water purification and gas separation, as well as many more niche applications of successful membrane-based separation and processing of fluid mixtures. The membrane industry is currently employing a portfolio of established materials, mostly standard polymers or inorganic materials (not originally developed for membranes), and easily scalable manufacturing processes such as phase inversion, interfacial polymerization and coating. Innovations in membranes and their manufacturing processes must meet the desired intrinsic properties that determine selectivity and flux, for specific applications. However, tunable and stable performance, as well as sustainability over the entire life cycle of membrane products are becoming increasingly important. Membrane manufacturers are progressively required to share the carbon footprint of their membrane modules with their customers. Environmental awareness among the world's population is a growing phenomenon and finds its reflection in product development and manufacturing processes. In membrane technology one can see initial steps in this direction with the replacement of hazardous solvents, the utilization of renewable materials for membrane production and the reuse of membrane modules. Other examples include increasing the stability of organic membrane polymers and lowering the cost of inorganic membranes. In a long-term perspective, many more developments in materials science will be required for making new, advanced membranes. These include “tools” such as self-assembly or micro- and nano-fabrication, and “building blocks”, e.g. tailored block copolymers or 1D, 2D and 3D materials. Such membranes must be fabricated in a simpler manner and be more versatile than existing ones. In this perspective paper, a vision of such LEGO®-like membranes with precisely adjustable properties will be illustrated with, where possible, examples that already demonstrate feasibility. These include the possibility to switch properties using an external stimulus, adapting a membrane's selectivity to a given separation, or providing the ability to assemble, disassemble and reassemble the membrane on a suitable support as scaffold, in situ, in place and on-demand. Overall, it is foreseen that the scope of future membrane applications will become much wider, based on improved existing membrane materials and manufacturing processes, as well as the combination of novel, tailor-made “building blocks” and “tools” for the fabrication of next-generation membranes tuned to specific applications.}
}
@article{TOUSSAINT20102,
title = {Computational geometric aspects of rhythm, melody, and voice-leading},
journal = {Computational Geometry},
volume = {43},
number = {1},
pages = {2-22},
year = {2010},
note = {Special Issue on the 14th Annual Fall Workshop},
issn = {0925-7721},
doi = {https://doi.org/10.1016/j.comgeo.2007.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S092577210900042X},
author = {Godfried Toussaint},
keywords = {Musical rhythm, Melody, Voice-leading, Evenness measures, Rhythm similarity, Sequence comparison, Necklaces, Convolution, Computational geometry, Music information retrieval, Algorithms, Computational music theory},
abstract = {Many problems concerning the theory and technology of rhythm, melody, and voice-leading are fundamentally geometric in nature. It is therefore not surprising that the field of computational geometry can contribute greatly to these problems. The interaction between computational geometry and music yields new insights into the theories of rhythm, melody, and voice-leading, as well as new problems for research in several areas, ranging from mathematics and computer science to music theory, music perception, and musicology. Recent results on the geometric and computational aspects of rhythm, melody, and voice-leading are reviewed, connections to established areas of computer science, mathematics, statistics, computational biology, and crystallography are pointed out, and new open problems are proposed.}
}
@article{BERGSTRA200855,
title = {Parallel Processes with Implicit Computational Capital},
journal = {Electronic Notes in Theoretical Computer Science},
volume = {209},
pages = {55-81},
year = {2008},
note = {Proceedings of the LIX Colloquium on Emerging Trends in Concurrency Theory (LIX 2006)},
issn = {1571-0661},
doi = {https://doi.org/10.1016/j.entcs.2008.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S1571066108002193},
author = {J.A. Bergstra and C.A. Middelburg},
keywords = {Process algebra, Implicit computational capital, Preservation of computational money},
abstract = {We propose a process algebra which is concerned with processes that have an implicit computational capital. This process algebra is intended to be helpful when designing computer-based systems of which the behaviour is related to money handling. It goes along with the development that the behaviour of computer-based systems, organizations and persons is increasingly more related to money handling.}
}
@incollection{ZHENG202411,
title = {Chapter Two - Reviewing the past enables us to learn},
editor = {Wenbo Zheng and Fei-Yue Wang},
booktitle = {Computational Knowledge Vision},
publisher = {Academic Press},
pages = {11-38},
year = {2024},
isbn = {978-0-443-21619-0},
doi = {https://doi.org/10.1016/B978-0-44-321619-0.00008-X},
url = {https://www.sciencedirect.com/science/article/pii/B978044321619000008X},
author = {Wenbo Zheng and Fei-Yue Wang},
keywords = {Computer vision, Artificial intelligence, Knowledge, Knowledge-based vision, Visual information},
abstract = {This chapter reviews the history of computer vision and artificial intelligence. Computer vision is the field of artificial intelligence that studies how computers can simulate the visual system of humans or other living things. It aims to enable computers to perceive and understand through the processing of visual information based on images or videos. From the 20th century onward, computer vision theory has been progressively developed. King-Sun Fu proposed syntactically structured representation and computation and constructed a top-down computational theory of vision. In the 1970s, David Marr then combined the knowledge of neuroscience, psychology, and other subjects of his time to systematically formulate a computational theory of vision, which made it possible to develop a more rigorous theory of the processing of visual information. Since then, computer vision has been flourishing.}
}
@incollection{VALERIO201385,
title = {Chapter 6 - Computational Translation and Integration of Test Data to Meet Risk Assessment Goals},
editor = {Bruce A. Fowler},
booktitle = {Computational Toxicology},
publisher = {Academic Press},
address = {San Diego},
pages = {85-112},
year = {2013},
isbn = {978-0-12-396461-8},
doi = {https://doi.org/10.1016/B978-0-12-396461-8.00008-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780123964618000087},
author = {Luis G. Valerio},
keywords = { toxicology,  methods, translational research, QSAR, computational toxicology, drug safety, safety assessment},
abstract = {The remarkable advances of high-performance computing to facilitate and increase efficiency in helping to resolve or support assessments on the toxic effects of chemicals on tissues and genomic material have led to development of novel in silico methods. These methods can support risk assessment via integration of study data that can be translated into meaningful predictive information. This chapter describes some methods in computational toxicology and how to integrate experimental data with computational assessments for supporting risk assessment.}
}
@article{VAMVOUDAKIS20226,
title = {Nonequilibrium dynamical games: A control systems perspective},
journal = {Annual Reviews in Control},
volume = {53},
pages = {6-18},
year = {2022},
issn = {1367-5788},
doi = {https://doi.org/10.1016/j.arcontrol.2022.03.006},
url = {https://www.sciencedirect.com/science/article/pii/S1367578822000128},
author = {Kyriakos G. Vamvoudakis and Filippos Fotiadis and Aris Kanellopoulos and Nick-Marios T. Kokolakis},
abstract = {Dynamical games model interactions between agents that take place in ever-shifting environments. Due to the increasing penetration of autonomous systems to society, understanding and predicting the outcomes of these games has become crucial. In this work, we highlight the importance of nonequilibrium solutions to dynamical games through the lens of bounded rationality. We describe the principles of level-k thinking and cognitive hierarchy – concepts developed in the field of economics – via mathematical tools and formulation of control theory. We describe the main principles of bounded rationality for nonequilibrium differential games in both nonlinear non-zero-sum and linear zero-sum settings. The importance of those approaches is highlighted in problems of pursuit evasion between Unmanned Aerial Vehicles, while the core of the bounded rationality principles that we employ are extended to discrete stochastic dynamical games. The versatility of the proposed approach is complemented by rigorous mathematical guarantees that enable predictability of the games’ outcomes.}
}
@article{IOANNIDOU2009236,
title = {AgentCubes: Incremental 3D end-user development},
journal = {Journal of Visual Languages & Computing},
volume = {20},
number = {4},
pages = {236-251},
year = {2009},
note = {Special Issue on Best Papers from VL/HCC2008},
issn = {1045-926X},
doi = {https://doi.org/10.1016/j.jvlc.2009.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S1045926X09000238},
author = {Andri Ioannidou and Alexander Repenning and David C. Webb},
keywords = {Incremental 3D, Game design, Visual programming, End-user development, IT fluency, Computational thinking},
abstract = {3D game development can be an enticing way to attract K-12 students to computer science, but designing and programming 3D games is far from trivial. Students need to achieve a certain level of 3D fluency in modeling, animation, and programming to be able to create compelling 3D content. The combination of innovative end-user development tools and standards-based curriculum that promotes IT fluency by shifting the pedagogical focus from programming to design, can address motivational aspects without sacrificing principled educational goals. The AgentCubes 3D game-authoring environment raises the ceiling of end-user development without raising the threshold. Our formal user study shows that with Incremental 3D, the gradual approach to transition from 2D to 3D authoring, middle school students can build sophisticated 3D games including 3D models, animations, and programming.}
}
@article{GARFIELD198447,
title = {Artificial intelligence: Using computers to think about thinking, Part 2: Some practical applications of Al},
journal = {Computer Compacts},
volume = {2},
number = {2},
pages = {47-53},
year = {1984},
issn = {0167-7136},
doi = {https://doi.org/10.1016/0167-7136(84)90041-6},
url = {https://www.sciencedirect.com/science/article/pii/0167713684900416},
author = {Eugene Garfield}
}
@article{KAWITI2025100213,
title = {Indigenous knowledge, architecture, and nature in the context of Oceania},
journal = {Nature-Based Solutions},
volume = {7},
pages = {100213},
year = {2025},
issn = {2772-4115},
doi = {https://doi.org/10.1016/j.nbsj.2025.100213},
url = {https://www.sciencedirect.com/science/article/pii/S2772411525000035},
author = {Derek Kawiti and Albert Refiti and Amanda Yates and Elisapeta Heta and Sibyl Bloomfield and Victoria Chanse and Maibritt Pedersen Zari},
keywords = {Pacific, Indigenous, Architecture, Ecology, Climate change adaptation, Māori, Samoan},
abstract = {This perspective article is derived from conversations between leading Indigenous academics and practitioners in the fields of architecture and urban design recorded at a keynote panel at the 2023 NUWAO International Symposium on Nature-based Urban Climate Adaptation for Wellbeing, held at Te Herenga Waka Victoria University of Wellington, Aotearoa New Zealand. The focus of the discussion was Indigenous design for adaptation to climate change in Moananui Oceania with an emphasis on relationships to nature. Given the diversity of Moananui Oceania in terms of languages, cultures, histories, and worldviews, this discussion represented a unique convergence of Indigenous leadership and thought in the field. It highlighted key themes related to Indigenous design for climate change adaptation and offered a novel, distinctive perspective aimed at advancing thinking around nature-based solutions (NbS). It is important to recognise and integrate Indigenous values and approaches to knowledge generation, particularly within academic settings. In the context of Moananui Oceania this can require adapting oral traditions and formats, such as talanoa, and hui or kōrero, into conventional Western-based research formats such as the journal article. This paper is an attempt to capture important Indigenous knowledge and discussion in a western format to enable further dissemination and sharing. This means the format and methodologies described in the paper do not align exactly with traditional scientific journal article formats, however the discussions and findings help to meet the motivation of the authors, which is to transform traditional Indigenous ways of sharing information into a perspective article format and share insights with a wider audience. This methodology aligns well with the special issue call that this paper resides in (Just, Socio-ecological Urban Transformation: Nature-based Solutions and Traditional Ecological Knowledge), underpinning the relevance and potential contribution to the field. Two key themes were explored within the context of the importance of working with nature; relationships between ecologies and tikanga (customary practices), and looking backwards to generate innovation and resilience.}
}
@article{HUANG2006567,
title = {An integrated computational intelligence approach to product concept generation and evaluation},
journal = {Mechanism and Machine Theory},
volume = {41},
number = {5},
pages = {567-583},
year = {2006},
issn = {0094-114X},
doi = {https://doi.org/10.1016/j.mechmachtheory.2005.07.006},
url = {https://www.sciencedirect.com/science/article/pii/S0094114X05001333},
author = {Hong-Zhong Huang and Ruifeng Bo and Wei Chen},
keywords = {Conceptual design, Computational intelligence, Optimal concept, Genetic algorithm, Fuzzy neural network},
abstract = {Product concept generation and evaluation are two major activities for obtaining an optimal concept in conceptual design. In this paper, an integrated computational intelligence approach is proposed for dealing with these two aspects. A group of satisfactory concepts are generated first by using genetic algorithm and incorporating the information from knowledge base. Then concept evaluation and decision making are implemented using fuzzy neural network to obtain an optimal concept. Our procedure of using computational intelligence in conceptual design is described. The key issues in implementing the proposed approach are discussed, and finally the applicability of the proposed method is illustrated with an engineering example.}
}
@article{RAHMAN201872,
title = {Hybrid bio-Inspired computational intelligence techniques for solving power system optimization problems: A comprehensive survey},
journal = {Applied Soft Computing},
volume = {69},
pages = {72-130},
year = {2018},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2018.04.051},
url = {https://www.sciencedirect.com/science/article/pii/S1568494618302424},
author = {Imran Rahman and Junita Mohamad-Saleh},
keywords = {Computational intelligence, Hybrid optimization, Optimization, Bio-inspired computation, Power system},
abstract = {Optimization problems of modern day power system are very challenging to resolve because of its design complexity, wide geographical dispersion and influence from many unpredictable factors. For that reason, it is essential to apply most effective optimization techniques by taking full benefits of simplified formulation and execution of a particular problem. This study presents a summary of significant hybrid bio-inspired computational intelligence (CI) techniques utilized for power system optimization. Authors have reviewed an extensive range of hybrid CI techniques and examined the motivations behind their improvements. Various applications of hybrid bio-inspired CI algorithms have been highlighted in this paper. In addition, few drawbacks regarding the hybrid CI algorithms are explained. Current trends in CI techniques from the past researches have also been discussed in the domain of power system optimization. Lastly, some future research directions are suggested for further advancement of hybrid techniques.}
}
@article{DIAS2007382,
title = {Philosophical grounding and computational formalization for practice based engineering knowledge},
journal = {Knowledge-Based Systems},
volume = {20},
number = {4},
pages = {382-387},
year = {2007},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2006.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S0950705106001675},
author = {W.P.S. Dias},
keywords = {Practice based knowledge, Connectionist AI techniques, Tacit knowing, Shared practice},
abstract = {Michael Polanyi’s idea of tacit knowing and Martin Heidegger’s concept of pre-theoretical shared practice are presented as providing a strong rationale for the notion of practice based knowledge. Artificial Intelligence (AI) approaches such as Artificial Neural Networks (ANN), Case Based Reasoning (CBR) and Grounded Theory (with Interval Probability Theory) are able to model these philosophical concepts related to practice based knowledge. The AI techniques appropriate for modeling Polanyi’s and Heidegger’s ideas should be founded more on a connectionist rather than a cognitivist paradigm. Examples from engineering practice are used to demonstrate how the above techniques can capture, structure and make available such knowledge to practitioners.}
}
@incollection{SIEGLER20051,
title = {A computational model of conscious and unconscious strategy discovery},
editor = {Robert V. Kail},
series = {Advances in Child Development and Behavior},
publisher = {JAI},
volume = {33},
pages = {1-42},
year = {2005},
issn = {0065-2407},
doi = {https://doi.org/10.1016/S0065-2407(05)80003-5},
url = {https://www.sciencedirect.com/science/article/pii/S0065240705800035},
author = {Robert Siegler and Roberto Araya},
abstract = {Publisher Summary
This chapter deals with a computational model of conscious and conscious strategy discovery and advocates a triangulation strategy for attaining a better understanding of change mechanisms. This triangulation strategy involves going back and forth among traditional studies of age-related change, microgenetic studies of children's gleaming, and computer simulations that generate the changes documented in the other two approaches. The chapter describes a new computational model of conscious and unconscious strategy discovery. Apart from being a crucial component of one of the examples of the triangulation strategy, this simulation significantly extends previous models of strategy choice and discovery. A large majority of studies of cognitive development have been devoted to describe age-related changes. The studies of age-related change have succeeded in providing excellent descriptions of many aspects of cognitive growth. Each of these three approaches—descriptions of age-related change, descriptions of learning, and formal modeling—provides unique information critical to a well-grounded account of developmental change.}
}
@article{HODGENS2021102149,
title = {Solving the puzzle of Fe homeostasis by integrating molecular, mathematical, and societal models},
journal = {Current Opinion in Plant Biology},
volume = {64},
pages = {102149},
year = {2021},
note = {Cell biology},
issn = {1369-5266},
doi = {https://doi.org/10.1016/j.pbi.2021.102149},
url = {https://www.sciencedirect.com/science/article/pii/S1369526621001503},
author = {Charles Hodgens and Belinda S. Akpa and Terri A. Long},
keywords = {Iron homeostasis, Simulation-based inference (SBI), Inclusivity},
abstract = {To ensure optimal utilization and bioavailability, iron uptake, transport, subcellular localization, and assimilation are tightly regulated in plants. Herein, we examine recent advances in our understanding of cellular responses to Fe deficiency. We then use intracellular mechanisms of Fe homeostasis to discuss how formalizing cell biology knowledge via a mathematical model can advance discovery even when quantitative data is limited. Using simulation-based inference to identify plausible systems mechanisms that conform to known emergent phenotypes can yield novel, testable hypotheses to guide targeted experiments. However, this approach relies on the accurate encoding of domain-expert knowledge in exploratory mathematical models. We argue that this would be facilitated by fostering more “systems thinking” life scientists and that diversifying your research team may be a practical path to achieve that goal.}
}
@article{CHAUNCEY2023100182,
title = {A framework and exemplars for ethical and responsible use of AI Chatbot technology to support teaching and learning},
journal = {Computers and Education: Artificial Intelligence},
volume = {5},
pages = {100182},
year = {2023},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2023.100182},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X23000619},
author = {Sarah A. Chauncey and H. Patricia McKenna},
keywords = {AI ethics, AI responsibility, AI-Rich learning environments, Cognitive flexibility, Critical thinking, Self-regulation},
abstract = {The aim of this paper is to investigate the ethical and responsible use of AI chatbots in education in support of critical thinking, cognitive flexibility and self-regulation in terms of their potential to enhance and motivate teaching and learning in contemporary education environments. AI chatbots such as ChatGPT by OpenAI appear to be improving in conversational and other capabilities and this paper explores such advances using version 4. Based on a review of the research literature, a conceptual framework is formulated for responsible use of AI chatbots in education supporting cognitive flexibility in AI-rich learning environments. The framework is then operationalized for use in this paper through the development of exemplars for math, english language arts (ELA), and studying with ChatGPT to close learning gaps in an effort to foster more ethical and responsible approaches to the design and development of AI chatbots for application and use in teaching and learning environments. This paper extends earlier foundational work on cognitive flexibility and AI chatbots as well as work on cognitive flexibility in support of creativity and innovation with AI chatbots in urban civic spaces.}
}
@article{DECARVALHO2021107887,
title = {A process for designing innovative mechatronic products},
journal = {International Journal of Production Economics},
volume = {231},
pages = {107887},
year = {2021},
issn = {0925-5273},
doi = {https://doi.org/10.1016/j.ijpe.2020.107887},
url = {https://www.sciencedirect.com/science/article/pii/S0925527320302504},
author = {Rogerio Atem {de Carvalho} and Henrique {da Hora} and Rodrigo Fernandes},
keywords = {Mechatronics, Product design, Design thinking, Concurrent engineering, Agilism, Product life cycle, Intellectual property, Innovation management},
abstract = {This article presents a process for the design of innovative mechatronic products that integrates techniques of Design Thinking, Concurrent Engineering and Agilism to Intellectual Property Management activities. Design Thinking is employed in the early stages in order to better explore creativity, whereas Concurrent Engineering and Agilism are applied during the development of the product, in order to deal with emerging requirements and shrinking development times. The product development process is accompanied by Intellectual Property Management activities that address the protection of the project's intellectual assets. In this way, the proposed process represents an addition to theory and practice by smoothly integrating the three most influential product design philosophies of today, while, at the same time, introduces a direction for managing intellectual assets throughout the product lifecycle.}
}
@incollection{MISHRA2024231,
title = {Chapter Twelve - Unravelling the gut microbiome: Connecting with AI for deeper insights},
editor = {Akanksha Srivastava and Vaibhav Mishra},
series = {Methods in Microbiology},
publisher = {Academic Press},
volume = {55},
pages = {231-246},
year = {2024},
booktitle = {Artificial Intelligence in Microbiology: Scope and Challenges Volume 1},
issn = {0580-9517},
doi = {https://doi.org/10.1016/bs.mim.2024.05.012},
url = {https://www.sciencedirect.com/science/article/pii/S058095172400028X},
author = {Vaibhav Mishra and Chhavi Atri and Raj Pandey and Akanksha Srivastava},
keywords = {Artificial intelligence, Gut microbes, Microbiology, Gastroenterology, Machine learning, Deep learning},
abstract = {Artificial intelligence (AI) remains a relatively unfamiliar concept for many, but its significance in the biomedical field is gaining recognition as the world undergoes transformative changes. Furthermore, AI possesses the potential to emulate critical thinking, reasoning, problem-solving abilities, and logical capacities of machines. Additionally, in the realm of gut microbiota research, AI emerges as a valuable asset. The synergy between gut microbes and AI not only holds promise for treating diverse gastroenterological diseases but also aids in comprehending the intricate relationships between gut microbes and microbes of resides into the other body parts. Moreover, AI facilitates a deeper understanding of different facets within gut-microbes interaction research. These direct communications are governed by chemical messengers, hormones, and neurotransmitters, detectable through biosensor chips employing machine learning (ML). Additionally, the indirect regulation of gut function by the brain via the hypothalamic-pituitary-adrenal (HPA) axis can be analysed using different computational models. This promising prospect remains largely unexplored, and in this chapter, our aim is to delve into and harness the potential of AI in gut microbial research.}
}
@article{PATAHUDDIN2022100988,
title = {Subtleties in spatial visualization maneuvers: Insights from numerical solutions},
journal = {The Journal of Mathematical Behavior},
volume = {67},
pages = {100988},
year = {2022},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2022.100988},
url = {https://www.sciencedirect.com/science/article/pii/S0732312322000566},
author = {Sitti Maesuri Patahuddin and Ajay Ramful and Tom Lowrie and Ajeevsing Bholoa},
keywords = {Spatial visualization, Spatial reasoning, Mathematics, Geometry, Measurement, Pre-service teacher},
abstract = {This study aimed to identify the role and nature of spatial visualization in the problem solutions of pre-service teachers solving school-mathematics tasks requiring measurement reasoning. The nuances in the pre-service teachers’ strategies were examined for the role of spatial visualization in the solution process. The findings suggest that inadequacies in visualizing the spatial configurations of the tasks led to incorrect numerical solutions despite the presence of conceptual knowledge. Furthermore, the tendency to rely on formula-based approaches appeared to have suppressed the preliminary spatial processing of the configurations. Theoretically, the paper offers insights into the mechanism that may be involved in the solution of spatially-related mathematical tasks. The findings imply that pre-service teachers need to be sufficiently engaged in spatial reasoning activities.}
}
@article{SALVATORE2024143,
title = {The affective grounds of the mind. The Affective Pertinentization (APER) model},
journal = {Physics of Life Reviews},
volume = {50},
pages = {143-165},
year = {2024},
issn = {1571-0645},
doi = {https://doi.org/10.1016/j.plrev.2024.07.008},
url = {https://www.sciencedirect.com/science/article/pii/S1571064524000903},
author = {Sergio Salvatore and Arianna Palmieri and Raffaele {De Luca Picione} and Vincenzo Bochicchio and Matteo Reho and Maria Rita Serio and Giampaolo Salvatore},
keywords = {Affective Pertinentization model, Affective Landscape, Phase Space of Meaning, Meaning dimensionality},
abstract = {The paper presents the Affective Pertinentization model (APER), a theory of the affect and its role it plays in meaning-making. APER views the affect as the basic form of making sense of reality. It consists of a global, bipolar pattern of neurophysiological activity through which the organism maps the instant-by-instant variation of its environment. Such a pattern of neuropsychological activity is constituted by a plurality of bipolar affective dimensions, each of which maps a component of the environmental variability. The affect has a pluri-componential structure defining a multidimensional affective landscape that foregrounds (i.e., makes pertinent) a certain pattern of facets of the environment (e.g., its pleasantness/unpleasantness) relevant to survival, while backgrounding the others. Doing so, the affect grounds the following cognitive processes. Accordingly, meaning-making can be modeled as a function of the dimensionality of the affective landscape. The greater the dimensionality of the affective landscape, the more differentiated the system of meaning is. Following a brief review of current theories pertaining to the affect, the paper proceeds discussing the APER's core tenets – the multidimensional view of the affect, its semiotic function, and the concepts of Affective Landscape and Phase Space of Meaning. The paper then proceeds deepening the relationship between the APER model and other theories, highlighting how the APER succeeds in framing original conceptualizations of several challenging issues – the intertwinement between affect and sensory modalities, the manner in which the mind constitutes the content of the experience, the determinants of psychopathology, the intertwinement of mind and culture, and the spreading of affective forms of thinking and behaving in society. Finally, the unsolved issues and future developments of the model are briefly envisaged.}
}
@article{NOWROOZI201252,
title = {A general computational recognition primed decision model with multi-agent rescue simulation benchmark},
journal = {Information Sciences},
volume = {187},
pages = {52-71},
year = {2012},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2011.09.039},
url = {https://www.sciencedirect.com/science/article/pii/S0020025511005330},
author = {Alireza Nowroozi and Mohammad E. Shiri and Angeh Aslanian and Caro Lucas},
keywords = {Naturalistic decision making, Recognition primed decision model, Computational modeling, Disaster management, RoboCup, Multi-agent rescue simulation benchmark},
abstract = {Analytical decision making strategies rely on weighing pros and cons of multiple options in an unbounded rationality manner. Contrary to these strategies, recognition primed decision (RPD) model which is a primary naturalistic decision making (NDM) approach assumes that experienced and professional decision makers when encounter problems in real operating conditions are able to use their previous experiences and trainings in order to diagnose the problem, recall the appropriate solution, evaluate it mentally, and implement it to handle the problem in a satisficing manner. In this paper, a computational form of RPD, now called C-RPD, is presented. Unified Modeling Language was used as a modeling language to represent the proposed C-RPD model in order to make the implementation easy and obvious. To execute the model, RoboCup Rescue agent simulation environment, which is one of the best and the most famous complex and multi-agent large-scale environments, was selected. The environment simulates the incidence of fire and earthquakes in urban areas where it is the duty of the police forces, firefighters and ambulance teams to control the crisis. Firefighters of SOS team are first modeled and implemented by utilizing C-RPD and then the system is trained using an expert’s experience. There are two evaluations. To find out the convergence of different versions developed during experience adding, some of the developed versions are chosen and evaluated on seven maps. Results show performance improvements. The SOS team ranked first in an official world championship and three official open tournaments.}
}
@article{GILROY201643,
title = {Inherently irrational? A computational model of escalation of commitment as Bayesian Updating},
journal = {Behavioural Processes},
volume = {127},
pages = {43-51},
year = {2016},
note = {SQAB 2015: Choice and Consequences},
issn = {0376-6357},
doi = {https://doi.org/10.1016/j.beproc.2016.02.017},
url = {https://www.sciencedirect.com/science/article/pii/S0376635716300389},
author = {Shawn P. Gilroy and Donald A. Hantula},
keywords = {Escalation, Computer simulation, Decision-making, Bayes theorem},
abstract = {Monte Carlo simulations were performed to analyze the degree to which two-, three- and four-step learning histories of losses and gains correlated with escalation and persistence in extended extinction (continuous loss) conditions. Simulated learning histories were randomly generated at varying lengths and compositions and warranted probabilities were determined using Bayesian Updating methods. Bayesian Updating predicted instances where particular learning sequences were more likely to engender escalation and persistence under extinction conditions. All simulations revealed greater rates of escalation and persistence in the presence of heterogeneous (e.g., both Wins and Losses) lag sequences, with substantially increased rates of escalation when lags comprised predominantly of losses were followed by wins. These methods were then applied to human investment choices in earlier experiments. The Bayesian Updating models corresponded with data obtained from these experiments. These findings suggest that Bayesian Updating can be utilized as a model for understanding how and when individual commitment may escalate and persist despite continued failures.}
}
@article{GOLDSTONE2005424,
title = {Computational models of collective behavior},
journal = {Trends in Cognitive Sciences},
volume = {9},
number = {9},
pages = {424-430},
year = {2005},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2005.07.009},
url = {https://www.sciencedirect.com/science/article/pii/S1364661305002147},
author = {Robert L. Goldstone and Marco A. Janssen},
abstract = {Computational models of human collective behavior offer promise in providing quantitative and empirically verifiable accounts of how individual decisions lead to the emergence of group-level organizations. Agent-based models (ABMs) describe interactions among individual agents and their environment, and provide a process-oriented alternative to descriptive mathematical models. Recent ABMs provide compelling accounts of group pattern formation, contagion and cooperation, and can be used to predict, manipulate and improve upon collective behavior. ABMs overcome an assumption that underlies much of cognitive science – that the individual is the crucial unit of cognition. The alternative advocated here is that individuals participate in collective organizations that they might not understand or even perceive, and that these organizations affect and are affected by individual behavior.}
}
@article{HASUO2017404,
title = {Semantics of higher-order quantum computation via geometry of interaction},
journal = {Annals of Pure and Applied Logic},
volume = {168},
number = {2},
pages = {404-469},
year = {2017},
note = {Eighth Games for Logic and Programming Languages Workshop (GaLoP)},
issn = {0168-0072},
doi = {https://doi.org/10.1016/j.apal.2016.10.010},
url = {https://www.sciencedirect.com/science/article/pii/S0168007216301336},
author = {Ichiro Hasuo and Naohiko Hoshino},
keywords = {Higher-order computation, Quantum computation, Programming language, Geometry of interaction, Denotational semantics, Categorical semantics},
abstract = {While much of the current study on quantum computation employs low-level formalisms such as quantum circuits, several high-level languages/calculi have been recently proposed aiming at structured quantum programming. The current work contributes to the semantical study of such languages by providing interaction-based semantics of a functional quantum programming language; the latter is, much like Selinger and Valiron's, based on linear lambda calculus and equipped with features like the ! modality and recursion. The proposed denotational model is the first one that supports the full features of a quantum functional programming language; we prove adequacy of our semantics. The construction of our model is by a series of existing techniques taken from the semantics of classical computation as well as from process theory. The most notable among them is Girard's Geometry of Interaction (GoI), categorically formulated by Abramsky, Haghverdi and Scott. The mathematical genericity of these techniques—largely due to their categorical formulation—is exploited for our move from classical to quantum.}
}
@incollection{MOL2015158,
title = {Chapter 5 - Computational Design of Biological Systems: From Systems to Synthetic Biology},
editor = {Zaheer Ul-Haq and Jeffry D. Madura},
booktitle = {Frontiers in Computational Chemistry},
publisher = {Bentham Science Publishers},
pages = {158-196},
year = {2015},
isbn = {978-1-60805-865-5},
doi = {https://doi.org/10.1016/B978-1-60805-865-5.50005-8},
url = {https://www.sciencedirect.com/science/article/pii/B9781608058655500058},
author = {Milsee Mol and Shailza Singh},
keywords = {Abstraction, bioengineering, bioinspired, biological parts, computational modelling, computational tools, constructs, dynamic, infectious disease, interdisciplinary, linearization, mathematical framework, nextgen therapeutics, omics, ordinary differential equations, parameters, physical systems, reactions, regulatory circuits, simulation},
abstract = {Abstract:
Today biology is overwhelmed with ‘big data’, amassed from genomic projects carried out in various laboratories around the world using efficient high throughput technologies. Biologists are co-opting mathematical and computational techniques developed to address these data and derive meaningful interpretations. These developments have led to new disciplines: systems and synthetic biology. To explore these two evolving branches of biology one needs to be familiar with technologies such as genomics, bioinformatics and proteomics, mathematical and computational modeling techniques that help predict the dynamic behavior of the biological system, ruling out the trial-and-error methods of traditional genetic engineering. Systems and synthetic biology have developed hand-in-hand towards building artificial biological devices using engineered biological units as basic building blocks. Systems biology is an integrated approach for studying the dynamic and complex behaviors of biological components, which may be difficult to interpret and predict from properties of individual constituents making up the biological systems. While, synthetic biology aims to engineer biologically inspired devices, such as cellular regulatory circuits that do not exist in nature but are designed using well characterized genes, proteins and other biological components in appropriate combinations to perform a desired function. This is analogous to an electronic circuit board design that is fabricated using well characterized electrical components such as resistors, capacitors and so on. The in silico abstractions and predictions should be tightly linked to experimentation to be proved in vitro and in vivo systems for their successful applications in biotechnology. This chapter focuses on mathematical approaches and computational tools available to engineer biological regulatory circuits and how they can be implemented as next generation therapeutics in infectious disease.}
}
@article{ROSSITER20241,
title = {MATLAB files to support learning of simple frequency response design},
journal = {IFAC-PapersOnLine},
volume = {58},
number = {26},
pages = {1-6},
year = {2024},
note = {4th IFAC Workshop on Internet Based Control Education - IBCE 2024},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2024.10.261},
url = {https://www.sciencedirect.com/science/article/pii/S2405896324020366},
author = {J.A. Rossiter},
keywords = {Control101 toolbox, frequency response, lead and lag compensation, virtual laboratories, independent learning},
abstract = {This paper presents a small number of MATLAB APPs and livescript files designed to help students both understand and implement frequency response tools into feedback design. The paper presents the thinking behind the use of MATLAB and the topic itself before then describing the proposed resources in detail.}
}
@incollection{ISMAIL2018165,
title = {Chapter 6 - High-Throughput Screening of Phytochemicals: Application of Computational Methods},
editor = {Satyajit D. Sarker and Lutfun Nahar},
booktitle = {Computational Phytochemistry},
publisher = {Elsevier},
pages = {165-192},
year = {2018},
isbn = {978-0-12-812364-5},
doi = {https://doi.org/10.1016/B978-0-12-812364-5.00006-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128123645000067},
author = {Fyaz M.D. Ismail and Lutfun Nahar and Satyajit D. Sarker},
keywords = {High-throughput screening (HTS), Robotics, Dereplication, Liquid handling systems, Screening , Natural product prototypes, Drug discovery and development, , , },
abstract = {This chapter reviews the origin and evolution of high-throughput screening (HTS) through the experience of the authors, who have either consulted for and/or provided courses to various pharmaceutical companies. It focuses on the role of HTS in natural product (phytochemicals) drug screening and drug discovery. Application of computational methods in HTS for phytochemical is highlighted. Commonly encountered difficulties and solutions to some of the problems are discussed together with selected ‘how to’ protocols to ensure investigators can set up and productively use HTS in their own natural product research. Relevant failures and successes in identifying interesting natural products are also outlined.}
}
@article{DUBLJEVIC2024102480,
title = {Colleges and universities are important stakeholders for regulating large language models and other emerging AI},
journal = {Technology in Society},
volume = {76},
pages = {102480},
year = {2024},
issn = {0160-791X},
doi = {https://doi.org/10.1016/j.techsoc.2024.102480},
url = {https://www.sciencedirect.com/science/article/pii/S0160791X24000289},
author = {Veljko Dubljević},
keywords = {Artificial intelligence (AI), Ethics, Public policy, Legitimacy, Oversight},
abstract = {AI technology has already gone through one “winter,” and alarmist thinking may cause yet another one. Calls for a moratorium on AI research increase the salience of the public request for comment on “AI accountability.” Prohibitive approaches are an overreaction, especially when leveraged on virtual (non-embodied) AI agents. While there are legitimate concerns regarding expansion of AI models like ChatGPT in society, a better approach would be to forge a partnership between academia and industry, and utilize infrastructure of campuses to authenticate users and oversee new AI research. The public could also be involved with public libraries authenticating users. This staged approach to embedding AI in society would facilitate addressing ethical concerns, and implementing virtual AI agents in society in a responsible and safe manner.}
}
@article{JACKSON20091397,
title = {There may be more to reaching than meets the eye: Re-thinking optic ataxia},
journal = {Neuropsychologia},
volume = {47},
number = {6},
pages = {1397-1408},
year = {2009},
note = {Perception and Action},
issn = {0028-3932},
doi = {https://doi.org/10.1016/j.neuropsychologia.2009.01.035},
url = {https://www.sciencedirect.com/science/article/pii/S0028393209000475},
author = {Stephen R. Jackson and Roger Newport and Masud Husain and Jane E. Fowlie and Michael O’Donoghue and Nin Bajaj},
keywords = {Optic ataxia, Neuropsychology of action, Reaching},
abstract = {Optic ataxia (OA) is generally thought of as a disorder of visually guided reaching movements that cannot be explained by any simple deficit in visual or motor processing. In this paper we offer a new perspective on optic ataxia; we argue that the popular characterisation of this disorder is misleading and is unrepresentative of the pattern of reaching errors typically observed in OA patients. We begin our paper by reviewing recent neurophysiological, neuropsychological, and functional brain imaging studies that have led to the proposal that the medial parietal cortex in the vicinity of the parietal-occipital junction (POJ) – the key anatomical site associated with OA – represents reaching movements in eye-centred coordinates, and that this ability is impaired in optic ataxia. Our perspective stresses the importance of the POJ and superior parietal regions of the human PPC for representing reaching movements in both extrinsic (eye-centred) and intrinsic (postural) coordinates, and proposes that it is the ability to simultaneously represent multiple spatial locations that must be directly compared with one another that is impaired in non-foveal OA patients. In support of this idea we review recent fMRI and behavioural studies conducted by our group that have investigated the anatomical correlates of posturally guided movements, and the movements guided by postural cues in patients presenting with optic ataxia.}
}
@article{MITROVIC2025100115,
title = {Glitch(ing)! A refusal and gateway to more caring techno-urban worlds?},
journal = {Digital Geography and Society},
volume = {8},
pages = {100115},
year = {2025},
issn = {2666-3783},
doi = {https://doi.org/10.1016/j.diggeo.2025.100115},
url = {https://www.sciencedirect.com/science/article/pii/S2666378325000042},
author = {Mirjana Mitrović and Maja-Lee Voigt},
keywords = {Glitch, Feminism, Computational cities, Hacking, flâneuse, Care, Refusal},
abstract = {With code connecting to concrete in ‘smart’ cities, oppressive, patriarchal, and binary architectures of the urban have been translated into their algorithmic counterparts, too. This particularly excludes people who do not conform to these inscribed norms. In the public realm of streets and screens, their bodies now become misidentified as glitches by digitalized welfare services, techno-politics, and passersby. Primarily known as a visual or audible phenomenon of disruption in the technological environment, this paper advocates for conceptualizing the glitch as more than that: it understands the glitch as three-part: 1. a fleeting, but potentially violent error – either by mistake (technical) or by design (social); 2. a moment of refusal of prevailing systems; and 3. as a gateway for changing what it reveals as flawed. Drawing on (auto-)ethnographic fieldwork from 2020 to 2022 on flâneuses* and hackfeminist collectives we will show how these grassroots urbanist actors turn the painful error of their bodies not being considered in techno-urban environments into practices of refusal and change. Creatively and collectively, they manage to turn glitches ‘by design’ into entry points to technologically and socially fight for spaces centering care instead. The portrayed bottom-up practices are important examples for breaking with social and technical binaries: Through strolling and scrolling, they dismantle tools of (digital) domination and provoke to think of who actually participates in ‘smartified’ spaces. Celebrating glitching as refusal, flâneuses* and hackfeminists alike open up questions about the authorship and implemented ideologies hardcoded into the fabric of the cities of today. Moreover, alone and together, their refusal mobilizes alternative, plural futures and makes glitch(ing) a gateway to more caring techno-urban worlds.}
}
@article{PALKOVICS2016144,
title = {Exploration of cognition–affect and Type 1–Type 2 dichotomies in a computational model of decision making},
journal = {Cognitive Systems Research},
volume = {40},
pages = {144-160},
year = {2016},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2016.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S1389041715300115},
author = {Michael Anton Palkovics and Martin Takáč},
keywords = {Affective computing, Dual process theory, Decision-making},
abstract = {This paper studies the role of cognition and affect in decision-making as well as notions of Type 1 and 2 processes and behaviors typically used in dual process theories. In order to demonstrate that there is no 1:1 correspondence between types of observed behavior and internal processes causing them, and that Type 1 and Type 2 processes can be produced by a single system, we implemented a computational model integrating affective and cognitive processing. Our model is based on the model of Marinier, Laird, and Lewis (2009). We modified it by increasing the agent’s visual field, adding a GOFAI-style cognitive module (sub-goal management) and expanding the environment by a high-threat tile, to which the agent responds with a hard-wired automatic reaction. This allowed us to generate and observe different types of behavior and study interesting interactions between cognitive and affective control. By comparing our re-implementation to the modified agent, we demonstrated clear cases of Type 1 (fast, automatic) and Type 2 (slow, deliberative) behavior, providing further evidence for the “single-system, two processes” hypothesis.}
}
@article{HOU2025141,
title = {Data-driven modeling of 600 MW supercritical unit under full operating conditions based on Transformer-XL},
journal = {ISA Transactions},
volume = {158},
pages = {141-166},
year = {2025},
issn = {0019-0578},
doi = {https://doi.org/10.1016/j.isatra.2024.12.049},
url = {https://www.sciencedirect.com/science/article/pii/S0019057824006359},
author = {Guolian Hou and Tianhao Zhang and Ting Huang},
keywords = {Supercritical unit, Transformer-XL, Once-through/recirculation/shut-down mode, Quantum chaotic nutcracker optimization algorithm},
abstract = {Improving the flexible and deep peak shaving capability of supercritical (SC) unit under full operating conditions to adapt a larger-scale renewable energy integrated into the power grid is the main choice of novel power system. However, it is particularly challenging to establish an accurate SC unit model under large-scale variable loads and deep peak shaving. To this end, a data-driven modeling strategy combining Transformer-Extra Long (Transformer-XL) and quantum chaotic nutcracker optimization algorithm is proposed. Firstly, three models of the SC unit under once-through/recirculation/shut-down are built via analyzing its mechanism of the operation process, respectively. Secondly, the superior performance of Transformer-XL in obtaining global feature information is employed to effectively solve the problem of high information dependence caused by the strong coupling and nonlinearity of SC unit. Then, the improved quantum chaotic nutcracker optimization algorithm with higher search accuracy is proposed to obtain the optimal parameters of Transformer-XL based on the logistic chaotic mapping and quantum thinking. Feature information dependencies and optimal parameter settings are fully considered in the proposed modeling scheme, which results in an accurate model of SC unit under full operating conditions. Finally, various simulations and comparisons are conducted based on the on-site data of 600 MW SC unit to demonstrate the superiority of the proposed data-driven modeling strategy. According to the improved Transformer-XL, the mean square errors of the proposed SC unit model under once-through/recirculation/shut-down modes are less than 2.500E-03, which verifies the high accuracy of the model. Consequently, the developed model is suitable for application in the controller designing and the operating efficiency and flexibility improvement of SC unit.}
}
@article{VARGO2017260,
title = {A systems perspective on markets – Toward a research agenda},
journal = {Journal of Business Research},
volume = {79},
pages = {260-268},
year = {2017},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2017.03.011},
url = {https://www.sciencedirect.com/science/article/pii/S014829631730098X},
author = {Stephen L. Vargo and Kaisa Koskela-Huotari and Steve Baron and Bo Edvardsson and Javier Reynoso and Maria Colurcio},
keywords = {Markets, Systems thinking, Marketing, Complex systems, Research agenda},
abstract = {This paper addresses the implications of an emerging, increasingly important way of thinking about markets: systems thinking. A market is one of the most founational abstractions in marketing and business research; yet, it often receives too little attention. As a result, the taken-for-granted assumptions about markets spur from over-simplified conceptualizations of neoclassical economics that depict markets as static and mechanistic. Systems thinking represents a major change in perspective that involves transcending this mechanistic worldview and thinking instead in terms of wholes, relationships, processes, and patterns. We argue that building a theory of markets based on systems thinking, would enable scholars to develop more realistic models that correspond with fast-changing business environment and therefore, increase both the rigor and relevance of future research. To further this aim, we identify the main implications of systems thinking and formulate them into a research agenda to further the systemic understanding of markets.}
}
@incollection{REIMERS2006119,
title = {[8] Bioconductor: An Open Source Framework for Bioinformatics and Computational Biology},
series = {Methods in Enzymology},
publisher = {Academic Press},
volume = {411},
pages = {119-134},
year = {2006},
booktitle = {DNA Microarrays, Part B: Databases and Statistics},
issn = {0076-6879},
doi = {https://doi.org/10.1016/S0076-6879(06)11008-3},
url = {https://www.sciencedirect.com/science/article/pii/S0076687906110083},
author = {Mark Reimers and Vincent J. Carey},
abstract = {This chapter describes the Bioconductor project and details of its open source facilities for analysis of microarray and other high‐throughput biological experiments. Particular attention is paid to concepts of container and workflow design, connections of biological metadata to statistical analysis products, support for statistical quality assessment, and calibration of inference uncertainty measures when tens of thousands of simultaneous statistical tests are performed.}
}
@article{KE201426,
title = {An implementation of design-based learning through creating educational computer games: A case study on mathematics learning during design and computing},
journal = {Computers & Education},
volume = {73},
pages = {26-39},
year = {2014},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2013.12.010},
url = {https://www.sciencedirect.com/science/article/pii/S0360131513003345},
author = {Fengfeng Ke},
keywords = {Learning by design, Game-based learning, Mathematical disposition, Thinking mathematically, Computer game making},
abstract = {This mixed-method case study examined the potential of computer-assisted, math game making activities in facilitating design-based math learning for school children. Sixty-four middle school children participated in Scratch-based, math game making activities. Data were collected via activity and conversation observation, artifact analysis, interviewing, and survey. The study findings indicated that participants developed significantly more positive dispositions toward mathematics after computer game making. The study also found that experience-driven game design processes helped to activate children's reflection on everyday mathematical experiences. Mathematical thinking and content experience were intertwined within the process of computer game authoring. On the other hand, children designers were involved in game-world and story crafting more than mathematical representation. And it was still challenging for them to perform computer game coding with abstract reasoning.}
}
@article{IOAKIMIDIS2017280,
title = {Caustics, pseudocaustics and the related illuminated and dark regions with the computational method of quantifier elimination},
journal = {Optics and Lasers in Engineering},
volume = {88},
pages = {280-300},
year = {2017},
issn = {0143-8166},
doi = {https://doi.org/10.1016/j.optlaseng.2016.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S0143816616301348},
author = {Nikolaos I. Ioakimidis},
keywords = {Caustics, Pseudocaustics, Illuminated and dark regions, Cracks, Plates, Elasticity},
abstract = {The method of caustics is a powerful experimental method in elasticity and particularly in fracture mechanics for crack problems. The related method of pseudocaustics is also of interest. Here we apply the computational method of quantifier elimination implemented in the computer algebra system Mathematica in order to determine (i) the non-parametric equation and two properties of the caustic at a crack tip and especially (ii) the illuminated and the dark regions related to caustics and pseudocaustics in plane elasticity and plate problems. The present computations concern: (i) The derivation of the non-parametric equation of the classical caustic about a crack tip through the elimination of the parameter involved (here the polar angle) as well as two geometrical properties of this caustic. (ii) The derivation of the inequalities defining the illuminated region on the screen in the problem of an elastic half-plane loaded normally by a concentrated load with the boundary of this illuminated region related to some extent to the caustic formed. (iii) Similarly for the problem of a clamped circular plate under a uniform loading with respect to the caustic and the pseudocaustic formed. (iv) Analogously for the problem of an equilateral triangular plate loaded by uniformly distributed moments along its whole boundary, which defines the related pseudocaustic. (v) The determination of quantities of interest in mechanics from the obtained caustics or pseudocaustics. The kind of computations in the applications (ii) to (iv), i.e. the derivation of inequalities defining the illuminated region on the screen, seems to be completely new independently of the use here of the method of quantifier elimination. Additional applications are also possible, but some of them require the expansion of the present somewhat limited power of the quantifier elimination algorithms in Mathematica. This is expected to take place in the future.}
}
@article{BJORNE2005193,
title = {A model of attentional impairments in autism: first steps toward a computational theory},
journal = {Cognitive Systems Research},
volume = {6},
number = {3},
pages = {193-204},
year = {2005},
note = {Epigenetic Robotics},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2004.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S1389041704000749},
author = {Petra Björne and Christian Balkenius},
keywords = {Autism, Attention, Computational model},
abstract = {A computational model with three interacting components for context sensitive reinforcement learning, context processing and automation can autonomously learn a focus attention and a shift attention task. The performance of the model is similar to that of normal children, and when a single parameter is changed, the performance on the two tasks approaches that of autistic children.}
}
@article{GILHOOLY2024100071,
title = {AI vs humans in the AUT: Simulations to LLMs},
journal = {Journal of Creativity},
volume = {34},
number = {1},
pages = {100071},
year = {2024},
issn = {2713-3745},
doi = {https://doi.org/10.1016/j.yjoc.2023.100071},
url = {https://www.sciencedirect.com/science/article/pii/S2713374523000304},
author = {Ken Gilhooly},
keywords = {AI, Alternative uses, Divergent thinking},
abstract = {This paper reviews studies of proposed creative machines applied to a prototypical creative task, i.e., the Alternative Uses Task (AUT). Although one system (OROC) did simulate some aspects of human strategies for the AUT, most recent attempts have not been simulation-oriented, but rather have used Large Language Model (LLM) systems such as GPT-3 which embody extremely large connectionist networks trained on huge volumes of textual data. Studies reviewed here indicate that LLM based systems are performing on the AUT at near or somewhat above human levels in terms of scores on originality and usefulness. Moreover, similar patterns are found in the data of humans and LLM models in the AUT, such as output order effects and a negative association between originality and value or utility. However, it is concluded that GPT-3 and similar systems, despite generating novel and useful responses, do not display creativity as they lack agency and are purely algorithmic. LLM studies so far in this area have largely been exploratory and future studies should guard against possible training data contamination.}
}
@incollection{MARS20253,
title = {What every cognitive neuroscientist should know about prefrontal cortex evolution},
editor = {Jordan Henry Grafman},
booktitle = {Encyclopedia of the Human Brain (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {3-11},
year = {2025},
isbn = {978-0-12-820481-8},
doi = {https://doi.org/10.1016/B978-0-12-820480-1.00127-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128204801001273},
author = {Rogier B. Mars},
keywords = {Prefrontal cortex, Brain evolution, Foraging, Granular prefrontal cortex, Dorsolateral prefrontal cortex, Cognitive control, Comparative neuroscience, Primate, Connectivity, Human},
abstract = {Most theories of cognitive control assign a vital role to human prefrontal cortex (PFC). Although models of PFC function are abundant, most fail to capture the complexity of this part of the brain. Here we argue that an improved understanding of the evolution of PFC can aid in the formulation of better models. By better understanding what PFC is, why it evolved, and what benefit it provided to our ancestors, we can constrain our thinking and put the plethora of neuroimaging data showing PFC activation into context.}
}
@article{TOFFOLI20103,
title = {From Such Simple a Beginning: The Momentous Consequences of Physics' Microscopic Reversibility for Communication and Computation—and Almost Anything Else},
journal = {Electronic Notes in Theoretical Computer Science},
volume = {253},
number = {6},
pages = {3-16},
year = {2010},
note = {Proceedings of the Workshop on Reversible Computation (RC 2009)},
issn = {1571-0661},
doi = {https://doi.org/10.1016/j.entcs.2010.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S1571066110000150},
author = {Tommaso Toffoli},
keywords = {invertibility, irreversibility, computation, dynamics, thermodynamics, entropy, second law of thermodynamics},
abstract = {Darwin concludes The Origin of Species with a splendid one-phrase poem,From so simple a beginningendless forms most beautiful and most wonderfulhave been, and are being, evolved. Darwin's “simple beginning” may be identified, in today's terminology, with dissipation—evolution's basic fuel. All the rest is commentary—or, more precisely, corollary. One can aptly apply Darwin's phrase to another kind of “simple beginning,” from which as well “endless forms most beautiful and most wonderful have been, and are being, evolved.” What I have in mind is a concept that is apparently the very antithesis of dissipation, namely, physics' fundamental assumption of invertibility—or “microscopic reversibility.” To paraphrase Dobzhansky, no sensible step can be taken today in information, communication, and computer sciences, as well as in fundamental physics, except in the light of invertibility.}
}