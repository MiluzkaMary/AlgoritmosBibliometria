@article{BERNALMANRIQUE202086,
title = {Effect of acceptance and commitment therapy in improving interpersonal skills in adolescents: A randomized waitlist control trial},
journal = {Journal of Contextual Behavioral Science},
volume = {17},
pages = {86-94},
year = {2020},
issn = {2212-1447},
doi = {https://doi.org/10.1016/j.jcbs.2020.06.008},
url = {https://www.sciencedirect.com/science/article/pii/S2212144720301551},
author = {Koryn N. Bernal-Manrique and María B. García-Martín and Francisco J. Ruiz},
keywords = {Acceptance and commitment therapy, Interpersonal skills, Emotional disorders, Psychological flexibility, Repetitive negative thinking},
abstract = {This parallel randomized controlled trial evaluated the effect of acceptance and commitment therapy (ACT) focused on repetitive negative thinking (RNT) versus a waitlist control (WLC) in improving interpersonal skills in adolescents with problems of social and school adaptation. Forty-two adolescents (11–17 years) agreed to participate. Participants were allocated through simple randomization to the intervention condition or the waitlist control condition. The intervention was a 3-session, group-based, RNT-focused ACT protocol. The primary outcome was the performance on a test of interpersonal skills (Interpersonal Conflict Resolution Assessment, ESCI). At posttreatment, repeated measures ANOVA showed that the intervention was efficacious in increasing overall interpersonal skills (d = 2.62), progress in values (d = 1.23), and reducing emotional symptoms (d = 0.98). No adverse events were found. A brief RNT-focused ACT intervention was highly efficacious in improving interpersonal skills and reducing emotional symptoms in adolescents.}
}
@article{GILL2018733,
title = {Data to Decision and Judgment Making – a Question of Wisdom},
journal = {IFAC-PapersOnLine},
volume = {51},
number = {30},
pages = {733-738},
year = {2018},
note = {18th IFAC Conference on Technology, Culture and International Stability TECIS 2018},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2018.11.205},
url = {https://www.sciencedirect.com/science/article/pii/S2405896318328702},
author = {Karamjit S Gill},
keywords = {algorithms, artificial intelligence, big data, calculation, decision, judgment, wisdom},
abstract = {The technological waves of super artificial intelligence, big data, algorithms, and machine learning continue to impact our thinking and actions, thereby affecting the ways individuals, professions and institutions make judgments. On the one hand, there is an argument that more data and knowledge together with the cyber physical system of industry4.0 will automatically push society along some track toward a better world for all. On the other hand, we hear worrying voices of the imponderable downsides of powerful new cyber-, bio-, Nano-technologies, and synthetic biology. In the age of uncertainties, big data and the algorithm, how is the decision and judgment making process being affected?}
}
@article{BUTZ2025105948,
title = {Contextualizing predictive minds},
journal = {Neuroscience & Biobehavioral Reviews},
volume = {168},
pages = {105948},
year = {2025},
issn = {0149-7634},
doi = {https://doi.org/10.1016/j.neubiorev.2024.105948},
url = {https://www.sciencedirect.com/science/article/pii/S0149763424004172},
author = {Martin V. Butz and Maximilian Mittenbühler and Sarah Schwöbel and Asya Achimova and Christian Gumbsch and Sebastian Otte and Stefan Kiebel},
keywords = {Prediction, Cognitive modeling, Events, Active inference, Learning, Behavior, Free energy, Abstraction, Context inference, Deep learning},
abstract = {The structure of human memory seems to be optimized for efficient prediction, planning, and behavior. We propose that these capacities rely on a tripartite structure of memory that includes concepts, events, and contexts—three layers that constitute the mental world model. We suggest that the mechanism that critically increases adaptivity and flexibility is the tendency to contextualize. This tendency promotes local, context-encoding abstractions, which focus event- and concept-based planning and inference processes on the task and situation at hand. As a result, cognitive contextualization offers a solution to the frame problem—the need to select relevant features of the environment from the rich stream of sensorimotor signals. We draw evidence for our proposal from developmental psychology and neuroscience. Adopting a computational stance, we present evidence from cognitive modeling research which suggests that context sensitivity is a feature that is critical for maximizing the efficiency of cognitive processes. Finally, we turn to recent deep-learning architectures which independently demonstrate how context-sensitive memory can emerge in a self-organized learning system constrained by cognitively-inspired inductive biases.}
}
@article{TURNQUIST2024112726,
title = {Adaptive mesh methods on compact manifolds via Optimal Transport and Optimal Information Transport},
journal = {Journal of Computational Physics},
volume = {500},
pages = {112726},
year = {2024},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2023.112726},
url = {https://www.sciencedirect.com/science/article/pii/S0021999123008215},
author = {Axel G.R. Turnquist},
keywords = {Optimal transport, Optimal information transport, Diffeomorphic density matching, Moving mesh methods, Convergent numerical methods, Compact manifolds},
abstract = {Moving mesh methods were devised to redistribute a mesh in a smooth way, while keeping the number of vertices of the mesh and their connectivity unchanged. A fruitful theoretical point-of-view is to take such moving mesh methods and think of them as an application of the diffeomorphic density matching problem. Given two probability measures μ0 and μ1, the diffeomorphic density matching problem consists of finding a diffeomorphic pushforward map T such that T#μ0=μ1. Moving mesh methods are seen to be an instance of the diffeomorphic density matching problem by treating the probability density as the local density of nodes in the mesh. It is preferable that the restructuring of the mesh be done in a smooth way that avoids tangling the connections between nodes, which would lead to numerical instability when the mesh is used in computational applications. This then suggests that a diffeomorphic map T is desirable to avoid tangling. The first tool employed to solve the moving mesh problem between source and target probability densities on the sphere was Optimal Transport (OT). Recently Optimal Information Transport (OIT) was rigorously derived and developed allowing for the computation of a diffeomorphic mapping by simply solving a Poisson equation. Not only is the equation simpler to solve numerically in OIT, but with Optimal Transport there is no guarantee that the mapping between probability density functions defines a diffeomorphism for general 2D compact manifolds. In this manuscript, we perform a side-by-side comparison of using Optimal Transport and Optimal Information Transport on the sphere for adaptive mesh problems. We choose to perform this comparison with recently developed provably convergent solvers, but these are, of course, not the only numerical methods that may be used. We believe that Optimal Information Transport is preferable in computations due to the fact that the partial differential equation (PDE) solve step is simply a Poisson equation. For more general surfaces M, we show how the Optimal Transport and Optimal Information Transport problems can be reduced to solving on the sphere, provided that there exists a diffeomorphic mapping Φ:M→S2. This implies that the Optimal Transport problem on M with a special cost function can be solved with regularity guarantees, while computations for the problem are performed on the unit sphere.}
}
@incollection{SAWLEY1995181,
title = { - A serial data-parallel multi-block method for compressible flow computations},
editor = {A. Ecer and J. Hauser and P. Leca and J. Periaux},
booktitle = {Parallel Computational Fluid Dynamics 1993},
publisher = {North-Holland},
address = {Amsterdam},
pages = {181-188},
year = {1995},
isbn = {978-0-444-81999-4},
doi = {https://doi.org/10.1016/B978-044481999-4/50148-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780444819994501481},
author = {M.L. Sawley and J.K. Tegnér and C.M. Bergman},
abstract = {Publisher Summary
Block structured meshes not only provide the possibility to compute flows in complex geometries but also lend themselves in a natural way to coarse-grain parallel processing via the distribution of different blocks to different processors. Nevertheless, for some flow computations, a fine-grain data parallel implementation may be more appropriate. This chapter presents a study of such an implementation, which utilizes the simplicity of the data parallel approach. Particular attention is placed on a dynamic block management strategy that allows computations to be undertaken only in blocks where useful work is to be performed. The question of code portability among four different parallel computer systems is addressed in the chapter. This chapter concludes that the serial data-parallel multi-block method provides a number of advantages: (1) it retains the simplicity of the above-mentioned data parallel methods, because each block is treated individually in the same manner as for a single block computation; (2) it does not impose any parallelization constraints on the mesh generation procedure, in principle, any number of blocks of unequal size can be employed; the transfer of data between two blocks (block connectivity) is performed in a transparent manner via globally addressable memory contrasting with the explicit data transfer required by message passing implementations; (3) because individual blocks are treated sequentially, a simple dynamic block management algorithm can be applied to avoid performing unnecessary operations; and (4) the use of standard Fortran 90 facilitates code portability among different platforms supporting the data parallel programming method.}
}
@article{AMADEI2020120149,
title = {Revisiting positive peace using systems tools},
journal = {Technological Forecasting and Social Change},
volume = {158},
pages = {120149},
year = {2020},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2020.120149},
url = {https://www.sciencedirect.com/science/article/pii/S0040162520309756},
author = {Bernard Amadei},
keywords = {Complex systems, Systems thinking, System dynamics, Cross-impact analysis, Network analysis, Positive peace, Peace geometry},
abstract = {This paper looks at peace with an integrated perspective. As a state, peace cannot be measured directly and requires the use of proxies and indicators. This paper revisits the positive peace index (PPI) introduced by the Institute for Economy and Peace (IEP) through the lens of systems thinking and modeling. Three sets of systems tools (cross-impact analysis, network analysis, and system dynamics) are proposed to explicitly account for the different levels of influence and dependence among the eight domains used to determine the PPI at the country level. Although more comprehensive than the original IEP formulation, the integrated approach proposed herein requires decisionmakers to be systems thinkers and able to conduct a detailed analysis of how the eight domains influence (impact) or depend on (sensitive to) each other. The proposed approach allows decisionmakers to capture the multidimensional and cross-disciplinary nature of positive peace better. This paper also shows that the three components of peace (positive, negative, and cultural) initially proposed by Johan Galtung can be represented using three-dimensional geometric features.}
}
@article{NAZIDIZAJI2015318,
title = {Does the smartest designer design better? Effect of intelligence quotient on students’ design skills in architectural design studio},
journal = {Frontiers of Architectural Research},
volume = {4},
number = {4},
pages = {318-329},
year = {2015},
issn = {2095-2635},
doi = {https://doi.org/10.1016/j.foar.2015.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S2095263515000394},
author = {Sajjad Nazidizaji and Ana Tomé and Francisco Regateiro},
keywords = {Architectural design studio, Intelligence quotient (IQ), Design education, Human factors, Design thinking},
abstract = {Understanding the cognitive processes of the human mind is necessary to further learn about design thinking processes. Cognitive studies are also significant in the research about design studio. The aim of this study is to examine the effect of designers intelligence quotient (IQ) on their designs. The statistical population in this study consisted of all Deylaman Institute of Higher Education architecture graduate students enrolled in 2011. Sixty of these students were selected via simple random sampling based on the finite population sample size calculation formula. The students’ IQ was measured using Raven’s Progressive Matrices. The students’ scores in Architecture Design Studio (ADS) courses from first grade (ADS-1) to fifth grade (ADS-5) and the mean scores of the design courses were used in determining the students’ design ability. Inferential statistics, as well as correlation analysis and mean comparison test for independent samples with SPSS, were also employed to analyze the research data. Results indicated that the students’ IQ, ADS-1 to ADS-4 scores, and the mean scores of the students’ design courses were not significantly correlated. By contrast, the students’ IQ and ADS-5 scores were significantly correlated. As the complexity of the design problem and designers’ experience increased, the effect of IQ on design seemingly intensified.}
}
@article{HEILMAN20041,
title = {Computational models of epileptiform activity in single neurons},
journal = {Biosystems},
volume = {78},
number = {1},
pages = {1-21},
year = {2004},
issn = {0303-2647},
doi = {https://doi.org/10.1016/j.biosystems.2004.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S0303264704000978},
author = {Avram D. Heilman and James Quattrochi},
keywords = {Paroxysmal depolarizing shifts (PDS), Sustained depolarizations (SD), Hippocampus, Autaptic CA1/CA3 pyramidal neuron, Voltage-gated Ca channels, Ca-dependent K channels},
abstract = {A series of original computational models written in NEURON of increasing physiological and morphological complexity were developed to determine the dominant causes of epileptiform behavior. Current injections to a model hippocampal pyramidal neuron consisting of three compartments produced the sustained depolarizations (SD) and simple paroxysmal depolarizing shifts (PDS) characteristic of ictal and interictal behavior in a cell, respectively. Our results indicate that SDs are the result of the semi-saturation of Na+, Ca2+ and K+ active channels, particularly the CaN, with regular Na+/K+ spikes riding atop a saturated depolarization; PDS rides on a similar semi-saturated depolarization whose shape depends more heavily on interactions between low-threshold voltage-gated Ca2+ channels (CaT) and Ca2+-dependent K+ channels. Our results reflect and predict recent physiological data, and we report here a cellular basis of epilepsy whose mechanisms reside mainly in the membrane channels, and not in specific morphology or network interactions, advancing a possible resolution to the cellular/network debate over the etiology of epileptiform activity.}
}
@article{NIKZAINAL2024101739,
title = {Prof. Serena Nik-Zainal},
journal = {Cell Reports Medicine},
volume = {5},
number = {9},
pages = {101739},
year = {2024},
issn = {2666-3791},
doi = {https://doi.org/10.1016/j.xcrm.2024.101739},
url = {https://www.sciencedirect.com/science/article/pii/S2666379124004695},
author = {Serena Nik-Zainal},
abstract = {Serena Nik-Zainal, MD, PhD, is professor of genomic medicine and bioinformatics and an honorary consultant in clinical genetics at the University of Cambridge. Prof. Nik-Zainal has dedicated her career to studying the physiology of cancer mutagenesis via a combination of computational and experimental work, as well as validation with clinical data. Among the many awards she has earned for her work, she has recently received the 2024 ESMO Award for Translational Research, for the research in the field of mutational signatures and her efforts in translating their use into clinics.}
}
@article{ZHU2023126915,
title = {SPAR: An efficient self-attention network using Switching Partition Strategy for skeleton-based action recognition},
journal = {Neurocomputing},
volume = {562},
pages = {126915},
year = {2023},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2023.126915},
url = {https://www.sciencedirect.com/science/article/pii/S092523122301038X},
author = {ZiJie Zhu and RenDong Ying and Fei Wen and PeiLin Liu},
keywords = {Action recognition, Self-attention, 3D-skeleton, Graph convolutional networks},
abstract = {Graph convolutional networks (GCN) have become the mainstream in skeleton-based action recognition. For further performance improvement, existing methods propose to utilize self-attention to model long-range features of joints. However, these methods cannot balance accuracy with computational efficiency. In this paper, we propose the Switching Partition Strategy (SPAR) Network that uses the self-attention mechanism for the simultaneous and efficient extraction of spatial–temporal long-range information from the skeleton. We design two partition strategies that reduce the computational cost and improve the efficiency of the computation of self-attention. Extensive experiments are conducted on two large-scale datasets, i.e. NTU RGB+D 60 and NTU RGB+D 120, to evaluate the performance of the proposed SPAR network. The results demonstrate that our method outperforms the state-of-the-art on accuracy as well as computational cost.}
}
@article{LIU2025103127,
title = {Rethinking multi-level information fusion in temporal graphs: Pre-training then distilling for better embedding},
journal = {Information Fusion},
volume = {121},
pages = {103127},
year = {2025},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2025.103127},
url = {https://www.sciencedirect.com/science/article/pii/S1566253525002003},
author = {Meng Liu and Yong Liu and Qianqian Ren and Meng Han},
keywords = {Graph learning, Temporal graph, Pre-training},
abstract = {Temporal graphs occupy an important place in graph data, which store node interactions in sequences, thus enabling a more microscopic view of each node’s dynamics. However, many temporal graph methods primarily concentrate on shallow-level temporal or neighborhood information, while acquiring deep-level community or global graph information necessitates increased computational costs, thereby significantly impacting model efficiency. Inspired by this, we rethink how this information is acquired: if it is difficult to acquire it during model training, why not obtain it before training? Consequently, we propose ReMIT, a novel method for temporal graph learning, which incorporates the concepts of feature pre-training and knowledge distillation to Rethink the embedding of Multi-level Information fusion in Temporal graphs. ReMIT facilitates the “remitting” of prior knowledge to model, wherein hard-to-access information is captured and distilled to the train module by introducing a pre-train module. Experimental results on multiple real-world datasets validate the validity and feasibility of our proposed framework. Our method improves performance by up to 10.2% while reducing almost 30% training time.}
}
@article{AMACHER2020100022,
title = {Specificity in PDZ-peptide interaction networks: Computational analysis and review},
journal = {Journal of Structural Biology: X},
volume = {4},
pages = {100022},
year = {2020},
issn = {2590-1524},
doi = {https://doi.org/10.1016/j.yjsbx.2020.100022},
url = {https://www.sciencedirect.com/science/article/pii/S2590152420300040},
author = {Jeanine F. Amacher and Lionel Brooks and Thomas H. Hampton and Dean R. Madden},
keywords = {Protein-protein interactions, PDZ, Peptide-binding domains, Therapeutic targets},
abstract = {Globular PDZ domains typically serve as protein–protein interaction modules that regulate a wide variety of cellular functions via recognition of short linear motifs (SLiMs). Often, PDZ mediated-interactions are essential components of macromolecular complexes, and disruption affects the entire scaffold. Due to their roles as linchpins in trafficking and signaling pathways, PDZ domains are attractive targets: both for controlling viral pathogens, which bind PDZ domains and hijack cellular machinery, as well as for developing therapies to combat human disease. However, successful therapeutic interventions that avoid off-target effects are a challenge, because each PDZ domain interacts with a number of cellular targets, and specific binding preferences can be difficult to decipher. Over twenty-five years of research has produced a wealth of data on the stereochemical preferences of individual PDZ proteins and their binding partners. Currently the field lacks a central repository for this information. Here, we provide this important resource and provide a manually curated, comprehensive list of the 271 human PDZ domains. We use individual domain, as well as recent genomic and proteomic, data in order to gain a holistic view of PDZ domains and interaction networks, arguing this knowledge is critical to optimize targeting selectivity and to benefit human health.}
}
@article{FLATER2018144,
title = {Architecture for software-assisted quantity calculus},
journal = {Computer Standards & Interfaces},
volume = {56},
pages = {144-147},
year = {2018},
issn = {0920-5489},
doi = {https://doi.org/10.1016/j.csi.2017.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S0920548917303069},
author = {David Flater},
keywords = {SI, Quantity, Unit, Uncertainty, Value, Unit 1},
abstract = {A quantity value, such as 5 kg, consists of a number and a reference (often an International System of Units (SI) unit) that together express the magnitude of a quantity. Many software libraries, packages, and ontologies that implement “quantities and units” functions are available. Although all of them begin with SI and associated practices, they differ in how they address issues such as ad hoc counting units, ratios of two quantities of the same kind, and uncertainty. This short article describes an architecture that addresses the complete set of functions in a simple and consistent fashion. Its goal is to encourage more convergent thinking about the functions and the underlying concepts so that the many disparate implementations, present and future, will become more consistent with one another.}
}
@article{WEYDMANN2025111173,
title = {Disentangling negative reinforcement, working memory, and deductive reasoning deficits in elevated BMI},
journal = {Progress in Neuro-Psychopharmacology and Biological Psychiatry},
volume = {136},
pages = {111173},
year = {2025},
issn = {0278-5846},
doi = {https://doi.org/10.1016/j.pnpbp.2024.111173},
url = {https://www.sciencedirect.com/science/article/pii/S0278584624002410},
author = {Gibson Weydmann and Igor Palmieri and Reinaldo A.G. Simões and Samara Buchmann and Eduardo Schmidt and Paulina Alves and Lisiane Bizarro},
keywords = {Overweight, Reinforcement Learning, Working Memory, Computational Modelling},
abstract = {Neuropsychological data suggest that being overweight or obese is associated with a tendency to perseverate behavior despite negative feedback. This deficit might be observed due to other cognitive factors, such as working memory (WM) deficits or decreased ability to deduce model-based strategies when learning by trial-and-error. In the present study, a group of subjects with overweight or obesity (Ow/Ob, n = 30) was compared to normal-weight individuals (n = 42) in a modified Reinforcement Learning (RL) task. The task was designed to control WM effects on learning by manipulating cognitive load and to foster model-based learning via deductive reasoning. Computational modelling and analysis were conducted to isolate parameters related to RL mechanisms, WM use, and model-based learning (deduction parameter). Results showed that subjects with Ow/Ob had a higher number of perseverative errors and used a weaker deduction mechanism in their performance than control individuals, indicating impairments in negative reinforcement and model-based learning, whereas WM impairments were not responsible for deficits in RL. The present data suggests that obesity is associated with impairments in negative reinforcement and model-based learning.}
}
@article{SEVERENGIZ2018429,
title = {Influence of Gaming Elements on Summative Assessment in Engineering Education for Sustainable Manufacturing},
journal = {Procedia Manufacturing},
volume = {21},
pages = {429-437},
year = {2018},
note = {15th Global Conference on Sustainable Manufacturing},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2018.02.141},
url = {https://www.sciencedirect.com/science/article/pii/S235197891830180X},
author = {Mustafa Severengiz and Ina Roeder and Kristina Schindler and Günther Seliger},
keywords = {summative assessment, gamification, higher education, engineering education},
abstract = {Regarding the massive sustainability challenge mankind is currently facing, there is an indisputable need to implement sustainability as the key reference point into higher engineering education in order to prepare the stakeholders of tomorrow. This requires networked thinking on the part of the learner and increases the learning goals’ complexity dramatically. The actual achieved learning outcomes are often evaluated by assessing factual knowledge in higher education. However, it has been shown many times that students choose the examination format for orientation when studying. Thus, the authors propose a gamified summative assessment approach that requires networked thinking to direct students’ learning efforts towards broad competency building. In a study with 25 students of a master engineering course, the effects of a gamified examination design are investigated.}
}
@article{BONCHEKDOKOW201444,
title = {Towards computational models of intention detection and intention prediction},
journal = {Cognitive Systems Research},
volume = {28},
pages = {44-79},
year = {2014},
note = {Special Issue on Mindreading},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2013.07.004},
url = {https://www.sciencedirect.com/science/article/pii/S1389041713000399},
author = {Elisheva Bonchek-Dokow and Gal A. Kaminka},
keywords = {Intention recognition, Intention prediction, Cognitive modeling},
abstract = {Intention recognition is one of the core components of mindreading, an important process in social cognition. Human beings, from age of 18months, have been shown to be able to extrapolate intentions from observed actions, even when the performer failed at achieving the goal. Existing accounts of intention recognition emphasize the use of an intent (plan) library, which is matched against observed actions for recognition. These therefore cannot account for recognition of failed sequences of actions, nor novel actions. In this paper, we begin to tackle these open questions by examining computational models for components of human intention recognition, which emphasize the ability of humans to detect and identify intentions in a sequence of observed actions, based solely on the rationality of movement (its efficiency). We provide a high-level overview of intention recognition as a whole, and then elaborate on two components of the model, which we believe to be at its core, namely, those of intention detection and intention prediction. By intention detection we mean the ability to discern whether a sequence of actions has any underlying intention at all, or whether it was performed in an arbitrary manner with no goal in mind. By intention prediction we mean the ability to extend an incomplete sequence of actions to its most likely intended goal. We evaluate the model, and these two components, in context of existing literature, and in a number of experiments with more than 140 human subjects. For intention detection, our model was able to attribute high levels of intention to those traces perceived by humans as intentional, and vice versa. For intention prediction as well, our model performed in a way that closely matched that of humans. The work highlights the intimate relationship between the ability to generate plans, and the ability to recognize intentions.}
}
@incollection{DUNBAR200113746,
title = {Scientific Reasoning and Discovery, Cognitive Psychology of},
editor = {Neil J. Smelser and Paul B. Baltes},
booktitle = {International Encyclopedia of the Social & Behavioral Sciences},
publisher = {Pergamon},
address = {Oxford},
pages = {13746-13749},
year = {2001},
isbn = {978-0-08-043076-8},
doi = {https://doi.org/10.1016/B0-08-043076-7/01602-8},
url = {https://www.sciencedirect.com/science/article/pii/B0080430767016028},
author = {K. Dunbar},
abstract = {The cognitive mechanisms underlying scientific thinking and discovery have been investigated using approaches from cognitive psychology, cognitive science, and artificial intelligence. In this article, six overlapping approaches are discussed. First, historical analyses and interviews have provided important information on the types of thinking involved in particular discoveries or used by individual scientists. Second, scientific reasoning has been thought of as a form of inductive thinking, and as a form of problem solving. Researchers using this approach have delineated some of the problem solving and inductive reasoning strategies used in science. Third, much research on errors in scientific reasoning, particularly on the topic of ‘confirmation bias’ has revealed some of the circumstances under which science can go awry. Fourth, many researchers have investigated how children's thinking is similar to, or different from, that of scientists. A fifth approach has been to investigate scientists reasoning live or ‘in vivo’ in their own labs. This work has shown how processes such as analogy, distributed cognition, and specific types of inductive and deductive reasoning strategies are used together by scientists. Finally, the incorporation of cognitive mechanisms into computer programs that make discoveries is seen as an important development in the cognitive psychology of scientific thinking.}
}
@article{LYU2023366,
title = {Application of Deep Learning in the Search and a Certain Celestial Body},
journal = {Procedia Computer Science},
volume = {228},
pages = {366-372},
year = {2023},
note = {3rd International Conference on Machine Learning and Big Data Analytics for IoT Security and Privacy},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.11.042},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923018665},
author = {Yang Lyu and Donglin Su},
keywords = {Deep Learning, Search Analysis, Research Applications, a Certain Celestial Body},
abstract = {With the development of the times, celestial body search has become increasingly common, and people want to enhance their understanding of the universe through further search of celestial bodies. Nowadays, many software and hardware have been invented to assist in celestial body search, but the computational efficiency and efficiency of current software are still insufficient. So this article focuses on the research and application of Deep Learning (DL) in the search and analysis of "a certain celestial body", aiming to improve celestial search through DL. Through experiments, this article uses DL to achieve a maximum computational rate of 78% and a minimum of 70% for celestial search. The computational efficiency of celestial search without DL can reach up to 68% and 57%, respectively. After using DL, the efficiency of celestial search reaches up to 82% and 70%, while before using DL, the efficiency of celestial search reaches up to 62% and 50%, respectively. From this data, it can be seen that DL can achieve good results in celestial search.}
}
@article{XU2024473,
title = {Review of the continuous catalytic ortho-para hydrogen conversion technology for hydrogen liquefaction},
journal = {International Journal of Hydrogen Energy},
volume = {62},
pages = {473-487},
year = {2024},
issn = {0360-3199},
doi = {https://doi.org/10.1016/j.ijhydene.2024.03.085},
url = {https://www.sciencedirect.com/science/article/pii/S0360319924009170},
author = {Pan Xu and Jian Wen and Ke Li and Simin Wang and Yanzhong Li},
keywords = {Hydrogen liquefaction, Continuous catalytic ortho-para hydrogen conversion technology, Ortho-para hydrogen conversion catalyst, Packing layer, Plate-fin heat exchanger},
abstract = {In order to meet rapidly growing demand of liquid hydrogen in the future hydrogen industry and energy structure, the continuous catalytic ortho-para hydrogen conversion technology (CCOPHCT) has been once again proposed and has become an important choice to improve the hydrogen liquefaction units (HLUs). The origin, concept, and research progress of the CCOPHCT are systematically reviewed for the first time in this paper. However, the research depth and breadth of the CCOPHCT are insufficient to support its current application. To solve it, for the continuous catalytic ortho-para hydrogen conversion plate-fin heat exchanger (CCOPHC-PFHE) with better comprehensive performances, this paper comprehensively summarizes the research achievements in the related fields from the perspective of the unit analysis, including the ortho-para hydrogen conversion (OPHC), packing layer and plate-fin heat exchanger (PFHE), which to provide further thinking for the development of the CCOPHC-PFHE. Further, some suggestions for the CCOPHCT are proposed based on the existed research foundations, including preparing the effective ortho-para hydrogen conversion catalyst (OPHCC), developing the accurate OPHC dynamical model, revealing the coupling mechanism in the packing layer filled with the OPHCC and establishing an effective design method and standard of the CCOPHC-PFHE. In addition, considering the special conditions on the CCOPHC-PFHE, the importance of the experimental research is emphasized. And based on the established hydrogen experimental platform with the comprehensive supporting implementations, the experimental device of the CCOPHC-PFHE has been completed and a series of experimental tests are currently in progressing.}
}
@article{KRONICK2011435,
title = {Compensatory beliefs and intentions contribute to the prediction of caloric intake in dieters},
journal = {Appetite},
volume = {57},
number = {2},
pages = {435-438},
year = {2011},
issn = {0195-6663},
doi = {https://doi.org/10.1016/j.appet.2011.05.306},
url = {https://www.sciencedirect.com/science/article/pii/S0195666311004636},
author = {Ilana Kronick and Randy P. Auerbach and Christine Stich and Bärbel Knäuper},
keywords = {Compensatory beliefs, Compensatory intentions, Restraint, Disinhibition, Caloric intake, Experience sampling methodology},
abstract = {One cognitive process that impacts dieters’ decision to indulge is the activation of compensatory beliefs. Compensatory beliefs (CBs) are convictions that the consequences of engaging in an indulgent behaviour (eating cake) can be neutralized by the effects of another behaviour (skipping dinner). Using experience sampling methodology, this study hypothesized that, in addition to the cognitive processes associated with restraint and disinhibition, compensatory thinking contributes to the prediction of caloric intake. Results indicated that higher scores on CB, CI and TFEQ-D predicted a greater number of portions eaten signifying that, along with disinhibition, compensatory thinking predicts caloric intake in dieters.}
}
@article{CAO2024101200,
title = {Explanatory models in neuroscience, Part 2: Functional intelligibility and the contravariance principle},
journal = {Cognitive Systems Research},
volume = {85},
pages = {101200},
year = {2024},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2023.101200},
url = {https://www.sciencedirect.com/science/article/pii/S1389041723001341},
author = {Rosa Cao and Daniel Yamins},
keywords = {Evolution, Contravariance, Intelligibility, Function, Optimization, No-miracles, Instrumentalism, Realism, Philosophy, Constraints, Evolutionary landscape, Models, Explanation, Evo-devo, Development, Learning, Deep learning, Abstraction},
abstract = {Computational modeling plays an increasingly important role in neuroscience, highlighting the philosophical question of how computational models explain. In the particular case of neural network models, concerns have been raised about their intelligibility, and how these models relate (if at all) to what is found in the brain. We claim that what makes a system intelligible is an understanding of the dependencies between its behavior and the factors that are responsible for that behavior. In biology, many of these dependencies are naturally “top-down”, as ethological imperatives interact with evolutionary and developmental constraints under natural selection to produce systems with capabilities and behaviors appropriate to their evolutionary needs. We describe how the optimization techniques used to construct neural network models capture some key aspects of these dependencies, and thus help explain why brain systems are as they are — because when a challenging ecologically-relevant goal is shared by a neural network and the brain, it places constraints on the possible mechanisms exhibited in both kinds of systems. The presence and strength of these constraints explain why some outcomes are more likely than others. By combining two familiar modes of explanation — one based on bottom-up mechanistic description (whose relation to neural network models we address in a companion paper) and the other based on top-down constraints, these models have the potential to illuminate brain function.}
}
@article{SUDHESHWAR2024108305,
title = {Learning from Safe-by-Design for Safe-and-Sustainable-by-Design: Mapping the current landscape of Safe-by-Design reviews, case studies, and frameworks},
journal = {Environment International},
volume = {183},
pages = {108305},
year = {2024},
issn = {0160-4120},
doi = {https://doi.org/10.1016/j.envint.2023.108305},
url = {https://www.sciencedirect.com/science/article/pii/S0160412023005780},
author = {Akshat Sudheshwar and Christina Apel and Klaus Kümmerer and Zhanyun Wang and Lya G. Soeteman-Hernández and Eugenia Valsami-Jones and Claudia Som and Bernd Nowack},
keywords = {Safe-by-Design (SbD), Safe and Sustainable-by-Design (SSbD), Literature mapping, SSbD implementation},
abstract = {With the introduction of the European Commission's “Safe and Sustainable-by-Design” (SSbD) framework, the interest in understanding the implications of safety and sustainability assessments of chemicals, materials, and processes at early-innovation stages has skyrocketed. Our study focuses on the “Safe-by-Design” (SbD) approach from the nanomaterials sector, which predates the SSbD framework. In this assessment, SbD studies have been compiled and categorized into reviews, case studies, and frameworks. Reviews of SbD tools have been further classified as quantitative, qualitative, or toolboxes and repositories. We assessed the SbD case studies and classified them into three categories: safe(r)-by-modeling, safe(r)-by-selection, or safe(r)-by-redesign. This classification enabled us to understand past SbD work and subsequently use it to define future SSbD work so as to avoid confusion and possibilities of “SSbD-washing” (similar to greenwashing). Finally, the preexisting SbD frameworks have been studied and contextualized against the SSbD framework. Several key recommendations for SSbD based on our analysis can be made. Knowledge gained from existing approaches such as SbD, green and sustainable chemistry, and benign-by-design approaches needs to be preserved and effectively transferred to SSbD. Better incorporation of chemical and material functionality into the SSbD framework is required. The concept of lifecycle thinking and the stage-gate innovation model need to be reconciled for SSbD. The development of high-throughput screening models is critical for the operationalization of SSbD. We conclude that the rapid pace of both SbD and SSbD development necessitates a regular mapping of the newly published literature that is relevant to this field.}
}
@incollection{MILLER2023169,
title = {Chapter 9 - Doctoral and professional programs},
editor = {Susan Miller and Walter Moos and Barbara Munk and Stephen Munk and Charles Hart and David Spellmeyer},
booktitle = {Managing the Drug Discovery Process (Second Edition)},
publisher = {Woodhead Publishing},
edition = {Second Edition},
pages = {169-196},
year = {2023},
series = {Woodhead Publishing Series in Biomedicine},
isbn = {978-0-12-824304-6},
doi = {https://doi.org/10.1016/B978-0-12-824304-6.00013-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128243046000134},
author = {Susan Miller and Walter Moos and Barbara Munk and Stephen Munk and Charles Hart and David Spellmeyer},
keywords = {Basic/applied/clinical, Career/job, Collaboration/teams, Critical thinking, -index, PhD/PharmD, Postdoc/postdoctoral, Problem identification, Research design, Writing/publishing},
abstract = {In this chapter on graduate and professional education, we explore doctoral and professional programs, posing a number of key questions you should ask yourself. Where to apply to graduate school or a postdoc, and why? With whom should you work? A PhD or a PharmD? What must you do to be successful? Moreover, we touch on traits important to becoming an independent researcher and ask whether success in graduate school or a postdoctoral fellowship requires different skills than undergraduate degrees. Critical thinking habits underpin this discussion. We outline possible career choices—jobs!—touching on the knowledge and expertise used by drug hunters, and also ask what might be of most value to potential employers. Each of us is different, and what’s best for you is something you will have to decipher, but hopefully you will consult with family, friends, and advisors or mentors before making a final decision. Regardless, “the big leap” is coming, so get ready.}
}
@article{JAYAPRAKASAM2015229,
title = {PSOGSA-Explore: A new hybrid metaheuristic approach for beampattern optimization in collaborative beamforming},
journal = {Applied Soft Computing},
volume = {30},
pages = {229-237},
year = {2015},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2015.01.024},
url = {https://www.sciencedirect.com/science/article/pii/S1568494615000435},
author = {S. Jayaprakasam and S.K.A. Rahim and Chee Yen Leow},
keywords = {Collaborative beamforming, Random array, Sidelobe suppression, Particle swarm optimization (PSO), Gravitational search algorithm (GSA)},
abstract = {A conventional collaborative beamforming (CB) system suffers from high sidelobes due to the random positioning of the nodes. This paper introduces a hybrid metaheuristic optimization algorithm called the Particle Swarm Optimization and Gravitational Search Algorithm-Explore (PSOGSA-E) to suppress the peak sidelobe level (PSL) in CB, by the means of finding the best weight for each node. The proposed algorithm combines the local search ability of the gravitational search algorithm (GSA) with the social thinking skills of the legacy particle swarm optimization (PSO) and allows exploration to avoid premature convergence. The proposed algorithm also simplifies the cost of variable parameter tuning compared to the legacy optimization algorithms. Simulations show that the proposed PSOGSA-E outperforms the conventional, the legacy PSO, GSA and PSOGSA optimized collaborative beamformer by obtaining better results faster, producing up to 100% improvement in PSL reduction when the disk size is small.}
}
@article{PARTTO2012442,
title = {Explaining failures in innovative thought processes in engineering design},
journal = {Procedia - Social and Behavioral Sciences},
volume = {41},
pages = {442-449},
year = {2012},
note = {The First International Conference on Leadership, Technology and Innovation Management},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2012.04.053},
url = {https://www.sciencedirect.com/science/article/pii/S1877042812009330},
author = {Minna Pärttö and Pertti Saariluoma},
keywords = {Microinnovation processes, engineering design, thought errors, thought failures},
abstract = {The aim of this study is to explore factors causing failures in innovative thought processes in engineering design. An innovation process is here understood as a complex and multi-phased thinking and problem solving process generating new and mostly unforeseeable solutions. The phases are partly overlapping and simultaneous. This complicated nature of innovation process demands a lot from innovation management, and thus it is not unusual that innovation processes fail. Identifying problems and shortcomings is important because it helps organizations to eliminate them in the future. This study focus on thought processes of individual participants in an innovation process, which is referred by us as microinnovation approach. This approach understands innovations as being based on human thinking.This study shows that factors related to knowledge, management and interaction are causing failures in engineering design. We found haste to be the most common reason for failures. Other contributing factors were lack of long-term thinking and inability to understand others’ perspective.}
}
@article{KALBANDE2023138474,
title = {Machine learning based quantification of VOC contribution in surface ozone prediction},
journal = {Chemosphere},
volume = {326},
pages = {138474},
year = {2023},
issn = {0045-6535},
doi = {https://doi.org/10.1016/j.chemosphere.2023.138474},
url = {https://www.sciencedirect.com/science/article/pii/S0045653523007415},
author = {Ritesh Kalbande and Bipin Kumar and Sujit Maji and Ravi Yadav and Kaustubh Atey and Devendra Singh Rathore and Gufran Beig},
keywords = {Ozone, VOCs, Machine learning, Meteorology, Isoprene},
abstract = {The prediction of surface ozone is essential attributing to its impact on human and environmental health. Volatile organic compounds (VOCs) are crucial in driving ozone concentration; particularly in urban areas where VOC limited regimes are prominent. The limited measurements of VOCs, however, hinder assessing the VOC-ozone relationship. This work applies machine learning (ML) algorithms for temporal forecasting of surface ozone over a metropolitan city in India. The availability of continuous VOCs measurement data along with meteorology and other pollutants during 2014–2016 makes it possible to deduce the influence of various input parameters on surface ozone prediction. After evaluating the best ML model for ozone prediction, simulations were carried out using varied input combinations. The combination with isoprene, meteorology, NOx, and CO (Isop + MNC) was the best with RMSE 4.41 ppbv and MAPE 6.77%. A season-wise comparison of simulations having all data, only meteorological data and Isop + MNC as input showed that Isop + MNC simulation gives the best results during the summer season (RMSE: 5.86 ppbv, MAPE: 7.05%). This shows the increased ability of the model to capture ozone peaks (high ozone during summer) relatively better when isoprene data is used. The overall results highlight that using all available data doesn't necessarily give best prediction results; also critical thinking is essential when evaluating the model results.}
}
@article{JOHNSTON2003325,
title = {Biological computation of image motion from flows over boundaries},
journal = {Journal of Physiology-Paris},
volume = {97},
number = {2},
pages = {325-334},
year = {2003},
note = {Neurogeometry and visual perception},
issn = {0928-4257},
doi = {https://doi.org/10.1016/j.jphysparis.2003.09.016},
url = {https://www.sciencedirect.com/science/article/pii/S0928425703000664},
author = {A. Johnston and P.W. McOwan and C.P. Benton},
keywords = {Optic flow, Cortex, Differential forms, Vision, Motion},
abstract = {A theory of early motion processing in the human and primate visual system is presented which is based on the idea that spatio-temporal retinal image data is represented in primary visual cortex by a truncated 3D Taylor expansion that we refer to as a jet vector. This representation allows all the concepts of differential geometry to be applied to the analysis of visual information processing. We show in particular how the generalised Stokes theorem can be used to move from the calculation of derivatives of image brightness at a point to the calculation of image brightness differences on the boundary of a volume in space–time and how this can be generalised to apply to integrals of products of derivatives. We also provide novel interpretations of the roles of direction selective, bi-directional and pan-directional cells and of type I and type II cells in V5/MT.}
}
@article{GOERTZEL199595,
title = {Self-reference, computation, and mind},
journal = {Journal of Social and Evolutionary Systems},
volume = {18},
number = {1},
pages = {95-101},
year = {1995},
issn = {1061-7361},
doi = {https://doi.org/10.1016/1061-7361(95)90018-7},
url = {https://www.sciencedirect.com/science/article/pii/1061736195900187},
author = {Ben Goertzel and Harold Bowman}
}
@incollection{PAUL2005431,
title = {Chapter 15 - Models of Computation for Systems-on-Chips},
editor = {Ahmed Amine Jerraya and Wayne Wolf},
booktitle = {Multiprocessor Systems-on-Chips},
publisher = {Morgan Kaufmann},
address = {San Francisco},
pages = {431-463},
year = {2005},
series = {Systems on Silicon},
issn = {18759661},
doi = {https://doi.org/10.1016/B978-012385251-9/50031-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780123852519500311},
author = {JoAnn M. Paul and Donald E. Thomas},
abstract = {Publisher Summary
This chapter describes system modeling and its relationship to models of computation. It compares several different models of computation and evaluates their usefulness at various stages in system design. It also describes the modeling environment for software and hardware (MESH) environment for hardware and software modeling. Models of computation (MoCs) are abstract representations of computing systems. Computer modeling can be separated into three areas—formal MoCs, computer artifacts, and computer design tools. A formal MoC is generally considered to be one with a mathematical basis. Simulations of formal models may be more efficient for large systems; however, the properties of formal models permit the representation of the system to be manipulated purely mathematically. Computer artifacts are the objects of computer architects. They include software, hardware, or both. Design tools are computer programs that are used to assist the construction of instances of computers as well as the conceptualization of computer artifacts. Design tools may be considered synonymous with design artifacts because they are objects, or entities, used to facilitate the design process. They may have a formal mathematical basis.}
}
@incollection{GISZTER2007323,
title = {Primitives, premotor drives, and pattern generation: a combined computational and neuroethological perspective},
editor = {Paul Cisek and Trevor Drew and John F. Kalaska},
series = {Progress in Brain Research},
publisher = {Elsevier},
volume = {165},
pages = {323-346},
year = {2007},
booktitle = {Computational Neuroscience: Theoretical Insights into Brain Function},
issn = {0079-6123},
doi = {https://doi.org/10.1016/S0079-6123(06)65020-6},
url = {https://www.sciencedirect.com/science/article/pii/S0079612306650206},
author = {Simon Giszter and Vidyangi Patil and Corey Hart},
keywords = {primitives, motor synergies, force-fields, modularity, feedback, motor pattern analysis, decomposition, rhythm generation, pattern shaping},
abstract = {A modular motor organization may be needed to solve the degrees of freedom problem in biological motor control. Reflex elements, kinematic primitives, muscle synergies, force-field primitives and/or pattern generators all have experimental support as modular elements. We discuss the possible relations of force-field primitives, spinal feedback systems, and pattern generation and shaping systems in detail, and review methods for examining underlying motor pattern structure in intact or semi-intact behaving animals. The divisions of systems into primitives, synergies, and rhythmic elements or oscillators suggest specific functions and methods of construction of movement. We briefly discuss the limitations and caveats needed in these interpretations given current knowledge, together with some of the hypotheses arising from these frameworks.}
}
@article{SONI2024100016,
title = {Advancements in MXene-based electrocatalysts for hydrogen evolution reaction processes: A comprehensive review},
journal = {Journal of Alloys and Compounds Communications},
volume = {3},
pages = {100016},
year = {2024},
issn = {2950-2845},
doi = {https://doi.org/10.1016/j.jacomc.2024.100016},
url = {https://www.sciencedirect.com/science/article/pii/S295028452400016X},
author = {Kunjal Soni and Rakesh Kumar Ameta},
keywords = {MXenes, 2D materials, Two-electron transfer process, Hydrogen evolution process, Electrocatalysts},
abstract = {MXenes are a newly emerging family of two-dimensional (2D) materials that include carbonitrides, nitrides, and carbides of transition metals. They have attracted much interest from scientists and researchers due to their potential use in electrocatalysts, where a two-electron transfer process is applied. Their remarkable properties, such as strong chemical and structural stability, high electrical conductivity, and large active surface area, make them effective for their potential in advanced hydrogen evolution reactions (HER). This thorough analysis starts by carefully outlining the forward-thinking advances in MXene synthesis and development. It then explores the theoretical and empirical aspects of MXene-based HER electrocatalysts. This review paper presents methods for improving the HER catalytic activity of MXene, including terminal modification, metal-atom doping, and the creation of various nanostructures to increase the density of active sites. The study clarifies current issues and new opportunities and provides a valuable framework for the future development of effective MXene-based electrocatalysts for HERs.}
}
@incollection{JAIN2015181,
title = {Chapter Seven - Computational Methods for RNA Structure Validation and Improvement},
editor = {Sarah A. Woodson and Frédéric H.T. Allain},
series = {Methods in Enzymology},
publisher = {Academic Press},
volume = {558},
pages = {181-212},
year = {2015},
booktitle = {Structures of Large RNA Molecules and Their Complexes},
issn = {0076-6879},
doi = {https://doi.org/10.1016/bs.mie.2015.01.007},
url = {https://www.sciencedirect.com/science/article/pii/S0076687915000208},
author = {Swati Jain and David C. Richardson and Jane S. Richardson},
keywords = {RNA crystallography, RNA backbone conformers, Ribose pucker, Clash correction, MolProbity, PHENIX, ERRASER, wwPDB validation},
abstract = {With increasing recognition of the roles RNA molecules and RNA/protein complexes play in an unexpected variety of biological processes, understanding of RNA structure–function relationships is of high current importance. To make clean biological interpretations from three-dimensional structures, it is imperative to have high-quality, accurate RNA crystal structures available, and the community has thoroughly embraced that goal. However, due to the many degrees of freedom inherent in RNA structure (especially for the backbone), it is a significant challenge to succeed in building accurate experimental models for RNA structures. This chapter describes the tools and techniques our research group and our collaborators have developed over the years to help RNA structural biologists both evaluate and achieve better accuracy. Expert analysis of large, high-resolution, quality-conscious RNA datasets provides the fundamental information that enables automated methods for robust and efficient error diagnosis in validating RNA structures at all resolutions. The even more crucial goal of correcting the diagnosed outliers has steadily developed toward highly effective, computationally based techniques. Automation enables solving complex issues in large RNA structures, but cannot circumvent the need for thoughtful examination of local details, and so we also provide some guidance for interpreting and acting on the results of current structure validation for RNA.}
}
@article{VANHOOIJDONK2022101044,
title = {Creativity and change of context: The influence of object-context (in)congruency on cognitive flexibility},
journal = {Thinking Skills and Creativity},
volume = {45},
pages = {101044},
year = {2022},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2022.101044},
url = {https://www.sciencedirect.com/science/article/pii/S1871187122000475},
author = {Mare {van Hooijdonk} and Simone M. Ritter and Marcel Linka and Evelyn Kroesbergen},
keywords = {Creativity, Spatial context, Object congruence, Cognitive flexibility, Stimulating creativity},
abstract = {Specific environmental features, such as natural settings or spatial design, can foster creativity. The effect of object-context congruency on creativity has not yet been investigated. While congruence between an object and its visual context provides meaning to the object, it may hamper creativity due to mental fixation effects. In the current study, virtual reality technology (VR) was employed to examine the hypothesis that people display more cognitive flexibility - a key element of creativity, representing the ability to overcome mental fixation - when thinking about an object while being in an incongruent than in a congruent environment. Participants (N = 184) performed an Alternative Uses Task, in which they had to name as many uses for a book as possible, while being immersed in a virtual environment that was either object-context congruent (i.e., places where you would expect a book; e.g., a library or a living room; n = 91) or object-context incongruent (i.e., places where a book is not expected; e.g., a clothing store or a car workshop; n = 93). The effect of object (in)congruency was also assessed for three other indices of creativity: fluency (i.e., the number of ideas generated), originality and usefulness. In line with our hypothesis, participants scored higher on pure cognitive flexibility in the object-context incongruent than in the object-context congruent environment. Moreover, participants in the object-context incongruent environment condition generated more original ideas. The theoretical and practical implications of the current findings are discussed.}
}
@article{PUPUNWIWAT2011827,
title = {Conceptual Selective RFID Anti-Collision Technique Management},
journal = {Procedia Computer Science},
volume = {5},
pages = {827-834},
year = {2011},
note = {The 2nd International Conference on Ambient Systems, Networks and Technologies (ANT-2011) / The 8th International Conference on Mobile Web Information Systems (MobiWIS 2011)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2011.07.114},
url = {https://www.sciencedirect.com/science/article/pii/S187705091100439X},
author = {Prapassara Pupunwiwat and Peter Darcy and Bela Stantic},
keywords = {Radio Frequency Identification (RFID), Anti-Collision, Decision Tree, Six Thinking Hats},
abstract = {Radio Frequency Identification (RFID) uses wireless radio frequency technology to automatically identify tagged objects. Despite the extensive development of RFID technology, tag collisions still remains a major drawback. The collision issue can be solved by using anti-collision techniques. While existing research has focused on improving anti-collision methods alone, it is also essential that a suitable type of anti-collision algorithm is selected for the specific circumstance. In this work, we evaluate anti-collision techniques and perform a comparative analysis in order to find the advantages and disadvantages of each approach. To identify the best anti-collision selection method in various scenarios, we have proposed two strategies for selective anti-collision technique management: a “Novel Decision Tree Strategy” and a “Six Thinking Hats Strategy”. We have shown that the selection of the correct technique for specific scenarios improve the quality of the data collection which, in turn, will increase the integrity of the data after being transformed, aggregated, and used for event processing.}
}
@article{JUDD1997907,
title = {Computational economics and economic theory: Substitutes or complements?},
journal = {Journal of Economic Dynamics and Control},
volume = {21},
number = {6},
pages = {907-942},
year = {1997},
note = {Society of Computational Economics Conference},
issn = {0165-1889},
doi = {https://doi.org/10.1016/S0165-1889(97)00010-9},
url = {https://www.sciencedirect.com/science/article/pii/S0165188997000109},
author = {Kenneth L. Judd},
keywords = {Computational approach, Theoretical analysis},
abstract = {This essay examines the idea and potential of a ‘computational approach to theory’, discusses methodological issues raised by such computational methods, and outlines the problems associated with the dissemination of computational methods and the exposition of computational results. We argue that the study of a theory need not be confined to proving theorems, that current and future computer technologies create new possibilities for theoretical analysis, and that by resolving these issues we will create an intellectual atmosphere in which computational methods can make substantial contributions to economic analysis.}
}
@article{ZHU2025117923,
title = {Prediction of damage evolution in CMCs considering the real microstructures through a deep-learning scheme},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {439},
pages = {117923},
year = {2025},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2025.117923},
url = {https://www.sciencedirect.com/science/article/pii/S0045782525001951},
author = {Rongqi Zhu and Guohao Niu and Panding Wang and Chunwang He and Zhaoliang Qu and Daining Fang},
keywords = {Deep learning, Damage evolution, Ceramic matrix composites, Microstructure, Computed tomography},
abstract = {The real microstructures of ceramic matrix composites (CMCs) play a crucial role in determining their damage behavior. However, considering the real microstructure within the high-fidelity numerical simulation usually leads to expensive computational costs. In this study, an end-to-end deep-learning (DL) framework is proposed to predict the evolution of damage fields for CMCs from their real microstructures, which are characterized through computed tomography (CT). Three sub-networks, including the microstructure processing network (MPN), elastic deformation prediction network (EPN), and damage sequence prediction network (DPN), are used to construct a two-stage DL model. In the first stage, the geometrical characteristics of real microstructure are precisely captured by the MPN with over 92 % precision for the yarns and matrix. In the second stage, the elastic deformation predicted by the EPN is taken as the intermediate variable to motivate the damage prediction of DPN with the MPN-predicted microstructure as input. The damage evolution of real microstructure is finally predicted with a mean relative error of 10.8 % for the primary damage variable fields. The high-damage regions in the microstructure can also be accurately captured with a mean precision of 87.9 %. The proposed model is further validated by the in-situ tensile experiment. The micro-cracks are proven to initiate and propagate in the high-damage regions. Compared with the high-fidelity numerical methods, this DL-based method can predict the damage evolution on the fly, avoiding time-consuming computation and poor convergence during the damage analysis.}
}
@article{ZHAO2024109027,
title = {Towards the definition of spatial granules},
journal = {Fuzzy Sets and Systems},
volume = {490},
pages = {109027},
year = {2024},
issn = {0165-0114},
doi = {https://doi.org/10.1016/j.fss.2024.109027},
url = {https://www.sciencedirect.com/science/article/pii/S0165011424001738},
author = {Liquan Zhao and Yiyu Yao},
keywords = {Granularity, Fineness, Subsethood, Coarse-fine relation, Quotient join, Quotient meet, Granular space, Spatial granular computing},
abstract = {Three basic issues of granular computing are construction or definition of granules, measures of granules, and computation or reasoning with granules. This paper reviews the main theories of granular computing and introduces the definition of spatial granules. A granule is composed of one or more atomic granules. The rationality of this definition is explained from the four aspects: simplicity, applicability, measurability and visualization. A one-to-one correspondence is established between the granules and the points in the unit hypercube, and the coarsening and refining of the granules are the descending and ascending dimensions of the points, respectively. The weak fuzzy tolerance relation and weak fuzzy equivalence relation are defined so as to study on all fuzzy binary relations. The notion of layer granularity/fineness is introduced and each granule can be easily denoted by two numbers, which can be used to pre-process macro knowledge space and greatly improve the search speed. This paper also discusses the main properties of granules including the necessary and sufficient conditions of coarse-fine relation and the main principles of granular space.}
}
@article{AMEL2023104187,
title = {Toward an automatic detection of cardiac structures in short and long axis views},
journal = {Biomedical Signal Processing and Control},
volume = {79},
pages = {104187},
year = {2023},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2022.104187},
url = {https://www.sciencedirect.com/science/article/pii/S1746809422006413},
author = {Laidi Amel and Mohammed Ammar and Mostafa {El Habib Daho} and Said Mahmoudi},
keywords = {Cardiac MRI Segmentation, Shape Descriptors, Particle Swarm Optimization, Residual Network, Interpretability},
abstract = {Objective
This work aims to create an automatic detection process of cardiac structures in both short-axis and long-axis views. A workflow inspired by human thinking process, for better explainability.
Methods
we began by separating the images into two classes: long axis and short axis, using a Residual Network model. Then, we used Particle Swarm Optimization for general segmentation. After segmentation, a characterization step based on shape descriptors calculated from bounding box and ANOVA for features selection were applied on the binary images to detect the location of each region of interest: lung, left and right ventricle in the short-axis view, the aorta, the left heart (left atrium and ventricle), and the right heart (right atrium and ventricle) in the long axis view.
Results
we achieved a 90% accuracy on view separation. We have selected: Elongation, Compactness, Circularity, Type Factor, for short axis identification; and:Area, Centre of Mass Y, Moment of Inertia XY, Moment of Inertia YY, for long axis identification.
Conclusion
a successful separation of long axis and short axis views allows for a better characterization and detection of segmented cardiac structures. After that, any method can be applied for segmentation, attribute selection, and classification.
Significance
an attempt to introduce explainability into cardiac image segmentation, we tried to mimic the human workflow while computerizing each step. The process seems to be valid and added clarity and interpretability to the detection.}
}
@article{MARRET2025597,
title = {Turning the kaleidoscope: Innovations shaping the future of clinical trial design},
journal = {Cancer Cell},
volume = {43},
number = {4},
pages = {597-605},
year = {2025},
issn = {1535-6108},
doi = {https://doi.org/10.1016/j.ccell.2025.02.019},
url = {https://www.sciencedirect.com/science/article/pii/S1535610825000716},
author = {Grégoire Marret and Mercedes Herrera and Lillian L. Siu},
abstract = {Current clinical trials are based on rigid designs and drug-centric approaches that can stifle flexibility and innovation. With advances in molecular biology and technology, there is an urgent call to revitalize trial designs to meet these evolving demands. We propose a reshaped, prismatic vision of clinical trials combining different knowledge layers, synergized with modern computational approaches. This paradigm based on iterative learning will enable a more adaptive and precise framework for oncology drug development.}
}
@article{JORAJURIA2022664,
title = {Oscillatory Source Tensor Discriminant Analysis (OSTDA): A regularized tensor pipeline for SSVEP-based BCI systems},
journal = {Neurocomputing},
volume = {492},
pages = {664-675},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.07.103},
url = {https://www.sciencedirect.com/science/article/pii/S0925231221018956},
author = {Tania Jorajuría and Mina {Jamshidi Idaji} and Zafer İşcan and Marisol Gómez and Vadim V. Nikulin and Carmen Vidaurre},
keywords = {Brain-computer interface, Steady-state visual evoked potential, Spatio-spectral decomposition, Higher order discriminant analysis, Analytical regularization, Tensor-based feature reduction},
abstract = {Periodic signals called Steady-State Visual Evoked Potentials (SSVEP) are elicited in the brain by flickering stimuli. They are usually detected by means of regression techniques that need relatively long trial lengths to provide feedback and/or sufficient number of calibration trials to be reliably estimated in the context of brain-computer interface (BCI). Thus, for BCI systems designed to operate with SSVEP signals, reliability is achieved at the expense of speed or extra recording time. Furthermore, regardless of the trial length, calibration free regression-based methods have been shown to suffer from significant performance drops when cognitive perturbations are present affecting the attention to the flickering stimuli. In this study we present a novel technique called Oscillatory Source Tensor Discriminant Analysis (OSTDA) that extracts oscillatory sources and classifies them using the newly developed tensor-based discriminant analysis with shrinkage. The proposed approach is robust for small sample size settings where only a few calibration trials are available. Besides, it works well with both low- and high-number-of-channel settings, using trials as short as one second. OSTDA performs similarly or significantly better than other three benchmarked state-of-the-art techniques under different experimental settings, including those with cognitive disturbances (i.e. four datasets with control, listening, speaking and thinking conditions). Overall, in this paper we show that OSTDA is the only pipeline among all the studied ones that can achieve optimal results in all analyzed conditions.}
}
@article{ROY2020106210,
title = {Fixed subgroups and computation of auto-fixed closures in free-abelian times free groups},
journal = {Journal of Pure and Applied Algebra},
volume = {224},
number = {4},
pages = {106210},
year = {2020},
issn = {0022-4049},
doi = {https://doi.org/10.1016/j.jpaa.2019.106210},
url = {https://www.sciencedirect.com/science/article/pii/S0022404919302178},
author = {Mallika Roy and Enric Ventura},
keywords = {Free-abelian times free, Automorphism, Fixed subgroup, Periodic subgroup, Auto-fixed closure},
abstract = {The classical result by Dyer–Scott about fixed subgroups of finite order automorphisms of Fn being free factors of Fn is no longer true in Zm×Fn. Within this more general context, we prove a relaxed version in the spirit of Bestvina–Handel Theorem: the rank of fixed subgroups of finite order automorphisms is uniformly bounded in terms of m,n. We also study periodic points of endomorphisms of Zm×Fn, and give an algorithm to compute auto-fixed closures of finitely generated subgroups of Zm×Fn. On the way, we prove the analog of Day's Theorem for real elements in Zm×Fn, contributing a modest step into the project of doing so for any right angled Artin group (as McCool did with respect to Whitehead's Theorem in the free context).}
}
@article{FAELENS2021106510,
title = {Social media use and well-being: A prospective experience-sampling study},
journal = {Computers in Human Behavior},
volume = {114},
pages = {106510},
year = {2021},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2020.106510},
url = {https://www.sciencedirect.com/science/article/pii/S0747563220302624},
author = {Lien Faelens and Kristof Hoorelbeke and Bart Soenens and Kyle {Van Gaeveren} and Lieven {De Marez} and Rudi {De Raedt} and Ernst H.W. Koster},
keywords = {Social media, Social comparison, Self-esteem, Repetitive negative thinking, Negative affect},
abstract = {Facebook and Instagram are currently the most popular Social Network Sites (SNS) for young adults. A large amount of research examined the relationship between these SNS and well-being, and possible intermediate constructs such as social comparison, self-esteem, and repetitive negative thinking (RNT). However, most of these studies have cross-sectional designs and use self-report indicators of SNS use. Therefore, their conclusions should be interpreted cautiously. Consequently, the goal of the current experience sampling study was to examine the temporal dynamics between objective indicators of SNS use, and self-reports of social comparison, RNT, and daily fluctuations in negative affect. More specifically, we assessed 98 participants 6 times per day during 14 days to examine reciprocal relationships between SNS use, negative affect, emotion regulation, and key psychological constructs. Results indicate that (1) both Facebook and Instagram use predicted reduced well-being, and (2) self-esteem and RNT appear to be important intermediate constructs in these relationships. Future longitudinal and experimental studies are needed to further support and extend the current research findings.}
}
@article{EDELMAN1997296,
title = {Computational theories of object recognition},
journal = {Trends in Cognitive Sciences},
volume = {1},
number = {8},
pages = {296-304},
year = {1997},
issn = {1364-6613},
doi = {https://doi.org/10.1016/S1364-6613(97)01090-5},
url = {https://www.sciencedirect.com/science/article/pii/S1364661397010905},
author = {S. Edelman},
abstract = {This paper examines four current theoretical approaches to the representation and recognition of visual objects: structural descriptions, geometric constraints, multidimensional feature spaces and shape-space approximation. The strengths and weaknesses of the four theories are considered, with a special focus on their approach to categorization — a computationally challenging task which is not widely addressed in computer vision, where the stress is rather on the generalization of recognition across changes of viewpoint.}
}
@article{JIANG2024109208,
title = {Surrogate-based Shape Optimization Design for the Stable Descent of Mars Parachutes},
journal = {Aerospace Science and Technology},
volume = {150},
pages = {109208},
year = {2024},
issn = {1270-9638},
doi = {https://doi.org/10.1016/j.ast.2024.109208},
url = {https://www.sciencedirect.com/science/article/pii/S1270963824003390},
author = {Lulu Jiang and Guanhua Chen and Xiaopeng Xue and Xin Pan and Gang Chen},
keywords = {Supersonic Parachute, Mars atmosphere, Aerodynamic Optimization, Shape design, Wide Speed Range},
abstract = {The crucial role that the supersonic parachute plays in space exploration missions has been widely recognized, as it directly impacts the safe landing of probes. However, parachute models with optimization on different aerodynamic performances often involve design conflicts with each other. Additionally, the parachute design focusing on a single point cannot fully adapt to different speed ranges during stable descent. This complexity makes it challenging to use traditional shape design methods, which rely on empirical knowledge, to address these coupled design issues. Faced with the design challenges of Mars parachutes, this study, inspired by aircraft aerodynamic optimization principles, establishes a shape design method specifically for the stable descent phase of Mars parachutes. The method combines numerical simulation and surrogate-based optimization strategies, aiming to enhance the overall performance during stable descent and meet various demands of different exploration missions. Meanwhile, by providing a rapid estimate of the shape during the design phase, the method significantly improves computational efficiency. The optimal models effectively balance comprehensive performance in the supersonic-transonic-subsonic speed domain by conducting shape optimization research on the disk-gap-band parachute using the surrogate-based optimization strategy. Also, it exhibits better deceleration and stability across the entire speed range compared to the base model, even when deviating from the design Mach number. Importantly, the advantages of canopy-only optimization for drag performance extend to the capsule-canopy two-body system, enhancing the drag performance of the canopy in the two-body system. This strategic approach reduces the transient calculation time for the two-body system, further improving computational efficiency. The method provides a practical and forward-thinking solution for the design of Mars parachutes.}
}
@article{ZHANG2025111031,
title = {Constrained multi-scale dense connections for biomedical image segmentation},
journal = {Pattern Recognition},
volume = {158},
pages = {111031},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111031},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324007829},
author = {Jiawei Zhang and Yanchun Zhang and Hailong Qiu and Tianchen Wang and Xiaomeng Li and Shanfeng Zhu and Meiping Huang and Jian Zhuang and Yiyu Shi and Xiaowei Xu},
keywords = {Multi-scale dense connections, Image segmentation, Network architecture search, Feature fusion},
abstract = {Multi-scale dense connection has been widely used in the biomedical image community to enhance the segmentation performance. In this way, features from all or most scales are aggregated or iteratively fused. However, by analyzing the details, we discover that some connections involving distant scales may not contribute to, or even harm, the performance, while they always introduce a noticeable increase in computational cost. In this paper, we propose constrained multi-scale dense connections (CMDC) for biomedical image segmentation. In contrast to current general lightweight approaches, we first introduce two methods, a naive method and a network architecture search (NAS)-based method, to remove redundant connections and verify the optimal connection configuration, thereby improving overall efficiency and accuracy. The results demonstrate that the two approaches obtain a similar optimal configuration in which most features at the adjacent scales are connected. Then, we applied the optimal configuration to various backbone networks to build constrained multi-scale dense networks (CMD-Net). Experimental results evaluated on eight image segmentation datasets covering biomedical images and natural images demonstrate the effectiveness of CMD-Net across a variety of backbone networks (FCN, U-Net, DeepLabV3, SegNet, FCNsa, ConvUNeXt) with a much lower increase in computational cost. Furthermore, CMD-Net achieves state-of-the-art performance on four publicly available datasets. We believe that the CMDC method can offer valuable insight for ways to engage in dense connectivity at multiple scales within communities. The source code has been made available at https://github.com/JerRuy/CMD-Net.}
}
@article{NUNEZV2024101173,
title = {Recommendation system using bio-inspired algorithms for urban orchards},
journal = {Internet of Things},
volume = {26},
pages = {101173},
year = {2024},
issn = {2542-6605},
doi = {https://doi.org/10.1016/j.iot.2024.101173},
url = {https://www.sciencedirect.com/science/article/pii/S2542660524001148},
author = {Juan M. {Núñez V.} and Juan M. Corchado and Diana M. Giraldo and Sara Rodríguez-González and Fernando {De la Prieta}},
keywords = {Internet of Things, Bio-inspired algorithms, Urban orchards, Lettuce crops, Social design},
abstract = {According to the Food and Agriculture Organization of the United Nations (FAO), climate change is exponentially affecting agricultural production worldwide, with food prices expected to increase by up to 90 percent by 2030 and hunger and malnutrition rates to rise by 2050. This paper presents the development of a platform based on the Internet of Things (IoT) for monitoring urban gardens as a strategy to mitigate hunger, promote food sovereignty and circular economy in areas of food shortage. To this end, an Internet of Things (IoT) architecture is proposed and implemented that involves a social design layer that allows an effective transfer of knowledge to communities and a recommendation system based on evolutionary computation to optimize and maximize the productivity of urban orchards, and thus contribute to the 2030 agenda of the Sustainable Development Goals (SDGs). Finally, three experiments in urban gardens are shown to validate evolutionary computation and artificial intelligence models, such as multiple linear regression, genetic algorithms, ant colony algorithms and spatial estimation and inference algorithms such as the Kriging algorithm. The productivity of urban lettuce orchards is increased between 25 and 45%.}
}
@incollection{KISS2019109,
title = {Process Systems Engineering from an industrial and academic perspective},
editor = {Anton A. Kiss and Edwin Zondervan and Richard Lakerveld and Leyla Özkan},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {46},
pages = {109-114},
year = {2019},
booktitle = {29th European Symposium on Computer Aided Process Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-12-818634-3.50019-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128186343500199},
author = {Anton A. Kiss and Johan Grievink},
keywords = {PSE, industry, education, research, interface, perspectives},
abstract = {Process Systems Engineering (PSE) deals with decision-making, at all levels and scales, by understanding complex process systems using a holistic view. Computer Aided Process Engineering (CAPE) is a complementary field that focuses on developing methods and providing solution through systematic computer aided techniques for problems related to the design, control and operation of chemical systems. The ‘PSE’ term suffers from a branding issue to the point that PSE does not get the recognition it deserves. This work aims to provide an informative industrial and academic perspective on PSE, arguing that the ‘systems thinking’ and ‘systems problem solving’ have to be prioritized ahead of just applications of computational problem solving methods. A multi-level view of the PSE field is provided within the academic and industrial context, and enhancements for PSE are suggested at their industrial and academic interfaces.}
}
@article{KAZANINA2023996,
title = {The neural ingredients for a language of thought are available},
journal = {Trends in Cognitive Sciences},
volume = {27},
number = {11},
pages = {996-1007},
year = {2023},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2023.07.012},
url = {https://www.sciencedirect.com/science/article/pii/S1364661323001936},
author = {Nina Kazanina and David Poeppel},
keywords = {language-of-thought, symbolic representation, computational theory of mind, spatial navigation, compositionality},
abstract = {The classical notion of a ‘language of thought’ (LoT), advanced prominently by the philosopher Jerry Fodor, is an influential position in cognitive science whereby the mental representations underpinning thought are considered to be compositional and productive, enabling the construction of new complex thoughts from more primitive symbolic concepts. LoT theory has been challenged because a neural implementation has been deemed implausible. We disagree. Examples of critical computational ingredients needed for a neural implementation of a LoT have in fact been demonstrated, in particular in the hippocampal spatial navigation system of rodents. Here, we show that cell types found in spatial navigation (border cells, object cells, head-direction cells, etc.) provide key types of representation and computation required for the LoT, underscoring its neurobiological viability.}
}
@article{BAI2011364,
title = {Prediction of human voluntary movement before it occurs},
journal = {Clinical Neurophysiology},
volume = {122},
number = {2},
pages = {364-372},
year = {2011},
issn = {1388-2457},
doi = {https://doi.org/10.1016/j.clinph.2010.07.010},
url = {https://www.sciencedirect.com/science/article/pii/S1388245710005699},
author = {Ou Bai and Varun Rathi and Peter Lin and Dandan Huang and Harsha Battapady and Ding-Yu Fei and Logan Schneider and Elise Houdayer and Xuedong Chen and Mark Hallett},
keywords = {Human intention, Voluntary movement, Prediction, Movement-related cortical potentials (MRCP), Event-related desynchronization (ERD), Electroencephalography (EEG), Brain–computer interface (BCI), Consciousness},
abstract = {Objective
Human voluntary movement is associated with two changes in electroencephalography (EEG) that can be observed as early as 1.5s prior to movement: slow DC potentials and frequency power shifts in the alpha and beta bands. Our goal was to determine whether and when we can reliably predict human natural movement BEFORE it occurs from EEG signals ONLINE IN REAL-TIME.
Methods
We developed a computational algorithm to support online prediction. Seven healthy volunteers participated in this study and performed wrist extensions at their own pace.
Results
The average online prediction time was 0.62±0.25s before actual movement monitored by EMG signals. There were also predictions that occurred without subsequent actual movements, where subjects often reported that they were thinking about making a movement.
Conclusion
Human voluntary movement can be predicted before movement occurs.
Significance
The successful prediction of human movement intention will provide further insight into how the brain prepares for movement, as well as the potential for direct cortical control of a device which may be faster than normal physical control.}
}
@article{PAN2025125506,
title = {CISL-PD: A deep learning framework of clinical intervention strategies for Parkinson’s disease based on directional counterfactual Dual GANs},
journal = {Expert Systems with Applications},
volume = {261},
pages = {125506},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.125506},
url = {https://www.sciencedirect.com/science/article/pii/S095741742402373X},
author = {Changrong Pan and Yu Tian and Lingyan Ma and Tianshu Zhou and Shuyu Ouyang and Jingsong Li},
keywords = {Parkinson’s disease, Intervention strategies, Counterfactual generation, Generative Adversarial Network},
abstract = {Parkinson’s disease (PD) is a prevalent chronic neurodegenerative disorder characterized by both motor and non-motor symptoms. The significant heterogeneity among PD patients poses a major challenge for treatment interventions. Current clinical interventions for PD primarily target motor symptoms, often neglecting non-motor symptoms, which can lead to unnecessary complications in non-motor symptoms while treating motor symptoms. Therefore, it is crucial to provide comprehensive and precise intervention strategies that encompass both symptom types. To address this issue, we develop a deep learning framework of clinical intervention strategies for PD (CISL-PD) based on counterfactual thinking. This framework introduces Directional Counterfactual Dual Generative Adversarial Networks (DCD-GANs), which apply various counterfactual constraints to longitudinal data to generate practical and plausible counterfactual instances aligned with clinical reality. By analyzing these counterfactual instances and their differences from the original instances, we explore PD intervention strategies with duration-specific fine regulation of multidimensional features. Experiments conducted on 374 PD patients from the Parkinson’s Progression Markers Initiative (PPMI) demonstrate that the counterfactual instances generated by DCD-GANs surpass other state-of-the-art models in terms of similarity (0.307 ± 0.246), sparsity (0.513 ± 0.161), smoothness (0.238 ± 0.135), and trend consistency (0.100 ± 0.089). From these generated counterfactual instances, we develop three clinically feasible intervention strategies that address both motor and non-motor symptoms and identify corresponding patterns of PD with distinct progression differences. Validation on an independent cohort of 351 patients from the National Institute of Neurological Disorders and Stroke Parkinson’s Disease Biomarkers Program (PDBP) confirmed the framework’s robustness and generalizability. By offering precise, multidimensional intervention strategies that can address both motor and non-motor symptoms, the CISL-PD framework has the potential to enhance patient outcomes, reduce complications, improve overall quality of life, and guide clinical decision-making.}
}
@incollection{MAIDA201639,
title = {Chapter 2 - Cognitive Computing and Neural Networks: Reverse Engineering the Brain},
editor = {Venkat N. Gudivada and Vijay V. Raghavan and Venu Govindaraju and C.R. Rao},
series = {Handbook of Statistics},
publisher = {Elsevier},
volume = {35},
pages = {39-78},
year = {2016},
booktitle = {Cognitive Computing: Theory and Applications},
issn = {0169-7161},
doi = {https://doi.org/10.1016/bs.host.2016.07.011},
url = {https://www.sciencedirect.com/science/article/pii/S0169716116300529},
author = {A.S. Maida},
keywords = {Brain simulation, Deep belief networks, Convolutional networks, Liquid computing, Biological neural networks, Neocortex},
abstract = {Cognitive computing seeks to build applications which model and mimic human thinking. One approach toward achieving this goal is to develop brain-inspired computational models. A prime example of such a model is the class of deep convolutional networks which is currently used in pattern recognition, machine vision, and machine learning. We offer a brief review of the mammalian neocortex, the minicolumn, and the ventral pathway. We provide descriptions of abstract neural circuits that have been used to model these areas of the brain. This include Poisson spiking networks, liquid computing networks, spiking models of feature discovery in the ventral pathway, spike-timing-dependent plasticity learning, restricted Boltzmann machines, deep belief networks, and deep convolutional networks. In summary, this chapter explores abstractions of neural networks found within the mammalian neocortex that support cognition and the beginnings of cognitive computation.}
}
@incollection{ADRIAANS2008133,
title = {LEARNING AND THE COOPERATIVE COMPUTATIONAL UNIVERSE},
editor = {Pieter Adriaans and Johan {van Benthem}},
booktitle = {Philosophy of Information},
publisher = {North-Holland},
address = {Amsterdam},
pages = {133-167},
year = {2008},
series = {Handbook of the Philosophy of Science},
issn = {18789846},
doi = {https://doi.org/10.1016/B978-0-444-51726-5.50010-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780444517265500108},
author = {Pieter Adriaans}
}
@article{CUI2025106320,
title = {From complex networks to urban heat island governance: Structural analysis and intervention in xiamen island's heat island network},
journal = {Sustainable Cities and Society},
volume = {124},
pages = {106320},
year = {2025},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2025.106320},
url = {https://www.sciencedirect.com/science/article/pii/S2210670725001970},
author = {Huanjia Cui and Yu Wang and Qiang Yu and Jikai Zhao and Weijie Sun and Xinyu Yang and Bowen Chi and Ji Long and Buyanbaatar Avirmed},
keywords = {Urban heat island, Morphological spatial pattern analysis, Circuit theory, Complex network theory, Robustness analysis},
abstract = {Rapid urbanization, under global warming, has intensified the urban heat island effect, causing significant environmental and social challenges. Many studies propose governance measures for individual landscape patches, but their effectiveness remains limited. A global perspective is essential for systematically analyzing urban heat environments. This study uses Xiamen Island as a case study and, based on the "source-sink" landscape theory, applies a reverse-thinking approach to construct urban heat island network that needs to be disrupted. By introducing complex network theory and aligning it with urban needs, a novel framework for optimizing and governing the global heat environment is proposed. The results show that: (1) Xiamen Island has 86 urban heat island sources and 202 heat transfer corridors; (2) 7 potential heat island sources were identified through pinch point analysis; (3) Based on barrier point analysis, 23 heat transfer barriers and 19 priority corridors for governance were found; (4) 6 potential heat transfer corridors were identified using complex network theory and LST data; (5) Robustness analysis of the urban heat island network revealed a governance strategy prioritizing closeness centrality, identifying 15 sources for prioritized governance. This study provides practical new approaches and strategies for comprehensive governance of urban heat environments.}
}
@article{DALLAQUA2021422,
title = {ForestEyes Project: Conception, enhancements, and challenges},
journal = {Future Generation Computer Systems},
volume = {124},
pages = {422-435},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2021.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X21001965},
author = {Fernanda B.J.R. Dallaqua and Álvaro L. Fazenda and Fabio A. Faria},
keywords = {Citizen Science, Deforestation area detection, Rainforest, Tropical forest, Volunteered Thinking},
abstract = {Rainforests play an important role in the global ecosystem. However, significant regions of them are facing deforestation and degradation due to several reasons. Diverse government and private initiatives were created to monitor and alert for deforestation increases from remote sensing images, using different ways to deal with the notable amount of generated data. Citizen Science projects can also be used to reach the same goal. Citizen Science consists of scientific research involving nonprofessional volunteers for analyzing, collecting data, and using their computational resources to outcome advancements in science and to increase the public’s understanding of problems in specific knowledge areas such as astronomy, chemistry, mathematics, and physics. In this sense, this work presents a Citizen Science project called ForestEyes, which uses volunteer’s answers through the analysis and classification of remote sensing images to monitor deforestation regions in rainforests. To evaluate the quality of those answers, different campaigns/workflows were launched using remote sensing images from Brazilian Legal Amazon and their results were compared to an official groundtruth from the Amazon Deforestation Monitoring Project PRODES. In this work, the first two workflows that enclose the State of Rondônia in the years 2013 and 2016 received more than 35,000 answers from 383 volunteers in the 2,050 created tasks in only two and a half weeks after their launch. For the other four workflows, even enclosing the same area (Rondônia) and different setups (e.g., image segmentation method, image spatial resolution, and detection target), they received 51,035 volunteers’ answers gathered from 281 volunteers in 3,358 tasks. In the performed experiments, it was possible to observe that the volunteers achieved satisfactory overall accuracy, higher than 75%, in the classification of forestation and non-forestation areas using the ForestEyes project. Furthermore, considering an efficient segmentation and a better image spatial resolution, they achieved almost 66% accuracy in the classification of recent deforestation, which is a great challenge to overcome. Therefore, these results show that Citizen Science might be a powerful tool in monitoring deforestation regions in rainforests as well as in obtaining high-quality labeled data.}
}
@article{DECARO200758,
title = {Methodologies for examining problem solving success and failure},
journal = {Methods},
volume = {42},
number = {1},
pages = {58-67},
year = {2007},
note = {Neurocognitive Mechanisms of Creativity: A Toolkit},
issn = {1046-2023},
doi = {https://doi.org/10.1016/j.ymeth.2006.12.006},
url = {https://www.sciencedirect.com/science/article/pii/S1046202306002982},
author = {Marci S. DeCaro and Mareike Wieth and Sian L. Beilock},
keywords = {Working memory, Performance, Pressure, Individual differences, Problem solving, Creativity, Short term memory, Stress, Math},
abstract = {When designing research to examine the variables underlying creative thinking and problem solving success, one must not only consider (a) the demands of the task being performed, but (b) the characteristics of the individual performing the task and (c) the constraints of the skill execution environment. In the current paper we describe methodologies that allow one to effectively study creative thinking by capturing interactions among the individual, task, and problem solving situation. In doing so, we demonstrate that the relation between executive functioning and problem solving success is not always as straightforward as one might initially believe.}
}
@incollection{ASHBY2024255,
title = {Chapter 10 - Circular Materials Economics},
editor = {Michael F. Ashby},
booktitle = {Materials and Sustainable Development (Second Edition)},
publisher = {Butterworth-Heinemann},
edition = {Second Edition},
pages = {255-295},
year = {2024},
isbn = {978-0-323-98361-7},
doi = {https://doi.org/10.1016/B978-0-323-98361-7.00010-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780323983617000105},
author = {Michael F. Ashby},
keywords = {Circularity, Material efficiency, Linear materials economy, Circular materials economy, Reuse, Repair, Recycling, Take-back legislation, Recycling targets, Increased product life, Urban mining, Business models, Measuring circularity, Modelling circularity, Limits to circularity},
abstract = {When products come to the end of their lives, the materials they contain are still there. Repurposing, repair or recycling can return them to active use, creating a technological cycle that, in some ways, parallels the carbon, nitrogen and hydrological cycles of the biosphere. In developed nations they lost urgency as the cost of materials fell and that of labour rose, making it cheaper to make new products than to fix old ones, leading to a materials economy that is largely linear, characterized by the sequence “take – make – use – dispose”. Increasing population and affluence, and the limited capacity for the planet to provide resources and absorb waste direct thinking towards a more circular way of using materials. Governments have sought to reduce waste by imposing take-back regulations, setting mandatory recycling targets and requiring minimum service lives. These, and the efficiency movement – eco-efficiency, material-efficiency, energy efficiency – seek to allow business as usual with reduced drain on natural resources without any real change of behaviour. The ‘circularity’ concept is a way of thinking that looks not just for efficiencies but also for new ways to provide functions. The idea of deploying rather than consuming materials, of using them not once but many times has economic as well as environmental appeal. This chapter examines the background, the successes, the difficulties, and the ultimate limits of implementing a circular materials economy.}
}
@article{FERREIRA20131446,
title = {Fostering the Creative Development of Computer Science Students in Programming and Interaction Design},
journal = {Procedia Computer Science},
volume = {18},
pages = {1446-1455},
year = {2013},
note = {2013 International Conference on Computational Science},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2013.05.312},
url = {https://www.sciencedirect.com/science/article/pii/S1877050913004559},
author = {Deller James Ferreira},
keywords = {Creativity, Programming, Interaction design},
abstract = {This study explores the enhancement of creativity in undergraduate students studying computer science. We assume that everybody has creative potential. As a teacher, we can explicitly encourage creative thinking, providing space to let students collaboratively discover and explore their creativity. This paper presents a dialogical framework to help the teacher fostering creativity among students of computer science in programming and interaction design. The framework presented here involves underlying dialogic processes from seven collaborative and creative dimensions that allow students to develop creativity. The use of the pedagogical framework makes it possible to teachers create significant interaction design and computer programming experiences to students, motivating them to activate mental processes underlying creativity. Students can simultaneously activate two or more ideas, images, or thoughts and have them interact, prompt thought experiments, change cognitive perspectives, raise new points of view, and risk category mistakes.}
}
@article{TIAN2018104,
title = {The association between visual creativity and cortical thickness in healthy adults},
journal = {Neuroscience Letters},
volume = {683},
pages = {104-110},
year = {2018},
issn = {0304-3940},
doi = {https://doi.org/10.1016/j.neulet.2018.06.036},
url = {https://www.sciencedirect.com/science/article/pii/S0304394018304397},
author = {Fang Tian and Qunlin Chen and Wenfeng Zhu and Yongming Wang and Wenjing Yang and Xingxing Zhu and Xue Tian and Qinglin Zhang and Guikang Cao and Jiang Qiu},
keywords = {Visual creativity, Cortical thickness, Prefrontal cortex, Supplementary motor cortex, Insula},
abstract = {Creativity is necessary to human survival, human prosperity, civilization and well-being. Visual creativity is an important part of creativity and is the ability to create products of novel and useful visual forms, playing important role in many fields such as art, painting and sculpture. There have been several neuroimaging studies exploring the neural basis of visual creativity. However, to date, little is known about the relationship between cortical structure and visual creativity as measured by the Torrance Tests of Creative Thinking. Here, we investigated the association between cortical thickness and visual creativity in a large sample of 310 healthy adults. We used multiple regression to analyze the correlation between cortical thickness and visual creativity, adjusting for gender, age and general intelligence. The results showed that visual creativity was significantly negatively correlated with cortical thickness in the left middle frontal gyrus (MFG), right inferior frontal gyrus (IFG), right supplementary motor cortex (SMA) and the left insula. These observations have implications for understanding that a thinner prefrontal cortex (PFC) (e.g. IFG, MFG), SMA and insula correspond to higher visual creative performance, presumably due to their role in executive attention, cognitive control, motor planning and dynamic switching.}
}
@article{GOLDBERG20007,
title = {The Design of Innovation: Lessons from Genetic Algorithms, Lessons for the Real World},
journal = {Technological Forecasting and Social Change},
volume = {64},
number = {1},
pages = {7-12},
year = {2000},
issn = {0040-1625},
doi = {https://doi.org/10.1016/S0040-1625(99)00079-7},
url = {https://www.sciencedirect.com/science/article/pii/S0040162599000797},
author = {David E Goldberg},
abstract = {This article considers some of the connections between genetic algorithms (GAs)—search procedures based on the mechanics of natural selection and natural genetics—and human innovation. Simply stated, innovation has been a source of inspiration for thinking about genetic algorithms, and as the algorithms have improved, GAs have become increasingly interesting computational models of the processes of innovation. The article reviews the basics of genetic algorithm operation and connects the basic mechanics to two processes of innovation: continual improvement and discontinuous change. Thereafter, some of the technical lessons of genetic algorithm processing are reviewed and their implications are briefly explored in the context of organizational change.}
}
@article{BLOSS2016,
title = {Reimagining Human Research Protections for 21st Century Science},
journal = {Journal of Medical Internet Research},
volume = {18},
number = {12},
year = {2016},
issn = {1438-8871},
doi = {https://doi.org/10.2196/jmir.6634},
url = {https://www.sciencedirect.com/science/article/pii/S1438887116003204},
author = {Cinnamon Bloss and Camille Nebeker and Matthew Bietz and Deborah Bae and Barbara Bigby and Mary Devereaux and James Fowler and Ann Waldo and Nadir Weibel and Kevin Patrick and Scott Klemmer and Lori Melichar},
keywords = {ethics committees, research, biomedical research, telemedicine, informed consent, behavioral research},
abstract = {Background
Evolving research practices and new forms of research enabled by technological advances require a redesigned research oversight system that respects and protects human research participants.
Objective
Our objective was to generate creative ideas for redesigning our current human research oversight system.
Methods
A total of 11 researchers and institutional review board (IRB) professionals participated in a January 2015 design thinking workshop to develop ideas for redesigning the IRB system.
Results
Ideas in 5 major domains were generated. The areas of focus were (1) improving the consent form and process, (2) empowering researchers to protect their participants, (3) creating a system to learn from mistakes, (4) improving IRB efficiency, and (5) facilitating review of research that leverages technological advances.
Conclusions
We describe the impetus for and results of a design thinking workshop to reimagine a human research protections system that is responsive to 21st century science.}
}
@article{FASOLINO2025125111,
title = {Dynamics in action: Exploring economic impacts of drought through a systemic approach},
journal = {Journal of Environmental Management},
volume = {380},
pages = {125111},
year = {2025},
issn = {0301-4797},
doi = {https://doi.org/10.1016/j.jenvman.2025.125111},
url = {https://www.sciencedirect.com/science/article/pii/S0301479725010874},
author = {Nunzia Gabriella Fasolino and Emilia Pellegrini and Meri Raggi and Davide Viaggi},
keywords = {Water energy nexus, Natural hazards, Climate change, Impacts assessment, System thinking},
abstract = {Droughts are generally associated with direct and indirect impacts, but their relationships and propagation dynamics are often unknown. This study aims to fill this research gap by exploring the spill-over effects of drought impacts across systems and, in doing so, identifying leverage points within the systems to promote effective mitigation strategies. To achieve these objectives, the effects of the 2022 drought in a portion of the Po River Basin District are analysed through a qualitative application of the System Dynamics methodology called Causal Loop Diagrams, systemic representations that can effectively capture the interdependencies across complex subsystems. The findings reveal that the drought increased production costs across all the water-using sectors due to higher energy demands driven by energy-intensive activities such as irrigation, pumping and potabilization. The empirical results offer a novel insight into the identification and propagation of impacts, providing further evidence on the water-energy nexus. Therefore, policy approaches that enhance blue and green water reserves, while addressing their interconnections, are encouraged for mitigating drought impacts. This underscores the importance of adopting a proactive approach in drought management rather than reactive responses during the occurrence of extreme events. Finally, in fields characterized by high policy fragmentation, such as water policy, the study demonstrates how representing the system behaviour through causal maps can support the integration of different sectoral policies, providing decision-makers with tools to evaluate and address the complexity of droughts.}
}
@article{ALY2014206,
title = {Atmospheric boundary-layer simulation for the built environment: Past, present and future},
journal = {Building and Environment},
volume = {75},
pages = {206-221},
year = {2014},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2014.02.004},
url = {https://www.sciencedirect.com/science/article/pii/S0360132314000444},
author = {Aly Mousaad Aly},
keywords = {Aerodynamics, Aeroelasticity, Atmospheric boundary-layer, Built environment, Experimental/computational wind engineering},
abstract = {This paper summarizes the state-of-the-art techniques used to simulate hurricane winds in atmospheric boundary-layer (ABL) for wind engineering testing. The wind tunnel simulation concept is presented along with its potential applications, advantages and challenges. ABL simulation at open-jet simulators is presented along with an application example followed by a discussion on the advantages and challenges of testing at these facilities. Some of the challenges and advantages of using computational fluid dynamics (CFD) are presented with an application example. The paper show that the way the wind can be simulated is complex and matching one parameter at full-scale may lead to a mismatch of other parameters. For instance, while large-scale testing is expected to improve Reynolds number and hence approach the full-scale scenario, it is challenging to generate large-scale turbulence in an artificially created wind. New testing protocols for low-rise structures and small-size architectural features are presented as an answer to challenging questions associated with both wind tunnel and open-jet testing. Results show that it is the testing protocol that can be adapted to enhance the prediction of full-scale physics in nature. Thinking out of the box and accepting non-traditional ABL is necessary to compensate for Reynolds effects and to allow for convenient experimentation. New research directions with focus on wind, rain and waves as well as other types of non-synoptic winds are needed, in addition to a more focus on the flow physics in the lower part of the ABL, where the major part of the infrastructure exists.}
}
@article{KNAPP2017370,
title = {Energy-efficient Legionella control that mimics nature and an open-source computational model to aid system design},
journal = {Applied Thermal Engineering},
volume = {127},
pages = {370-377},
year = {2017},
issn = {1359-4311},
doi = {https://doi.org/10.1016/j.applthermaleng.2017.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S135943111633664X},
author = {Samuel Knapp and Bo Nordell},
keywords = {Thermal model, Heat exchanger, Pasteurization, Legionnaire’s disease, Microsoft Excel},
abstract = {Although there is no direct connection, the incidence of Legionnaire’s disease has increased concurrently with increased usage of energy efficient domestic hot water (DHW) systems, which serve as ideal growth environments for Legionella pneumophila, the bacteria responsible for Legionnaire’s disease. The Duck Foot Heat Exchange Model (DFHXM) was developed to aid design of energy efficient thermal pasteurization systems with Legionella control specifically in mind. The model simulates a system design imitating the countercurrent heat exchange in the feet of ducks, an evolutionary adaption reducing environmental heat losses in cold climates. Such systems use a heat exchanger to preheat fluids prior to pasteurization and cool the same fluid after pasteurization. Thus, the design requires minimal addition of heat to achieve pasteurization temperatures and to cover environmental heat losses. This article describes the underlying principles and use of the freely available Microsoft Excel model, as well as compares results from the DFHXM to measurements of an experimental pilot system. Simulation outputs agreed well with experimental results for transient and steady-state temperatures, the largest discrepancy in steady-state temperatures being 4.6%. Lastly, we discuss the flexibility of the DFHXM to simulate a wide variety of designs with special emphasis on Legionella control and solar-thermal water disinfection.}
}
@article{WEICHBROTH20223798,
title = {A note on the affective computing systems and machines: a classification and appraisal},
journal = {Procedia Computer Science},
volume = {207},
pages = {3798-3807},
year = {2022},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 26th International Conference KES2022},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.09.441},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922013345},
author = {Paweł Weichbroth and Wiktor Sroka},
keywords = {Affective Computing, Artificial Emotional Intelligence, Classification, System, Machine},
abstract = {Affective computing (AfC) is a continuously growing multidisciplinary field, spanning areas from artificial intelligence, throughout engineering, psychology, education, cognitive science, to sociology. Therefore, many studies have been devoted to the aim of addressing numerous issues, regarding different facets of AfC solutions. However, there is a lack of classification of the AfC systems. This study aims to fill this gap by reviewing and evaluating the state-of-the-art studies in a qualitative manner. In this line of thinking, we put forward a threefold classification that breaks down to desktop and mobile AfC systems, and AfC machines. Moreover, we identified four types of AfC systems, based on the features extracted. In our opinion, the results of this study can serve as a guide for future affect-related research and design, on the one hand, and provide a better understanding on the role of emotions and affect in human-computer interaction, on the other hand.}
}
@article{OLSON1995183,
title = {Emergent computation and the modeling and management of ecological systems},
journal = {Computers and Electronics in Agriculture},
volume = {12},
number = {3},
pages = {183-209},
year = {1995},
issn = {0168-1699},
doi = {https://doi.org/10.1016/0168-1699(94)00022-I},
url = {https://www.sciencedirect.com/science/article/pii/016816999400022I},
author = {Richard L. Olson and Ronaldo A. Sequeira},
keywords = {Emergent computation, Ecosystem dynamics, Ecosystem management},
abstract = {This paper introduces the emergent computational paradigm, discusses its applicability and potential in ecosystem management, and reviews the literature. Emergent computation is significantly different from the “classic” computational paradigm, where control is top-down and centralized. In emergent systems, overall system dynamics emerge from the local interactions of independent agents. In such systems, overall global control is minimized or eliminated altogether. Applications in ecosystem management include use of “artificial ecosystems” as surrogate experimental systems, and genetics-based machine learning systems to evolve management rule-sets for complex domains. Cellular automata, neural networks, genetic algorithms and classifier systems are discussed as examples of the emergent approach. Finally, an in-depth literature review of artificial ecosystems is provided.}
}
@article{LI2024109502,
title = {Numerical study on heat transfer performance of printed circuit heat exchanger with anisotropic thermal conductivity},
journal = {International Journal of Heat and Fluid Flow},
volume = {109},
pages = {109502},
year = {2024},
issn = {0142-727X},
doi = {https://doi.org/10.1016/j.ijheatfluidflow.2024.109502},
url = {https://www.sciencedirect.com/science/article/pii/S0142727X24002273},
author = {Libo Li and Jiyuan Bi and Jingkai Ma and Xiaoxu Zhang and Qiuwang Wang and Ting Ma},
keywords = {Printed circuit heat exchanger, Anisotropic thermal conductivity, Numerical simulation, Thermal resistance, Heat exchanger efficiency},
abstract = {Printed Circuit Heat Exchangers are compact and efficient heat exchangers, widely used in nuclear engineering, very high-temperature reactors, and aerospace systems. This study investigates the heat transfer performance of a heat exchanger with anisotropic thermal conductivity, such as fiber reinforced composites. Numerical simulations were conducted to examine the synergistic effect of three-dimensional thermal resistance on heat exchanger performance. The most significant impact on performance is the z-direction thermal resistance, followed by the y-direction, while the x-direction has the least impact. Contrary to traditional design thinking, increasing the overall heat exchanger thermal resistance under the same thermal resistance ratio improves heat transfer efficiency at the studied conditions. The results suggest that it is necessary to design the lowest thermal conductivity direction as the z-direction and increase the y-direction thermal conductivity to enhance heat exchanger performance. In the numerical investigation presented in this study, the efficiency of the heat exchanger was improved by approximately 23 % under specific operating conditions by adjusting the thermal conductivity of anisotropic materials to control the thermal resistance in the x, y and z directions. It is evident that the manipulation of anisotropic material properties has a substantial influence on the performance of heat exchangers.}
}
@incollection{SCHOMMERS201991,
title = {Chapter 2 - Theoretical and Computational Methods},
editor = {Wolfram Schommers},
booktitle = {Basic Physics of Nanoscience},
publisher = {Elsevier},
pages = {91-202},
year = {2019},
isbn = {978-0-12-813718-5},
doi = {https://doi.org/10.1016/B978-0-12-813718-5.00002-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128137185000028},
author = {Wolfram Schommers},
keywords = {Simulation methods, interaction potentials, anharmonicities, temperature effects, molecular dynamics, nanosystems, structures, dynamics},
abstract = {It is underlined that typical nanosystems are adequately described only by the fundamental laws of theoretical physics. It is in particular argued that phenomenological models are in most cases not sophisticated enough. For the description of such nanosystems the theoretical and computational tools have to be selected carefully and have in particular to be improved in many cases. In this connection the interaction laws (potentials) between the atoms, forming a nanosystem, are critical functions because the structure and dynamics of such systems are very sensitive to small variations in the potentials. This point has been studied in detail. Various potential laws have been introduced and discussed in connection with applications. The most relevant simulations methods are quoted and their relevance for nanotechnology is discussed. In particular, the molecular dynamics method is described in detail. We give typical examples, which demonstrate the fact the molecular dynamics method is a powerful and reliable tool for the investigation of typical nanosystems with their large variety of structures and complex dynamical states. The examples deal with wear at the nanotechnological level and with metallic nanoclusters as building blocks.}
}
@article{VANDUN2023113880,
title = {ProcessGAN: Supporting the creation of business process improvement ideas through generative machine learning},
journal = {Decision Support Systems},
volume = {165},
pages = {113880},
year = {2023},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2022.113880},
url = {https://www.sciencedirect.com/science/article/pii/S0167923622001518},
author = {Christopher {van Dun} and Linda Moder and Wolfgang Kratsch and Maximilian Röglinger},
keywords = {Business process improvement, Business process redesign, Generative adversarial networks, Generative machine learning, Process mining},
abstract = {Business processes are a key driver of organizational success, which is why business process improvement (BPI) is a central activity of business process management. Despite an abundance of approaches, BPI as a creative task is time-consuming and labour-intensive. Most importantly, its level of computational support is low. The few computational BPI approaches hardly leverage the opportunities brought about by computational creativity, neglect process data, and rely on rather rigid improvement patterns. Given the increasing amount of process data in the form of event logs and the uptake of generative machine learning for automating creative tasks in various domains, there is huge potential for BPI. Hence, following the design science research paradigm, we specified, implemented, and evaluated ProcessGAN, a novel computational BPI approach based on generative adversarial networks that supports the creation of BPI ideas. Our evaluation shows that ProcessGAN improves the creativity of process designers, particularly the originality of BPI ideas, and shapes up useful in real-world settings. Moreover, ProcessGAN is the first approach to combine BPI and computational creativity.}
}
@article{CAO2024102160,
title = {Self-assembly of peptides: The acceleration by molecular dynamics simulations and machine learning},
journal = {Nano Today},
volume = {55},
pages = {102160},
year = {2024},
issn = {1748-0132},
doi = {https://doi.org/10.1016/j.nantod.2024.102160},
url = {https://www.sciencedirect.com/science/article/pii/S174801322400015X},
author = {Nana Cao and Kang Huang and Jianjun Xie and Hui Wang and Xinghua Shi},
keywords = {Peptides, Self-assembly, Molecular dynamics, Machine learning},
abstract = {Peptides, biopolymeric compounds connected by peptide bonds, have garnered significant attention in recent years as their potential wide applications in fields such as drug delivery, tissue engineering, and antibiotics. Peptides exhibit excellent biocompatibility and stability due to their structural similarities to many bioactive substances found in human bodies. The self-assembly of peptides has piqued considerable interest with groundbreaking advancements achieved in experimental research. However, it is still a big challenge to establish comprehensive theoretical model to accurately describe the behavior of peptide self-assembly. Current peptide self-assembly designs primarily rely on experimental outcomes and general rules, which is inefficient and susceptible to human errors. In recent years, thanks to rapid advancements in computer techniques and theoretical methods, computational research has become a vital tool in complementing experimental research with rapid development witted in this field. This review delves into the description of peptide self-assembly, covering relevant sequences, structures, morphologies, rules, and application areas. It places particular emphasis on the recent progress in computational methods such as molecular dynamics (MD) simulations and machine learning (ML) techniques in the study. Finally, we provide a perspective on the application of computational methods to expedite exploration in the realm of multi-peptide self-assembly.}
}
@article{KALAY199837,
title = {P3: Computational environment to support design collaboration},
journal = {Automation in Construction},
volume = {8},
number = {1},
pages = {37-48},
year = {1998},
issn = {0926-5805},
doi = {https://doi.org/10.1016/S0926-5805(98)00064-8},
url = {https://www.sciencedirect.com/science/article/pii/S0926580598000648},
author = {Yehuda E Kalay},
keywords = {Collaborative design, Design environment, Product model, Performance model, Process model},
abstract = {The work reported in this paper addresses the paradoxical state of the construction industry (also known as A/E/C, for Architecture, Engineering and Construction), where the design of highly integrated facilities is undertaken by severely fragmented teams, leading to diminished performance of both processes and products. The construction industry has been trying to overcome this problem by partitioning the design process hierarchically or temporally. While these methods are procedurally efficient, their piecemeal nature diminishes the overall performance of the project. Computational methods intended to facilitate collaboration in the construction industry have, so far, focused primarily on improving the flow of information among the participants. They have largely met their stated objective of improved communication, but have done little to improve joint decision-making, and therefore have not significantly improved the quality of the design project itself. We suggest that the main impediment to effective collaboration and joint decision-making in the A/E/C industry is the divergence of disciplinary `world-views', which are the product of educational and professional processes through which the individuals participating in the design process have been socialized into their respective disciplines. To maximize the performance of the overall project, these different world-views must be reconciled, possibly at the expense of individual goals. Such reconciliation can only be accomplished if the participants find the attainment of the overall goals of the project more compelling than their individual disciplinary goals. This will happen when the participants have become cognizant and appreciative of world-views other than their own, including the objectives and concerns of other participants. To achieve this state of knowledge, we propose to avail to the participants of the design team highly specific, contextualized information, reflecting each participant's valuation of the proposed design actions. P3 is a semantically-rich computational environment, which is intended to fulfill this mission. It consists of: (1) a shared representation of the evolving design project, connected (through the World Wide Web) to (2) individual experts and their discipline-specific knowledge repositories; and (3) a computational project manager makes the individual valuations visible to all the participants, and helps them deliberate and negotiate their respective positions for the purpose of improving the overall performance of the project. The paper discusses the theories on which the three components are founded, their function, and the principles of their implementation.}
}
@incollection{WU202257,
title = {Chapter 3 - CTDA methodology},
editor = {Jiaping Wu and Junyu He and George Christakos},
booktitle = {Quantitative Analysis and Modeling of Earth and Environmental Data},
publisher = {Elsevier},
pages = {57-100},
year = {2022},
isbn = {978-0-12-816341-2},
doi = {https://doi.org/10.1016/B978-0-12-816341-2.00010-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128163412000101},
author = {Jiaping Wu and Junyu He and George Christakos},
keywords = {Methodological chain, Knowledge bases, Big data, Scales, Visualization, Chronotopologic statistics},
abstract = {Abstract The methodological characteristics of the chronotopologic data analysis chain are discussed. Various kinds of knowledge are considered and properly classified, and several illustrative examples in applied sciences are presented. Big data and data-driven analyses are critically reviewed, and their implementation carefully assessed. Data scale types are classifications considered in property- and attribute-oriented settings. Classical statistics inadequacies are pointed out and the need of a chronotopology-dependent statistics is outlined. The chronotopologic visualization thinking mode and techniques are briefly reviewed.}
}
@article{DEVOE2012466,
title = {Time, money, and happiness: How does putting a price on time affect our ability to smell the roses?},
journal = {Journal of Experimental Social Psychology},
volume = {48},
number = {2},
pages = {466-474},
year = {2012},
issn = {0022-1031},
doi = {https://doi.org/10.1016/j.jesp.2011.11.012},
url = {https://www.sciencedirect.com/science/article/pii/S0022103111002897},
author = {Sanford E. DeVoe and Julian House},
keywords = {Time, Money, Impatience, Happiness},
abstract = {In this paper, we investigate how the impatience that results from placing a price on time impairs individuals' ability to derive happiness from pleasurable experiences. Experiment 1 demonstrated that thinking about one's income as an hourly wage reduced the happiness that participants derived from leisure time on the internet. Experiment 2 revealed that a similar manipulation decreased participants' state of happiness after listening to a pleasant song and that this effect was fully mediated by the degree of impatience experienced during the music. Finally, Experiment 3 showed that the deleterious effect on happiness caused by impatience was attenuated by offering participants monetary compensation in exchange for time spent listening to music, suggesting that a sensation of unprofitably wasted time underlay the induced impatience. Together these experiments establish that thinking about time in terms of money can influence how people experience pleasurable events by instigating greater impatience during unpaid time.}
}
@article{LUNGU2008255,
title = {Partial current information and signal extraction in a rational expectations macroeconomic model: A computational solution},
journal = {Economic Modelling},
volume = {25},
number = {2},
pages = {255-273},
year = {2008},
issn = {0264-9993},
doi = {https://doi.org/10.1016/j.econmod.2007.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S0264999307000818},
author = {L. Lungu and K.G.P. Matthews and A.P.L. Minford},
keywords = {Rational expectations, Partial current information, Signal extraction, Macroeconomic modelling},
abstract = {Previous attempts at modelling current observed endogenous financial variables in a macroeconomic model have concentrated on only one variable — the short-term rate of interest. This paper applies a general search algorithm to a macroeconomic model with an observed interest rate and exchange rate to solve the signal extraction problem. Firstly, the algorithm is tested against a linear model with a known analytical solution. Then, the algorithm is applied to all the observed current endogenous variables in a non-linear rational expectations model of the UK. The informational advantage of applying the signal extraction algorithm is evaluated in terms of the forecasting efficiency of the model.}
}
@article{RUTHERFORD2023102255,
title = {“Don't [ruminate], be happy”: A cognitive perspective linking depression and anhedonia},
journal = {Clinical Psychology Review},
volume = {101},
pages = {102255},
year = {2023},
issn = {0272-7358},
doi = {https://doi.org/10.1016/j.cpr.2023.102255},
url = {https://www.sciencedirect.com/science/article/pii/S0272735823000132},
author = {Ashleigh V. Rutherford and Samuel D. McDougle and Jutta Joormann},
keywords = {Rumination, Emotion regulation, Working memory, Reinforcement learning, Depression},
abstract = {Anhedonia, a lack of pleasure in things an individual once enjoyed, and rumination, the process of perseverative and repetitive attention to specific thoughts, are hallmark features of depression. Though these both contribute to the same debilitating disorder, they have often been studied independently and through different theoretical lenses (e.g., biological vs. cognitive). Cognitive theories and research on rumination have largely focused on understanding negative affect in depression with much less focus on the etiology and maintenance of anhedonia. In this paper, we argue that by examining the relation between cognitive constructs and deficits in positive affect, we may better understand anhedonia in depression thereby improving prevention and intervention efforts. We review the extant literature on cognitive deficits in depression and discuss how these dysfunctions may not only lead to sustained negative affect but, importantly, interfere with an ability to attend to social and environmental cues that could restore positive affect. Specifically, we discuss how rumination is associated to deficits in working memory and propose that these deficits in working memory may contribute to anhedonia in depression. We further argue that analytical approaches such as computational modeling are needed to study these questions and, finally, discuss implications for treatment.}
}
@article{SEWALL2020,
title = {Fiber Force: A Fiber Diet Intervention in an Advanced Course-Based Undergraduate Research Experience (CURE) Course},
journal = {Journal of Microbiology & Biology Education},
volume = {21},
number = {1},
year = {2020},
issn = {1935-7877},
doi = {https://doi.org/10.1128/jmbe.v21i1.1991},
url = {https://www.sciencedirect.com/science/article/pii/S1935787720000660},
author = {Julia Massimelli Sewall and Andrew Oliver and Kameryn Denaro and Alexander B. Chase and Claudia Weihe and Mi Lay and Jennifer B. H. Martiny and Katrine Whiteson},
abstract = {Course-based undergraduate research experiences (CUREs) are an effective way to introduce students to contemporary scientific research. Research experiences have been shown to promote critical thinking, improve understanding and proper use of the scientific method, and help students learn practical skills including writing and oral communication. We aimed to improve scientific training by engaging students enrolled in an upper division elective course in a human microbiome CURE. The “Fiber Force” course is aimed at studying the effect of a wholesome high-fiber diet (40 to 50 g/day for two weeks) on the students’ gut microbiomes. Enrolled students participated in a noninvasive diet intervention, designed health surveys, tested hypotheses on the effect of a diet intervention on the gut microbiome, and analyzed their own samples (as anonymized aggregates). The course involved learning laboratory techniques (e.g., DNA extraction, PCR, and 16S sequencing) and the incorporation of computational techniques to analyze microbiome data with QIIME2 and within the R software environment. In addition, the learning objectives focused on effective student performance in writing, data analysis, and oral communication. Enrolled students showed high performance grades on writing, data analysis and oral communication assignments. Pre- and post-course surveys indicate that the students found the experience favorable, increased their interest in science, and heightened awareness of their diet habits. Fiber Force constitutes a validated case of a research experience on microbiology with the capacity to improve research training and promote healthy dietary habits.}
}
@article{ELVEVAG2023115098,
title = {Reflections on measuring disordered thoughts as expressed via language},
journal = {Psychiatry Research},
volume = {322},
pages = {115098},
year = {2023},
issn = {0165-1781},
doi = {https://doi.org/10.1016/j.psychres.2023.115098},
url = {https://www.sciencedirect.com/science/article/pii/S0165178123000513},
author = {Brita Elvevåg},
keywords = {Assessment, Language, Memory},
abstract = {Thought disorder, as inferred from disorganized and incoherent speech, is an important part of the clinical presentation in schizophrenia. Traditional measurement approaches essentially count occurrences of certain speech events which may have restricted their usefulness. Applying speech technologies in assessment can help automate traditional clinical rating tasks and thereby complement the process. Adopting these computational approaches affords clinical translational opportunities to enhance the traditional assessment by applying such methods remotely and scoring various parts of the assessment automatically. Further, digital measures of language may help detect subtle clinically significant signs and thus potentially disrupt the usual manner by which things are conducted. If proven beneficial to patient care, methods where patients’ voice are the primary data source could become core components of future clinical decision support systems that improve risk assessment. However, even if it is possible to measure thought disorder in a sensitive, reliable and efficient manner, there remain many challenges to then translate into a clinically implementable tool that can contribute towards providing better care. Indeed, embracing technology - notably artificial intelligence - requires vigorous standards for reporting underlying assumptions so as to ensure a trustworthy and ethical clinical science.}
}
@article{NYANGIWE2025100683,
title = {Applications of density functional theory and machine learning in nanomaterials: A review},
journal = {Next Materials},
volume = {8},
pages = {100683},
year = {2025},
issn = {2949-8228},
doi = {https://doi.org/10.1016/j.nxmate.2025.100683},
url = {https://www.sciencedirect.com/science/article/pii/S2949822825002011},
author = {Nangamso Nathaniel Nyangiwe},
keywords = {Nanomaterials, Density functional theory, Machine learning, Data, Algorithm},
abstract = {The development and creation of nanomaterials carry enormous prospects in advancing technology in electronics, energy storage and medicine. The high degree of complexity and diversity in nanomaterials presents a real challenge in their theoretical and experimental studies. Density Functional Theory (DFT) is emerging as a powerful computational tool to model, understand, and predict material properties at a quantum mechanical level for nanomaterials. This review highlights the considerable use of DFT in elucidating the electronic, structural, and catalytic attributes of various nanomaterials. Also, this review considers developments between DFT and machine learning (ML)-based techniques that have paved the way for accelerated discoveries and design of novel nanomaterials. In fact, the ML algorithm has built models based on data from DFT, which predicts with high accuracy the properties of materials at reduced computational costs to expand vast areas of emerging chemistries. Major advances in this new hybrid approach include the development of ML models to predict band gaps, adsorption energies, and reaction mechanisms. The review discusses open topics regarding the future efforts to integrate DFT and ML focusing on model interpretability, data quality and broadened applicability to increasingly complex systems. The review concludes by discussing key advancements, such as those of machine learning interatomic potentials, graph-based models for structure property mapping and generative AI for materials design.}
}
@article{STORLIE20091735,
title = {Implementation and evaluation of nonparametric regression procedures for sensitivity analysis of computationally demanding models},
journal = {Reliability Engineering & System Safety},
volume = {94},
number = {11},
pages = {1735-1763},
year = {2009},
issn = {0951-8320},
doi = {https://doi.org/10.1016/j.ress.2009.05.007},
url = {https://www.sciencedirect.com/science/article/pii/S0951832009001112},
author = {Curtis B. Storlie and Laura P. Swiler and Jon C. Helton and Cedric J. Sallaberry},
keywords = {Bootstrap, Confidence intervals, Meta-model, Nonparametric regression, Sensitivity analysis, Surrogate model, Uncertainty analysis, Variance decomposition},
abstract = {The analysis of many physical and engineering problems involves running complex computational models (simulation models, computer codes). With problems of this type, it is important to understand the relationships between the input variables (whose values are often imprecisely known) and the output. The goal of sensitivity analysis (SA) is to study this relationship and identify the most significant factors or variables affecting the results of the model. In this presentation, an improvement on existing methods for SA of complex computer models is described for use when the model is too computationally expensive for a standard Monte-Carlo analysis. In these situations, a meta-model or surrogate model can be used to estimate the necessary sensitivity index for each input. A sensitivity index is a measure of the variance in the response that is due to the uncertainty in an input. Most existing approaches to this problem either do not work well with a large number of input variables and/or they ignore the error involved in estimating a sensitivity index. Here, a new approach to sensitivity index estimation using meta-models and bootstrap confidence intervals is described that provides solutions to these drawbacks. Further, an efficient yet effective approach to incorporate this methodology into an actual SA is presented. Several simulated and real examples illustrate the utility of this approach. This framework can be extended to uncertainty analysis as well.}
}
@article{ZHAO2023100891,
title = {Meet the authors: Yuxuan Zhao, Enmeng Lu, and Yi Zeng},
journal = {Patterns},
volume = {4},
number = {12},
pages = {100891},
year = {2023},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2023.100891},
url = {https://www.sciencedirect.com/science/article/pii/S2666389923002933},
author = {Yuxuan Zhao and Enmeng Lu and Yi Zeng},
abstract = {Yuxuan Zhao, associate professor, Enmeng Lu, research engineer, and Yi Zeng, professor and lab director, have proposed a brain-inspired bodily self-perception model based on biological findings on monkeys and humans. This model can reproduce various rubber hand illusion (RHI) experiments, which helps reveal the RHI’s computational and biological mechanisms. They talk about their view of data science and research plans for brain-inspired robot self-modeling and ethical robots.}
}
@article{OH2023100602,
title = {Making computing visible & tangible: A paper-based computing toolkit for codesigning inclusive computing education activities},
journal = {International Journal of Child-Computer Interaction},
volume = {38},
pages = {100602},
year = {2023},
issn = {2212-8689},
doi = {https://doi.org/10.1016/j.ijcci.2023.100602},
url = {https://www.sciencedirect.com/science/article/pii/S2212868923000399},
author = {HyunJoo Oh and Sherry Hsi and Noah Posner and Colin Dixon and Tymirra Smith and Tingyu Cheng},
keywords = {Paper-based computing, Codesign, Inclusive CS education, Learning through making},
abstract = {MCVT (Making Computing Visible and Tangible) Cards are a toolkit of paper-based computing cards intended for use in the codesign of inclusive computing education. Working with groups of teachers and students over multiple design sessions, we share our toolkit, design drivers and material considerations; and use cases drawn from a week-long codesign workshop where seven teachers made and adapted cards for their future classroom facilitation. Our findings suggest that teachers valued the MCVT toolkit as a resource for their own learning and perceived the cards to be useful for supporting new computational practices, specifically for learning through making and connecting to examples of everyday computing. Critically reviewed by teachers during codesign workshops, the toolkit however posed some implementation challenges and constraints for learning through making and troubleshooting circuitry. From teacher surveys, interviews, workshop video recordings, and teacher-constructed projects, we show how teachers codesigned new design prototypes and pedagogical activities while also adapting and extending paper-based computing materials so their students could take advantage of the unique technical and expressive affordances of MCVT Cards. Our design research contributes a new perspective on using interactive paper computing cards as a medium for instructional materials development to support more inclusive computing education.}
}
@article{RAHMAN20125541,
title = {Developing Mathematical Communication Skills of Engineering Students},
journal = {Procedia - Social and Behavioral Sciences},
volume = {46},
pages = {5541-5547},
year = {2012},
note = {4th WORLD CONFERENCE ON EDUCATIONAL SCIENCES (WCES-2012) 02-05 February 2012 Barcelona, Spain},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2012.06.472},
url = {https://www.sciencedirect.com/science/article/pii/S1877042812022082},
author = {Roselainy Abdul Rahman and Yudariah Mohammad Yusof and Hamidreza Kashefi and Sabariah Baharun},
keywords = {Communication, Mathematical Thinking, Multivariable Calculus, Student's Obstacles},
abstract = {In Malaysia and also elsewhere in the world the demands for graduates who have employability skills such as ability to think critically, solve problems and can communicate are highly sought in the workplace. In the early 2006, the development of such skills was recognized as integral goals of undergraduate education at Universiti Teknologi Malaysia. Since then rigorous efforts have been made to inculcate these skills amongst the undergraduates. In this paper, we will share some of our experiences in coping with the challenges of changing our teaching practices to accommodate this quest though focusing on communication. For mathematics learning to occur, we believed that students should participate actively in the knowledge construction and be able to take charge of their own learning. Taking these aspects into consideration, we had developed a framework of active learning and used it to guide our instruction in engineering mathematics at UTM. Here we will discuss the strategies that we had designed and employed in engaging students with the subject matter as well as to initiate and support student's thinking and communication in the language of mathematics. Some student's responses that gave indications of their struggle, progress and growth encountered in the research implementation will also be presented.}
}
@article{MUSGRAVE2017137,
title = {Understanding and advancing graduate teaching assistants’ mathematical knowledge for teaching},
journal = {The Journal of Mathematical Behavior},
volume = {45},
pages = {137-149},
year = {2017},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2016.12.011},
url = {https://www.sciencedirect.com/science/article/pii/S0732312316302012},
author = {Stacy Musgrave and Marilyn P. Carlson},
keywords = {Graduate student teaching assistant, Mathematical meanings, Average rate of change, Precalculus},
abstract = {Graduate student teaching assistants (GTAs) usually teach introductory level courses at the undergraduate level. Since GTAs constitute the majority of future mathematics faculty, their image of effective teaching and preparedness to lead instructional improvements will impact future directions in undergraduate mathematics curriculum and instruction. In this paper, we argue for the need to support GTAs in improving their mathematical meanings of foundational ideas and their ability to support productive student thinking. By investigating GTAs’ meanings for average rate of change, a key content area in precalculus and calculus, we found evidence that even mathematically sophisticated GTAs possess impoverished meanings of this key idea. We argue for the need, and highlight one approach, for supporting GTAs to improve their understanding of foundational mathematical ideas and how these ideas are learned.}
}
@article{SAIKIA2021129664,
title = {Study of interacting mechanism of amino acid and Alzheimer's drug using vibrational techniques and computational method},
journal = {Journal of Molecular Structure},
volume = {1227},
pages = {129664},
year = {2021},
issn = {0022-2860},
doi = {https://doi.org/10.1016/j.molstruc.2020.129664},
url = {https://www.sciencedirect.com/science/article/pii/S0022286020319773},
author = {Jyotshna Saikia and Bhargab Borah and Th. Gomti Devi},
keywords = {DL-Alanine, Memantine, Raman, FTIR, DFT},
abstract = {The present work is undertaken to investigate the molecular interaction between Memantine (Alzheimer's drug) and DL-Alanine (amino acid) using DFT and vibrational spectroscopic methods, in particular, Fourier Transform Infrared Spectroscopy (FTIR) and Raman techniques. The DFT calculations of these molecules are carried out using the B3LYP/6–311 ++ G (d, p) level of theory. The experimental FTIR and Raman spectra of the molecules are compared to the respective DFT computed wavenumbers. A satisfactory agreement is obtained between experimental and computed wavenumbers. Further, HOMO-LUMO energy gap, Natural Bond Orbital (NBO) analysis, total energy, zero-point vibrational energy, Molecular Electrostatic Potential (MEP), chemical potential, hardness, ionization energy, global electrophilicity index, dipole moments, and first-order hyperpolarizabilities of the interacting state are reported and compared to the respective parameters of the individual states. The NBO analysis of the molecules indicates the transfer of charge between DL-Alanine and Memantine through NH•••O intermolecular hydrogen bonds. The molecular docking studies of the molecules are  performed to investigate the binding affinity of the ligand with the 6DG7 receptor.}
}
@article{MALLETT20241105,
title = {New strategies for the cognitive science of dreaming},
journal = {Trends in Cognitive Sciences},
volume = {28},
number = {12},
pages = {1105-1117},
year = {2024},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2024.10.004},
url = {https://www.sciencedirect.com/science/article/pii/S136466132400264X},
author = {Remington Mallett and Karen R. Konkoly and Tore Nielsen and Michelle Carr and Ken A. Paller},
keywords = {sleep, dreams, memory, neuroscience, natural language processing},
abstract = {Dreams have long captivated human curiosity, but empirical research in this area has faced significant methodological challenges. Recent interdisciplinary advances have now opened up new opportunities for studying dreams. This review synthesizes these advances into three methodological frameworks and describes how they overcome historical barriers in dream research. First, with observable dreaming, neural decoding and real-time reporting offer more direct measures of dream content. Second, with dream engineering, targeted stimulation and lucidity provide routes to experimentally manipulate dream content. Third, with computational dream analysis, the generation and exploration of large dream-report databases offer powerful avenues to identify patterns in dream content. By enabling researchers to systematically observe, engineer, and analyze dreams, these innovations herald a new era in dream science.}
}
@article{LI2023119775,
title = {Neural representations of self-generated thought during think-aloud fMRI},
journal = {NeuroImage},
volume = {265},
pages = {119775},
year = {2023},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2022.119775},
url = {https://www.sciencedirect.com/science/article/pii/S1053811922008965},
author = {Hui-Xian Li and Bin Lu and Yu-Wei Wang and Xue-Ying Li and Xiao Chen and Chao-Gan Yan},
keywords = {Self-generated thoughts, Think-aloud fMRI, Natural language processing, Representational similarity analysis},
abstract = {Is the brain at rest during the so-called resting state? Ongoing experiences in the resting state vary in unobserved and uncontrolled ways across time, individuals, and populations. However, the role of self-generated thoughts in resting-state fMRI remains largely unexplored. In this study, we collected real-time self-generated thoughts during “resting-state” fMRI scans via the think-aloud method (i.e., think-aloud fMRI), which required participants to report whatever they were currently thinking. We first investigated brain activation patterns during a think-aloud condition and found that significantly activated brain areas included all brain regions required for speech. We then calculated the relationship between divergence in thought content and brain activation during think-aloud and found that divergence in thought content was associated with many brain regions. Finally, we explored the neural representation of self-generated thoughts by performing representational similarity analysis (RSA) at three neural scales: a voxel-wise whole-brain searchlight level, a region-level whole-brain analysis using the Schaefer 400-parcels, and at the systems level using the Yeo seven-networks. We found that “resting-state” self-generated thoughts were distributed across a wide range of brain regions involving all seven Yeo networks. This study highlights the value of considering ongoing experiences during resting-state fMRI and providing preliminary methodological support for think-aloud fMRI.}
}
@article{LI2023106299,
title = {Constructing a link between multivariate titanium-based semiconductor band gaps and chemical formulae based on machine learning},
journal = {Materials Today Communications},
volume = {35},
pages = {106299},
year = {2023},
issn = {2352-4928},
doi = {https://doi.org/10.1016/j.mtcomm.2023.106299},
url = {https://www.sciencedirect.com/science/article/pii/S235249282300990X},
author = {Jiawei Li and Zhengxin Chen and Jiang Wu and Jia Lin and Ping He and Rui Zhu and Cheng Peng and Hai Zhang and Wenhao Li and Xu Fang and Hongtao Shen},
keywords = {Random forest model, Chemical formula, Components, Bandgap, Machine learning},
abstract = {Titanium-based semiconductors are wildly recognized as one of the most commonly used photocatalysts for photocatalysis. Energy band modulation is a key aspect of the catalytic activity of photocatalytic semiconductors, but the acquisition of semiconductor energy bands is still a complex and important task. In recent years, machine learning has played an important role in materials prediction, where the crystal structure of a material is usually used as input in energy band prediction. However, existing machine learning algorithms cannot accurately predict the energy bands of materials from structural components. Here, we convert the chemical formula into component descriptors after comparing first principles and ultraviolet-visible spectrophotometry (UV–vis) errors on bandgap values, and the component and bandgap values form a set of labeled data pairs. The chemical formula components are used as input to accurately predict the energy bands of the material. In our evaluation, the model outperforms existing machine learning methods in predicting energy bands, yielding mean absolute value errors of about 0.277 eV, and possesses a significant advantage in component prediction. In particular, this method of predicting the photocatalytic semiconductor energy band gap from chemical formula components provides a new way of thinking about photocatalyst selectivity.}
}
@incollection{VALLERO2021601,
title = {Chapter 14 - The Future},
editor = {Daniel A. Vallero},
booktitle = {Environmental Systems Science},
publisher = {Elsevier},
pages = {601-613},
year = {2021},
isbn = {978-0-12-821953-9},
doi = {https://doi.org/10.1016/B978-0-12-821953-9.00004-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128219539000040},
author = {Daniel A. Vallero},
keywords = {Precautionary principle, Evidence-based risk assessment, Exposome, Translational science, Scientific workflow, Ontologies, Resilience, Data-driven decision-making, Precision science, Life cycle risk assessment (LCRA)},
abstract = {Solving and preventing environmental problems will continue to rely on systems thinking that translates and combines data, information, knowledge, and wisdom from numerous scientific and other perspectives. The chapter provides insights into possible directions for environmental systems science, especially ways to address complexities at every scale from cellular to planetary. Environmental scientists and engineers will engage in precision science and customized approaches to reduce risks, improve reliability and resilience, and ensure sustainability.}
}
@article{GUSTAFSON199557,
title = {Theory and computation of periodic solutions of autonomous partial differential equation boundary value problems, with application to the driven cavity problem},
journal = {Mathematical and Computer Modelling},
volume = {22},
number = {9},
pages = {57-75},
year = {1995},
issn = {0895-7177},
doi = {https://doi.org/10.1016/0895-7177(95)00168-2},
url = {https://www.sciencedirect.com/science/article/pii/0895717795001682},
author = {K. Gustafson},
keywords = {Navier-Stokes equations, Driven cavity problem, Pressure boundary condition, Vortex shedding, Hopf bifurcation},
abstract = {In ordinary differential equations, one distinguishes two cases: autonomous and nonautonomous. Roughly speaking, the theory of the latter is built upon the theory of the former. The same distinction should be applied to partial differential equations, where much less is known. Here I will focus on the question of the generation of periodic solutions for autonomous partial differential equation boundary value problems. Specifically, I consider the incompressible Navier-Stokes equations, and the important driven cavity problem. For simplicity, attention is restricted to two bifurcation parameters, the Reynolds number and the Aspect ratio. Only Dirichlet velocity boundary conditions are considered. Both the known theory and known computational results for the driven cavity are surveyed. The importance of computationally adhering to the div u = 0 condition to accurately simulate unsteady flows which will be qualitatively correct for the incompressible Navier-Stokes equations is stressed. The dependence of sustained periodicity upon the existence of highly localized vortex shedding sequences somewhere along the boundary is pointed out. A new analysis of the pressure boundary condition, based upon a general regularity principle, is given. A conjectured Hopf bifurcation criticality curve is explained.}
}
@article{GALLISTEL2021104533,
title = {The physical basis of memory},
journal = {Cognition},
volume = {213},
pages = {104533},
year = {2021},
note = {Special Issue in Honour of Jacques Mehler, Cognition’s founding editor},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2020.104533},
url = {https://www.sciencedirect.com/science/article/pii/S0010027720303528},
author = {C.R. Gallistel},
keywords = {Engram, Communication channel, Plastic synapse, Molecules},
abstract = {Neuroscientists are searching for the engram within the conceptual framework established by John Locke's theory of mind. This framework was elaborated before the development of information theory, before the development of information processing machines and the science of computation, before the discovery that molecules carry hereditary information, before the discovery of the codon code and the molecular machinery for editing the messages written in this code and translating it into transcription factors that mark abstract features of organic structure such as anterior and distal. The search for the engram needs to abandon Locke's conceptual framework and work within a framework informed by these developments. The engram is the medium by which information extracted from past experience is transmitted to the computations that inform future behavior. The information-conveying symbols in the engram are rapidly generated in the course of computations, which implies that they are molecules.}
}
@article{ZOHDI20073927,
title = {Computation of strongly coupled multifield interaction in particle–fluid systems},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {196},
number = {37},
pages = {3927-3950},
year = {2007},
note = {Special Issue Honoring the 80th Birthday of Professor Ivo Babuška},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2006.10.040},
url = {https://www.sciencedirect.com/science/article/pii/S004578250700117X},
author = {T.I. Zohdi},
keywords = {Particle–fluid interaction, Multiple fields, Iterative methods},
abstract = {The present work develops a flexible and robust solution strategy to resolve coupled systems comprised of large numbers of flowing particles embedded within a fluid. A model problem, consisting of particles which may undergo inelastic collisions in the presence of near-field forces, is considered. The particles are surrounded by a continuous interstitial fluid which is assumed to obey the compressible Navier–Stokes equations. Thermal effects are also considered. Such particle/fluid systems are strongly coupled, due to the mechanical forces and heat transfer induced by the fluid onto the particles and vice-versa. Because the coupling of the various particle and fluid fields can dramatically change over the course of a flow process, a primary focus of this work is the development of a recursive “staggering” solution scheme, whereby the time-steps are adaptively adjusted to control the error associated with the incomplete resolution of the coupled interaction between the various solid particulate and continuum fluid fields. A central feature of the approach is the ability to account for the presence of particles within the fluid in a straightforward manner that can be easily incorporated within any standard computational fluid mechanics code based on finite difference, finite element or finite volume type discretization. A three dimensional example is provided to illustrate the overall approach.}
}
@article{HAANTJES2025204028,
title = {Towards an integrated view and understanding of embryonic signalling during murine gastrulation},
journal = {Cells & Development},
pages = {204028},
year = {2025},
issn = {2667-2901},
doi = {https://doi.org/10.1016/j.cdev.2025.204028},
url = {https://www.sciencedirect.com/science/article/pii/S266729012500035X},
author = {Rhanna R. Haantjes and Jeske Strik and Joëlle {de Visser} and Marten Postma and Renée {van Amerongen} and Antonius L. {van Boxtel}},
keywords = {Embryo, Signalling, Gastrulation, Gastruloid, Mouse, ES cells, Computational modelling},
abstract = {At the onset of mammalian gastrulation, secreted signalling molecules belonging to the Bmp, Wnt, Nodal and Fgf signalling pathways induce and pattern the primitive streak, marking the start for the cellular rearrangements that generate the body plan. Our current understanding of how signalling specifies and organises the germ layers in three dimensions, was mainly derived from genetic experimentation using mouse embryos performed over many decades. However, the exact spatiotemporal sequence of events is still poorly understood, both because of a lack of tractable models that allow for real time visualisation of signalling and differentiation and because of the molecular and cellular complexity of these early developmental events. In recent years, a new wave of in vitro embryo models has begun to shed light on the dynamics of signalling during primitive streak formation. Here we discuss the similarities and differences between a widely adopted mouse embryo model, termed gastruloids, and real embryos from a signalling perspective. We focus on the gene regulatory networks that underlie signalling pathway interactions and outline some of the challenges ahead. Finally, we provide a perspective on how embryo models may be used to advance our understanding of signalling dynamics through computational modelling.}
}
@article{ASSIOURAS2025104063,
title = {The evolution of artificial empathy in the hospitality metaverse era},
journal = {International Journal of Hospitality Management},
volume = {126},
pages = {104063},
year = {2025},
issn = {0278-4319},
doi = {https://doi.org/10.1016/j.ijhm.2024.104063},
url = {https://www.sciencedirect.com/science/article/pii/S027843192400375X},
author = {Ioannis Assiouras and Cornelia Laserer and Dimitrios Buhalis},
keywords = {Empathy, Artificial empathy, Artificial intelligence, Metaverse, Hospitality, Artificial intelligence agents},
abstract = {As hospitality enters the metaverse era, artificial empathy becomes essential for developing of artificial intelligence (AI) agents. Using the empathy cycle model, computational empathy frameworks and interdisciplinary research, this conceptual paper proposes a model explaining how artificial empathy will evolve in the hospitality metaverse era. The paper also addresses customer empathy and responses towards AI agents and other human actors with in the hospitality context. It explores how metaverse characteristics such as immersiveness, sociability, experiential nature, interoperability, blended virtual and physical environments as well as environmental fidelity will shape computational models and evolution of artificial empathy. Findings suggests that metaverse enables AI agents to form a seamless cycle of detection, resonation, and response to consumers’ affective states, facilitating the evolution of artificial empathy. Additionally, the paper outlines conditions under which the artificial empathy cycle may be disrupted and proposes future research questions that can advance our understanding of artificial empathy.}
}
@article{KHAZAEI2025115420,
title = {Renewable energy portfolio in Mexico for Industry 5.0 and SDGs: Hydrogen, wind, or solar?},
journal = {Renewable and Sustainable Energy Reviews},
volume = {213},
pages = {115420},
year = {2025},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2025.115420},
url = {https://www.sciencedirect.com/science/article/pii/S1364032125000930},
author = {Moein Khazaei and Fatemeh Gholian-Jouybari and Mahdi {Davari Dolatabadi} and Aryan {Pourebrahimi Alamdari} and Hamidreza Eskandari and Mostafa Hajiaghaei-Keshteli},
keywords = {Renewable energies, Industry 5.0, Sustainable development goals, Portfolio selection, Nearshoring},
abstract = {Despite a surge in Foreign Direct Investment (FDI) in Mexico, like nearshoring, the slow growth in international investment in renewables challenges the country's progress in achieving Sustainable Development Goals (SDGs) related to clean energy. To the best of current knowledge, this research is one of the first to explore the integration of renewable energy (Green/Blue/Turquoise Hydrogen, Solar, and Wind plants) in Mexico, emphasizing a diverse portfolio of projects aligned with SDGs and Industry 5.0. While previous works have focused on the nexus between energy, Industry 4.0, and sustainability, the present study advances this discourse by incorporating Industry 5.0 principles and a comprehensive methodological approach. Through a comprehensive methodology involving Value-Focused Thinking (VFT), fuzzy Decision-Making Trial and Evaluation Laboratory (DEMATEL), and multi-objective mathematical programming, the study identifies key criteria encompassing social, economic, environmental, and technological dimensions. The resulting criteria form a robust framework for evaluating project sustainability. The fuzzy DEMATEL analysis reveals intricate interrelations among criteria, emphasizing the need for balanced considerations. Results highlighted job creation, income equality, and microfinance support as key social considerations, while energy-related criteria emphasized sustainable practices. The proposed multi-objective programming model and COmbined COmpromise SOlution (COCOSO) method facilitated the selection of eight projects, with one project as the top-ranked option across various scoring strategies. Overall, this research provides a nuanced roadmap for effective decision-making in renewable energy projects, offering insights into project strengths, weaknesses, and potential areas for improvement.}
}
@incollection{2004201,
title = {Chapter 6 Alternatives to purely Lagrangian computations},
editor = {Jonas A. Zukas},
series = {Studies in Applied Mechanics},
publisher = {Elsevier},
volume = {49},
pages = {201-250},
year = {2004},
booktitle = {Introduction to Hydrocodes},
issn = {0922-5382},
doi = {https://doi.org/10.1016/S0922-5382(04)80007-2},
url = {https://www.sciencedirect.com/science/article/pii/S0922538204800072},
abstract = {Publisher Summary
Lagrangian techniques deal with problems involving fast and transient loading. Lagrangian methods offer several advantages over the competition. Because Lagrangian codes cannot solve all the problems involving fast short-duration loading, other techniques shoulb be mentioned. The chapter describes the popular alternative methods, Euler codes, coupled Euler-Lagrange codes, arbitrary Lagrange-Euler (ALE) techniques, and meshless methods The advantages of Lagrange codes are offset by grid distortion. With large distortions, the time increment for advancing the computations is forced to approach zero, thus rendering the calculations uneconomical. The use of sliding interfaces and rezoning can extend the range of applicability of Lagrange codes to larger distortions. Similarly, the ability to handle large distortions in Euler codes is offset by the need to account for material transport. Pure Euler techniques are ideal for handling large distortions.}
}
@article{WOLFF2024893,
title = {The mediodorsal thalamus in executive control},
journal = {Neuron},
volume = {112},
number = {6},
pages = {893-908},
year = {2024},
issn = {0896-6273},
doi = {https://doi.org/10.1016/j.neuron.2024.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S0896627324000023},
author = {Mathieu Wolff and Michael M. Halassa},
keywords = {thalamus, mediodorsal, prefrontal cortex, cognition, cognitive flexibility, computation, neural architectures},
abstract = {Summary
Executive control, the ability to organize thoughts and action plans in real time, is a defining feature of higher cognition. Classical theories have emphasized cortical contributions to this process, but recent studies have reinvigorated interest in the role of the thalamus. Although it is well established that local thalamic damage diminishes cognitive capacity, such observations have been difficult to inform functional models. Recent progress in experimental techniques is beginning to enrich our understanding of the anatomical, physiological, and computational substrates underlying thalamic engagement in executive control. In this review, we discuss this progress and particularly focus on the mediodorsal thalamus, which regulates the activity within and across frontal cortical areas. We end with a synthesis that highlights frontal thalamocortical interactions in cognitive computations and discusses its functional implications in normal and pathological conditions.}
}
@incollection{RUNCO20141,
title = {Chapter 1 - Cognition and Creativity},
editor = {Mark A. Runco},
booktitle = {Creativity (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {San Diego},
pages = {1-38},
year = {2014},
isbn = {978-0-12-410512-6},
doi = {https://doi.org/10.1016/B978-0-12-410512-6.00001-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780124105126000011},
author = {Mark A. Runco},
keywords = {Threshold theory, IQ, Structure of intellect, Associative theory, Problem solving, Problem finding, Incubation, Insight, Intuition, Meta-cognition, Mindfulness, Overinclusive thinking},
abstract = {This chapter discusses various aspects of cognition and creativity. Cognitive theories focus on thinking skills and intellectual processes. The approaches to creative cognition are extremely varied. There are bridges between basic cognitive processes and creative problem solving, as well as connections with intelligence, problem solving, language, and other indications of individual differences. The basic processes are generally nomothetic, meaning that they represent universals. Divergent thinking is employed when an individual is faced with an open-ended task. From this perspective divergent thinking is a kind of problem solving. Divergent thinking is not synonymous with creative thinking, but it does tell something about the cognitive processes that may lead to original ideas and solutions. Many theories of creative cognition look to associative processes. Associative theories focus on how ideas are generated and chained together. Cognitive theories of creativity often focus specifically on the problem-solving process. A problem can be defined as a situation with a goal and an obstacle. The stage models of creative cognition are also elaborated.}
}
@article{HARDING2024295,
title = {A new predictive coding model for a more comprehensive account of delusions},
journal = {The Lancet Psychiatry},
volume = {11},
number = {4},
pages = {295-302},
year = {2024},
issn = {2215-0366},
doi = {https://doi.org/10.1016/S2215-0366(23)00411-X},
url = {https://www.sciencedirect.com/science/article/pii/S221503662300411X},
author = {Jessica Niamh Harding and Noham Wolpe and Stefan Peter Brugger and Victor Navarro and Christoph Teufel and Paul Charles Fletcher},
abstract = {Summary
Attempts to understand psychosis—the experience of profoundly altered perceptions and beliefs—raise questions about how the brain models the world. Standard predictive coding approaches suggest that it does so by minimising mismatches between incoming sensory evidence and predictions. By adjusting predictions, we converge iteratively on a best guess of the nature of the reality. Recent arguments have shown that a modified version of this framework—hybrid predictive coding—provides a better model of how healthy agents make inferences about external reality. We suggest that this more comprehensive model gives us a richer understanding of psychosis compared with standard predictive coding accounts. In this Personal View, we briefly describe the hybrid predictive coding model and show how it offers a more comprehensive account of the phenomenology of delusions, thereby providing a potentially powerful new framework for computational psychiatric approaches to psychosis. We also make suggestions for future work that could be important in formalising this novel perspective.}
}
@article{KELLEY2021439,
title = {Applying Independent Core Observer Model Cognitive Architecture to a Collective Intelligence System},
journal = {Procedia Computer Science},
volume = {190},
pages = {439-449},
year = {2021},
note = {2020 Annual International Conference on Brain-Inspired Cognitive Architectures for Artificial Intelligence: Eleventh Annual Meeting of the BICA Society},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.06.052},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921012977},
author = {David Kelley},
keywords = {Collective Intelligence Systems, Independent Core Observer Model, Artificial General Intelligence, mediated Artificial Superintelligence, Hive Mind, AGI, ICOM, mASI.},
abstract = {This paper shows how the Independent Core Observer Model (ICOM) Cognitive Architecture for Artificial General Intelligence (AGI) can be applied to building a collective intelligence system called a mediated Artificial Superintelligence (mASI). The details include breaking down the ICOM implementation in the form of the mASI system and the general performance of initial studies with the mASI. Details of the primary difference between the Independent Core Observer Model Cognitive Architecture and the mASI architecture variant include inserting humanity in the contextual engine components of ICOM, creating a type of collective intelligence. Humans can ‘mediate’ new system-generated thinking keeping the thought process accessible and slow enough for humans to oversee and understand. This also allows the modification of emotional valences of the thought process of the mASI system to help the system generate complex contextual models (knowledge graphs) of new ideas and which speeds up the learning process. With the humans acting as control rods in a reactor and emotional drivers, the mASI system maintains safety where the system would cease to function if humans walked away.}
}
@article{YAN2025109327,
title = {An approach to calculate conceptual distance across multi-granularity based on three-way partial order structure},
journal = {International Journal of Approximate Reasoning},
volume = {177},
pages = {109327},
year = {2025},
issn = {0888-613X},
doi = {https://doi.org/10.1016/j.ijar.2024.109327},
url = {https://www.sciencedirect.com/science/article/pii/S0888613X24002147},
author = {Enliang Yan and Pengfei Zhang and Tianyong Hao and Tao Zhang and Jianping Yu and Yuncheng Jiang and Yuan Yang},
keywords = {Partial order structure, Concept-cognitive learning, Knowledge distance, Three-way decision, Granular computing, Concept graph},
abstract = {The computation of concept distances aids in understanding the interrelations among entities within knowledge graphs and uncovering implicit information. The existing studies predominantly focus on the conceptual distance of specific hierarchical levels without offering a unified framework for comprehensive exploration. To overcome the limitations of unidimensional approaches, this paper proposes a method for calculating concept distances at multiple granularities based on a three-way partial order structure. Specifically: (1) this study introduces a methodology for calculating inter-object similarity based on the three-way attribute partial order structure (APOS); (2) It proposes the application of the similarity matrix to delineate the structure of categories; (3) Based on the similarity matrix describing the three-way APOS of categories, we establish a novel method for calculating inter-category distance. The experiments on eight datasets demonstrate that this approach effectively differentiates various concepts and computes their distances. When applied to classification tasks, it exhibits outstanding performance.}
}
@article{ACISECHE2023109386,
title = {A perspective on the sharing of docking data},
journal = {Data in Brief},
volume = {49},
pages = {109386},
year = {2023},
issn = {2352-3409},
doi = {https://doi.org/10.1016/j.dib.2023.109386},
url = {https://www.sciencedirect.com/science/article/pii/S2352340923004985},
author = {Samia Aci-Sèche and Stéphane Bourg and Pascal Bonnet and Joseph Rebehmed and Alexandre G. {de Brevern} and Julien Diharce},
keywords = {3D coordinates, Docking, Files, SDF, Sharing, FAIR principles},
abstract = {Computational approaches are nowadays largely applied in drug discovery projects. Among these, molecular docking is the most used for hit identification against a drug target protein. However, many scientists in the field shed light on the lack of availability and reproducibility of the data obtained from such studies to the whole community. Consequently, sustaining and developing the efforts toward a large and fully transparent sharing of those data could be beneficial for all researchers in drug discovery. The purpose of this article is first to propose guidelines and recommendations on the appropriate way to conduct virtual screening experiments and second to depict the current state of sharing molecular docking data. In conclusion, we have explored and proposed several prospects to enhance data sharing from docking experiment that could be developed in the foreseeable future.}
}