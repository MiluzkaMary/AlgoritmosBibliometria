@article{YAN2024120835,
title = {CPS-3WS: A critical pattern supported three-way sampling method for classifying class-overlapped imbalanced data},
journal = {Information Sciences},
volume = {676},
pages = {120835},
year = {2024},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2024.120835},
url = {https://www.sciencedirect.com/science/article/pii/S0020025524007497},
author = {Yuanting Yan and Zhong Zheng and Yiwen Zhang and Yanping Zhang and Yiyu Yao},
keywords = {Three-way sampling, Class-imbalance problem, Critical pattern, Class overlap},
abstract = {Class-imbalance problem widely exists in real applications ranging from medial diagnosis to economic fraud detection, etc. As one of the mainstream techniques in dealing with imbalanced data, SMOTE (Synthetic Minority Over-sampling TEchnique) and its extensions mainly rebalance the datasets via generation of observations in specific regions with various adapted strategies. Many of them do not consider the cost of role assignment of samples, and the intractable data complexity (overlap, small disjuncts, etc.) poses additional challenges to them. This paper proposes a critical pattern supported three-way sampling method (CPS-3WS) for classifying class-overlapped imbalanced data, introducing the philosophy of thinking in threes to effective classification in imbalanced learning. Specifically, CPS-3WS uses a three-way sample partition strategy with the Bayes posterior probability by dividing majority and minority classes into three disjoint subsets: risky, critical and safe patterns. CPS-3WS conducts a three-way hybrid sampling through (i) evaluating the risky majority pattern to be eliminated and (ii) selecting critical minority pattern to synthesize new samples under local information constraint. Extensive experiments on 42 UCI benchmark datasets demonstrate the superiority of the proposed CPS-3WS compared with 11 data-level methods. The source code of CPS-3WS is available at https://github.com/ytyancp/CPS-3WS.}
}
@article{GOUTAUDIER2021113755,
title = {Proper Generalized Decomposition with time adaptive space separation for transient wave propagation problems in separable domains},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {380},
pages = {113755},
year = {2021},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2021.113755},
url = {https://www.sciencedirect.com/science/article/pii/S0045782521000918},
author = {Dimitri Goutaudier and Laurent Berthe and Francisco Chinesta},
keywords = {Proper Generalized Decomposition (PGD), Transient wave propagation, Time adaptive space separation, Separable domain, Scalar wave equation, Elastodynamics},
abstract = {Transient wave propagation problems may involve rich discretizations, both in space and in time, leading to computationally expensive simulations, even for simple spatial domains. The Proper Generalized Decomposition (PGD) is an attractive model order reduction technique to address this issue, especially when the spatial domain is separable. In this work, we propose a space separation with a time adaptive number of modes to efficiently capture transient wave propagation in separable domains. We combine standard time integration schemes with this original space separated representation for empowering standard procedures. The numerical behavior of the proposed method is explored through several 2D wave propagation problems involving radial waves, propagation on long time analyses, and wave conversions. We show that the PGD solution approximates its standard finite element solution counterpart with acceptable accuracy, while reducing the storage needs and the computation time (CPU time). Numerical results show that the CPU time per time step linearly increases when refining the mesh, even with implicit time integration schemes, which is not the case with standard procedures.}
}
@article{BOLER201875,
title = {The affective politics of the “post-truth” era: Feeling rules and networked subjectivity},
journal = {Emotion, Space and Society},
volume = {27},
pages = {75-85},
year = {2018},
issn = {1755-4586},
doi = {https://doi.org/10.1016/j.emospa.2018.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S1755458617301585},
author = {Megan Boler and Elizabeth Davis},
keywords = {Affect, Emotion, Post-truth, Feeling rules, Truthiness, Digital media, Algorithmic governance, Computational propaganda},
abstract = {This essay maps interdisciplinary lines of inquiry to assess current research on affect and emotion in relation to digital and social media, in the context of the fractured news media landscape and increasingly visible emotionality in political life. The essay sketches the context of polarized emotionality and the crisis of truth characterizing current U.S. politics, centrally engaging Arlie Hochschild's concept of “feeling rules”. We explore the limitations of “affect theory” for researching mediatized politics, contending that the stark differentiation of “affect” from “emotion” reifies the rational, autonomous, liberal conception of the subject, and is of limited value for political communications research. Instead, we emphasize the relational nature of affect and emotion, and the value of feminist politics of emotion research. Our analysis evaluates the limitations of contemporary scholarship on affect, social media, and politics in the context of the grave challenges posed by algorithmic governance and computational propaganda. We conclude by suggesting the concept of “networked subjectivity” for understanding mediatized politics, and the importance of the “affective feedback loop” within the context of the social media “culture of likes.”}
}
@article{BACK202423,
title = {Accelerated chemical science with AI},
journal = {Digital Discovery},
volume = {3},
number = {1},
pages = {23-33},
year = {2024},
issn = {2635-098X},
doi = {https://doi.org/10.1039/d3dd00213f},
url = {https://www.sciencedirect.com/science/article/pii/S2635098X24000858},
author = {Seoin Back and Alán Aspuru-Guzik and Michele Ceriotti and Ganna Gryn'ova and Bartosz Grzybowski and Geun Ho Gu and Jason Hein and Kedar Hippalgaonkar and Rodrigo Hormázabal and Yousung Jung and Seonah Kim and Woo Youn Kim and Seyed Mohamad Moosavi and Juhwan Noh and Changyoung Park and Joshua Schrier and Philippe Schwaller and Koji Tsuda and Tejs Vegge and O. Anatole {von Lilienfeld} and Aron Walsh},
abstract = {In light of the pressing need for practical materials and molecular solutions to renewable energy and health problems, to name just two examples, one wonders how to accelerate research and development in the chemical sciences, so as to address the time it takes to bring materials from initial discovery to commercialization. Artificial intelligence (AI)-based techniques, in particular, are having a transformative and accelerating impact on many if not most, technological domains. To shed light on these questions, the authors and participants gathered in person for the ASLLA Symposium on the theme of ‘Accelerated Chemical Science with AI’ at Gangneung, Republic of Korea. We present the findings, ideas, comments, and often contentious opinions expressed during four panel discussions related to the respective general topics: ‘Data’, ‘New applications’, ‘Machine learning algorithms’, and ‘Education’. All discussions were recorded, transcribed into text using Open AI's Whisper, and summarized using LG AI Research's EXAONE LLM, followed by revision by all authors. For the broader benefit of current researchers, educators in higher education, and academic bodies such as associations, publishers, librarians, and companies, we provide chemistry-specific recommendations and summarize the resulting conclusions.}
}
@article{STAMMITTI2013e58,
title = {Spreadsheets for assisting Transport Phenomena Laboratory experiences},
journal = {Education for Chemical Engineers},
volume = {8},
number = {2},
pages = {e58-e71},
year = {2013},
issn = {1749-7728},
doi = {https://doi.org/10.1016/j.ece.2013.02.005},
url = {https://www.sciencedirect.com/science/article/pii/S1749772813000067},
author = {Aurelio Stammitti},
keywords = {Educational spreadsheets, Transport Phenomena Laboratory, Laboratory experience quality, Data processing task, Hands-on learning, Student analytical thinking},
abstract = {Academic laboratories have been traditionally used for complementing and reinforcing in a practical way the theoretical instruction received in classroom lectures. However, data processing and model evaluation tasks are time consuming and do not add much value to the student's learning experience as they reduce available time for result analysis, critical thinking and report writing skills development. Therefore, this project addressed this issue by selecting three experiences of the Transport Phenomena Laboratory, namely: metallic bar temperature profiles, transient heat conduction and fixed and fluidised bed behaviour, and developed a spreadsheet for each one of them. These spreadsheets, without demanding programming skills, easily process experimental data sets, evaluate complex analytical and numerical models and correlations, not formerly considered and, convey results in tables and plots. Chemical engineering students that tested the spreadsheets were surveyed and expressed the added value of the sheets, being user-friendly, helped them to fulfil lab objectives by reducing their workload and, allowed them to complete deeper analyses that instructors could not request before, as they were able to quickly evaluate, compare and validate different model assumptions and correlations. Students also provided valuable suggestions for improving the spreadsheet experience. Through these sheets, students’ lab learning experience was updated.}
}
@article{KAMATH2020100944,
title = {Making grammars for material and tectonic complexity: An example of a thin-tile vault},
journal = {Design Studies},
volume = {69},
pages = {100944},
year = {2020},
issn = {0142-694X},
doi = {https://doi.org/10.1016/j.destud.2020.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X20300326},
author = {Ayodh Vasant Kamath},
keywords = {affordance, architectural design, creativity, reflective practice, making grammars},
abstract = {Shape grammars are a framework to view design as non-deterministic, creative, visual computation, and making as the deterministic execution of a design in the material world. Making grammars conceive of making as creative, multi-sensory, material computation. However, examples in the literature on making grammar are insufficiently complex to demonstrate the creativity of non-visual senses in making. This paper develops a making grammar for thin-tile vault construction as a sensory ethnography to ‘show making’ to designers as being a creative practice involving visual and non-visual senses. To do so, the role of drawing in shape grammar and making grammar is differentiated, and environmental psychology is used to develop a framework for the use of drawing to depict multi-sensory processes in making grammar.}
}
@article{LILI20171611,
title = {An Inverse Optimization Model for Human Resource Allocation Problem Considering Competency Disadvantage Structure},
journal = {Procedia Computer Science},
volume = {112},
pages = {1611-1622},
year = {2017},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 21st International Conference, KES-20176-8 September 2017, Marseille, France},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2017.08.248},
url = {https://www.sciencedirect.com/science/article/pii/S1877050917316575},
author = {Zhang Lili},
keywords = {inverse optimization, linear programming, human resource allocation, competency, evaluation according to disadvantage structure},
abstract = {Most of serious and major accidents that happened during the production procedure of process industry are caused by improper equipment operations, which is further owing to inappropriate human resources allocation and ignorance of individual competencies differences. In order to take both of competency disadvantage and adjustment requirement into consideration, we use an inverse optimization method to solve a human resource allocation problem, and furthermore, adjust equipment operating parameters to make the per-defined settings optimized, such as the total number of jobs, security-related parameters and so on.In the solving process, firstly a standard competence hierarchy system is conducted; secondly we propose an assessment method according to disadvantage structure; thirdly we use inverse optimization method to solve the problem and optimize the predefined allocation plan. Lastly, we give an example to prove its feasibility and effectiveness. In this paper a novel formulation of human resource allocation problem is proposed, in which some of main individual characteristics are considered and described mathematically, including psychology, behaviour and characteristics diged from them such as weakness. The other contribution of this paper is using inverse optimization to adjust parameters based on the given ideal allocation plan. Both of these propositions have a positive significance on promoting development and security construction for process industries.This research incorporates the academic thinking of inverse optimization, it not only puts psychology and behavior into optimization model, but also data mines weakness characteristics under the psychology and behavior data, and find a new way to introducing the weakness characteristics into decision making model. It provides a new thought for the following decision making problem, that is the ideal decision plan is known, and optimization parameters are changeable. It promotes the combining of psychology, behavior and operations research, it is good for process industries to develop in a safety and efficiency way.}
}
@incollection{WHANGBO2005765,
title = {Chapter 26 - Concepts of perturbation, orbital interaction, orbital mixing and orbital occupation},
editor = {Clifford E. Dykstra and Gernot Frenking and Kwang S. Kim and Gustavo E. Scuseria},
booktitle = {Theory and Applications of Computational Chemistry},
publisher = {Elsevier},
address = {Amsterdam},
pages = {765-784},
year = {2005},
isbn = {978-0-444-51719-7},
doi = {https://doi.org/10.1016/B978-044451719-7/50069-X},
url = {https://www.sciencedirect.com/science/article/pii/B978044451719750069X},
author = {Myung-Hwan Whangbo},
abstract = {Publisher Summary
This chapter describes the concepts of perturbation, orbital interaction, orbital mixing, and orbital occupation work at all levels of electronic structure. These qualitative concepts provide a conceptual framework in which to rationalize experimental/theoretical observations and to generate qualitative predictions. An important role of an electronic structure theory is to provide quantitative predictions. In this role theoretical predictions require developments of efficient programs for theoretical computations. Another important role of an electronic structure theory is to provide a conceptual framework in which to think and organize. In this role, the theoretical predictions need not be quantitative but should provide a bias toward correct thinking about further experimental and theoretical studies. When combined with the ideas of symmetry and overlap, the concepts of perturbation, orbital interaction, orbital mixing, and orbital occupation have been indispensable not only in understanding structure – property relationships in various chemical compounds but also in interpreting results of electronic structure calculations. These qualitative concepts work at all levels of electronic structure descriptions from one-electron theory neglecting self-consistent-field adjustments of orbitals to theories including electron correlation and to those including relativistic effects.}
}
@article{ZEITHAMMER2024,
title = {Strange Case of Dr. Bidder and Mr. Entrant: Consumer Preference Inconsistencies in Costly Price Offers},
journal = {International Journal of Research in Marketing},
year = {2024},
issn = {0167-8116},
doi = {https://doi.org/10.1016/j.ijresmar.2024.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S016781162400079X},
author = {Robert Zeithammer and Lucas Stich and Martin Spann and Gerald Häubl},
keywords = {Pricing, Auctions, Entry costs, Behavioral economics, Experiments},
abstract = {Consumers make price offers to sellers in a variety of domains, such as when buying cars or houses or when bidding in auctions for airline upgrades, art, or collectibles. Submitting an offer typically entails administrative, waiting, and opportunity costs. Making such costly price offers involves two intertwined decisions—in addition to determining how much to offer, consumers must also decide whether to make an offer in the first place. We examine the impact of offer-submission costs on consumer behavior using a series of incentive-compatible experiments. Our findings reveal a preference inconsistency, whereby the preferences implied by one of the decisions do not align with the preferences implied by the other. In particular, potential buyers enter more often than their offer amounts would predict based on standard economic models. This preference inconsistency is robust to two interventions designed to help consumers make offer-amount and entry decisions—(1)the provision of interactive-feedback decision aids and (2)the sequencing of the two sub-decisions in the normative order. Neither of these interventions resolves the inconsistency. Instead, the patterns of results suggest that consumers approach the offer-amount and entry decisions as if they were unrelated. We discuss the implications of our findings for the design of offer-submission interfaces, as well as for econometric attempts to infer consumer preferences from offer and bidding data.}
}
@article{ALRAKHAMI2021107573,
title = {A deep learning-based edge-fog-cloud framework for driving behavior management},
journal = {Computers & Electrical Engineering},
volume = {96},
pages = {107573},
year = {2021},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2021.107573},
url = {https://www.sciencedirect.com/science/article/pii/S0045790621005127},
author = {Mabrook S. Al-Rakhami and Abdu Gumaei and Mohammad Mehedi Hassan and Atif Alamri and Musaed Alhussein and Md. Abdur Razzaque and Giancarlo Fortino},
keywords = {Deep learning, Car mobile edge (CME), Fog and cloud computing, Aggressive driving behaviors},
abstract = {Among the various reasons behind vehicle accidents, drivers' aggressiveness and distractions play a significant role. Deep learning (DL) algorithms inside a car mobile edge (CME) have been used for driver monitoring and to perform automated decision-making processes. Training and retraining the DL models in resource-constrained CME devices come with several challenges, especially regarding computational and memory space costs. Moreover, training the DL models periodically on representative data nearest to CME without imposing communication overheads on the cloud improves the quality of service (QoS) parameters, such as memory demand, processing time, power consumption, and bandwidth. This paper investigates the deployment of a deep neural network (DNN) model on a cloud-fog-edge computing framework for aggressive driver behavior detection and monitoring. To reach this goal, our framework proposes utilizing effective systems and databases of sensor-based metrics and data, cost-effective wireless networks, cloud-and fog-edge computing technologies, and the Internet. Experimental results of the DNN model showed that the accuracy of detection is improved by 1.84% compared with the current related work without any pre-processing window on data points that come from bio-signal sensors. Moreover, the experimental results of the networking part prove the efficiency and effectiveness of the proposed framework.}
}
@incollection{FARMER2008228,
title = {Chapter 11 - Fragment-Based Drug Discovery},
editor = {Camille Georges Wermuth},
booktitle = {The Practice of Medicinal Chemistry (Third Edition)},
publisher = {Academic Press},
edition = {Third Edition},
address = {New York},
pages = {228-243},
year = {2008},
isbn = {978-0-12-374194-3},
doi = {https://doi.org/10.1016/B978-0-12-374194-3.00011-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780123741943000111},
author = {Bennett T. Farmer and Allen B. Reitz},
abstract = {Publisher Summary
Although target proteins are flexible and can adopt one or more of a manifold of induced conformations, binding sites on proteins have evolved to recognize a limited number of endogenous modulators and substrates and to exclude others. This chapter reviews fragment-based drug discovery (FBDD) on the historical and operational level and explains how it can be applied on a case-by-case basis. FBBD determines which molecular substructures or fragments interact with targets of interest and how they bind, and then uses that information to obtain drugs for therapy. It represents a paradigm shift in thinking of how to approach the lead generation process in drug discovery, and is an attempt to get more information rapidly while doing the same amount of work overall. The study draws comparison between the FBDD and HTS/HTL approaches. In the FBDD approach, the medicinal chemist plays the role of a combined synthetic and structural chemist. The emphasis on informatics is greatly reduced because there is less data overall and most of it, such as from NMR or X-ray crystal structures, is visually analyzed, typically being complemented only by functional assay data on just the target itself. Several different computational methods have been developed to prescreen fragment libraries as a way to select members for further study and consideration.}
}
@article{BAMOROVAT202321,
title = {Poor adherence is a major barrier to the proper treatment of cutaneous leishmaniasis: A case-control field assessment in Iran},
journal = {International Journal for Parasitology: Drugs and Drug Resistance},
volume = {21},
pages = {21-27},
year = {2023},
issn = {2211-3207},
doi = {https://doi.org/10.1016/j.ijpddr.2022.11.006},
url = {https://www.sciencedirect.com/science/article/pii/S2211320722000331},
author = {Mehdi Bamorovat and Iraj Sharifi and Setareh {Agha Kuchak Afshari} and Ali Karamoozian and Amirhossein Tahmouresi and Amireh Heshmatkhah and Ehsan Salarkia and Ahmad Khosravi and Maryam {Hakimi Parizi} and Maryam Barghi},
keywords = {Poor adherence, Cutaneous leishmaniasis, Major barrier, Treatment, Iran},
abstract = {Leishmaniasis is an overlooked, poverty-stricken, and complex disease with growing social and public health problems. In general, leishmaniasis is a curable disease; however, there is an expansion of unresponsive cases to treatment in cutaneous leishmaniasis (CL). One of the effective and ignored determinants in the treatment outcome of CL is poor treatment adherence (PTA). PTA is an overlooked and widespread phenomenon to proper Leishmania treatment. This study aimed to explore the effect of poor adherence in unresponsiveness to treatment in patients with anthroponotic CL (ACL) by comparing conventional statistical modalities and machine learning analyses in Iran. Overall, 190 cases consisting of 50 unresponsive patients (case group), and 140 responsive patients (control group) with ACL were randomly selected. The data collecting form that included 25 queries (Q) was recorded for each case and analyzed by R software and genetic algorithm (GA) approaches. Complex treatment regimens (Q11), cultural and lay views about the disease and therapy (Q8), life stress, hopelessness and negative feelings (Q22), adverse effects of treatment (Q13), and long duration of the lesion (Q12) were the most prevalent significant variables that inhibited effective treatment adherence by the two methods, in decreasing order of significance. In the inherent algorithm approach, similar to the statistical approach, the most significant feature was complex treatment regimens (Q11). Providing essential knowledge about ACL and treatment of patients with chronic diseases and patients with misconceptions about chemical drugs are important issues directly related to the disease's unresponsiveness. Furthermore, early detection of patients to prevent the long duration of the disease and the process of treatment, efforts to minimize side effects of treatment, induction of positive thinking, and giving hope to patients with stress and anxiety by medical staff, and family can help patients adhere to the treatment.}
}
@article{MARSELLA200970,
title = {EMA: A process model of appraisal dynamics},
journal = {Cognitive Systems Research},
volume = {10},
number = {1},
pages = {70-90},
year = {2009},
note = {Modeling the Cognitive Antecedents and Consequences of Emotion},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2008.03.005},
url = {https://www.sciencedirect.com/science/article/pii/S1389041708000314},
author = {Stacy C. Marsella and Jonathan Gratch},
keywords = {Emotion, Cognitive models, Appraisal theory, Coping},
abstract = {A computational model of emotion must explain both the rapid dynamics of some emotional reactions as well as the slower responses that follow deliberation. This is often addressed by positing multiple levels of appraisal processes such as fast pattern directed vs. slower deliberative appraisals. In our view, this confuses appraisal with inference. Rather, we argue for a single and automatic appraisal process that operates over a person’s interpretation of their relationship to the environment. Dynamics arise from perceptual and inferential processes operating on this interpretation (including deliberative and reactive processes). This article discusses current developments in a computational model of emotion processes and illustrates how a single-level model of appraisal obviates a multi-level approach within the context of modeling a naturalistic emotional situation.}
}
@article{CHERNYSHOV2004535,
title = {A System Identification Approach to Assessing Airline Pilot Skills},
journal = {IFAC Proceedings Volumes},
volume = {37},
number = {6},
pages = {535-540},
year = {2004},
note = {16th IFAC Symposium on Automatic Control in Aerospace 2004, Saint-Petersburg, Russia, 14-18 June 2004},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)32230-9},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017322309},
author = {Kirill Chernyshov},
keywords = {Aircraft control, Skill, Human factors, Performance monitoring, Identification, Stochastic systems, Coupling coefficients, Cross correlation functions, Estimation algorithms, Sampled data},
abstract = {The paper presents a new approach to eliciting information on current professional airline pilot skills and pilotage experience as a decision making person (DMP). Such an approach is regarded to the heuristic regularities of the DMP thinking process. In turn, the regularities are revealed on basis of recording the motions of the pilot eyes over the information field of the flight deck and processing the experimental data obtained. For the data mining, a probability theoretical approach is involved. Such an approach is based on applying the notion of consistency of measures of stochastic dependence of random variables; and a method of deriving almost sure converging estimate of such a measure by sample data is proposed.}
}
@article{NUMRICH201169,
title = {Self-similarity of parallel machines},
journal = {Parallel Computing},
volume = {37},
number = {2},
pages = {69-84},
year = {2011},
issn = {0167-8191},
doi = {https://doi.org/10.1016/j.parco.2010.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S0167819110001444},
author = {Robert W. Numrich and Michael A. Heroux},
keywords = {Parallel algorithms, Benchmark analysis, Computational intensity, Computational force, Dimensional analysis, Equivalence class, Self-similarity, Scaling, Mixing coefficient},
abstract = {Self-similarity is a property of physical systems that describes how to scale parameters such that dissimilar systems appear to be similar. Computer systems are self-similar if certain ratios of computational forces, also known as computational intensities, are equal. Two machines with different computational power, different network bandwidth and different inter-processor latency behave the same way if they have the same ratios of forces. For the parallel conjugate gradient algorithm studied in this paper, two machines are self-similar if and only if the ratio of one force describing latency effects to another force describing bandwidth effects is the same for both machines. For the two machines studied in this paper, this ratio, which we call the mixing coefficient, is invariant as problem size and processor count change. The two machines have the same mixing coefficient and belong to the same equivalence class.}
}
@article{SHANAHAN2005157,
title = {Applying global workspace theory to the frame problem},
journal = {Cognition},
volume = {98},
number = {2},
pages = {157-176},
year = {2005},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2004.11.007},
url = {https://www.sciencedirect.com/science/article/pii/S0010027704002288},
author = {Murray Shanahan and Bernard Baars},
keywords = {Frame problem, Relevance problem, Global workspace theory, Consciousness, Analogical reasoning},
abstract = {The subject of this article is the frame problem, as conceived by certain cognitive scientists and philosophers of mind, notably Fodor for whom it stands as a fundamental obstacle to progress in cognitive science. The challenge is to explain the capacity of so-called informationally unencapsulated cognitive processes to deal effectively with information from potentially any cognitive domain without the burden of having to explicitly sift the relevant from the irrelevant. The paper advocates a global workspace architecture, with its ability to manage massively parallel resources in the context of a serial thread of computation, as an answer to this challenge. Analogical reasoning is given particular attention, since it exemplifies informational unencapsulation in its most extreme form. Because global workspace theory also purports to account for the distinction between conscious and unconscious information processing, the paper advances the tentative conclusion that consciousness may go hand-in-hand with a solution to the frame problem in the biological brain.}
}
@incollection{HADAP2023319,
title = {Chapter 16 - Theories methods and the parameters of quantitative structure–activity relationships and artificial neural network},
editor = {Dakeshwar Kumar Verma and Chandrabhan Verma and Jeenat Aslam},
booktitle = {Computational Modelling and Simulations for Designing of Corrosion Inhibitors},
publisher = {Elsevier},
pages = {319-335},
year = {2023},
isbn = {978-0-323-95161-6},
doi = {https://doi.org/10.1016/B978-0-323-95161-6.00019-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780323951616000199},
author = {Arti Hadap and Ashutosh Pandey and Bhawana Jain and Reena Rawat},
keywords = {Corrosion, QSAR (quantitative structure and activity relationship), ANN (artificial neural network), inhibitor adsorption, metal surface},
abstract = {Quantitative Structure–Activity Relationships (QSAR) is a computational model used to describe and anticipate the interaction and surface interactions of substances. In addition, Artificial Neural Network (ANN) has been used for the development of linear and sigmoidal functionals, aiming to predict low-carbon steel, copper, and aluminum corrosion rates corresponding to environmental parameters. Thus this chapter explains the theory behind the QSAR/ANN and demonstrates its effectiveness related to corrosion. QSAR aims to draw an attention to the link between the effectiveness of prevention (any function) and the structural features (adjectives). It involves finding one or more items that, by mathematical equation, link these definitions to their blocking function. ANN (artificial neural network) shows excellent performance in the prediction (output) for various complex characteristic data (input). The obtained results give deep insight into the corrosion systems by analyzing the point of surface corrosion attack, the more stable site of the inhibitor adsorption, and the binding power of the adsorbed coating.}
}
@incollection{PROFILLIDIS2019383,
title = {Chapter 9 - Fuzzy Methods},
editor = {V.A. Profillidis and G.N. Botzoris},
booktitle = {Modeling of Transport Demand},
publisher = {Elsevier},
pages = {383-417},
year = {2019},
isbn = {978-0-12-811513-8},
doi = {https://doi.org/10.1016/B978-0-12-811513-8.00009-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128115138000091},
author = {V.A. Profillidis and G.N. Botzoris},
keywords = {4-step model, Accidents, Airports, Crisp, Fuzzification, Fuzzy logic, Fuzzy model, Fuzzy regression, Gaussian, Linear programming, Membership degree, Membership function, Objective function, Railways, Traffic, Transport economics, Trapezoidal, Triangular},
abstract = {This chapter deals with applications of fuzzy methods, which give the ability to study quantitatively problems characterized by ambiguity, imprecision, uncertainty, linguistic variables, and missing or few or no data. The fuzzy method introduces another way of thinking: a statement, instead of being true or false, may be partially true or false. Thus, instead of taking into account the typically used fixed numerical values (such as, e.g., 2.34), the fuzzy method employs a set of plausible values (e.g., around the value 2.34) within a specific domain. Although this approach may look similar to the error of statistical methods, the fuzzy method can tackle situations (such as missing or vague data), for which classic methods are inefficient. The principles of fuzzy numbers, fuzzy sets, and fuzzy logic are presented. The case of symmetric triangular fuzzy numbers is analyzed in detail. Next, linear regression analysis with the use of fuzzy numbers is explained. A detailed application of fuzzy linear regression for a transport demand problem is surveyed analytically. The chapter includes many applications of fuzzy linear regression for the forecast of a variety of transport demand problems: air transport, rail transport, road transport, transport at urban level, and transport economics. Applications of the fuzzy method to other transport problems are explained: route choice, road safety, accident analysis, logistics and routing of freight vehicles, and the optimization of capacity of airports.}
}
@article{LAMB2023101031,
title = {Flexibility across and flexibility within: The domain of integer addition and subtraction},
journal = {The Journal of Mathematical Behavior},
volume = {70},
pages = {101031},
year = {2023},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2023.101031},
url = {https://www.sciencedirect.com/science/article/pii/S0732312323000019},
author = {Lisa Lamb and Jessica Bishop and Ian Whitacre and Randolph Philipp},
keywords = {Number concepts and operations, Cognition, Flexibility, Adaptive expertise, Strategy variability, Integers},
abstract = {To better understand the role that flexibility plays in students’ success on integer addition and subtraction problems, we examined students’ flexibility when solving open number sentences. We define flexibility as the degree to which a learner uses more than one strategy to solve a single task when prompted, as well as the degree to which a learner changes strategies when solving a range of tasks to accommodate task differences. We introduce the categorizations of flexibility within and flexibility across to distinguish these two ways of operationalizing flexibility. We examined flexibility and performance within and among three groups of students — 2nd and 4th graders who had negative numbers in their numerical domains, 7th graders, and college-track 11th graders. Profiles of five students are shared to provide insight in relation to the quantitative findings.}
}
@article{HUNTER198763,
title = {What is fundamental in an information age? A focus on curriculum},
journal = {Education and Computing},
volume = {3},
number = {1},
pages = {63-73},
year = {1987},
note = {Special Issue on Educational Computer Policy Alternatives in the United States},
issn = {0167-9287},
doi = {https://doi.org/10.1016/S0167-9287(87)80513-7},
url = {https://www.sciencedirect.com/science/article/pii/S0167928787805137},
author = {Beverly Hunter},
keywords = {Curriculum change, Knowledge-creative Learning, Problem Solving Tools, Information Handling, Algorithmic Thinking, Critical Thinking Skills, Higher-order Thinking Skills, Information Age, Computer Literacy, Problem Solving, Decision Making, Inquiry, Reasoning, Valuing},
abstract = {Systematic reassessment of both overt and covert curriculum content and methods is needed in response to broader social change involved in the information revolution. The educational system in the United States is moving (unevenly) through three overlapping stages of curriculum change: (1) focus on technology, (2) integration of technology into curriculum, and (3) focus on fundamental change in curriculum. Indicators of the current state of change in elementary and secondary schools include state education agency mandates, teacher-oriented publications, past and present surveys of computer use in schools, teacher attitudes, private industry initiatives, and recommendations of national study groups and commissions.}
}
@article{GAO2023101631,
title = {Key nodes identification in complex networks based on subnetwork feature extraction},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {35},
number = {7},
pages = {101631},
year = {2023},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2023.101631},
url = {https://www.sciencedirect.com/science/article/pii/S1319157823001854},
author = {Luyuan Gao and Xiaoyang Liu and Chao Liu and Yihao Zhang and Giacomo Fiumara and Pasquale De Meo},
keywords = {Key nodes identification, Complex network, Subnetwork feature extraction, Graph convolutional networks},
abstract = {The problem of detecting key nodes in a network (i.e. nodes with the greatest ability to spread an infection) has been studied extensively in the past. Some approaches to key node detection compute node centrality, but there is no formal proof that central nodes also have the greatest spreading capacity. Other methods use epidemiological models (e.g., the SIR model) to describe the spread of an infection and rely on numerical simulations to find out key nodes; these methods are highly accurate but computationally expensive. To efficiently but accurately detect key nodes, we propose a novel deep learning method called Rank by Graph Convolutional Network, RGCN. Our method constructs a subnetwork around each node to estimate its spreading power; then RGCN applies a graph convolutional network to each subnetwork and the adjacency matrix of the network to learn node embeddings. Finally, a neural network is applied to the node embeddings to detect key nodes. Our RGCN method outperforms state-of-the-art approaches such as RCNN and MRCNN by 11.84% and 13.99%, respectively, when we compare the Kendall’s τ coefficient between the node ranking produced by each method with the true ranking obtained by SIR simulations.}
}
@article{FURNHAM2025102637,
title = {Personality and the education process: Individual difference preferences for teacher, technology, testing, time and topic},
journal = {Learning and Individual Differences},
volume = {119},
pages = {102637},
year = {2025},
issn = {1041-6080},
doi = {https://doi.org/10.1016/j.lindif.2025.102637},
url = {https://www.sciencedirect.com/science/article/pii/S1041608025000135},
author = {Adrian Furnham},
keywords = {Preferences, Time of teaching, Teacher, Technology, Testing method},
abstract = {The present paper looks at the relationship between well-established personality traits and five different features of the educational process. Specifically, I explore the relationship between pupil Extraversion, Neuroticism, Openness, Agreeableness and Conscientiousness and personal preferences for Teacher (who the instructor is), Technology (the mode of instruction used), Testing (how the learning is evaluated), Time (the pace, length and time-of-day of the instruction period), and Topic (what is taught/discipline). There is a scattered literature on these topics which is briefly reviewed with a particular interest in how they relate to personality trait correlates. Evidence suggests the importance of understanding the role personality trait preferences in various educational choices and outcomes.}
}
@article{MARGINEANU20161,
title = {Neuropharmacology beyond reductionism – A likely prospect},
journal = {Biosystems},
volume = {141},
pages = {1-9},
year = {2016},
issn = {0303-2647},
doi = {https://doi.org/10.1016/j.biosystems.2015.11.010},
url = {https://www.sciencedirect.com/science/article/pii/S0303264715002026},
author = {Doru Georg Margineanu},
keywords = {Neuropharmacology, Systems pharmacology, Reductionism, Multi-target drug, Phenotypic screening, Emergent properties, Serendipity},
abstract = {Neuropharmacology had several major past successes, but the last few decades did not witness any leap forward in the drug treatment of brain disorders. Moreover, current drugs used in neurology and psychiatry alleviate the symptoms, while hardly curing any cause of disease, basically because the etiology of most neuro-psychic syndromes is but poorly known. This review argues that this largely derives from the unbalanced prevalence in neuroscience of the analytic reductionist approach, focused on the cellular and molecular level, while the understanding of integrated brain activities remains flimsier. The decline of drug discovery output in the last decades, quite obvious in neuropharmacology, coincided with the advent of the single target-focused search of potent ligands selective for a well-defined protein, deemed critical in a given pathology. However, all the widespread neuro-psychic troubles are multi-mechanistic and polygenic, their complex etiology making unsuited the single-target drug discovery. An evolving approach, based on systems biology considers that a disease expresses a disturbance of the network of interactions underlying organismic functions, rather than alteration of single molecular components. Accordingly, systems pharmacology seeks to restore a disturbed network via multi-targeted drugs. This review notices that neuropharmacology in fact relies on drugs which are multi-target, this feature having occurred just because those drugs were selected by phenotypic screening in vivo, or emerged from serendipitous clinical observations. The novel systems pharmacology aims, however, to devise ab initio multi-target drugs that will appropriately act on multiple molecular entities. Though this is a task much more complex than the single-target strategy, major informatics resources and computational tools for the systemic approach of drug discovery are already set forth and their rapid progress forecasts promising outcomes for neuropharmacology.}
}
@article{DRABECK202591,
title = {Disability in ecology and evolution},
journal = {Trends in Ecology & Evolution},
volume = {40},
number = {2},
pages = {91-95},
year = {2025},
issn = {0169-5347},
doi = {https://doi.org/10.1016/j.tree.2024.12.009},
url = {https://www.sciencedirect.com/science/article/pii/S0169534724003173},
author = {Danielle Drabeck and Chris Rensing and Kat {Van der Poorten}}
}
@article{MOTA2023985,
title = {Speech as a Graph: Developmental Perspectives on the Organization of Spoken Language},
journal = {Biological Psychiatry: Cognitive Neuroscience and Neuroimaging},
volume = {8},
number = {10},
pages = {985-993},
year = {2023},
note = {Natural Language Processing in Psychiatry and Clinical Neuroscience Research},
issn = {2451-9022},
doi = {https://doi.org/10.1016/j.bpsc.2023.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S2451902223000988},
author = {Natália Bezerra Mota and Janaina Weissheimer and Ingrid Finger and Marina Ribeiro and Bárbara Malcorra and Lilian Hübner},
keywords = {Clinical high risk, Cognitive development, Computational psychiatry, Dementia, Diagnosis, Psychosis},
abstract = {Language has been used as a privileged window to investigate mental processes. More recently, descriptions of psychopathological symptoms have been analyzed with the help of natural language processing tools. An example is the study of speech organization using graph theoretical approaches that began approximately 10 years ago. After its application in different areas, there is a need to better characterize what aspects can be associated with typical and atypical behavior throughout the lifespan, given the variables related to aging as well as biological and social contexts. The precise quantification of mental processes assessed through language may allow us to disentangle biological/social markers by looking at naturalistic protocols in different contexts. In this review, we discuss 10 years of studies in which word recurrence graphs were adopted to characterize the chain of thoughts expressed by individuals while producing discourse. Initially developed to understand formal thought disorder in the context of psychotic syndromes, this line of research has been expanded to understand the atypical development in different stages of psychosis and differential diagnosis (such as dementia) as well as the typical development of thought organization in school-age children/teenagers in naturalistic and school-based protocols. We comment on the effects of environmental factors, such as education and reading habits (in monolingual and bilingual contexts), in clinical and nonclinical populations at different developmental stages (from childhood to older adulthood, considering aging effects on cognition). Looking toward the future, there is an opportunity to use word recurrence graphs to address complex questions that consider biological/social factors within a developmental perspective in typical and atypical contexts.}
}
@article{IZMALKOV2011121,
title = {Perfect implementation},
journal = {Games and Economic Behavior},
volume = {71},
number = {1},
pages = {121-140},
year = {2011},
note = {Special Issue In Honor of John Nash},
issn = {0899-8256},
doi = {https://doi.org/10.1016/j.geb.2010.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S0899825610000758},
author = {Sergei Izmalkov and Matt Lepinski and Silvio Micali},
keywords = {Mechanism design, Trust, Privacy},
abstract = {Privacy and trust affect our strategic thinking, yet have not been precisely modeled in mechanism design. In settings of incomplete information, traditional implementations of a normal-form mechanism—by disregarding the players' privacy, or assuming trust in a mediator—may fail to reach the mechanism's objectives. We thus investigate implementations of a new type. We put forward the notion of a perfect implementation of a normal-form mechanism M: in essence, a concrete extensive-form mechanism exactly preserving all strategic properties of M, without relying on trusted mediators or violating the players' privacy. We prove that any normal-form mechanism can be perfectly implemented by a verifiable mediator using envelopes and an envelope-randomizing device. Differently from a trusted mediator, a verifiable one only performs prescribed public actions, so that everyone can verify that he is acting properly, and that he never learns any information that should remain private.}
}
@article{LIANG20242457,
title = {Adaptive Video Dual Domain Watermarking Scheme Based on PHT Moment and Optimized Spread Transform Dither Modulation},
journal = {Computers, Materials and Continua},
volume = {81},
number = {2},
pages = {2457-2492},
year = {2024},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2024.056438},
url = {https://www.sciencedirect.com/science/article/pii/S1546221824008051},
author = {Yucheng Liang and Ke Niu and Yingnan Zhang and Yifei Meng and Fangmeng Hu},
keywords = {Dual-domain, H.264, group of pictures, polar harmonic transform, spread transform dither modulation},
abstract = {To address the challenges of video copyright protection and ensure the perfect recovery of original video, we propose a dual-domain watermarking scheme for digital video, inspired by Robust Reversible Watermarking (RRW) technology used in digital images. Our approach introduces a parameter optimization strategy that incrementally adjusts scheme parameters through attack simulation fitting, allowing for adaptive tuning of experimental parameters. In this scheme, the low-frequency Polar Harmonic Transform (PHT) moment is utilized as the embedding domain for robust watermarking, enhancing stability against simulation attacks while implementing the parameter optimization strategy. Through extensive attack simulations across various digital videos, we identify the optimal low-frequency PHT moment using adaptive normalization. Subsequently, the embedding parameters for robust watermarking are adaptively adjusted to maximize robustness. To address computational efficiency and practical requirements, the unnormalized high-frequency PHT moment is selected as the embedding domain for reversible watermarking. We optimize the traditional single-stage extended transform dithering modulation (STDM) to facilitate multi-stage embedding in the dual-domain watermarking process. In practice, the video embedded with a robust watermark serves as the candidate video. This candidate video undergoes simulation according to the parameter optimization strategy to balance robustness and embedding capacity, with adaptive determination of embedding strength. The reversible watermarking is formed by combining errors and other information, utilizing recursive coding technology to ensure reversibility without attacks. Comprehensive analyses of multiple performance indicators demonstrate that our scheme exhibits strong robustness against Common Signal Processing (CSP) and Geometric Deformation (GD) attacks, outperforming other advanced video watermarking algorithms under similar conditions of invisibility, reversibility, and embedding capacity. This underscores the effectiveness and feasibility of our attack simulation fitting strategy.}
}
@article{SAEED2022122012,
title = {A simple approach for short-term wind speed interval prediction based on independently recurrent neural networks and error probability distribution},
journal = {Energy},
volume = {238},
pages = {122012},
year = {2022},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2021.122012},
url = {https://www.sciencedirect.com/science/article/pii/S036054422102260X},
author = {Adnan Saeed and Chaoshun Li and Zhenhao Gan and Yuying Xie and Fangjie Liu},
keywords = {Wind speed interval prediction, Independently recurrent neural networks, Quantile regression, Error prediction, Distribution estimation},
abstract = {Improving the quality of Wind Speed Interval prediction is important to maximize the usage of integrated wind energy as well as to reduce the adverse effects of the uncertainties, introduced by the random fluctuations of wind, to the power systems. This paper utilizes independently recurrent neural network to propose two new interval prediction frameworks. This network possesses the ability to retain memory at different lengths, which is helpful in capturing temporal features, especially for multi-horizon forecasts where the local dynamics get quite involved. In the first approach, we integrated a quantile regression loss function into this network to generate the intervals. This framework however, require to train different regressors to generate the conditional quantiles. Removing this limitation, a new simple and intuitive approach, is proposed which estimates the prediction intervals using a Gaussian function centered on the prediction and estimated error by a point prediction model and an error prediction model respectively. In our computational experiments, which involve two different wind fields contributing to eight different cases, an improvement of 43% and 12%, in average coverage width criterion index, over traditional models and LSTM based model respectively is remarkable. Thus, the proposed framework is able to produce high quality PIs while simultaneously reducing the computational cost.}
}
@article{SCHUH20181,
title = {Exact satisfiability of linear CNF formulas},
journal = {Discrete Applied Mathematics},
volume = {251},
pages = {1-4},
year = {2018},
issn = {0166-218X},
doi = {https://doi.org/10.1016/j.dam.2018.05.018},
url = {https://www.sciencedirect.com/science/article/pii/S0166218X18302762},
author = {Bernd R. Schuh},
keywords = {Complexity, XSAT, Exact linear formula, l-regularity, k-uniformity, NP-completeness},
abstract = {Open questions with respect to the computational complexity of linear CNF (LCNF) formulas are addressed. Focus lies on exact linear CNF formulas (XLCNF), in which any two clauses have exactly one variable in common. It is shown that l-regularity, i.e. each variable occurs exactly l times in the formula, imposes severe restrictions on the structure of XLCNF formulas. In particular it is proven that l-regularity in XLCNF implies k-uniformity, i.e. all clauses have the same number k of literals. Allowed k- values obey k (k−1)=0 (mod l), and the number of clauses m is given by m =kl-(k−1). Then the computational complexity of monotone l-regular XLCNF formulas with respect to exact satisfiability (XSAT) is determined. XSAT turns out to be either trivial, if m is not a multiple of l, or it can be decided in sub-exponential time, namely O(nn).}
}
@article{RUAN2022103133,
title = {Closed-form Minkowski sums of convex bodies with smooth positively curved boundaries},
journal = {Computer-Aided Design},
volume = {143},
pages = {103133},
year = {2022},
issn = {0010-4485},
doi = {https://doi.org/10.1016/j.cad.2021.103133},
url = {https://www.sciencedirect.com/science/article/pii/S0010448521001445},
author = {Sipu Ruan and Gregory S. Chirikjian},
keywords = {Minkowski sums, Computer-aided design, Computational geometry},
abstract = {This article derives closed-form parametric formulas for the Minkowski sums of convex bodies in d-dimensional Euclidean space with boundaries that are smooth and have all positive sectional curvatures at every point. Under these conditions, there is a unique relationship between the position of each boundary point and the surface normal. The main results are presented as two theorems. The first theorem directly parameterizes Minkowski sum boundaries using the unit normal vector at each surface point. Although simple to express mathematically, such a parameterization is not always practical to obtain computationally. Therefore, the second theorem derives a more useful parametric closed-form expression using the gradient that is not normalized. In the special case of two ellipsoids, the proposed expressions are identical to those derived previously using geometric interpretations. In order to examine the results, numerical validations and comparisons of the Minkowski sums between two superquadric bodies are conducted. Applications to generate configuration space obstacles in motion planning problems and to improve optimization-based collision detection algorithms are introduced and demonstrated.}
}
@article{WANDELL2017298,
title = {Diagnosing the Neural Circuitry of Reading},
journal = {Neuron},
volume = {96},
number = {2},
pages = {298-311},
year = {2017},
issn = {0896-6273},
doi = {https://doi.org/10.1016/j.neuron.2017.08.007},
url = {https://www.sciencedirect.com/science/article/pii/S0896627317306980},
author = {Brian A. Wandell and Rosemary K. Le},
keywords = {reading, diffusion imaging, development, fMRI, computational modeling},
abstract = {We summarize the current state of knowledge of the brain’s reading circuits, and then we describe opportunities to use quantitative and reproducible methods for diagnosing these circuits. Neural circuit diagnostics—by which we mean identifying the locations and responses in an individual that differ significantly from measurements in good readers—can help parents and educators select the best remediation strategy. A sustained effort to develop and share diagnostic methods can support the societal goal of improving literacy.}
}
@article{DELIGKAS2022103784,
title = {Two's company, three's a crowd: Consensus-halving for a constant number of agents},
journal = {Artificial Intelligence},
volume = {313},
pages = {103784},
year = {2022},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2022.103784},
url = {https://www.sciencedirect.com/science/article/pii/S0004370222001242},
author = {Argyrios Deligkas and Aris Filos-Ratsikas and Alexandros Hollender},
keywords = {Consensus-halving, Fair division, Computational complexity, Query complexity, Robertson-Webb},
abstract = {We consider the ε-Consensus-Halving problem, in which a set of heterogeneous agents aim at dividing a continuous resource into two (not necessarily contiguous) portions that all of them simultaneously consider to be of approximately the same value (up to ε). This problem was recently shown to be PPA-complete, for n agents and n cuts, even for very simple valuation functions. In a quest to understand the root of the complexity of the problem, we consider the setting where there is only a constant number of agents, and we consider both the computational complexity and the query complexity of the problem. For agents with monotone valuation functions, we show a dichotomy: for two agents the problem is polynomial-time solvable, whereas for three or more agents it becomes PPA-complete. Similarly, we show that for two monotone agents the problem can be solved with polynomially-many queries, whereas for three or more agents, we provide exponential query complexity lower bounds. These results are enabled via an interesting connection to a monotone Borsuk-Ulam problem, which may be of independent interest. For agents with general valuations, we show that the problem is PPA-complete and admits exponential query complexity lower bounds, even for two agents.}
}
@article{AGUIRRE2011305,
title = {Geovisual evaluation of public participation in decision making: The grapevine},
journal = {Journal of Visual Languages & Computing},
volume = {22},
number = {4},
pages = {305-321},
year = {2011},
note = {Part Special Issue on Challenging Problems in Geovisual Analytics},
issn = {1045-926X},
doi = {https://doi.org/10.1016/j.jvlc.2010.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S1045926X10000790},
author = {Robert Aguirre and Timothy Nyerges},
keywords = {Grapevine, Geovisual analytics, Public participation, Decision making, Spatio-temporal events, Human–computer–human interaction},
abstract = {This article reports on a three-dimensional (time–space) geovisual analytic called a “grapevine.” People often use metaphors to describe the temporal and spatial structure of online discussions, e.g., “threads” growing as a result of message exchanges. We created a visualization to evaluate the temporal and spatial structure of online message exchanges based on the shape of a grapevine naturally cultivated in a vineyard. Our grapevine visualization extends up through time with features like buds, nodes, tendrils, and leaves produced as a result of message posting, replying, and voting. Using a rotatable and fully interactive three-dimensional GIS (Geographic Information System) environment, a geovisual analyst can evaluate the quality of deliberation in the grapevine visualization by looking for productive patterns in fine-grained human–computer–human interaction (HCHI) data and then sub-sampling the productive parts for content analysis. We present an example of how we used the technique in a study of participatory interactions during an online field experiment about improving transportation in the central Puget Sound region of Washington called the Let's Improve Transportation (LIT) Challenge. We conclude with insights about how our grapevine could be applied as a general purpose technique for evaluation of any participatory learning, thinking, or decision making situation.}
}
@article{CHARLES20243693,
title = {Weaving innovative fabrics of knowledge between institutionalized sciences and Indigenous ways of knowing},
journal = {Matter},
volume = {7},
number = {11},
pages = {3693-3698},
year = {2024},
issn = {2590-2385},
doi = {https://doi.org/10.1016/j.matt.2024.10.013},
url = {https://www.sciencedirect.com/science/article/pii/S259023852400540X},
author = {Michael Charles},
abstract = {In the rapid chase to address humanity’s grand challenges, we must embrace multiple knowledge systems, including Indigenous ways of knowing, to fuel innovation, translate science into practice, and invite institutional sciences to evolve in an increasingly globalized world.}
}
@incollection{OLSON200116640,
title = {Writing Systems, Psychology of},
editor = {Neil J. Smelser and Paul B. Baltes},
booktitle = {International Encyclopedia of the Social & Behavioral Sciences},
publisher = {Pergamon},
address = {Oxford},
pages = {16640-16643},
year = {2001},
isbn = {978-0-08-043076-8},
doi = {https://doi.org/10.1016/B0-08-043076-7/01563-1},
url = {https://www.sciencedirect.com/science/article/pii/B0080430767015631},
author = {D.R. Olson},
abstract = {The writing systems of the world differ importantly in how they relate to spoken language. Tokening and pictographic scripts relate to meanings or intentions directly. So-called full writing systems represent properties of the spoken language but in completely different ways. Morphophonemic (logographic) scripts represent the words or morphemes of the language, syllabic scripts represent the syllables of the language whether or not they also represent word boundaries. Alphabetic scripts represent, with varying degrees of success, the phonemes of the language, but also by means of spaces, the words of the language. Not only do these differences have an effect on learning to read, they also have an important effect on the ways in which one thinks about language and consequently about the world and the mind. Writing systems provide models for thinking about speech.}
}
@article{HEUNG2025100361,
title = {How ChatGPT impacts student engagement from a systematic review and meta-analysis study},
journal = {Computers and Education: Artificial Intelligence},
volume = {8},
pages = {100361},
year = {2025},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2025.100361},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X25000013},
author = {Yuk Mui Elly Heung and Thomas K.F. Chiu},
keywords = {Generative artificial intelligence, Student engagement, ChatGPT, Systematic review, Meta-analysis},
abstract = {Generative artificial intelligence, such as ChatGPT, has been increasingly integrated into education to change student learning experience. Current empirical studies have mixed results on how ChatGPT impacts student behavioral, cognitive, and emotional engagement. This systematic literature review and meta-analysis explores whether and how ChatGPT impacts student behavioral, cognitive, and emotional engagement. We used the PRISMA method to select, analyze, and report the results. We screened 766 articles from four databases and identified 17 empirical studies with 1735 students for analysis. We compared the effect on student engagement between ChatGPT-based and non-ChatGPT learning. We found a medium effect size on overall student engagement in ChatGPT-based learning in the random effects model. Our analyses further suggest that ChatGPT-based learning is more effective in fostering student behavioral (medium effective size), cognitive (large effective size), and emotional engagement (medium effective size) than non-ChatGPT learning. Our findings revealed ChatGPT is an effective tool for engaging students in learning. We also suggested three roles ChatGPT plays in fostering student engagement: personalized tutoring, programming and technical assistance, and content generation and collaboration. Our systematic literature review revealed potential risks and results in student disengagement, such as over-reliance.}
}
@article{ZHUANG2024e29830,
title = {Artificial multi-verse optimisation for predicting the effect of ideological and political theory course},
journal = {Heliyon},
volume = {10},
number = {9},
pages = {e29830},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e29830},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024058614},
author = {Xingzhong Zhuang and Zhaodi Yi and Yuqing Wang and Yi Chen and Sudan Yu},
keywords = {Teaching sufficiency, Artificial multi-verse optimizer, Classification, Art ideological and political theory course},
abstract = {Enhancing teaching sufficiency is crucial because low teaching efficiency has always been a widespread issue in ideological and political theory course. Evaluating data on the course is obtained from a freshmen class of 2022 using questionnaires. The data is organised and condensed for mining and analysis. Subsequently, an intelligent artificial multi-verse optimizer (AMVO) method s developed to predict the effect of ideological and political theory course. The proposed AMVO approach was tested against various cutting-edge algorithms to demonstrate its effectiveness and stability on the benchmark functions. The experimental results indicated that AMVO ranked first among the 23 test functions. Furthermore, the binary AMVO enhanced k-nearest neighbour classifier had excellent performance in the art ideological and political theory course in terms of error rate, accuracy, specificity and sensitivity. This model can predict the overall evaluation attitude of freshmen towards the course based on the dataset. In addition, we can further analyse the potential correlations between factors that enhance the intellectual and political content of the course. This model can further refine the evaluation of ideological and political courses by teachers and students in our school, thereby achieving the fundamental goal of moral cultivation.}
}
@article{BARTH201937,
title = {Progressive Circuit Changes during Learning and Disease},
journal = {Neuron},
volume = {104},
number = {1},
pages = {37-46},
year = {2019},
issn = {0896-6273},
doi = {https://doi.org/10.1016/j.neuron.2019.09.032},
url = {https://www.sciencedirect.com/science/article/pii/S0896627319308347},
author = {Alison L. Barth and Ajit Ray},
abstract = {A critical step toward understanding cognition, learning, and brain dysfunction will be identification of the underlying cellular computations that occur in and across discrete brain areas, as well as how they are progressively altered by experience or disease. These computations will be revealed by targeted analyses of the neurons that perform these calculations, defined not only by their firing properties but also by their molecular identity and how they are wired within the local and broad-scale network of the brain. New studies that take advantage of sophisticated genetic tools for cell-type-specific identification and control are revealing how learning and neurological disorders initiate and successively change the properties of defined neural circuits. Understanding the temporal sequence of adaptive or pathological synaptic changes across multiple synapses within a network will shed light into how small-scale neural circuits contribute to higher cognitive functions during learning and disease.}
}
@article{DIESTER20242265,
title = {Internal world models in humans, animals, and AI},
journal = {Neuron},
volume = {112},
number = {14},
pages = {2265-2268},
year = {2024},
issn = {0896-6273},
doi = {https://doi.org/10.1016/j.neuron.2024.06.019},
url = {https://www.sciencedirect.com/science/article/pii/S0896627324004549},
author = {Ilka Diester and Marlene Bartos and Joschka Bödecker and Adam Kortylewski and Christian Leibold and Johannes Letzkus and Matthew M. Nour and Monika Schönauer and Andrew Straw and Abhinav Valada and Andreas Vlachos and Thomas Brox},
abstract = {Summary
How do brains—biological or artificial—respond and adapt to an ever-changing environment? In a recent meeting, experts from various fields of neuroscience and artificial intelligence met to discuss internal world models in brains and machines, arguing for an interdisciplinary approach to gain deeper insights into the underlying mechanisms.}
}
@article{STENNING1988143,
title = {Knowledge-rich solutions to the binding problem: a simulation of some human computational mechanisms},
journal = {Knowledge-Based Systems},
volume = {1},
number = {3},
pages = {143-152},
year = {1988},
issn = {0950-7051},
doi = {https://doi.org/10.1016/0950-7051(88)90072-X},
url = {https://www.sciencedirect.com/science/article/pii/095070518890072X},
author = {Keith Stenning and Joe Levy},
keywords = {binding, memory, PDP system, knowledge-rich, human memory, representations},
abstract = {The binding problem, how properties are represented as belonging to individuals, is identified as a severe problem for human memory, for which the memory adopts knowledge-rich solutions. It is argued that it is the nature of these solutions that endows human memory with many of its positive properties, particularly rapid retrieval on the basis of unreliable search clues. Parallel Distributed Processing (PDP) systems offer some insight into how human memory systems may work, as they also have to solve the binding problem by knowledge-rich methods. Experimental analysis and statistical models of Memory for Individuals Task (MIT) are presented, which provide evidence that the memory representations underlying human performance consist of sets of existential facts containing no referential terms. It is shown that the proposed representations can be incorporated directly into a PDP simulation of the inference from representation to response, and that the resulting system produces human-like errors when subjected to noisy input. The PDP simulation captures some of the asymmetries between stimulus and response which the statistical model cannot.}
}
@article{DAHL20231039,
title = {A Learning Approach for Future Competencies in Manufacturing using a Learning Factory},
journal = {Procedia CIRP},
volume = {118},
pages = {1039-1043},
year = {2023},
note = {16th CIRP Conference on Intelligent Computation in Manufacturing Engineering},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2023.06.178},
url = {https://www.sciencedirect.com/science/article/pii/S2212827123004055},
author = {Håkon Dahl and Nina Tvenge and Carla Susana A Assuad and Kristian Martinsen},
keywords = {Learning factory, Work Related Learning, Industry 4.0, Learning Method, Manufacturing, Future Work Competencies},
abstract = {This paper describes a study on future competence needs in manufacturing and how a learning factory utilising a Connective Model for Didactic Design can be used in teaching and learning of these competencies. The paper briefly reports on a literature study, and a set of interviews in Norwegian manufacturing companies to get a better understanding on the expected future competence needs. This was used to design a learning process with four steps: 1: Exploration, 2: Product and process design, 3: Problem solving and 4: Debriefing. The method was tested in a case study where undergraduate students are learners following the 4-step method. The approach was evaluated through feedback from the learners. The case utilised a Festo CP-Factory learning factory at NTNU.}
}
@article{WANG2020223,
title = {Anonymous data collection scheme for cloud-aided mobile edge networks},
journal = {Digital Communications and Networks},
volume = {6},
number = {2},
pages = {223-228},
year = {2020},
issn = {2352-8648},
doi = {https://doi.org/10.1016/j.dcan.2019.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S2352864819300574},
author = {Anxi Wang and Jian Shen and Chen Wang and Huijie Yang and Dengzhi Liu},
keywords = {Cloud-aided mobile edge networks, Anonymous data collection, Communication model, Path selection},
abstract = {With the rapid spread of smart sensors, data collection is becoming more and more important in Mobile Edge Networks (MENs). The collected data can be used in many applications based on the analysis results of these data by cloud computing. Nowadays, data collection schemes have been widely studied by researchers. However, most of the researches take the amount of collected data into consideration without thinking about the problem of privacy leakage of the collected data. In this paper, we propose an energy-efficient and anonymous data collection scheme for MENs to keep a balance between energy consumption and data privacy, in which the privacy information of senors is hidden during data communication. In addition, the residual energy of nodes is taken into consideration in this scheme in particular when it comes to the selection of the relay node. The security analysis shows that no privacy information of the source node and relay node is leaked to attackers. Moreover, the simulation results demonstrate that the proposed scheme is better than other schemes in aspects of lifetime and energy consumption. At the end of the simulation part, we present a qualitative analysis for the proposed scheme and some conventional protocols. It is noteworthy that the proposed scheme outperforms the existing protocols in terms of the above indicators.}
}
@article{PROCTOR2021142852,
title = {Gateway to the perspectives of the Food-Energy-Water nexus},
journal = {Science of The Total Environment},
volume = {764},
pages = {142852},
year = {2021},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2020.142852},
url = {https://www.sciencedirect.com/science/article/pii/S0048969720363828},
author = {Kyle Proctor and Seyed M.H. Tabatabaie and Ganti S. Murthy},
keywords = {Food-energy–water nexus, Water security, Life cycle assessment, Problem archetype, Resource governance, Systems thinking},
abstract = {The Food-Energy–Water (FEW) nexus has been promoted as a tool for improving food, energy, and water resource security via an interdisciplinary approach that acknowledges the inherent synergies and tradeoffs involved in managing these resources. Over the past decade discussion of the nexus has increased rapidly, along with research funding and output. However, because the nexus encompasses so many different disciplines, researchers engage with and study the nexus from differing perspectives with distinct motivations and analytical methodologies. Understanding these motivations is critical to understanding the value of a given work. This paper first uses a narrative review to identify the motivations and toolsets of five key perspectives used to view the nexus, including: ecosystem health, waste management, public and private institutional change, stakeholder trust, and the learning process. Then, a systematic review is conducted to examine how publication trends have changed over the past decade, both generally and for each of these perspectives. The Food-Energy-Water nexus is not the first systems-based approach for addressing resource management and critiques of the nexus as a “Buzzword” or simply a reinvention of previous systems are growing in the literature. Challenging authors to explicitly define the role and motivations of their research within the broader category of the FEW nexus can improve the actionability of the research, better allow researchers to build from each other's work, and help reduce the ambiguity surrounding the nexus.}
}
@article{LI2023103984,
title = {Improving short-term bike sharing demand forecast through an irregular convolutional neural network},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {147},
pages = {103984},
year = {2023},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2022.103984},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X22003977},
author = {Xinyu Li and Yang Xu and Xiaohu Zhang and Wenzhong Shi and Yang Yue and Qingquan Li},
keywords = {Bike sharing, Deep learning, Travel demand forecast, Spatial–temporal analysis, Irregular convolution},
abstract = {As an important task for the management of bike sharing systems, accurate forecast of travel demand could facilitate dispatch and relocation of bicycles to improve user satisfaction. In recent years, many deep learning algorithms have been introduced to improve bicycle usage forecast. A typical practice is to integrate convolutional (CNN) and recurrent neural network (RNN) to capture spatial–temporal dependency in historical travel demand. For typical CNN, the convolution operation is conducted through a kernel that moves across a “matrix-format” city to extract features over spatially adjacent urban areas. This practice assumes that areas close to each other could provide useful information that improves prediction accuracy. However, bicycle usage in neighboring areas might not always be similar, given spatial variations in built environment characteristics and travel behavior that affect cycling activities. Yet, areas that are far apart can be relatively more similar in temporal usage patterns. To utilize the hidden linkage among these distant urban areas, the study proposes an irregular convolutional Long-Short Term Memory model (IrConv+LSTM) to improve short-term bike sharing demand forecast. The model modifies traditional CNN with irregular convolutional architecture to leverage the hidden linkage among “semantic neighbors”. The proposed model is evaluated with a set of benchmark models in five study sites, which include one dockless bike sharing system in Singapore, and four station-based systems in Chicago, Washington, D.C., New York, and London. We find that IrConv+LSTM outperforms other benchmark models in the five cities. The model also achieves superior performance in areas with varying levels of bicycle usage and during peak periods. The findings suggest that “thinking beyond spatial neighbors” can further improve short-term travel demand prediction of urban bike sharing systems.}
}
@article{LISANA2025100896,
title = {Playing to learn: Game-based approach to financial literacy for generation Z},
journal = {Entertainment Computing},
volume = {52},
pages = {100896},
year = {2025},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2024.100896},
url = {https://www.sciencedirect.com/science/article/pii/S1875952124002647},
author = {Lisana Lisana and Hendra Dinata and Gabriela {Valencia Tanudjaja}},
keywords = {Gamification, Digital learning, Simulation, Evaluation},
abstract = {This quantitative study assessed the effectiveness of game-based learning in improving financial literacy among Generation Z. Conducted with 32 urban participants, the study involved the use of a board game and a mobile application, designed with input from financial literacy experts. Participants underwent a pretest to gauge their initial financial knowledge, engaged with the game, and completed a posttest to measure learning outcomes. Statistical analysis, including paired sample t-tests, compared pretest and posttest scores, revealing a significant enhancement in financial literacy post-gameplay. Furthermore, a questionnaire evaluated user satisfaction regarding the game, assessing metrics like enjoyment, ease of use, and perceived usefulness. Results demonstrated not only an improvement in financial knowledge but also high satisfaction among users, indicating that game-based learning can be a valuable tool for teaching financial concepts to Generation Z. These findings contribute to the understanding of game-based learning’s potential in financial education and provide insights for educators and parents. However, the study’s limitations suggest areas for future research to explore and refine the use of educational games in financial literacy.}
}
@article{DEBRUIJNSMOLDERS2024e39439,
title = {Effective student engagement with blended learning: A systematic review},
journal = {Heliyon},
volume = {10},
number = {23},
pages = {e39439},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e39439},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024154709},
author = {M. {De Bruijn-Smolders} and F.R. Prinsen},
keywords = {Blended learning, Student engagement, Learning outcomes, Systematic review},
abstract = {Although student engagement is known to promote learning outcomes in higher education, what elements of blended learning designs impact effective student engagement and hereby learning outcomes, has not been clarified yet. Hence, it is unknown how to engage students with blended learning in an effective manner. The current study breaks down student engagement into four dimensions (academic, behavioral, cognitive, and affective), and reviews the evidence regarding blended learning that engages students effectively, whether this is academically, personally, socially, or with regard to citizenship. The studies reviewed (k = 15, N = 1,428) overall asserted that all blended learning interventions investigated had a moderate to high impact on student engagement and on learning outcomes. This review, a summary and insight into the evidence, is important for the field's understanding as well as for professionals in higher education: for lecturers and policy makers who want to introduce and monitor blended learning as a means to promote both student engagement and their learning outcomes in higher education. Further research is required to increase our knowledge of how blended learning impacts both multi-dimensional constructs: student engagement and learning outcomes.}
}
@article{WU2025109151,
title = {Examining the role and neural electrophysiological mechanisms of adjective cues in size judgment},
journal = {Neuropsychologia},
volume = {213},
pages = {109151},
year = {2025},
issn = {0028-3932},
doi = {https://doi.org/10.1016/j.neuropsychologia.2025.109151},
url = {https://www.sciencedirect.com/science/article/pii/S0028393225000867},
author = {Yihan Wu and Ronglian Zheng and Huili Xing and Yining Kou and Yufeng Wang and Xin Wu and Feng Zou and Yanyan Luo and Meng Zhang},
keywords = {Size judgement, Language, ERP, EEG microstate},
abstract = {Numerous influential theories have attempted to elucidate the relationship between language and thought. The debate persists on whether language and thought are distinct entities or if language is deeply embedded in individual cognitive processes. This study employs adjective cues combined with a mental imagery size judgment task as an experimental paradigm, utilizing neurophysiological techniques to preliminarily explore the role of adjectives in size judgment tasks and their underlying neurophysiological mechanisms. Findings reveal that performance is best when adjectives are congruent with the size of the object, with EEG microstate results indicating strong activity in Class A, related to language networks under this condition. Additionally, when adjectives conflict with object size, the discovery of the Ni component suggests that individuals monitor and inhibit the conflict between adjectives and object size, leading to decreased task performance in this condition. Moreover, when object size is ambiguous, individuals' size judgments do not benefit significantly from clear adjective cues. Event-related potentials and EEG microstate results suggest that under this condition, top-down cognitive resources are recruited more extensively. In conclusion, language plays a more crucial role in simpler judgment tasks; as tasks become more complex, judgment processes engage a greater number of distributed brain regions to collaborate, while the language system remains active. This study provides initial cognitive neuroscience evidence for understanding the relationship between language and simple forms of thought, offering preliminary insights for future investigations into the connection between language and thought.}
}
@article{SALOMATIN2021582,
title = {Web user identification based on browser fingerprints using machine learning methods},
journal = {IFAC-PapersOnLine},
volume = {54},
number = {13},
pages = {582-587},
year = {2021},
note = {20th IFAC Conference on Technology, Culture, and International Stability TECIS 2021},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2021.10.512},
url = {https://www.sciencedirect.com/science/article/pii/S2405896321019492},
author = {Alexander A. Salomatin and Andrey Y. Iskhakov and Anastasia O. Iskhakova},
keywords = {browser fingerprint, cybersecurity, identification, digital footprint, machine learning, web server},
abstract = {The article developed a method for identifying users on the network based on browser fingerprints using machine learning methods. The resulting method is a modification of the user identification method based on a digital footprint, which can be more efficient due to two components. First, the selection of attributes for a digital footprint is made from a limited set of attributes to form a user browser fingerprint. Secondly, the identification accuracy can be increased through the combined use of classification methods and the probabilistic-statistical approach. To check the successful operation of the method, a computational experiment is carried out on real data, which consists in solving the problem of classifying a user based on his browser fingerprint using the K nearest neighbors method.}
}
@article{KOHLER2016212,
title = {On GPU acceleration of common solvers for (quasi-) triangular generalized Lyapunov equations},
journal = {Parallel Computing},
volume = {57},
pages = {212-221},
year = {2016},
issn = {0167-8191},
doi = {https://doi.org/10.1016/j.parco.2016.05.010},
url = {https://www.sciencedirect.com/science/article/pii/S0167819116300436},
author = {Martin Köhler and Jens Saak},
keywords = {Lyapunov equations, BLAS level-3, Accelerator device},
abstract = {The solutions of Lyapunov and generalized Lyapunov equations are a key player in many applications in systems and control theory. Their stable numerical computation, when the full solution is sought, is considered solved since the seminal work of Bartels and Stewart [R. H. Bartels, G. W. Stewart, Solution of the matrix equation AX+XB=C: Algorithm 432, Comm. ACM 15 (1972) 820–826.]. A number of variants of their algorithm have been proposed, but none of them goes beyond BLAS level-2 style implementation. On modern computers, however, the formulation of BLAS level-3 type implementations is crucial to enable optimal usage of cache hierarchies and modern block scheduling methods based on directed acyclic graphs describing the interdependence of single block computations. In this contribution, we present the port of our recent BLAS level-3 algorithm [M. Köhler, J. Saak, On BLAS Level-3 implementations of common solvers for (quasi-) triangular generalized Lyapunov equations, SLICOT Working Note 2014-1, NICONET e.V., available from www.slicot.org (Sep. 2014).] to a GPU accelerator device.}
}
@article{FIELDS2022104714,
title = {Neurons as hierarchies of quantum reference frames},
journal = {Biosystems},
volume = {219},
pages = {104714},
year = {2022},
issn = {0303-2647},
doi = {https://doi.org/10.1016/j.biosystems.2022.104714},
url = {https://www.sciencedirect.com/science/article/pii/S0303264722000983},
author = {Chris Fields and James F. Glazebrook and Michael Levin},
keywords = {Activity-dependent remodeling, Bayesian inference, Bioelectricity, Computation, Learning, Memory},
abstract = {Conceptual and mathematical models of neurons have lagged behind empirical understanding for decades. Here we extend previous work in modeling biological systems with fully scale-independent quantum information-theoretic tools to develop a uniform, scalable representation of synapses, dendritic and axonal processes, neurons, and local networks of neurons. In this representation, hierarchies of quantum reference frames act as hierarchical active-inference systems. The resulting model enables specific predictions of correlations between synaptic activity, dendritic remodeling, and trophic reward. We summarize how the model may be generalized to nonneural cells and tissues in developmental and regenerative contexts.}
}
@article{GILBOA202196,
title = {The complexity of the consumer problem},
journal = {Research in Economics},
volume = {75},
number = {1},
pages = {96-103},
year = {2021},
issn = {1090-9443},
doi = {https://doi.org/10.1016/j.rie.2021.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S1090944321000016},
author = {Itzhak Gilboa and Andrew Postlewaite and David Schmeidler},
keywords = {Consumer theory, Computational complexity, Mental accounting},
abstract = {A literal interpretation of neo-classical consumer theory suggests that the consumer solves a very complex problem. In the presence of indivisible goods, the consumer problem is NP-Hard, and it appears unlikely that it can be optimally solved by a human. Two implications of this observation are that (i) households may imitate each other’s choices; (ii) households may adopt heuristics that give rise to the phenomenon of mental accounting.}
}
@incollection{DOLIVEIRACOELHO2020259,
title = {Chapter 5.1 - Osteomics: Decision support systems for forensic anthropologists},
editor = {Zuzana Obertová and Alistair Stewart and Cristina Cattaneo},
booktitle = {Statistics and Probability in Forensic Anthropology},
publisher = {Academic Press},
pages = {259-273},
year = {2020},
isbn = {978-0-12-815764-0},
doi = {https://doi.org/10.1016/B978-0-12-815764-0.00005-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128157640000058},
author = {João {d’Oliveira Coelho} and Francisco Curate and David Navega},
keywords = {Biological profile, Machine learning, Population data, Web-based applications, Age at death, Sex diagnosis, Biogeographic origins, Body parameters, Medicolegal contexts, Cross validation},
abstract = {The popularity of web-based analytical tools with an emphasis on improved statistical analyses within the landscape of forensic anthropology is increasing. Osteomics is a web-based platform composed of a suite of forensic decision support systems designed to contend with the challenges posed by the estimation of the biological profile of human skeletal remains and particularly the estimation of age at death, the diagnosis of sex, the calculation of body parameters, and the prediction of biogeographic origin. The web applications designed at Osteomics intend to make innovative and reliable statistical models freely available. The suggested models are grounded around traditional and advanced statistical thinking, data visualization and processing, and predictive modeling under the machine learning paradigm. This paper aims to introduce the potential of the web platforms as forensic decision support systems and to give a detailed description of the statistical techniques used in the web-based applications available at Osteomics.}
}
@article{KYRIAZOS2024105070,
title = {Quantum concepts in Psychology: Exploring the interplay of physics and the human psyche},
journal = {BioSystems},
volume = {235},
pages = {105070},
year = {2024},
issn = {0303-2647},
doi = {https://doi.org/10.1016/j.biosystems.2023.105070},
url = {https://www.sciencedirect.com/science/article/pii/S0303264723002459},
author = {Theodoros Kyriazos and Mary Poga},
keywords = {Quantum mechanics, Quantum psychology, Interdisciplinary, Human psyche},
abstract = {This paper delves into the innovative intersection of quantum mechanics and psychology, examining the potential of quantum principles to provide fresh insights into human emotions, cognition, and consciousness. Drawing parallels between quantum phenomena such as superposition, entanglement, tunneling, decoherence and their psychological counterparts, we present a quantum-psychological model that reimagines emotional states, cognitive breakthroughs, interpersonal relationships, and the nature of consciousness. The study uses computational models and simulations to explore this interdisciplinary fusion's implications and applications, highlighting its potential benefits and inherent challenges. While quantum concepts offer a rich metaphorical lens to view the intricacies of human experience, it is essential to approach this nascent framework with enthusiasm and skepticism. Rigorous empirical validation is paramount to realize its full potential in research and therapeutic contexts. This exploration stands as a promising thread in the tapestry of intellectual history, suggesting a deeper understanding of the human psyche through the lens of quantum mechanics.}
}
@incollection{GALLISTEL2017141,
title = {1.08 - Learning and Representation☆},
editor = {John H. Byrne},
booktitle = {Learning and Memory: A Comprehensive Reference (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {Oxford},
pages = {141-154},
year = {2017},
isbn = {978-0-12-805291-4},
doi = {https://doi.org/10.1016/B978-0-12-809324-5.21009-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128093245210092},
author = {Randy Gallistel},
keywords = {Associations, Cognitive map, Episodic memory, Information, Memory molecules, Path integration, Read–write memory, Signals, Sun compass, Symbols},
abstract = {Behavioral and electrophysiological evidence implies that brains compute representations of aspects of the experienced world. For example, they compute the animal's position in the world by integrating its velocity with respect to time. Other examples are the learning of the solar ephemeris, the construction of a cognitive map, and episodic memory in food caching. Representations require a symbolic read–write memory that carries information extracted from experience forward in time in a computationally accessible form. The analogy between the architecture of computer memory and the genetic architecture suggests the sort of memory structure to be looked for in the nervous system.}
}
@article{WANG2022103414,
title = {Cross-layer progressive attention bilinear fusion method for fine-grained visual classification},
journal = {Journal of Visual Communication and Image Representation},
volume = {82},
pages = {103414},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103414},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321002789},
author = {Chaoqing Wang and Yurong Qian and Weijun Gong and Junjong Cheng and Yongqiang Wang and Yuefei Wang},
keywords = {Fine-grained visual classification, Feature fusion, Attention, Progressive},
abstract = {Fine-grained visual classification (FGVC) is a critical task in the field of computer vision. However, FGVC is full of challenges due to the large intra-class variation and small inter-class variation of the classes to be classified on an image. The key in dealing with the problem is to capture subtle visual differences from the image and effectively represent the discriminative features. Existing methods are often limited by insufficient localization accuracy and insufficient feature representation capabilities. In this paper, we propose a cross-layer progressive attention bilinear fusion (CPABF in short) method, which can efficiently express the characteristics of discriminative regions. The CPABF method involves three components: 1) Cross-Layer Attention (CLA) locates and reinforces the discriminative region with low computational costs; 2) The Cross-Layer Bilinear Fusion Module (CBFM) effectively integrates the semantic information from the low-level to the high-level 3) Progressive Training optimizes the parameters in the network to the best state in a delicate way. The CPABF shows excellent performance on the four FGVC datasets and outperforms some state-of-the-art methods.}
}
@article{LI2022111937,
title = {Detection method of timber defects based on target detection algorithm},
journal = {Measurement},
volume = {203},
pages = {111937},
year = {2022},
issn = {0263-2241},
doi = {https://doi.org/10.1016/j.measurement.2022.111937},
url = {https://www.sciencedirect.com/science/article/pii/S0263224122011332},
author = {Dongjie Li and Zilei Zhang and Baogang Wang and Chunmei Yang and Liwei Deng},
keywords = {Wood defect detection, YOLOX, Target detection, Feature fusion},
abstract = {Deep learning has achieved certain results in the field of wood surface defect detection. To address the problems of low accuracy of the detection results of surface defects on boards, slow detection speed and large number of model parameters, this article take advantage of computer vision to improve the feature fusion module of YOLOX target detection algorithm, by adding efficient channel attention (ECA) mechanism, adaptive spatial feature fusion mechanism (ASFF) and improve the confidence loss and localization loss functions as Focal loss and Efficient Intersection over Union (EIoU) loss, to enhance the feature extraction ability and detection accuracy of the algorithm. Considering the depth and width of the model, the depth-separable convolution and optional multi-version algorithm are used to reduce the model parameters and computational effort to seek the optimal model. Experiments show that the improved model detects four types of defects in rubber timber with a considerable improvement and has significant advantages over other target detection algorithms.}
}
@article{PINEDA2024101204,
title = {The mode of computing},
journal = {Cognitive Systems Research},
volume = {84},
pages = {101204},
year = {2024},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2023.101204},
url = {https://www.sciencedirect.com/science/article/pii/S1389041723001389},
author = {Luis A. Pineda},
keywords = {Mode of computing, Natural computing, Representation, Interpretation, Consciousness},
abstract = {The Turing Machine is the paradigmatic case of computing machines, but there are others such as analogical, connectionist, quantum and diverse forms of unconventional computing, each based on a particular intuition of the phenomenon of computing. This variety can be captured in terms of system levels, re-interpreting and generalizing Newell’s hierarchy, which includes the knowledge level at the top and the symbol level immediately below it. In this re-interpretation the knowledge level consists of human knowledge and the symbol level is generalized into a new level that here is called The Mode of Computing. Mental processes performed by natural brains are often thought of informally as computing processes and that the brain is alike to computing machinery. However, if natural computing does exist it should be characterized on its own. A proposal to such an effect is that natural computing appeared when interpretations were first made by biological entities, so natural computing and interpreting are two aspects of the same phenomenon, or that consciousness and experience are the manifestations of computing/interpreting. By analogy with computing machinery, there must be a system level at the top of the neural circuitry and directly below the knowledge level that is named here The mode of Natural Computing. If it turns out that such putative object does not exist the proposition that the mind is a computing process should be dropped; but characterizing it would come with solving the hard problem of consciousness.}
}
@article{ZHU2021118730,
title = {From gratitude to injustice: Neurocomputational mechanisms of gratitude-induced injustice},
journal = {NeuroImage},
volume = {245},
pages = {118730},
year = {2021},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2021.118730},
url = {https://www.sciencedirect.com/science/article/pii/S1053811921010028},
author = {Ruida Zhu and Zhenhua Xu and Song Su and Chunliang Feng and Yi Luo and Honghong Tang and Shen Zhang and Xiaoyan Wu and Xiaoqin Mai and Chao Liu},
keywords = {Gratitude, Protection tendency, Injustice, Mentalizing, Reward processing},
abstract = {Gratitude shapes individuals’ behaviours and impacts the harmony of society. Many previous studies focused on its association with prosocial behaviours. A possibility that gratitude can lead to moral violation has been overlooked until recently. Nevertheless, the neurocognitive mechanisms of gratitude-induced moral violation are still unclear. On the other hand, though neural correlates of the gratitude's formation have been examined, the neural underpinnings of gratitude-induced behaviour remain unknown. For addressing these two overlapped research gaps, we developed novel tasks to investigate how participants who had received voluntary (Gratitude group) or involuntary help (Control group) punished their benefactors’ unfairness with functional magnetic resonance imaging (fMRI). The Gratitude group punished their benefactors less than the Control group. The self-report and computational modelling results demonstrated a crucial role of the boosted protection tendency on behalf of benefactors in the gratitude-induced injustice. The fMRI results showed that activities in the regions associated with mentalizing (temporoparietal junction) and reward processing (ventral medial prefrontal cortex) differed between the groups and were related to the gratitude-induced injustice. They suggest that grateful individuals concern for benefactors’ benefits, value chances to interact with benefactors, and refrain from action that perturbs relationship-building (i.e., exert less punishment on benefactors’ unfairness), which reveal a dark side of gratitude and enrich the gratitude theory (i.e., the find-bind-remind theory). Our findings provide psychological, computational, and neural accounts of the gratitude-induced behaviour and further the understanding of the nature of gratitude.}
}
@article{REN2025100774,
title = {Immersive E-learning mode application in Chinese language teaching system based on big data recommendation algorithm},
journal = {Entertainment Computing},
volume = {52},
pages = {100774},
year = {2025},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2024.100774},
url = {https://www.sciencedirect.com/science/article/pii/S1875952124001423},
author = {Chunjiao Ren},
keywords = {Big data, Interest recommendation algorithm, Immersive, E-Learning mode, Chinese teaching system},
abstract = {With the rapid development of information technology, E-Learning has become an innovative teaching method. However, in the field of Chinese teaching, how to provide effective learning resources and recommendation algorithms in E-Learning mode is still a challenge. This study aims to improve the effectiveness of Chinese teaching system and students’ learning outcomes through an immersive E-Learning model based on big data interest recommendation algorithm. This paper adopts an immersive E-Learning model based on big data interest recommendation algorithm, and constructs a Chinese teaching system. The web crawler is used to fully collect the experimental data and collate it in a targeted manner, and the required data is screened out by using a more efficient separation method. Adding big data recommendation algorithm to the system of this paper can not only record and analyze historical behaviors of users, but also recommend data information according to users’ interests, so that users can clarify their real information needs. By testing the system’s professional ability and recording the experimental data, this paper finds that the overall performance of this Chinese language teaching system is very good, and can achieve the original expected design purpose. In addition, the system largely solves the problem that the traditional system based on data recommendation algorithm is difficult to carry out effective recommendation smoothly when the total amount of data is too large.}
}
@article{VEZOLI2021117479,
title = {Cortical hierarchy, dual counterstream architecture and the importance of top-down generative networks},
journal = {NeuroImage},
volume = {225},
pages = {117479},
year = {2021},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2020.117479},
url = {https://www.sciencedirect.com/science/article/pii/S1053811920309642},
author = {Julien Vezoli and Loïc Magrou and Rainer Goebel and Xiao-Jing Wang and Kenneth Knoblauch and Martin Vinck and Henry Kennedy},
keywords = {Non-human primate, Human, Brain, Electrophysiology, Anatomy, Modeling, Connectivity, Predictive coding, Perception, Consciousness},
abstract = {Hierarchy is a major organizational principle of the cortex and underscores modern computational theories of cortical function. The local microcircuit amplifies long-distance inter-areal input, which show distance-dependent changes in their laminar profiles. Statistical modeling of these changes in laminar profiles demonstrates that inputs from multiple hierarchical levels to their target areas show remarkable consistency, allowing the construction of a cortical hierarchy based on a principle of hierarchical distance. The statistical modeling that is applied to structure can also be applied to laminar differences in the oscillatory coherence between areas thereby determining a functional hierarchy of the cortex. Close examination of the anatomy of inter-areal connectivity reveals a dual counterstream architecture with well-defined distance-dependent feedback and feedforward pathways in both the supra- and infragranular layers, suggesting a multiplicity of feedback pathways with well-defined functional properties. These findings are consistent with feedback connections providing a generative network involved in a wide range of cognitive functions. A dynamical model constrained by connectivity data sheds insight into the experimentally observed signatures of frequency-dependent Granger causality for feedforward versus feedback signaling. Concerted experiments capitalizing on recent technical advances and combining tract-tracing, high-resolution fMRI, optogenetics and mathematical modeling hold the promise of a much improved understanding of lamina-constrained mechanisms of neural computation and cognition. However, because inter-areal interactions involve cortical layers that have been the target of important evolutionary changes in the primate lineage, these investigations will need to include human and non-human primate comparisons.}
}
@article{WIECHA2024101129,
title = {Deep learning for nano-photonic materials – The solution to everything!?},
journal = {Current Opinion in Solid State and Materials Science},
volume = {28},
pages = {101129},
year = {2024},
issn = {1359-0286},
doi = {https://doi.org/10.1016/j.cossms.2023.101129},
url = {https://www.sciencedirect.com/science/article/pii/S1359028623000748},
author = {Peter R. Wiecha},
abstract = {Deep learning is currently being hyped as an almost magical tool for solving all kinds of difficult problems that computers have not been able to solve in the past. Particularly in the fields of computer vision and natural language processing, spectacular results have been achieved. The hype has now infiltrated several scientific communities. In (nano-) photonics, researchers are trying to apply deep learning to all kinds of forward and inverse problems. A particularly challenging problem is for instance the rational design of nanophotonic materials and devices. In this opinion article, I will first discuss the public expectations of deep learning and give an overview of the quite different scales at which actors from industry and research are operating their deep learning models. I then examine the weaknesses and dangers associated with deep learning. Finally, I’ll discuss the key strengths that make this new set of statistical methods so attractive, and review a personal selection of opportunities that shouldn’t be missed in the current developments.}
}
@article{IGELSTROM201770,
title = {The inferior parietal lobule and temporoparietal junction: A network perspective},
journal = {Neuropsychologia},
volume = {105},
pages = {70-83},
year = {2017},
note = {Special Issue: Concepts, Actions and Objects: Functional and Neural Perspectives},
issn = {0028-3932},
doi = {https://doi.org/10.1016/j.neuropsychologia.2017.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S0028393217300015},
author = {Kajsa M. Igelström and Michael S.A. Graziano},
keywords = {Angular gyrus, Supramarginal gyrus, Ventral parietal cortex, Posterior superior temporal sulcus, Internal cognition, Frontoparietal executive control network},
abstract = {Information processing in specialized, spatially distributed brain networks underlies the diversity and complexity of our cognitive and behavioral repertoire. Networks converge at a small number of hubs – highly connected regions that are central for multimodal integration and higher-order cognition. We review one major network hub of the human brain: the inferior parietal lobule and the overlapping temporoparietal junction (IPL/TPJ). The IPL is greatly expanded in humans compared to other primates and matures late in human development, consistent with its importance in higher-order functions. Evidence from neuroimaging studies suggests that the IPL/TPJ participates in a broad range of behaviors and functions, from bottom-up perception to cognitive capacities that are uniquely human. The organization of the IPL/TPJ is challenging to study due to the complex anatomy and high inter-individual variability of this cortical region. In this review we aimed to synthesize findings from anatomical and functional studies of the IPL/TPJ that used neuroimaging at rest and during a wide range of tasks. The first half of the review describes subdivisions of the IPL/TPJ identified using cytoarchitectonics, resting-state functional connectivity analysis and structural connectivity methods. The second half of the article reviews IPL/TPJ activations and network participation in bottom-up attention, lower-order self-perception, undirected thinking, episodic memory and social cognition. The central theme of this review is to discuss how network nodes within the IPL/TPJ are organized and how they participate in human perception and cognition.}
}
@article{THACKER2023101782,
title = {Climate change by the numbers: Leveraging mathematical skills for science learning online},
journal = {Learning and Instruction},
volume = {86},
pages = {101782},
year = {2023},
issn = {0959-4752},
doi = {https://doi.org/10.1016/j.learninstruc.2023.101782},
url = {https://www.sciencedirect.com/science/article/pii/S0959475223000518},
author = {Ian Thacker},
keywords = {Climate change, Conceptual change, Epistemic dispositions, Numerical estimation, Plausibility judgments, Learning technology},
abstract = {The purpose of this preregistered study was to test an online intervention that presents participants with novel numbers about climate change after they estimate those numbers. An experimental study design was used to investigate the impact of the intervention on undergraduate students’ climate change understanding and perceptions that human caused climate change is plausible. Findings revealed that posttest climate change knowledge and plausibility perceptions were higher among those randomly assigned to use the intervention compared with those assigned to a control condition, and that supplementing this experience with numeracy instruction was linked with the use of more explicit estimation strategies and greater learning gains for people with adaptive epistemic dispositions. Findings from this study replicate and extend prior research, support the idea that novel data can support knowledge revision, identify estimation strategies used in this context, and offer an open-source online intervention for sharing surprising data with students and teachers.}
}
@article{RINGE2023101268,
title = {Cation effects on electrocatalytic reduction processes at the example of the hydrogen evolution reaction},
journal = {Current Opinion in Electrochemistry},
volume = {39},
pages = {101268},
year = {2023},
issn = {2451-9103},
doi = {https://doi.org/10.1016/j.coelec.2023.101268},
url = {https://www.sciencedirect.com/science/article/pii/S2451910323000613},
author = {Stefan Ringe},
keywords = {Cation effects, Hydrogen evolution reaction, Hydrogen underpotential deposition, CO reduction, Electric double layer, Solid-liquid interface},
abstract = {Cation effects provide invaluable insights into electrochemistry. In this review, I discuss them with a main focus on the hydrogen evolution reaction and a summary of recent in situ spectroscopic and electrochemical measurements as well as advanced computational simulation results conducted at varying cation identities, concentrations, and pH. According to these works, the interfacial cation concentration is the main descriptor to explain cation and pH effects. The detailed mechanism (such as e.g. water polarization, water structure changes, field-stabilization of intermediates) depends strongly on potential, pH, oxophilicity of the electrode, or the nature of the rate-limiting step and proton donor. With growing convergence in this field, cation effects remain a highly challenging and promising topic for research.}
}
@incollection{YACKINOUS2015193,
title = {Chapter 11 - Cellular Automata Investigations and Emerging Complex System Principles},
editor = {William S. Yackinous},
booktitle = {Understanding Complex Ecosystem Dynamics},
publisher = {Academic Press},
address = {Boston},
pages = {193-212},
year = {2015},
isbn = {978-0-12-802031-9},
doi = {https://doi.org/10.1016/B978-0-12-802031-9.00011-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128020319000115},
author = {William S. Yackinous},
keywords = {Cellular automata, Cellular automata investigations, Explicit experimentation, Cellular automata classes, Simple programs/simple rules, Complex system principles, Computational view of systems, Computational universality, Principle of Computational Equivalence},
abstract = {This chapter is primarily about Stephen Wolfram's innovative cellular automata investigations and his associated ideas on emerging complex system principles. The chapter begins with some cellular automata history and background, and then provides a description of Wolfram's cellular automata “explicit experimentation” work. The experimentation work shows that simple programs with simple rules, repeated over and over, can yield highly complex behavior. Wolfram has identified four classes of cellular automata. Those classes and their characteristics are discussed. The correspondence between cellular automata classes and the attractors of nonlinear dynamics theory is also discussed. Another of Wolfram's important insights is that the behavior of cellular automata is indicative of the behavior of systems in general. That idea is addressed in some detail. The latter part of the chapter addresses Wolfram's computational view of systems. The topics covered include computation as a framework for system principles, the concept of computational universality, and the identification of computationally universal cellular automata. Wolfram's Principle of Computational Equivalence is then described and discussed. The chapter concludes with a summary of my perspectives on the emerging complex system principles.}
}
@article{HOU2025105329,
title = {Measuring undergraduate students' reliance on Generative AI during problem-solving: Scale development and validation},
journal = {Computers & Education},
volume = {234},
pages = {105329},
year = {2025},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2025.105329},
url = {https://www.sciencedirect.com/science/article/pii/S0360131525000971},
author = {Chenyu Hou and Gaoxia Zhu and Vidya Sudarshan and Fun Siong Lim and Yew Soon Ong},
keywords = {Human-AI collaboration, Problem-solving, Generative AI, Higher education, Reliance on AI, Scale development},
abstract = {Reliance on AI describes the behavioral patterns of when and how individuals depend on AI suggestions, and appropriate reliance patterns are necessary to achieve effective human-AI collaboration. Traditional measures often link reliance to decision-making outcomes, which may not be suitable for complex problem-solving tasks where outcomes are not binary (i.e., correct or incorrect) or immediately clear. Therefore, this study aims to develop a scale to measure undergraduate students' behaviors of using Generative AI during problem-solving tasks without directly linking them to specific outcomes. We conducted an exploratory factor analysis on 800 responses collected after students finished one problem-solving activity, which revealed four distinct factors: reflective use, cautious use, thoughtless use, and collaborative use. The overall scale has reached sufficient internal reliability (Cronbach's alpha = .84). Two confirmatory factor analyses (CFAs) were conducted to validate the factors using the remaining 730 responses from this activity and 1173 responses from another problem-solving activity. CFA indices showed adequate model fit for data from both problem-solving tasks, suggesting that the scale can be applied to various human-AI problem-solving tasks. This study offers a validated scale to measure students' reliance behaviors in different human-AI problem-solving activities and provides implications for educators to responsively integrate Generative AI in higher education.}
}
@article{MARZANO20231028,
title = {Manufacturing Ergonomics Improvements in Distillery Industry Using Digital Tools},
journal = {Procedia CIRP},
volume = {118},
pages = {1028-1032},
year = {2023},
note = {16th CIRP Conference on Intelligent Computation in Manufacturing Engineering},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2023.06.176},
url = {https://www.sciencedirect.com/science/article/pii/S2212827123004031},
author = {Adelaide Marzano},
keywords = {Digital manufacturing system, ergonomics, design},
abstract = {This paper presents the steps taken by distilleries to uphold years old traditions and how new design tools can streamlined the current manufacturing processes. Different methods for bung removal are explored and how they are used today within warehouses and distilleries worldwide. The aim is to test new designs to replace the current tools used in distillery process to perform heavily manual tasks. Models of the current and new design are produced, and both are tested in a digital environment for ergonomics and time efficiency purposes.}
}
@article{RAVISHANKAR20211,
title = {Time dependent network resource optimization in cyber–physical systems using game theory},
journal = {Computer Communications},
volume = {176},
pages = {1-12},
year = {2021},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2021.04.034},
url = {https://www.sciencedirect.com/science/article/pii/S0140366421001857},
author = {Monica Ravishankar and Thompson Stephan and Thinagaran Perumal},
keywords = {Critical infrastructures, Cyber–physical systems, Game theory, Reinforcement learning technique, Linguistic fuzzy variables},
abstract = {The social and economic stability of a country is dependent on critical infrastructures (CIs) whose services range from financial to healthcare and power to transportation and communications. Most of these CIs are cyber–physical systems (CPSs), which integrate the network’s computational and communication capabilities to facilitate the monitoring and controlling of physical processes. Such systems are vulnerable to damage due to natural disasters, physical incidents, or cyber-attacks impacting the CPS organizations managing complex industrial control systems and data acquisition systems. When these CPSs are exposed to systemic cyber risks and cascaded network failures, network administrators need to recover from the compromise under limited resources. This is formulated as an attacker-defender game model to emulate the decision-making process in choosing an appropriate attack/defence mechanism in response to cybersecurity incidents using game theory. To further improve the assumptions made in the pure game-theoretic model, we relax the constraints on the rationality of the players, monetary payoff, and completeness of information by incorporating learning in games using reinforcement learning technique and compute the expected payoff using linguistic fuzzy variables.}
}
@article{RODRIGUES2021406,
title = {Convolutional Neural Network for Respiratory Mechanics Estimation during Pressure Support Ventilation},
journal = {IFAC-PapersOnLine},
volume = {54},
number = {15},
pages = {406-411},
year = {2021},
note = {11th IFAC Symposium on Biological and Medical Systems BMS 2021},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2021.10.290},
url = {https://www.sciencedirect.com/science/article/pii/S2405896321016955},
author = {Adriano S. Rodrigues and Marcos R.O.A. Maximo and Marcus H. Victor},
keywords = {Mechanical Ventilation, Respiratory Mechanics, Respiratory Effort, Deep Learning, Convolutional Neural Networks},
abstract = {In mechanically ventilated patients, some lung injuries can be reduced or avoided with therapy individualization, while the lung function is evaluated continuously, breath by breath. However, obtaining information on respiratory mechanics (respiratory system resistance and compliance) in the presence of respiratory effort is challenging, even if using invasive and complex procedures. The contribution of this work is to predict both respiratory system resistance and compliance over time using a convolutional neural network (CNN) and estimate the respiratory effort profile using the respiratory dynamics. Therefore, the approach used in this work was to generate a large amount of simulated data to feed a CNN so it could learn how to predict the correct values of the respiratory system resistance and compliance. Then, the respiratory effort was estimated by solving a first-order linear model. The main results showed a normalized mean squared error of 5.7% for the respiratory system resistance and 11.56% for compliance from Bland-Altman plots derived from the computational simulator. Finally, the method was validated using real data from an active lung simulator within which respiratory mechanics varied, and some ventilator settings were adjusted to mimic actual patient situations. The active lung simulator effort profile was obtained with a normalized mean squared error of 8.31% considering the use of an active lung simulator. The results have shown that the simulated data were valuable for the CNN training, while the performance over the real data suggested that the network was generalized accordingly for estimating respiratory parameters and effort profile.}
}
@article{RAY2020106679,
title = {A framework for probabilistic model-based engineering and data synthesis},
journal = {Reliability Engineering & System Safety},
volume = {193},
pages = {106679},
year = {2020},
issn = {0951-8320},
doi = {https://doi.org/10.1016/j.ress.2019.106679},
url = {https://www.sciencedirect.com/science/article/pii/S0951832018312754},
author = {Douglas Ray and Jose Ramirez-Marquez},
keywords = {Modeling and Simulation (M&S), Design of experiments (DOE), Deterministic computer experiments, Space filling designs, Uncertainty Quantification (UQ), Probabilistic optimization, Verification, Validation, Calibration, Trade space, Sensitivity analysis, Statistical engineering},
abstract = {Modern computing resources provide scientists, engineers, and system design teams the ability to study phenomena, such as system behavior, in a virtual setting. Computational modeling and simulation (M&S) enables engineers to avoid many of the challenges encountered in traditional design engineering, including the design, manufacture, and testing of expensive prototypes prior to having an optimized design. However, the use of M&S carries its own challenges, such as the computational time and resources required to execute effective studies, and uncertainties arising from simplifying assumptions inherent to computer models, which are intended to be an approximate representation of reality. In recent year advances have been made in a number of areas related to the efficient and reliable use of M&S for system evaluations, including design & analysis of computer experiments, uncertainty quantification, probabilistic analysis, response optimization, and data synthesis techniques. In this review paper, a general framework for systematically executing efficient M&S studies at the component-level, product-level, system-level, and system-of-systems-level is described. A case study is used to demonstrate how statistical and probabilistic techniques can be integrated with M&S to address those challenges inherent to model-based engineering, and how this aligns with the proposed workflow. The example is a gun-launch dynamics model of an artillery projectile developed by US Army engineers, and illustrates the application of this workflow in the study of subsystem system reliability, performance, and end-to-end system-level characterization.}
}
@article{CIMBUROVA2023127839,
title = {Making trees visible: A GIS method and tool for modelling visibility in the valuation of urban trees},
journal = {Urban Forestry & Urban Greening},
volume = {81},
pages = {127839},
year = {2023},
issn = {1618-8667},
doi = {https://doi.org/10.1016/j.ufug.2023.127839},
url = {https://www.sciencedirect.com/science/article/pii/S1618866723000109},
author = {Zofie Cimburova and Stefan Blumentrath and David N. Barton},
keywords = {Cultural ecosystem services, GIS, Tree valuation, Urban trees, Visibility analysis},
abstract = {Tree visibility is a key determinant of cultural ecosystem services of urban trees. This paper develops a flexible, efficient and easy-to-use GIS method for modelling individual tree visibility to support tree valuation. The method is implemented as a GRASS GIS AddOn tool called v.viewshed.impact, making it available to a broad spectrum of users and purposes. Thanks to empirically validated underlying algorithms and parallel processing, the method is accurate and fast in analysing high-resolution datasets and large numbers of trees. We demonstrate the method in two use cases in Oslo, Norway, showing that it provides an alternative to field-based assessment of visibility indicators in tree valuation methods and facilitates the inclusion of complex visibility indicators not possible to assess in the field. We argue that the method could also be used for tree management and planning, urban ecosystem accounting and neighbour conflict resolution related to trees.}
}
@incollection{FRANTZ202025,
title = {3 - The “Big 3.” Simon, Katona, Leibenstein},
editor = {Roger Frantz},
booktitle = {The Beginnings of Behavioral Economics},
publisher = {Academic Press},
pages = {25-45},
year = {2020},
series = {Perspectivs in Behavioral Economics and the Economics of Beh},
isbn = {978-0-12-815289-8},
doi = {https://doi.org/10.1016/B978-0-12-815289-8.00003-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128152898000034},
author = {Roger Frantz},
keywords = {ECONS and HUMANS, Behavioral macroeconomics, Intervening variables, Gestalt psychology, Tit-for-tat, Parable of the ant, Bounded rationality, Satisficing, X-efficiency, Non-allocative efficiency, Das John Maynard Keynes rationality problem},
abstract = {Katona, Leibenstein, and Simon are the “Big 3” of the old behavioral economics. Why are they the Big 3? Their names are most often mentioned by others in terms of “early” behavioral economics. They wrote convincingly about homo economicus, and in doing so they began knocking him off his pedestal. Without this behavioral economicus would never exist. With respect to the Big 3’s writings, Leibenstein wrote about, among other things, multiple-selves, gift exchange, social norms, consumer interdependence, non-allocative efficiency, and less than perfect rationality. Katona wrote about, among other things, ECONS vs HUMANS, expectations, aspirations, adaptive behavior, macro-behavioral theory, procedural rationality, and less than perfect rationality. Among other things, Herbert Simon wrote about bounded rationality, intuition (System 1) and logical thinking (System 2), ECONS vs HUMANS, satisficing, rejection of as if theorizing, learning theories in economics and psychology, rationality in economics and psychology, the nature of human knowledge (tacit knowledge), and less than perfect rationality.}
}
@article{XIONG2020180,
title = {Construction of approximate reasoning model for dynamic CPS network and system parameter identification},
journal = {Computer Communications},
volume = {154},
pages = {180-187},
year = {2020},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2020.02.073},
url = {https://www.sciencedirect.com/science/article/pii/S0140366420301225},
author = {Juxia Xiong and Jinzhao Wu},
keywords = {Cyber physical system, Network, Event message modeling, Interactive multi-model algorithm},
abstract = {CPS (Cyber Physical System) is a large and complex real-time feedback system that integrates computing processes, physical processes, communication networks, sensor networks, and control systems. It has the powerful function of sensing and controlling the physical environment, which is a big wave following the Internet technology. Because the forms of communication, interaction, and collaboration between heterogeneous units within the CPS are intricate and complex, a comprehensive model needs to be established to describe and analyze the CPS. This paper analyzes the CPS architecture and proposes a new and more complete CPS architecture, decomposes according to this architecture, and classifies the physical entities in the CPS. At the same time, event-based modeling thinking is used to define, classify and formalize event messages. Considering the higher real-time requirements of CPS, an event weighting algorithm was designed according to the different priorities of real-time events. In order to reduce the congestion caused by the limited network bandwidth in the CPS system, improve the ability to identify abnormal data with great uncertainty, and fully guarantee the response rate of the CPS system to emergencies, this paper analyzes the complexity of the CPS system from the perspective of information theory. The average dynamic complexity of the CPS system is set as a threshold to determine the level of information entropy of the sensor data in a certain period of time. The CPS system selects high information entropy data to send first. The effectiveness is analyzed through experiments.}
}
@article{LIBERATORE2024103456,
title = {The ghosts of forgotten things: A study on size after forgetting},
journal = {Annals of Pure and Applied Logic},
volume = {175},
number = {8},
pages = {103456},
year = {2024},
issn = {0168-0072},
doi = {https://doi.org/10.1016/j.apal.2024.103456},
url = {https://www.sciencedirect.com/science/article/pii/S016800722400054X},
author = {Paolo Liberatore},
keywords = {Logical forgetting, Boolean minimization},
abstract = {Forgetting is removing variables from a logical formula while preserving the constraints on the other variables. In spite of reducing information, it does not always decrease the size of the formula and may sometimes increase it. This article discusses the implications of such an increase and analyzes the computational properties of the phenomenon. Given a propositional Horn formula, a set of variables and a maximum allowed size, deciding whether forgetting the variables from the formula can be expressed in that size is Dp-hard in Σ2p. The same problem for unrestricted CNF propositional formulae is D2p-hard in Σ3p.}
}
@article{STABLER1984155,
title = {Berwick and Weinberg on linguistics and computational psychology},
journal = {Cognition},
volume = {17},
number = {2},
pages = {155-179},
year = {1984},
issn = {0010-0277},
doi = {https://doi.org/10.1016/0010-0277(84)90017-9},
url = {https://www.sciencedirect.com/science/article/pii/0010027784900179},
author = {Edward P. Stabler}
}
@article{DEBOER2010502,
title = {Frame-based guide to situated decision-making on climate change},
journal = {Global Environmental Change},
volume = {20},
number = {3},
pages = {502-510},
year = {2010},
note = {Governance, Complexity and Resilience},
issn = {0959-3780},
doi = {https://doi.org/10.1016/j.gloenvcha.2010.03.003},
url = {https://www.sciencedirect.com/science/article/pii/S0959378010000245},
author = {Joop {de Boer} and J. Arjan Wardekker and Jeroen P. {van der Sluijs}},
keywords = {Climate change, Adaptation, Decision-making, Frames},
abstract = {The present paper describes a frame-based approach to situated-decision-making on climate change. Building on the multidisciplinary literature on the relationship between frames and decision-making, it argues that decision-makers may gain from making frames more explicit and using them for generating different visions about the central issues. Frames act as organizing principles that shape in a “hidden” and taken-for-granted way how people conceptualize an issue. Science-related issues, such as climate change, are often linked to only a few frames, which consistently appear across different policy areas. Indeed, it appears that there are some very contrasting ways in which climate change may be framed. These frames can be characterized in terms of a simple framework that highlights specific interpretations of climate issues. A second framework clarifies the built-in frames of decision tools. Using Thompson's two basic dimensions of decision, it identifies the main uncertainties that should be considered in developing a decision strategy. The paper characterizes four types of decision strategy, focusing on (1) computation, (2) compromise, (3) judgment, or (4) inspiration, and links each strategy to the appropriate methods and tools, as well as the appropriate social structures. Our experiences show that the frame-based guide can work as an eye-opener for decision-makers, particularly where it demonstrates how to add more perspectives to the decision.}
}
@article{DWYER20111021,
title = {An approach to quantitatively measuring collaborative performance in online conversations},
journal = {Computers in Human Behavior},
volume = {27},
number = {2},
pages = {1021-1032},
year = {2011},
note = {Web 2.0 in Travel and Tourism: Empowering and Changing the Role of Travelers},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2010.12.006},
url = {https://www.sciencedirect.com/science/article/pii/S0747563210003730},
author = {Paul Dwyer},
keywords = {Collaboration, Cognitive modeling, Collective thinking},
abstract = {Interpersonal dynamics often hinder people from optimizing collaboration. Researchers who monitor the intellectual activity of people as they converse online receive less value when such collaboration is impaired. How can they detect suboptimal collaboration? This study builds on a new metric for measuring collaborative value from the information content of participant contributions to propose a measure of collaborative efficiency, and demonstrates its utility by assessing collaboration around a sample of weblogs. The new collaborative value metric can augment qualitative research by highlighting for deeper investigation conversational themes that triggered elevated collaborative production. Identifying these themes may also define the cognitive box people have built within a collaborative venue. Challenging people to consider fresh ideas by deliberately introducing them into collaborative venues is recommended as the key to overcoming collaborative dysfunction.}
}
@article{BARLOW1983107,
title = {Vision: A computational investigation into the human representation and processing of visual information: David Marr. San Francisco: W. H. Freeman, 1982. pp. xvi + 397},
journal = {Journal of Mathematical Psychology},
volume = {27},
number = {1},
pages = {107-110},
year = {1983},
issn = {0022-2496},
doi = {https://doi.org/10.1016/0022-2496(83)90030-5},
url = {https://www.sciencedirect.com/science/article/pii/0022249683900305},
author = {H.B. Barlow}
}
@article{BEIGZADEH2025107877,
title = {Mental stress detection and performance enhancement using fNIRS and wrist vibrator biofeedback},
journal = {Biomedical Signal Processing and Control},
volume = {107},
pages = {107877},
year = {2025},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2025.107877},
url = {https://www.sciencedirect.com/science/article/pii/S174680942500388X},
author = {Anita Beigzadeh and Vahid Yazdnian and Seyed Kamaledin Setarehdan},
keywords = {Stress management, Performance enhancement, Machine learning, Learning model for real-time stress classification, Brain signal processing, Biofeedback, Brain–computer interface, Functional near infrared spectroscopy},
abstract = {Daily life activities frequently expose individuals to varying levels of mental stress, which can adversely affect their performance. Therefore, it is crucial to develop effective strategies for stress management and performance improvement. This paper presents a comprehensive, portable, and real-time biofeedback system aimed at improving individuals’ stress management capabilities, ultimately leading to enhanced mental task performance. The system consists of a real-time brain signal acquisition device, a wireless vibration biofeedback unit, and a software-based program for stress level classification. Notably, the system is designed to minimize the time delay by efficiently integrating all components. Various signal processing and feature extraction techniques combined with machine learning have been employed for online stress detection. The experimental results demonstrate an accuracy of 83% and a recall of 92% in detecting true levels of mental stress in the stress classification module. In addition, the complete biofeedback system is tested on 20 participants in a controlled experimental setup, revealing a 55% reduction in stress levels and a 24.5% improvement in task accuracy. These findings support the effectiveness of the proposed system in stress management and performance improvement, validating the core premises of stress reduction and performance improvement through reward-based learning.}
}
@article{MENG2025105534,
title = {Application of a boundary-type algorithm to the inverse problems of convective heat and mass transfer},
journal = {Progress in Nuclear Energy},
volume = {179},
pages = {105534},
year = {2025},
issn = {0149-1970},
doi = {https://doi.org/10.1016/j.pnucene.2024.105534},
url = {https://www.sciencedirect.com/science/article/pii/S0149197024004840},
author = {Xiangyuan Meng and Mei Huang and Jianghao Yang and Xiaoping Ouyang and Boxue Wang and Yanping Huang and Hiroshi Matsuda and Bo Cao},
keywords = {Half boundary method, Inverse problems, Convection-diffusion, Discontinuous coefficient, The Gaussian plume model},
abstract = {The inverse problems of the convection-diffusion equation (ICDE) have received extensive attention in incomplete boundary conditions and uncertain source terms. They can be applied in thermally stratified pipe elbows and so on. Many algorithms need to combine with optimization algorithms to repeatedly calculate the direct problem in the solution process. To solve such problems, this paper employs a boundary-type algorithm named the half-boundary method (HBM). The HBM does not require additional repeated optimization of the direct problem. To test the performance of the method, the numerical simulations of some problems have been carried out, including the inverse problems of heat convection, river pollution and air pollution. The results show that the HBM has the desired accuracy by comparing with the exact solution. If there are errors in the measurement process, the solution doesn't generate a large deviation from the result. It is worth noting that the placement of internal measurement points minimally impacts the numerical results within the solution domain. And the method is also able to handle with discontinuous problems. Because the Gaussian plume model verifies the accuracy of HBM, the HBM can quickly calculate the atmospheric diffusion of the non-Gaussian plume model.}
}
@article{BONSIGNORE2017298,
title = {Present and future approaches to lifetime prediction of superelastic nitinol},
journal = {Theoretical and Applied Fracture Mechanics},
volume = {92},
pages = {298-305},
year = {2017},
issn = {0167-8442},
doi = {https://doi.org/10.1016/j.tafmec.2017.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S0167844217300587},
author = {Craig Bonsignore}
}
@article{GIACHETTI2025103229,
title = {Preface for “Selected papers from the 26th Ibero-American Conference on Software Engineering (CIbSE 2023)”},
journal = {Science of Computer Programming},
volume = {243},
pages = {103229},
year = {2025},
issn = {0167-6423},
doi = {https://doi.org/10.1016/j.scico.2024.103229},
url = {https://www.sciencedirect.com/science/article/pii/S0167642324001527},
author = {Giovanni Giachetti and Breno {de França} and Marcela Genero and Renata Guizzardi}
}
@article{YETISEN2015724,
title = {Bioart},
journal = {Trends in Biotechnology},
volume = {33},
number = {12},
pages = {724-734},
year = {2015},
issn = {0167-7799},
doi = {https://doi.org/10.1016/j.tibtech.2015.09.011},
url = {https://www.sciencedirect.com/science/article/pii/S016777991500205X},
author = {Ali K. Yetisen and Joe Davis and Ahmet F. Coskun and George M. Church and Seok Hyun Yun},
keywords = {genetics, transgenic art, tissue engineering, ethics, aesthetics},
abstract = {Bioart is a creative practice that adapts scientific methods and draws inspiration from the philosophical, societal, and environmental implications of recombinant genetics, molecular biology, and biotechnology. Some bioartists foster interdisciplinary relationships that blur distinctions between art and science. Others emphasize critical responses to emerging trends in the life sciences. Since bioart can be combined with realistic views of scientific developments, it may help inform the public about science. Artistic responses to biotechnology also integrate cultural commentary resembling political activism. Art is not only about ‘responses’, however. Bioart can also initiate new science and engineering concepts, foster openness to collaboration and increasing scientific literacy, and help to form the basis of artists’ future relationships with the communities of biology and the life sciences.}
}
@article{PITTNAUER2023382,
title = {Observing the creation of new knowledge in the economics laboratory—Do participants discover how to learn from outcome feedback in a dynamic decision problem?},
journal = {Journal of Economic Behavior & Organization},
volume = {215},
pages = {382-405},
year = {2023},
issn = {0167-2681},
doi = {https://doi.org/10.1016/j.jebo.2023.09.015},
url = {https://www.sciencedirect.com/science/article/pii/S0167268123003311},
author = {Sabine Pittnauer and Martin Hohnisch},
keywords = {Learning, Outcome feedback, Discovery, Conjecture, Heuristic simplification, Dynamic decision making},
abstract = {Domain-general learning rules often enable decision makers to learn from outcome feedback which actions tend to achieve a desired goal. However, in novel and complex environments decision makers must explore how to learn, i.e., acquire procedural knowledge of how to elicit and evaluate outcome feedback that will enable them to navigate toward a desired goal despite the vastness of the set of possible policies. Using a dynamic business simulation, this study investigated: (1) whether and how frequently participants discovered an effective procedure to learn from outcome feedback that allowed them to navigate toward a policy that maximizes long-term business profit (and hence their monetary payoff from the experiment), and (2) whether high monetary incentives affected learning procedures and performance. We found that a number of participants discovered an effective learning procedure and succeeded in approximating the optimal policy. In line with the heuristic method, this learning procedure involved a simplification of the search space and the application of domain-general learning rules to this simplified space. Although the decision histories of about half of the participants feature the key aspect of the effective learning procedure—search among the different steady states of the dynamical system—implementation errors prevented many of the participants from realizing the full potential of the learning procedure. We found no evidence to suggest that high monetary incentives affect the effectiveness of learning. Overall, the study illustrates that a “prepared mind” can discover new, effective learning procedures, although their initial implementation may require substantial refinement.}
}
@article{WILKINS202440,
title = {We need to think differently about artificial intelligence},
journal = {New Scientist},
volume = {263},
number = {3509},
pages = {40-43},
year = {2024},
issn = {0262-4079},
doi = {https://doi.org/10.1016/S0262-4079(24)01696-8},
url = {https://www.sciencedirect.com/science/article/pii/S0262407924016968},
author = {Alex Wilkins},
abstract = {Will AI ever emulate human intelligence? Professor of machine intelligence Neil Lawrence tells Alex Wilkins it is misleading to compare the two}
}
@incollection{CHORAFAS200760,
title = {4 - Stress analysis and its tools},
editor = {Dimitris N. Chorafas},
booktitle = {Stress Testing for Risk Control Under Basel II},
publisher = {Butterworth-Heinemann},
address = {Oxford},
pages = {60-79},
year = {2007},
isbn = {978-0-7506-8305-0},
doi = {https://doi.org/10.1016/B978-075068305-0.50005-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780750683050500051},
author = {Dimitris N. Chorafas},
abstract = {Publisher Summary
This chapter explains the need for stress testing to take a scientific approach as an advanced analytical methodology for commendable results. The scientific method of investigation is the only basis for conducting tests and experiments. The chapter examines relatively novel approaches to surveys targeting a qualitative evaluation by experts, such as the Delphi method. The chapter discusses the contributions of the scientific method and financial technology to analytical thinking and testing. The characteristics of a sound methodology are discussed and the fundamentals of stress analysis under normal conditions or under stress are described. The chapter also discusses case studies with scenario analysis and talks about stress evaluation through sensitivity analysis and about the fundamentals of statistical analysis.}
}
@article{BRAMSON2023105397,
title = {Emotion regulation from an action-control perspective},
journal = {Neuroscience & Biobehavioral Reviews},
volume = {153},
pages = {105397},
year = {2023},
issn = {0149-7634},
doi = {https://doi.org/10.1016/j.neubiorev.2023.105397},
url = {https://www.sciencedirect.com/science/article/pii/S0149763423003664},
author = {Bob Bramson and Ivan Toni and Karin Roelofs},
keywords = {Emotion control, Emotion regulation, Emotional-action selection, Forward modelling},
abstract = {Despite increasing interest in emotional processes in cognitive science, theories on emotion regulation have remained rather isolated, predominantly focused on cognitive regulation strategies such as reappraisal. However, recent neurocognitive evidence suggests that early emotion regulation may involve sensorimotor control in addition to other emotion-regulation processes. We propose an action-oriented view of emotion regulation, in which feedforward predictions develop from action-selection mechanisms. Those can account for acute emotional-action control as well as more abstract instances of emotion regulation such as cognitive reappraisal. We argue the latter occurs in absence of overt motor output, yet in the presence of full-blown autonomic, visceral, and subjective changes. This provides an integrated framework with testable neuro-computational predictions and concrete starting points for intervention to improve emotion control in affective disorders.}
}
@article{GARAVAGLIA2010258,
title = {Modelling industrial dynamics with “History-friendly” simulations},
journal = {Structural Change and Economic Dynamics},
volume = {21},
number = {4},
pages = {258-275},
year = {2010},
issn = {0954-349X},
doi = {https://doi.org/10.1016/j.strueco.2010.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S0954349X10000573},
author = {Christian Garavaglia},
keywords = {Simulation, Industrial dynamics, Evolutionary economics, “History-Friendly” models, Complexity},
abstract = {The use of simulation techniques has increased greatly in recent years. In economics the industrial dynamics approach makes use of simulation techniques to understand the complexity of the industrial process of continuous change. Among these models, a new branch of studies known as “History-friendly” models aims at establishing a close link between formal theory, developing stand-alone theoretical simulation models, and empirical evidence. In this paper, we study “History-friendly” analyses and counterfactuals. Some examples of “History-friendly” models are widely examined. Finally, the paper makes a critical contribution to “History-friendly” methodology and defines the role of “History-friendly” models in the debate on the empirical validation of simulations.}
}
@article{SMOLENTSEV2020111671,
title = {On the role of integrated computer modelling in fusion technology},
journal = {Fusion Engineering and Design},
volume = {157},
pages = {111671},
year = {2020},
issn = {0920-3796},
doi = {https://doi.org/10.1016/j.fusengdes.2020.111671},
url = {https://www.sciencedirect.com/science/article/pii/S0920379620302192},
author = {Sergey Smolentsev and Gandolfo Alessandro Spagnuolo and Arkady Serikov and Jens Juul Rasmussen and Anders H. Nielsen and Volker Naulin and Jaime Marian and Matti Coleman and Lorenzo Malerba},
keywords = {Fusion technology, Computer modelling, Neutronics, Materials, Plasma, MHD thermofluids, Model integration},
abstract = {Computer modelling is expected to play an increasingly important role in fusion design and technology, where the complexity of the physical processes involved (plasma, materials, engineering), and the highly interconnected nature of systems and components (“system of systems” design), call for support from sophisticated and integrated computer simulation tools. In this paper, we review the contribution of coupled computer modelling to the design of the reactor, breeding blanket and integrated first wall in terms of neutronics, materials behaviour (including plasma-materials interaction, radiation effects and compatibility with fluids), magnetohydrodynamics thermofluid issues and thermo-hydraulic aspects, as well as simulations of plasma transport out of the confinement region to determine heat and particle loads on plasma facing components. The current capabilities and levels of maturity of existing simulation tools are critically analysed, having in mind the possibility of integrating several tools in a single computational suite in the future and highlighting the perspectives and difficulties of such an endeavour.}
}
@article{VALLE2025105242,
title = {Task-value motivational prompts in a descriptive dashboard can increase anxiety among anxious learners},
journal = {Computers & Education},
volume = {229},
pages = {105242},
year = {2025},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2025.105242},
url = {https://www.sciencedirect.com/science/article/pii/S0360131525000107},
author = {Natercia Valle and Pavlo Antonenko and Denis Valle and Benjamin Baiser},
keywords = {Data science applications in education, Distance education and online learning, Human-computer interface, Pedagogical issues, Post-secondary education},
abstract = {Despite the ubiquitous use of learning analytics dashboards in computer-mediated learning environments, there is still a knowledge gap on how these tools can support learners’ academic performance and motivation. This article describes an experimental study that investigated the influence of motivational prompts (task-value scaffolding) in a descriptive learning analytics dashboard on learners’ motivation, statistics anxiety, and learning performance in an authentic semester-long online statistics course. The study was based on a two-group experimental design during two semesters (Fall 2020 and Spring 2021). A total of 122 graduate students completed the study. The results showed that despite learners’ mostly positive perceptions of the dashboard, the use of motivational prompts did not influence learners’ cognitive outcomes. Test anxiety was the only affective outcome influenced by the intervention, with motivational prompts having a negative effect on learners who started the course with a higher level of test anxiety. This study provides needed empirical evidence on how the design of these tools can influence learners’ affective outcomes, with implications for theory and practice. However, additional experimental studies that account for sources of heterogeneity (e.g., intrapersonal characteristics, contextual factors) are necessary to uncover theoretical gaps and opportunities in the design of effective learning analytics dashboards.}
}
@article{NAKHALAKEL20252288,
title = {System-theoretic analysis for the identification of emerging risks in the storage of dangerous substances},
journal = {Procedia Computer Science},
volume = {253},
pages = {2288-2295},
year = {2025},
note = {6th International Conference on Industry 4.0 and Smart Manufacturing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2025.01.289},
url = {https://www.sciencedirect.com/science/article/pii/S1877050925002972},
author = {Antonio Javier {Nakhal Akel} and Francesco Simone and Elena Stefana and Lorenzo Fedele and Riccardo Patriarca},
keywords = {STAMP, cyber attacks, socio-technical systems, operations management},
abstract = {The energy transition process lets novel risks emerge, impacting safety of modern industrial settings. The introduction of automation and digitalization fosters the collaboration and the interconnection between system agents (both humans and technologies) to comply with new sustainability objectives. Cyber-physical systems are increasingly present in industries, stressing the need to consider safety and security jointly. Systemic approaches, such as System-Theoretic Process Analysis (STPA), have been shown to be effective tools for dealing with such problems. This paper employs STPA to identify and analyse emergent risks within an energy transition scenario. Performing STPA permitted to identify control flaws and unsafe interactions when integrating renewable energy technologies. Results highlight critical agents and actions that may lead accidents. Specifically, a case study related to the storage of dangerous substances is presented in this paper, showing how tank’s automated controls may be susceptible to disruptions.}
}
@article{GIERISCH200972,
title = {Factors associated with annual-interval mammography for women in their 40s},
journal = {Cancer Epidemiology},
volume = {33},
number = {1},
pages = {72-78},
year = {2009},
issn = {1877-7821},
doi = {https://doi.org/10.1016/j.cdp.2009.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S0361090X0900021X},
author = {Jennifer M. Gierisch and Suzanne C. O’Neill and Barbara K. Rimer and Jessica T. DeFrank and J. Michael Bowling and Celette Sugg Skinner},
keywords = {Breast neoplasms, Guideline adherence, Health behavior, Middle aged, Attitude to health, Patient compliance, Mass screening, Female, Risk factor, Health knowledge},
abstract = {Background: Evidence is mounting that annual mammography for women in their 40s may be the optimal schedule to reduce morbidity and mortality from breast cancer. Few studies have assessed predictors of repeat mammography on an annual interval among these women. Methods: We assessed mammography screening status among 596 insured Black and Non-Hispanic white women ages 43–49. Adherence was defined as having a second mammogram 10–14 months after a previous mammogram. We examined socio-demographic, medical and healthcare-related variables on receipt of annual-interval repeat mammograms. We also assessed barriers associated with screening. Results: 44.8% of the sample were adherent to annual-interval mammography. A history of self-reported abnormal mammograms, family history of breast cancer and never having smoked were associated with adherence. Saying they had not received mammography reminders and reporting barriers to mammography were associated with non-adherence. Four barrier categories were associated with women's non-adherence: lack of knowledge/not thinking mammograms are needed, cost, being too busy, and forgetting to make/keep appointments. Conclusions: Barriers we identified are similar to those found in other studies. Health professionals may need to take extra care in discussing mammography screening risk and benefits due to ambiguity about screening guidelines for women in their 40s, especially for women without family histories of breast cancer or histories of abnormal mammograms. Reminders are important in promoting mammography and should be coupled with other strategies to help women maintain adherence to regular mammography.}
}
@article{HAN2025101699,
title = {Understanding the role of virtual mobility on how and what people create in virtual reality},
journal = {Thinking Skills and Creativity},
volume = {56},
pages = {101699},
year = {2025},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2024.101699},
url = {https://www.sciencedirect.com/science/article/pii/S1871187124002372},
author = {Eugy Han and Portia Wang and Cyan DeVeaux and Gabriella M. Harari and Jeremy N. Bailenson},
keywords = {Virtual reality, Creativity, Virtual mobility, Design creations},
abstract = {Virtual reality (VR) is considered a compelling tool to foster creativity by allowing its users to create in 3D space. However, the challenge lies in understanding how people use these tools and what they create, hindering the drawing of meaningful conclusions about VR as a viable tool for creativity. Furthermore, past research has shown that contextual factors shape how people create within VR, suggesting the existence of other factors. Here, we analyze the 3D creations of 137 participants responding to different creativity activities across seven sessions on a social VR platform. Specifically, we evaluate the role of virtual mobility, the capacity to move freely or have restricted movement in virtual space. We additionally present a VR-specific creativity coding scheme that follows recommendations from previous literature. Using dimensions derived from this coding scheme, we examine how these dimensions relate to behaviors and features of the creations in the context of virtual mobility. Results showed the significant role of virtual mobility on the design process, such that participants iterated and revised more by deleting more when their avatars were allowed to teleport and translate freely, compared to when their avatar’s movements were restricted to sitting down in virtual chairs. Furthermore, participants built shorter creations and took up less projection space with restricted virtual mobility. Results also showed that participants created more practical, unique, and well-implemented creations the more 3D models they used. Similarly, the more participants deleted, the more well-implemented the creations were. We discuss implications for designers of creation-oriented VR platforms and pedagogy for instructors facilitating activities in educational contexts.}
}
@article{LIU2024122986,
title = {Tackling fuel poverty and decarbonisation in a distributed heating system through a three-layer whole system approach},
journal = {Applied Energy},
volume = {362},
pages = {122986},
year = {2024},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2024.122986},
url = {https://www.sciencedirect.com/science/article/pii/S0306261924003696},
author = {Xinyao Liu and Floris Bierkens and Ishanki {De Mel} and Matthew Leach and Michael Short and Mona Chitnis and Boyue Zheng and Lirong Liu},
keywords = {Residential heating decarbonisation, Fuel poverty, Cambridge housing model, Mixed-integer linear programming, Input-output-simulation},
abstract = {Residential heating displays huge decarbonisation potential towards Net-Zero. The complexity of heating system and socio-economic system appeals for a systematic design to avoid exacerbating fuel poverty. This study develops a three-layer heat-for-all model which integrates building stocks analysis, distributed heating system optimisation, economic and environmental impacts simulation to tackle heating decarbonisation and fuel poverty simultaneously. This whole system model is a powerful decision support tool that can help conceive heating decarbonisation strategies for wider regions and countries. More than 400,000 scenarios are created, considering the effects of future policy schemes (No Grant, Business as Usual, Proposed), minimum emission reduction target, carbon intensity of grid, future natural gas, and electricity prices. Results show that optimised heating system decarbonisation plan heavily relies on future energy prices. In the case study, only air source heat pumps are chosen when electricity price is lower than 3 times gas price. Secondly, investment in heating system could stimulate the greenhouse gas emission of whole supply chain, hedging the emission reduction achieved in heating system. This further reveals that life cycle thinking is imperative in GHG emission mitigation. Thirdly, electricity decarbonisation plays a vital role in achieving whole system emission reduction. The grid carbon intensity reduction makes substantial contribution to the emission reduction of heating system and industry system. In tackling fuel poverty, it's worth noticing that the fuel poverty is aggravated with more grant support under certain scenarios, since current policy schemes focus on capital investment in heating system but overlook the increased energy bills. It appeals for a more comprehensive policy design considering all stakeholders.}
}
@article{GAO2019242,
title = {Expert knowledge recommendation systems based on conceptual similarity and space mapping},
journal = {Expert Systems with Applications},
volume = {136},
pages = {242-251},
year = {2019},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2019.06.013},
url = {https://www.sciencedirect.com/science/article/pii/S0957417419304130},
author = {Li Gao and Kun Dai and Liping Gao and Tao Jin},
keywords = {Conceptual similarity, Space mapping, Core resource database (CRD), Institutional repository (IR), Expert Knowledge Recommendation System (EKRS)},
abstract = {The semantic analysis method of structured big data generated based on human knowledge is important in expert recommendation systems and scientific and technological information analysis. In these fields, the most important problem is the calculation of concept similarity. The study aims to explore the spatial mapping relationship between the general knowledge base and the professional knowledge base for the application of the general knowledge map in professional fields. With the core resource database (CRD) as the main body of the general knowledge and the institutional repository (IR) as the main body of the professional knowledge, the conceptual features of institutional expert knowledge were firstly abstracted from IR and inferred from small-scale datasets and the mathematical model was established based on the similarity of text concepts and related ranking results. Then, a two-set concept space mapping algorithm between CRD and IR was designed. In the algorithm, the more granular concept nodes were extracted from the information on the shortest paths among concepts to obtain a new knowledge set, the Expert Knowledge Recommendation System (EKRS). Finally, the simulation experiment was carried out with open datasets to verify the algorithm. The simulation results showed that the algorithm reduced the structural complexity in the calculation of large datasets. The proposed system model had a clear knowledge structure and the recommended accuracy of the text similarity was high. For small-scale knowledge base datasets with different sparsity, the system showed the stable performance, indicating the better convergence and robustness of the algorithm.}
}
@article{TRAUSANMATU20231052,
title = {Identification of creativity in collaborative conversations based on the polyphonic model},
journal = {Procedia Computer Science},
volume = {221},
pages = {1052-1057},
year = {2023},
note = {Tenth International Conference on Information Technology and Quantitative Management (ITQM 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.08.087},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923008451},
author = {Stefan Trausan-Matu},
keywords = {polyphonic model, creativity, brainstorming, collaboration, computer-supported collaborative learning, natural language processing, deep learning},
abstract = {The paper presents a theoretical approach and a set of experiments that operationalize it for the identification of creative moments in conversations. State-of-the-art artificial intelligence technology is used for the operationalization: natural language processing, machine learning, and deep neural networks The approach is based on the polyphonic model introduced by Trausan-Matu, which starts from Mikhail Bakhtin's analogy of discourse building in texts with polyphonic music. The divergent and convergent steps of creativity are related to the inter-animation of voices through dissonances and consonances in polyphonic, contrapuntal music.}
}
@article{MALINVERNI2021100305,
title = {Educational Robotics as a boundary object: Towards a research agenda},
journal = {International Journal of Child-Computer Interaction},
volume = {29},
pages = {100305},
year = {2021},
issn = {2212-8689},
doi = {https://doi.org/10.1016/j.ijcci.2021.100305},
url = {https://www.sciencedirect.com/science/article/pii/S2212868921000349},
author = {Laura Malinverni and Cristina Valero and Marie Monique Schaper and Isabel Garcia {de la Cruz}},
keywords = {Educational robotics, Children, Robots, Boundary object, Intelligent technologies},
abstract = {Educational robotics has become each time more present in the educational experiences of children and young people. Nonetheless, often, the way in which robotics is introduced in educational settings has been considered as unnecessarily narrow. The paper aims at widening the scope of Educational Robotics and expanding the pedagogical possibilities of this field. To this end, the paper draws on the outcomes of two case studies carried out with primary and secondary school children aimed at investigating their views about robots. These studies allow framing and identifying five themes we believe are particularly relevant to rethink the pedagogy of Educational Robotics. Using these themes as cornerstones for reflection, we delineate a set of dimensions and paths to move Educational Robotics beyond the focus on technical skills but instead explore its potential as a boundary object to involve children in reflective processes around the ethical, social and cultural implications of emerging intelligent technologies.}
}
@article{SCHWAB2018500,
title = {A Robust Fault Detection Method using a Zonotopic Kaucher Set-membership Approach},
journal = {IFAC-PapersOnLine},
volume = {51},
number = {24},
pages = {500-507},
year = {2018},
note = {10th IFAC Symposium on Fault Detection, Supervision and Safety for Technical Processes SAFEPROCESS 2018},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2018.09.623},
url = {https://www.sciencedirect.com/science/article/pii/S2405896318323358},
author = {Stefan Schwab and Vicenç Puig and Soeren Hohmann},
keywords = {Robust fault detection, set-membership approach, Kaucher arithmetic},
abstract = {This paper presents a robust fault detection method using a zonotopic Kaucher set-membership method. The fault detection approach is based on checking the consistency between the model and the data. Consistency is given if there is an intersection between the feasible parameter set and the nominal parameter set. To allow efficient computation the feasible set is approximated by a zonotope. Due to the usage of Kaucher interval arithmetic the results are mathematically guaranteed. The proposed approach is assessed using an illustrative application based on a well-known four-tank case study. The study shows that it is possible to detect even small errors in a noisy setting.}
}
@article{SHUKLA2024e31397,
title = {AI as a user of AI: Towards responsible autonomy},
journal = {Heliyon},
volume = {10},
number = {11},
pages = {e31397},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e31397},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024074280},
author = {Amit K. Shukla and Vagan Terziyan and Timo Tiihonen},
keywords = {Artificial Intelligence (AI), Autonomy, Responsible AI, ChatGPT, Prompt engineering, AI accountability},
abstract = {Recent advancements in Artificial Intelligence (AI), particularly in generative language models and algorithms, have led to significant impacts across diverse domains. AI capabilities to address prompts are growing beyond human capability but we expect AI to perform well also as a prompt engineer. Additionally, AI can serve as a guardian for ethical, security, and other predefined issues related to generated content. We postulate that enforcing dialogues among AI-as-prompt-engineer, AI-as-prompt-responder, and AI-as-Compliance-Guardian can lead to high-quality and responsible solutions. This paper introduces a novel AI collaboration paradigm emphasizing responsible autonomy, with implications for addressing real-world challenges. The paradigm of responsible AI-AI conversation establishes structured interaction patterns, guaranteeing decision-making autonomy. Key implications include enhanced understanding of AI dialogue flow, compliance with rules and regulations, and decision-making scenarios exemplifying responsible autonomy. Real-world applications envision AI systems autonomously addressing complex challenges. We have made preliminary testing of such a paradigm involving instances of ChatGPT autonomously playing various roles in a set of experimental AI-AI conversations and observed evident added value of such a framework.}
}
@article{BIENVENU202049,
title = {On low for speed oracles},
journal = {Journal of Computer and System Sciences},
volume = {108},
pages = {49-63},
year = {2020},
issn = {0022-0000},
doi = {https://doi.org/10.1016/j.jcss.2019.08.007},
url = {https://www.sciencedirect.com/science/article/pii/S0022000018305828},
author = {Laurent Bienvenu and Rod Downey},
keywords = {Oracle computations, Lowness for speed},
abstract = {Relativizing computations of Turing machines to an oracle is a central concept in the theory of computation, both in complexity theory and in computability theory(!). Inspired by lowness notions from computability theory, Allender introduced the concept of “low for speed” oracles. An oracle A is low for speed if relativizing to A has essentially no effect on computational complexity, meaning that if a decidable language can be decided in time f(n) with access to oracle A, then it can be decided in time poly(f(n)) without any oracle. The existence of non-computable such A's was later proven by Bayer and Slaman, who even constructed a computably enumerable one, and exhibited a number of properties of these oracles. In this paper, we pursue this line of research, answering the questions left by Bayer and Slaman and give further evidence that the class of low for speed oracles is a very rich one.}
}