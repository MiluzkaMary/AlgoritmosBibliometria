@article{KOMPALLI2016534,
title = {Clusters of Genetic-based Attributes Selection of Cancer Data},
journal = {Procedia Computer Science},
volume = {89},
pages = {534-539},
year = {2016},
note = {Twelfth International Conference on Communication Networks, ICCN 2016, August 19– 21, 2016, Bangalore, India Twelfth International Conference on Data Mining and Warehousing, ICDMW 2016, August 19-21, 2016, Bangalore, India Twelfth International Conference on Image and Signal Processing, ICISP 2016, August 19-21, 2016, Bangalore, India},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2016.06.098},
url = {https://www.sciencedirect.com/science/article/pii/S1877050916311632},
author = {Vijaya Sri Kompalli and K. Usha Rani},
keywords = {Cluster, Coupling, Cohesion, Genetic Algorithm, Fuzzy C-Means.},
abstract = {Clustering of data simplifies the task of data analysis and results in better disease diagnosis. Well-existing K-Means clustering hard computes clusters. Due to which the data may be centered to a specific cluster having less concentration on the effect of the coupling of clusters. Soft Computing methods are widely used in medical field as it contains fuzzy natured data. A Soft Computing approach of clustering called Fuzzy C-Means (FCM) deals with coupling. FCM clustering soft computes the clusters to determine the clusters based on the probability of having memberships in each of the clusters. The probability function used, determines the extent of coupling among the clusters. In order to achieve the computational efficiency and binding of features genetic evaluation is introduced. Genetic-based features are identified having more cohesion based on the fitness function values and then the coupling of the clusters is done using K-Means clustering in one trial and FCM in another trial. Analysis of coupling and cohesion is performed on Wisconsin Breast Cancer Dataset. Nature of clusters formations are observed with respect to coupling and cohesion.}
}
@article{MAJID2004108,
title = {Can language restructure cognition? The case for space},
journal = {Trends in Cognitive Sciences},
volume = {8},
number = {3},
pages = {108-114},
year = {2004},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2004.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S1364661304000208},
author = {Asifa Majid and Melissa Bowerman and Sotaro Kita and Daniel B.M. Haun and Stephen C. Levinson},
abstract = {Frames of reference are coordinate systems used to compute and specify the location of objects with respect to other objects. These have long been thought of as innate concepts, built into our neurocognition. However, recent work shows that the use of such frames in language, cognition and gesture varies cross-culturally, and that children can acquire different systems with comparable ease. We argue that language can play a significant role in structuring, or restructuring, a domain as fundamental as spatial cognition. This suggests we need to rethink the relation between the neurocognitive underpinnings of spatial cognition and the concepts we use in everyday thinking, and, more generally, to work out how to account for cross-cultural cognitive diversity in core cognitive domains.}
}
@incollection{ESFELD2001859,
title = {Atomism and Holism: Philosophical Aspects},
editor = {Neil J. Smelser and Paul B. Baltes},
booktitle = {International Encyclopedia of the Social & Behavioral Sciences},
publisher = {Pergamon},
address = {Oxford},
pages = {859-864},
year = {2001},
isbn = {978-0-08-043076-8},
doi = {https://doi.org/10.1016/B0-08-043076-7/01005-6},
url = {https://www.sciencedirect.com/science/article/pii/B0080430767010056},
author = {M. Esfeld},
abstract = {Social atomism is the thesis that an individual considered in isolation can have thoughts with a determinate conceptual content. Social holism, by contrast, is the thesis that social relations are essential for a human being in order to be a ‘thinking’ being. The discussion on atomism vs. holism extends to aspects of thoughts as well. Semantic atomism is the thesis that each thought has a meaning independently of other thoughts. Semantic holism, in reverse, is the thesis that the meaning of a thought consists in its inferential relations to other thoughts in a system of thoughts. Confirmation atomism is the thesis that thoughts can be empirically confirmed one by one. Confirmation holism, by contrast, is the thesis that only a whole system of thoughts or a whole theory can be confirmed by experience. Social atomism in modern philosophy goes back to Hobbes. Social holism comes up in romanticism and its predecessors; it is worked out by Hegel. In today's discussion, the rule-following considerations that are developed by Kripke on behalf of the later Wittgenstein are the main argument for social holism. Social atomists counter this argument by a naturalistic account of rule following in terms of certain dispositions to behavior.}
}
@article{MARSHALL2024R950,
title = {Where physics and biology meet},
journal = {Current Biology},
volume = {34},
number = {20},
pages = {R950-R960},
year = {2024},
issn = {0960-9822},
doi = {https://doi.org/10.1016/j.cub.2024.08.022},
url = {https://www.sciencedirect.com/science/article/pii/S0960982224011345},
author = {Wallace Marshall and Buzz Baum and Adrienne Fairhall and Carl-Philipp Heisenberg and Elena Koslover and Andrea Liu and Yanlan Mao and Alex Mogilner and Celeste M. Nelson and Ewa K. Paluch and Xavier Trepat and Alpha Yap}
}
@article{MILLI2021104881,
title = {A rational reinterpretation of dual-process theories},
journal = {Cognition},
volume = {217},
pages = {104881},
year = {2021},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2021.104881},
url = {https://www.sciencedirect.com/science/article/pii/S0010027721003024},
author = {Smitha Milli and Falk Lieder and Thomas L. Griffiths},
keywords = {Bounded rationality, Dual-process theories, Meta-decision making, Bounded optimality, Metareasoning, Resource-rationality},
abstract = {Highly influential “dual-process” accounts of human cognition postulate the coexistence of a slow accurate system with a fast error-prone system. But why would there be just two systems rather than, say, one or 93? Here, we argue that a dual-process architecture might reflect a rational tradeoff between the cognitive flexibility afforded by multiple systems and the time and effort required to choose between them. We investigate what the optimal set and number of cognitive systems would be depending on the structure of the environment. We find that the optimal number of systems depends on the variability of the environment and the difficulty of deciding when which system should be used. Furthermore, we find that there is a plausible range of conditions under which it is optimal to be equipped with a fast system that performs no deliberation (“System 1”) and a slow system that achieves a higher expected accuracy through deliberation (“System 2”). Our findings thereby suggest a rational reinterpretation of dual-process theories.}
}
@article{DELLACQUA2021199,
title = {Increased functional connectivity within alpha and theta frequency bands in dysphoria: A resting-state EEG study},
journal = {Journal of Affective Disorders},
volume = {281},
pages = {199-207},
year = {2021},
issn = {0165-0327},
doi = {https://doi.org/10.1016/j.jad.2020.12.015},
url = {https://www.sciencedirect.com/science/article/pii/S0165032720331049},
author = {Carola Dell'Acqua and Shadi Ghiasi and Simone {Messerotti Benvenuti} and Alberto Greco and Claudio Gentili and Gaetano Valenza},
keywords = {depression, depressive symptoms, dysphoria, functional connectivity, EEG, vulnerability},
abstract = {Background: The understanding of neurophysiological correlates underlying the risk of developing depression may have a significant impact on its early and objective identification. Research has identified abnormal resting-state electroencephalography (EEG) power and functional connectivity patterns in major depression. However, the entity of dysfunctional EEG dynamics in dysphoria is yet unknown. Methods: 32-channel EEG was recorded in 26 female individuals with dysphoria and in 38 age-matched, female healthy controls. EEG power spectra and alpha asymmetry in frontal and posterior channels were calculated in a 4-minute resting condition. An EEG functional connectivity analysis was conducted through phase locking values, particularly mean phase coherence. Results: While individuals with dysphoria did not differ from controls in EEG spectra and asymmetry, they exhibited dysfunctional brain connectivity. Particularly, in the theta band (4-8 Hz), participants with dysphoria showed increased connectivity between right frontal and central areas and right temporal and left occipital areas. Moreover, in the alpha band (8-12 Hz), dysphoria was associated with increased connectivity between right and left prefrontal cortex and between frontal and central-occipital areas bilaterally. Limitations: All participants belonged to the female gender and were relatively young. Mean phase coherence did not allow to compute the causal and directional relation between brain areas. Conclusions: An increased EEG functional connectivity in the theta and alpha bands characterizes dysphoria. These patterns may be associated with the excessive self-focus and ruminative thinking that typifies depressive symptoms. EEG connectivity patterns may represent a promising measure to identify individuals with a higher risk of developing depression.}
}
@article{NOBRE2019132,
title = {Premembering Experience: A Hierarchy of Time-Scales for Proactive Attention},
journal = {Neuron},
volume = {104},
number = {1},
pages = {132-146},
year = {2019},
issn = {0896-6273},
doi = {https://doi.org/10.1016/j.neuron.2019.08.030},
url = {https://www.sciencedirect.com/science/article/pii/S0896627319307366},
author = {Anna C. Nobre and Mark G. Stokes},
keywords = {memory, attention, decision-making, hippocampus, prefrontal cortex, priming, working memory, episodic memory, implicit memory},
abstract = {Memories are about the past, but they serve the future. Memory research often emphasizes the former aspect: focusing on the functions that re-constitute (re-member) experience and elucidating the various types of memories and their interrelations, timescales, and neural bases. Here we highlight the prospective nature of memory in guiding selective attention, focusing on functions that use previous experience to anticipate the relevant events about to unfold—to “premember” experience. Memories of various types and timescales play a fundamental role in guiding perception and performance adaptively, proactively, and dynamically. Consonant with this perspective, memories are often recorded according to expected future demands. Using working memory as an example, we consider how mnemonic content is selected and represented for future use. This perspective moves away from the traditional representational account of memory toward a functional account in which forward-looking memory traces are informationally and computationally tuned for interacting with incoming sensory signals to guide adaptive behavior.}
}
@incollection{DEDEOGLU2023251,
title = {Chapter Nine - Blockchain meets edge-AI for food supply chain traceability and provenance},
editor = {Joost Laurus Dinant Nelis and Aristeidis S. Tsagkaris},
series = {Comprehensive Analytical Chemistry},
publisher = {Elsevier},
volume = {101},
pages = {251-275},
year = {2023},
booktitle = {Smartphones for Chemical Analysis: From Proof-of-concept to Analytical Applications},
issn = {0166-526X},
doi = {https://doi.org/10.1016/bs.coac.2022.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S0166526X22001064},
author = {Volkan Dedeoglu and Sidra Malik and Gowri Ramachandran and Shantanu Pal and Raja Jurdak},
keywords = {Blockchain, Edge AI, Traceability, Provenance, Supply Chains},
abstract = {Food supply chains are increasingly digitised and automated through the use of technologies such as Internet-of-Things (IoT), blockchain and Artificial Intelligence (AI). Such digitization efforts often rely on cloud computing, which creates bandwidth overhead, high latency, security and privacy challenges. In this chapter, we propose the use of edge AI, which is a computing paradigm that combines edge computing and AI, to complete computing tasks close to the sensor data sources. Edge AI can promote greater scalability and avoid the security and privacy challenges of centralised cloud computing. The chapter introduces the provenance and traceability requirements of food supply chains and the digitization of these supply chains through blockchain, IoT, and AI. The chapter also proposes the use of smartphone integrated sensors to provide unique physical, chemical, or biological signatures of food supply chain products, and to conduct the necessary computations on the smartphone. The proposed Edge AI approach to supply chain digitization sets the scene for greater resilience in modern digital supply chains.}
}
@article{DELIGUORO2023114082,
title = {From semantics to types: The case of the imperative λ-calculus},
journal = {Theoretical Computer Science},
volume = {973},
pages = {114082},
year = {2023},
issn = {0304-3975},
doi = {https://doi.org/10.1016/j.tcs.2023.114082},
url = {https://www.sciencedirect.com/science/article/pii/S030439752300395X},
author = {Ugo de'Liguoro and Riccardo Treglia},
keywords = {State monad, Imperative lambda calculus, Type assignment systems, Filter models},
abstract = {We study the logical semantics of an untyped λ-calculus equipped with operators representing read and write operations from and to a global store. Such a logic consists of an intersection type assignment system, which we derive from the denotational semantics of the calculus, based on the monadic approach to model computational λ-calculi. The system is obtained by constructing a filter model in the category of ω-algebraic lattices, such that the typing rules can be recovered out of the term interpretation. By construction, the so-obtained type system satisfies the “type-semantics” property and completeness.}
}
@article{JAIN2023119859,
title = {Optimized levy flight model for heart disease prediction using CNN framework in big data application},
journal = {Expert Systems with Applications},
volume = {223},
pages = {119859},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.119859},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423003603},
author = {Arushi Jain and Annavarapu {Chandra Sekhara Rao} and Praphula {Kumar Jain} and Yu-Chen Hu},
keywords = {Optimization, Heart disease prediction, Convolution neural networks, Big data, Swarm intelligence algorithm},
abstract = {Cardiac disease is one of the most complex diseases globally. It affects the lives of humans critically. It is essential for accurate and timely diagnosis to treat heart failure and prevent the disease. In most aspects, it was not so successful with the traditional method, which uses past medical history. Many existing models had several types of the loss function in traditional CNN can lead to misidentification of the model. To solve this problem, so many scholars have used the swarm intelligence algorithm, but most of these techniques are stuck in the local minima and suffer from premature convergence. In the proposed method, we build up the Levy Flight – Convolutional Neural Network (LV-CNN) depending on the diagnostic system using heart disease image data set for heart disease assessment. Initially, the input Big Data images are resized to reduce the computational complexity of the system. Then, those resized images are subject to the proposed LV-CNN model. Therefore, the LV approach is integrated with the Sunflower Optimization Algorithm (SFO) to reduce loss function occurring in the CNN architecture. Such a combination helps the SFO algorithm avoid trapping in local minima due to the random walk of the levy flight. The proposed algorithm will be simulated using the MATLAB tool and tested experimentally in terms of accuracy is 95.74%, specificity is 0.96%, the error rate is 0.35, and time consumption is 9.71 s. This comparative analysis revealed that the excellence of the proposed model.}
}
@article{WANG2007254,
title = {An efficient algorithm for generalized discriminant analysis using incomplete Cholesky decomposition},
journal = {Pattern Recognition Letters},
volume = {28},
number = {2},
pages = {254-259},
year = {2007},
issn = {0167-8655},
doi = {https://doi.org/10.1016/j.patrec.2006.07.008},
url = {https://www.sciencedirect.com/science/article/pii/S0167865506001966},
author = {Haixian Wang and Zilan Hu and Yu’e Zhao},
keywords = {Generalized discriminant analysis, Nonlinear feature extraction, Eigenvalue decomposition, Gram–Schmidt orthonormalization, Incomplete Cholesky decomposition},
abstract = {Generalized discriminant analysis (GDA) has provided an extremely powerful approach to extracting nonlinear features via kernel trick. And it has been suggested for a number of applications, such as classification problem. Whereas the GDA could be solved by the utilization of Mercer kernels, a drawback of the standard GDA is that it may suffer from computational problem for large scale data set. Besides, there is still attendant problem of numerical accuracy when computing the eigenvalue problem of large matrices. Also, the GDA would occupy large memory (to store the kernel matrix). To overcome these deficiencies, we use Gram–Schmidt orthonormalization and incomplete Cholesky decomposition to find a basis for the entire training samples, and then formulate GDA as another eigenvalue problem of matrix whose size is much smaller than that of the kernel matrix by using the basis, while still working out the optimal discriminant vectors from all training samples. The theoretical analysis and experimental results on both artificial and real data set have shown the superiority of the proposed method for performing GDA in terms of computational efficiency and even the recognition accuracy, especially when the training samples size is large.}
}
@article{KOULADOUM2024200052,
title = {The role of institutional quality on the impact of Chinese foreign direct investments and human capital development on macroeconomic performance in the CEMAC zone},
journal = {Transnational Corporations Review},
volume = {16},
number = {2},
pages = {200052},
year = {2024},
issn = {1925-2099},
doi = {https://doi.org/10.1016/j.tncr.2024.200052},
url = {https://www.sciencedirect.com/science/article/pii/S1925209924005783},
author = {Jean-Claude Kouladoum},
keywords = {Institutional quality, Chinese foreign direct investments, Human capital development, Macroeconomic performance, CEMAC zone},
abstract = {This paper investigates the role of institutional quality in terms of governance on the impact of Chinese foreign direct investments and human capital development on the macroeconomic performance of the CEMAC zone between 2003 and 2020. The data were analyzed using descriptive statistics and the Correlated Panels Corrected Standard Errors (PCSEs) Approach. The findings indicated that poor governance performance in the CEMAC zone deteriorates the effects of Chinese foreign direct investments and human capital development on the macroeconomic performance of the CEMAC zone. At the same time, poor governance performance also deteriorates the impact of foreign aid and personal remittances received on the macroeconomic performance of the CEMAC zone. This study strongly recommends measures to improve institutional quality such as increased training on ethical thinking in all forms of education, meritocratic recruitment to the civil service, and auditing of public finances and services.}
}
@article{GUI2024111972,
title = {Molten salt-promoted MgO-based CO2 adsorbents: Selective adsorption on polycrystalline surfaces},
journal = {Journal of Environmental Chemical Engineering},
volume = {12},
number = {2},
pages = {111972},
year = {2024},
issn = {2213-3437},
doi = {https://doi.org/10.1016/j.jece.2024.111972},
url = {https://www.sciencedirect.com/science/article/pii/S2213343724001027},
author = {Changqing Gui and Zirui Wang and Changjian Ling and Zhongfeng Tang},
keywords = {CO, MgCl·6 HO, Molten salt, MgO, Capture},
abstract = {Molten salt-doped MgO adsorbent is considered one of the most promising CO2 adsorbents in the field. In this work, MgO-based adsorbents were prepared by one-step calcination using MgCl2·6 H2O as magnesium source. The CO2 adsorption performance of MgO-based adsorbents was investigated via different methods. Results showed that the maximum CO2 adsorption capacity of MgO doped by LiNO3-NaNO3-KNO3 was 57.1% at the CO2 concentration of 80% and 350 ℃, and the MgO-based adsorbents showed good regeneration. The nanosheet structure of the MgO-based adsorbents decreased with the increase in the number of cycles, whereas the crystal structures of MgO and alkali metal nitrates did not change because of multiple decarbonization. DFT computation revealed selective adsorption of CO2 on different crystal faces of MgO. The (200) crystal face of molten salt-doped MgO did not have CO2 trap ability. In addition, the doped nitrate did not directly participate in the reaction but reduced the adsorption energy of MgO carbonation. The adsorption energies of the MgO (220) and (222) crystal faces after doping with nitrate were reduced to − 2.07 and − 3.26 eV, respectively. The overall energy level of adsorption decreased as the number of resonance peaks and the stability of the structure increased. This study explains why MgO currently fails to reach the theoretical adsorption capacity and reveals the underlying mechanism of molten salts.}
}
@article{SCHNASE2017198,
title = {MERRA Analytic Services: Meeting the Big Data challenges of climate science through cloud-enabled Climate Analytics-as-a-Service},
journal = {Computers, Environment and Urban Systems},
volume = {61},
pages = {198-211},
year = {2017},
note = {Geospatial Cloud Computing and Big Data},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2013.12.003},
url = {https://www.sciencedirect.com/science/article/pii/S019897151300118X},
author = {John L. Schnase and Daniel Q. Duffy and Glenn S. Tamkin and Denis Nadeau and John H. Thompson and Cristina M. Grieg and Mark A. McInerney and William P. Webster},
keywords = {MapReduce, Hadoop, Data analytics, Data services, Cloud Computing, Generativity, iRODS, MERRA, ESGF, BAER},
abstract = {Climate science is a Big Data domain that is experiencing unprecedented growth. In our efforts to address the Big Data challenges of climate science, we are moving toward a notion of Climate Analytics-as-a-Service (CAaaS). We focus on analytics, because it is the knowledge gained from our interactions with Big Data that ultimately produce societal benefits. We focus on CAaaS because we believe it provides a useful way of thinking about the problem: a specialization of the concept of business process-as-a-service, which is an evolving extension of IaaS, PaaS, and SaaS enabled by Cloud Computing. Within this framework, Cloud Computing plays an important role; however, we see it as only one element in a constellation of capabilities that are essential to delivering climate analytics as a service. These elements are essential because in the aggregate they lead to generativity, a capacity for self-assembly that we feel is the key to solving many of the Big Data challenges in this domain. MERRA Analytic Services (MERRA/AS) is an example of cloud-enabled CAaaS built on this principle. MERRA/AS enables MapReduce analytics over NASA’s Modern-Era Retrospective Analysis for Research and Applications (MERRA) data collection. The MERRA reanalysis integrates observational data with numerical models to produce a global temporally and spatially consistent synthesis of 26 key climate variables. It represents a type of data product that is of growing importance to scientists doing climate change research and a wide range of decision support applications. MERRA/AS brings together the following generative elements in a full, end-to-end demonstration of CAaaS capabilities: (1) high-performance, data proximal analytics, (2) scalable data management, (3) software appliance virtualization, (4) adaptive analytics, and (5) a domain-harmonized API. The effectiveness of MERRA/AS has been demonstrated in several applications. In our experience, Cloud Computing lowers the barriers and risk to organizational change, fosters innovation and experimentation, facilitates technology transfer, and provides the agility required to meet our customers’ increasing and changing needs. Cloud Computing is providing a new tier in the data services stack that helps connect earthbound, enterprise-level data and computational resources to new customers and new mobility-driven applications and modes of work. For climate science, Cloud Computing’s capacity to engage communities in the construction of new capabilities is perhaps the most important link between Cloud Computing and Big Data.}
}
@incollection{MURRAY202119,
title = {Chapter Two - The neurocognitive mechanisms of responsibility: A framework for normatively relevant neuroscience},
editor = {Martín Hevia},
series = {Developments in Neuroethics and Bioethics},
publisher = {Academic Press},
volume = {4},
pages = {19-40},
year = {2021},
booktitle = {Regulating Neuroscience: Transnational Legal Challenges},
issn = {2589-2959},
doi = {https://doi.org/10.1016/bs.dnb.2021.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S2589295921000023},
author = {Samuel Murray and Felipe {De Brigard}},
keywords = {Moral responsibility, Autonomy of ethics, Moral neuroscience, Decision-making, Practical reasoning, Moral agency},
abstract = {We argue that research in cognitive neuroscience can contribute meaningfully to some normative theorizing. To make our case, we develop one instance where ethical inquiry progressed through empirical research into the computational basis of decision-making. From this, we draw some general considerations about the kinds of normative inquiry where research in cognitive neuroscience might be relevant.}
}
@article{KEBEDE2024131461,
title = {Transfer learning-based deep learning models for proton exchange membrane fuel remaining useful life prediction},
journal = {Fuel},
volume = {367},
pages = {131461},
year = {2024},
issn = {0016-2361},
doi = {https://doi.org/10.1016/j.fuel.2024.131461},
url = {https://www.sciencedirect.com/science/article/pii/S0016236124006094},
author = {Getnet Awoke Kebede and Shih-Che Lo and Fu-Kwun Wang and Jia-Hong Chou},
keywords = {Drop method, Long-short term memory with attention, Remaining useful life prediction, Transfer learning, Variational autoencoder},
abstract = {Proton exchange membrane fuel cells (PEMFCs) offer power generation capabilities for diverse applications including commercial enterprises, industrial sectors, and residential technologies. Nevertheless, the comprehensive integration of PEMFC applications could be improved by challenges related to degradation and durability. The imperative development of efficient performance prognostic models assumes a pivotal role in the prognosis of remaining useful life (RUL), health monitoring, and effective utilization of PEMFCs. This paper centers on the prognostication of critical components within PEMFCs and introduces a transfer learning approach based on variational autoencoder and bi-directional long short-term memory with an attention mechanism (Bi-LSTM-AM) model. This approach combines feature fusion, knee-point detection, and a sophisticated deep-learning-based predictive model. Notably, incorporating the variational autoencoder as the framework for feature fusion introduces a novel perspective previously unexplored. Identifying the knee point and knowing the start point on the training data, facilitates optimized parameter computation. The application of transfer learning facilitates the transfer of optimal model parameters and weights from a source to a target dataset. Conclusively, the estimation of stack voltage degradation and real-time RUL prediction based on the test dataset is executed by implementing our proposed method. The stack voltage prediction findings showcase the Bi-LSTM-AM model’s superior performance relative to comparison models. The proposed online rolling prediction model, utilizing a sliding window technique for RUL prediction, yields significantly enhanced accuracy, culminating in a relative error margin ranging from approximately 1.69% to 5.04%.}
}
@article{LEPP2025100642,
title = {Does generative AI help in learning programming: Students’ perceptions, reported use and relation to performance},
journal = {Computers in Human Behavior Reports},
volume = {18},
pages = {100642},
year = {2025},
issn = {2451-9588},
doi = {https://doi.org/10.1016/j.chbr.2025.100642},
url = {https://www.sciencedirect.com/science/article/pii/S2451958825000570},
author = {Marina Lepp and Joosep Kaimre},
keywords = {Artificial Intelligence (AI), Programming education, Higher education, Student perceptions, Academic performance},
abstract = {In 2022, the release of ChatGPT marked a significant advancement in the use of Artificial Intelligence (AI) chatbots, particularly impacting fields like computer science and education. The ability to generate code snippets using AI chatbots has introduced new opportunities and challenges in teaching programming. However, there is limited agreement on how students integrate them into their learning processes. This study aims to explore how students utilize AI chatbots in the "Object-Oriented Programming" course and examine the relationship between chatbot usage and academic performance. To address this, 231 students completed a survey assessing the frequency and manner of chatbot usage. Descriptive statistical methods were employed to analyze usage and perceptions, while Spearman's correlation was used to investigate the connection between chatbot usage and course performance. Results indicated that students primarily relied on AI chatbots for programming tasks. Interestingly, students' performance negatively correlates with the reported frequency of using these tools. These findings provide valuable insights for programming educators, offering a better understanding of students' perceptions and use of AI chatbots. This knowledge can inform strategies for integrating these tools effectively into computer science education.}
}
@article{SWARTZ2004773,
title = {A multimethod approach to the combat air forces mix and deployment problem},
journal = {Mathematical and Computer Modelling},
volume = {39},
number = {6},
pages = {773-797},
year = {2004},
note = {Defense transportation: Algorithms, models, and applications for the 21st century},
issn = {0895-7177},
doi = {https://doi.org/10.1016/S0895-7177(04)90554-7},
url = {https://www.sciencedirect.com/science/article/pii/S0895717704905547},
author = {S.M Swartz and A.W Johnson},
keywords = {Multiattribute decision analysis, Ranking and selection, Heuristics},
abstract = {The purpose of military logistics is to ensure that the material elements of combat capability come together at the right place and time and in the right configuration to be useful to the supported commander. These material elements are constrained in both quantity and location. The usefulness of any element to a commander is dependent upon both its extrinsic (qualitative; situation dependent) and intrinsic (quantitative; inherent) characteristics. Our research provides a methodology for rationally assigning relative value to material resources over time, in order to improve the linkage between what arrives (becomes available for use) in theater at any given time, and what is actually needed at that time. A blend of qualitative (value focused thinking and hierarchical weighting) and quantitative (a greedy matching algorithm) methods were used against the lift-constrained combat forces material selection/movement problem. The intent is to provide a decision support tool for the formulation of force mixes that best support desired time-phased battlefield objectives, given constraints on available transportation resources. This methodology is applicable to general crisis response planning, such as for disaster relief.}
}
@article{YAMANE2021102520,
title = {Humor meets morality: Joke generation based on moral judgement},
journal = {Information Processing & Management},
volume = {58},
number = {3},
pages = {102520},
year = {2021},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2021.102520},
url = {https://www.sciencedirect.com/science/article/pii/S0306457321000297},
author = {Hiroaki Yamane and Yusuke Mori and Tatsuya Harada},
keywords = {Computational humor, Morality, Recurrent neural networks, Joke generation},
abstract = {Although humor enriches human lives, some jokes fail to amuse people because of a lack of morality. In this paper, we propose a mechanism capable of selecting humor based on moral criteria. To this end, we first construct a model based on an N-gram corpus and generate joke candidates using various template patterns. We then employ a moral judgement classifier based on a recurrent neural network and utilize the trained model for humor selection. The experimental results obtained from best–worst scaling demonstrate that this scheme is able to generate jokes with moral category labels. We confirmed that jokes about the classifier categorized as Loyalty and Authority, which are regarded as good in our study, are funnier than jokes about Fairness, Purity, Harm, Cheating, and Degradation. Although we did not confirm that there was a difference in the funny level between good and bad moral jokes, the results demonstrate that moral categories of humor can affect the funny level.}
}
@article{ADAMOVIC2024100604,
title = {Streamlined approach to 2nd/3rd graders learning basic programming concepts},
journal = {Entertainment Computing},
volume = {48},
pages = {100604},
year = {2024},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2023.100604},
url = {https://www.sciencedirect.com/science/article/pii/S1875952123000599},
author = {Milan Đ. Adamović and Dragan V. Ivetić},
keywords = {Video games, Edutainment, Programming, School},
abstract = {There is a growing need to teach schoolchildren programming at an increasingly younger age. The goal of this study is to determine if it is possible to teach schoolchildren basic programming concepts in a streamlined manner. In order to present the new knowledge in a way schoolchildren could understand easily, analogies between basic programming concepts and traffic were used. A simple video game was developed with this in mind and an effort was made to avoid design pitfalls commonly found in edutainment titles. The study involved 112 schoolchildren ages 7 to 9. Test group and control group were given a pre-test, a re-test and a post-test. The re-test and the post-test respectively showed 16% and 7% score difference in favor of the test group. Focusing on questions featuring content analogous to basic programming concepts showed 36% and 20% difference in scores.}
}
@article{HE2025110716,
title = {The comprehensive safety assessment method for complex construction crane accidents based on scenario analysis – A case study of crane accidents},
journal = {Computers & Industrial Engineering},
volume = {199},
pages = {110716},
year = {2025},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2024.110716},
url = {https://www.sciencedirect.com/science/article/pii/S0360835224008386},
author = {Wei He and Zelong Lin and Wei Li and CJ Wong and Dewei Kong and W.M. Edmund Loh},
keywords = {Scenario Analysis Theory, Improved Bayesian Network, Crane Accidents, Safety Assessment, Emergency Management},
abstract = {Crane accidents pose a significant safety hazard in the infrastructure construction process, making a scientifically reliable safety assessment crucial. Addressing the limitations of traditional methods in adequately considering the complexity of crane accidents, this study proposes a safety assessment model based on Scenario Analysis Theory (SAT) and an improved Bayesian Network (BN) algorithm. The model constructs accident scenario elements, utilizes improved BN to model influencing factors and their interactions, and designs safety assessment functions for a quantitative analysis of crane accident safety. This study demonstrates that the proposed safety assessment model more comprehensively reflects the dynamic evolution of crane accidents. It provides more accurate and interpretable assessment outcomes, significantly aiding in risk prediction and decision-making for emergency management. Key stakeholders, including site management teams, and regulatory bodies, can leverage these findings to enhance emergency management capabilities and reduce the risk of accidents in construction projects.}
}
@article{RZHETSKY20089,
title = {Seeking a New Biology through Text Mining},
journal = {Cell},
volume = {134},
number = {1},
pages = {9-13},
year = {2008},
issn = {0092-8674},
doi = {https://doi.org/10.1016/j.cell.2008.06.029},
url = {https://www.sciencedirect.com/science/article/pii/S0092867408008167},
author = {Andrey Rzhetsky and Michael Seringhaus and Mark Gerstein},
abstract = {Tens of thousands of biomedical journals exist, and the deluge of new articles in the biomedical sciences is leading to information overload. Hence, there is much interest in text mining, the use of computational tools to enhance the human ability to parse and understand complex text.}
}
@incollection{ROSENBERG2023157,
title = {Chapter 9 - Machine learning and precision medicine},
editor = {Gary A. Rosenberg},
booktitle = {Neuroinflammation in Vascular Dementia},
publisher = {Academic Press},
pages = {157-173},
year = {2023},
isbn = {978-0-12-823455-6},
doi = {https://doi.org/10.1016/B978-0-12-823455-6.00005-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128234556000055},
author = {Gary A. Rosenberg},
keywords = {Principal component analysis (PCA), exploratory factor analysis (EFA), Binswanger’s disease score (BDS), The Alzheimer Disease Neuroimaging Initiative (ADNI), hierarchical clustering analysis (HCA)},
abstract = {Clinical medicine is experiencing a massive increase in the amount of information available to the physician caring for a patient. Certain medical fields have incorporated this deluge of information into patient care while others are lagging behind. Neurology has been slow to adopt the new methods to use the large amount of information, but it is rapidly learning from other fields. Cancer diagnosis and treatment has been at the forefront of this revolution; not only have there been an extensive number of genes associated with different cancers discovered, but this information has been used to formulate treatment plans. Other fields such as radiology and dermatology are using computer-aided imaging to diagnose illness by analysis of radiographs and to automate diagnoses of skin cancers. The concepts behind the use of machine learning in diagnosis originated from early work in the field of “cybernetics,” which is a transdisciplinary approach for exploring regulatory systems – their structures, constraints, and possibilities. Norbert Wiener defined cybernetics in 1948 as “the scientific study of control and communication in the animal and the machine.” From the early work on control theory by Wiener and others has slowly evolved the modern concepts of artificial intelligence (AI) and machine learning. There are various definitions of AI or machine learning. The term is used to describe computers that perform cognitive functions that we associate with the human mind; these “thinking machines” can beat experts in chess and the Chinese game of Go. In medicine, there are capable of analyzing large amounts of data to arrive at a diagnosis through pattern recognition. Antibiotic drugs have been designed by AI in ways that were unavailable to humans, pointing to the future of molecular discovery in medicine.}
}
@article{EVANS1989499,
title = {A review and synthesis of OR/MS and creative problem solving (Parts 1 and 2)},
journal = {Omega},
volume = {17},
number = {6},
pages = {499-524},
year = {1989},
issn = {0305-0483},
doi = {https://doi.org/10.1016/0305-0483(89)90055-8},
url = {https://www.sciencedirect.com/science/article/pii/0305048389900558},
author = {JR Evans},
keywords = {creativity, problem-solving, OR/MS methodology},
abstract = {Problem solving in operations research and management science is both a science and an art. While much has been written about the science of OR/MS, relatively little has been written about the art. Art, by its very nature, is a creative discipline. This implies that creative thinking should be an important component of OR/MS methodology. A rich literature on creative thinking exists, mostly in the domains of psychology and design. Creativity has been indirectly discussed in OR/MS research and practice, but seldom as a central theme. The purpose of this paper is to review the literature on creative thinking and problem solving that has special relevance to traditional OR/MS methodology. In this part we focus on problem solving, the need for creative thinking, and fundamental concepts of creativity. In Part 2 we synthesize the OR/MS literature that relates to creative thinking, and provide a framework for integrating structured creative thinking processes with OR/MS methodology. Finally, we discuss implications for education, research, and practice.}
}
@article{NESI2024184,
title = {Exploring enactivism: A scoping review of its key concepts and theorical approach},
journal = {Advances in Integrative Medicine},
volume = {11},
number = {4},
pages = {184-190},
year = {2024},
issn = {2212-9588},
doi = {https://doi.org/10.1016/j.aimed.2024.08.007},
url = {https://www.sciencedirect.com/science/article/pii/S221295882400082X},
author = {Jacson Nesi and Roberta Lemos {dos Santos} and Michele Benites},
keywords = {Enactivism, Enaction, body, Anatomy, Embodiment, Scoping Review},
abstract = {Enactivism is a theoretical perspective in the fields of philosophy of mind and cognition that emphasizes the active role of the organism in constructing and giving meaning to the world around it. It highlights that the mind is not merely a passive receiver of information from the environment, but rather an active participant in the creation of meaning and experience. The idea for this article arises from the uncertainty surrounding the distinction of practice, principles, and osteopathic concepts, which have been raised by various regarding osteopathic principles: the anachronism of their distinction, whether the role of these principles could still be accepted as a guide for osteopathy in the contemporary world, whether the biopsychosocial model could be the basis for a proposal to redefine them and even whether the use of these principles could do more harm than good. Objectives: Facilitate access to essential definitions and concepts related to enactivism, and make the understanding of these elements more accessible, as they play a crucial role in the reconceptualization of osteopathy. Materials and methods: The work was elaborated as a scoping review, using the PRISMA-P 2020 Checklist.}
}
@article{PHILLIPS2009597,
title = {Advances in evolution and genetics: Implications for technology strategy},
journal = {Technological Forecasting and Social Change},
volume = {76},
number = {5},
pages = {597-607},
year = {2009},
note = {Two Special Sections: Advances in Evolution and Genetics: Implications for Technology Strategy The Digital Economy in Asia},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2008.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S0040162508001522},
author = {Fred Phillips and Yu-Shan Su},
keywords = {Evolution, Selection, Genetics, Technology strategy, Technology forecasting},
abstract = {Genetic and evolutionary principles are of great importance to technology strategists, both directly (as in the forecasting of genetic engineering technologies) and as a source of metaphor and perspective on socio-technical change. Recent rapid progress in the molecular sciences have revealed new genetic mechanisms of evolution, and introduced new controversies of interpretation. How do these recent developments affect technology forecasting and our view of technological evolution? This paper provides a quick primer for TFSC readers on several new developments in evolution and genetics, comments upon a number of common misconceptions and pitfalls in evolutionary thinking, and critically describes some controversies and open questions, introducing key readings and sources. It relates genetic and evolutionary knowledge, analogies and metaphors to areas of interest to researchers in technology forecasting and assessment, noting possible future directions. The paper concludes with an overview of the other papers in this special section.}
}
@article{NOORMAN2017677,
title = {Biochemical engineering’s grand adventure},
journal = {Chemical Engineering Science},
volume = {170},
pages = {677-693},
year = {2017},
note = {13th International Conference on Gas-Liquid and Gas-Liquid-Solid Reactor Engineering},
issn = {0009-2509},
doi = {https://doi.org/10.1016/j.ces.2016.12.065},
url = {https://www.sciencedirect.com/science/article/pii/S0009250916307266},
author = {Henk J. Noorman and Joseph J. Heijnen},
keywords = {Lifeline modeling, Bioprocess design, Scale-down, Bio-economy, Renewable feedstocks, Bio-products},
abstract = {Building on the recent revolution in molecular biology, enabling a wealth of bio-product innovations made from renewable feedstocks, the biotechnology field is in a transition phase to bring the products to the market. This requires a shift from natural sciences to engineering sciences with first conception of new, efficient large-scale bioprocess designs, followed by implementation of the most promising design in practice. Inspired by a former publication by O. Levenspiel in 1988, an outline is presented of main challenges that the field of biochemical engineering is currently facing, in a context of major global sustainability trends. The critical stage is the conceptual design phase. Issues can best be addressed and overcome by adopting an attitude where one begins with the end in mind. This applies to three principal components: 1. the bioprocess value chain, where the product specifications and downstream purification schemes should be set before defining the upstream sections, 2. the time perspective, starting in the future assuming that feedstock and product-market combinations are already in place and then going back to today, and 3. the scale of operation, where the industrial operation sets the boundaries for all labscale research and development, and not vice versa. In this way, and ideal process is defined taking constraints from anticipated manufacturing into account. For illustration, three bioprocess design examples are provided, that show how new, ideal conceptual designs can be generated. These also make clear that the engineering sciences are undergoing a revolution, where bio-based approaches replace fossil routes, and gross simplification is replaced by highly detailed computational methods. For biochemical processes, lifeline modeling frameworks are highlighted as powerful means to reconcile the competing needs for high speed and high quality in biochemical engineering, both in the design and implementation stages, thereby enabling significant growth of the bio-based economy.}
}
@article{SUKHOBOKOV2024101279,
title = {A universal knowledge model and cognitive architectures for prototyping AGI},
journal = {Cognitive Systems Research},
volume = {88},
pages = {101279},
year = {2024},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2024.101279},
url = {https://www.sciencedirect.com/science/article/pii/S1389041724000731},
author = {Artem Sukhobokov and Evgeny Belousov and Danila Gromozdov and Anna Zenger and Ilya Popov},
keywords = {Cognitive architecture, AGI, Metagraph, Archigraph, Universal knowledge model, Machine consciousness, Machine subconsciousness, Machine reflection, Machine worldview},
abstract = {The article identified 56 cognitive architectures for creating general artificial intelligence (AGI) and proposed a set of interrelated functional blocks that an agent approaching AGI in its capabilities should possess. Since the required set of blocks is not found in any of the existing architectures, the article proposes a reference cognitive architecture for intelligent systems approaching AGI in their capabilities. As one of the key solutions within the framework of the architecture, a universal method of knowledge representation is proposed, which allows combining various non-formalized, partially and fully formalized methods of knowledge representation in a single knowledge base, such as texts in natural languages, images, audio and video recordings, graphs, algorithms, databases, neural networks, knowledge graphs, ontologies, frames, essence-property-relation models, production systems, predicate calculus models, conceptual models, and others. To combine and structure various fragments of knowledge, archigraph model are used, constructed as a development of annotated metagraphs. As other components, the reference cognitive architecture being developed includes following modules: machine consciousness, machine subconsciousness, interaction with the external environment, a goal management, an emotional control, social interaction, reflection, ethics, worldview, learning, monitoring, statement problems, solving problems, self-organization and meta learning. Based on the composition of the proposed reference architecture modules, existing cognitive architectures containing the following modules were analyzed: machine consciousness, machine subconsciousness, reflection, worldview.}
}
@article{PATON1997245,
title = {The organisations of hereditary information},
journal = {Biosystems},
volume = {40},
number = {3},
pages = {245-255},
year = {1997},
issn = {0303-2647},
doi = {https://doi.org/10.1016/S0303-2647(96)01652-8},
url = {https://www.sciencedirect.com/science/article/pii/S0303264796016528},
author = {Ray Paton},
keywords = {Gene, Syntax/semantics, Hierarchy, Epigenetic system, Talkback},
abstract = {The meaning of hereditary information is not simple. It includes not only what a system receives and transmits but particularly what it makes. The syntactic basis to hereditary information is also not straightforward. For example, is DNA instructions, or data, or both? The answer to this question requires an appreciation of the meaning of the information yet there are a number of possible semantic systems for describing hereditary information including proteins and development. The descriptive boundaries of hereditary information are examined by locating some general organising themes including hierarchy, ecology, regulation, epigenetic systems and talkback. Though metaphors have limits in terms of their explanatory power, a number have influenced the development of biological thinking and biosystems have variously been represented as chemical laboratories, computers, electromechanical machines and societies. In this article a further metaphor is discussed, that of life-as-a-play or dance in which the trio of script (genome), cast (metabolism) and stage (cellular structure) co-exist and pre-exist the phenotypic life history which inherits them. A fuller examination of this trio provides an important perspective on the study of the organisations of information processing in hereditary systems.}
}
@incollection{HERLIHY20211,
title = {Chapter 1 - Introduction},
editor = {Maurice Herlihy and Nir Shavit and Victor Luchangco and Michael Spear},
booktitle = {The Art of Multiprocessor Programming (Second Edition)},
publisher = {Morgan Kaufmann},
edition = {Second Edition},
address = {Boston},
pages = {1-18},
year = {2021},
isbn = {978-0-12-415950-1},
doi = {https://doi.org/10.1016/B978-0-12-415950-1.00009-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780124159501000094},
author = {Maurice Herlihy and Nir Shavit and Victor Luchangco and Michael Spear},
keywords = {parallelism, concurrent programming, shared-memory multiprocessors, safety, liveness, mutual exclusion, coordination protocol, producer—consumer problem, readers–writers problem, deadlock-freedom, starvation-freedom, Amdahl's law},
abstract = {This chapter introduces and motivates the study of shared-memory multiprocessor programming, or concurrent programming. It describes the overall plan of the book, and then presents some basic concepts of concurrent computation, and presents some of the fundamental problems—mutual exclusion, the producer–consumer problem, and the readers–writers problem—and some simple approaches to solve these problems. It ends with a brief discussion of Amdahl's law.}
}
@article{WILLMANN201616,
title = {Robotic timber construction — Expanding additive fabrication to new dimensions},
journal = {Automation in Construction},
volume = {61},
pages = {16-23},
year = {2016},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2015.09.011},
url = {https://www.sciencedirect.com/science/article/pii/S0926580515002046},
author = {Jan Willmann and Michael Knauss and Tobias Bonwetsch and Anna Aleksandra Apolinarska and Fabio Gramazio and Matthias Kohler},
keywords = {Non-standard timber structures, Automated assembly, Computational design, Industrial full scale implementation, Additive digital fabrication, Robotic Timber Construction},
abstract = {This paper presents a novel approach to non-standard timber assembly – Robotic Timber Construction (RTC) – where robotic fabrication is used to expand additive digital fabrication techniques towards industrial full scale dimensions. Featuring robotic systems that grasp, manipulate, and finally position building components according to a precise digital blueprint, RTC combines robotic assembly procedures and advanced digital design of non-standard timber structures. The resulting architectural morphologies allow for a convergence of aesthetic and functional concerns, enabling structural optimisation through the locally differentiated aggregation of material. Initiated by the group of Gramazio Kohler Research at ETH Zurich, this approach offers a new perspective on automated timber construction, where the focus is shifted from the processing of single parts towards the assembly of generic members in space. As such, RTC promotes unique advantages over conventional approaches to timber construction, such as, for example, CNC joinery and cutting: through the automated placement of material exactly where it is needed, RTC combines additive and largely waste-free construction with economic assembly procedures, it does not require additional external building reference, and it offers digital control across the entire building process, even when the design and assembly information are highly complex. This paper considers 1) research parameters for the individual components of RTC (such as computational design processes, construction methods and fabrication strategies), and 2) the architectural implications of integrating these components into a systemic, unifying process at the earliest stages of design. Overall, RTC leads to profound changes in the design, performance and expressive language of architecture and thus fosters the creation of architecture that profoundly reinvents its constructive repertoire.}
}
@article{RAMIREZPEDRAZA2021122,
title = {Decision-making bioinspired model for target definition and “satisfactor” selection for physiological needs},
journal = {Cognitive Systems Research},
volume = {66},
pages = {122-133},
year = {2021},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2020.10.020},
url = {https://www.sciencedirect.com/science/article/pii/S1389041720300942},
author = {Raymundo Ramirez-Pedraza and Felix Ramos},
keywords = {Decision-making, Brain model, Satisfactor selection, Physiological need, Goal-driven},
abstract = {Every person, from an early age, has to make decisions to resolve situations that arise in life. In general, different people make different decisions in the same situation, since decision-making takes into account different factors such as age, emotional state, experience, among others. We can make decisions about situations that we classify as: more important than others, routine, unexpected, or trivial. However, making the correct decision(s) in a timely manner for these situations is one of the most complex and delicate challenges that human beings face. This is due to the arduous mental process required to be carried out. Providing such behavior to a virtual entity is possible through the use of Cognitive Architectures (CAs). CAs are an approach for modeling human intelligence and behavior. This paper presents an functional bioinspired computational decision-making model to satisfy the physiological needs of hunger and thirst. Our proposal considers as black boxes other cognitive functions that are part of a general CA (named Cuäyöllötl or brain in Nahuatl). In the proposed case study, it is proved that the decision-making process plays an essential role in determining the objective and selecting the object that satisfies the established need.}
}
@article{RUBIN2023104955,
title = {Cartography of the multiple formal systems of molecular autopoiesis: from the biology of cognition and enaction to anticipation and active inference},
journal = {Biosystems},
volume = {230},
pages = {104955},
year = {2023},
issn = {0303-2647},
doi = {https://doi.org/10.1016/j.biosystems.2023.104955},
url = {https://www.sciencedirect.com/science/article/pii/S0303264723001302},
author = {Sergio Rubin},
keywords = {Self-fabrication, Operational closure, Closure to efficient causation, Calculus of self-reference, Non-algorithmic, Enaction, Final cause},
abstract = {A rich literature has grown up over the years that bears with autopoiesis, which tends to assume that it is a model, a theory, a principle, a definition of life, a property, refers to self-organization or even to hastily conclude that it is hylomorphic, hylozoist, in need of reformulation or to be overcome, making its status even more unclear. Maturana insists that autopoiesis is none of these and rather it is the causal organization of living systems as natural systems (NS) such that when it stops, they die. He calls this molecular autopoiesis (MA), which comprises two domains of existence: that of the self-producing organization (self-fabrication) and that of the structural coupling/enaction (cognition). Like all-NS in the universe, MA is amenable to be defined in theoretical terms, i.e. encoded in mathematical models and/or formal systems (FS). Framing the multiple formal systems of autopoiesis (FSA) into the Rosen's modeling relation (a process of bringing into equivalence the causality of NS and the inferential rules of FS), allows a classification of FSA into analytical categories, most importantly Turing machine (algorithmic) vs non-Turing machine (non-algorithmic) based, and FSA with a purely reactive mathematical image as cybernetic systems, i.e. feedbacks based, or conversely, as anticipatory systems making active inferences. It is thus the intent of the present work to advance the precision with which different FS may be observed to comply (preserve correspondence) with MA in its worldly state as a NS. The modeling relation between MA and the range of FS proposed as potentially illuminating their processes forecloses the applicability of Turing-based algorithmic computational models. This outcome indicates that MA, as modelled through Varela's calculus of self-reference or more especially through Rosen's (M,R)-system, is essentially anticipatory without violating structural determinism nor causality whatsoever, hence enaction may involve it. This quality may capture a fundamentally different mode of being in living systems as opposed to mechanical-computational systems. Implications in different fields of biology from the origin of life to planetary biology as well as in cognitive science and artificial intelligence are of interest.}
}
@article{HE2021117140,
title = {Understanding chemical short-range ordering/demixing coupled with lattice distortion in solid solution high entropy alloys},
journal = {Acta Materialia},
volume = {216},
pages = {117140},
year = {2021},
issn = {1359-6454},
doi = {https://doi.org/10.1016/j.actamat.2021.117140},
url = {https://www.sciencedirect.com/science/article/pii/S1359645421005206},
author = {Q.F. He and P.H. Tang and H.A. Chen and S. Lan and J.G. Wang and J.H. Luan and M. Du and Y. Liu and C.T. Liu and C.W. Pao and Y. Yang},
keywords = {Chemical Short Rang Order, High entropy alloy, Solid solution, Lattice distortion},
abstract = {Chemical short-range ordering (CSRO) or demixing in solid solution high entropy alloys (HEAs) is a fundamental issue yet to be fully understood. In this work, we first developed a generalized quasi-chemical solid solution model that enables quantitative computation of the local chemical ordering or demixing in solid solution HEAs. After that, we performed synchrotron diffraction experiments, extensive Reverse Monte Carlo (RMC) simulations, and first principles calculations on the CoCrFeNi model alloy to study the development of local chemical environments after long time thermal annealing. The outcome of the combined research demonstrates that the development of local chemical ordering or demixing in CoCrFeNi is not only affected by the heat of mixing between dislike atoms but also coupled with local lattice distortion.}
}
@article{TALAAT2021164,
title = {The validity of an artificial intelligence application for assessment of orthodontic treatment need from clinical images},
journal = {Seminars in Orthodontics},
volume = {27},
number = {2},
pages = {164-171},
year = {2021},
note = {Artificial Intelligence applications in Orthodontics -An update},
issn = {1073-8746},
doi = {https://doi.org/10.1053/j.sodo.2021.05.012},
url = {https://www.sciencedirect.com/science/article/pii/S1073874621000359},
author = {Sameh Talaat and Ahmed Kaboudan and Wael Talaat and Budi Kusnoto and Flavio Sanchez and Mohammed H. Elnagar and Christoph Bourauel and Ahmed Ghoneima},
abstract = {Aim: To assess the validity of a Convolutional Neural Network (CNN) digital model to detect and localize orthodontic malocclusions from intraoral clinical images. Materials and methods: The sample of this study consisted of the intraoral images of 700 Subjects. All images were intraoral clinical images, in one of the following views: Left Occlusion, Right Occlusion, Front Occlusion, Upper Occlusal, and Lower Occlusal. The following malocclusion conditions were localized: crowding, spacing, increased overjet, cross bite, open bite, deep bite. The images annotations were repeated by the same investigator (S.T) with a one week interval (ICC ≥ 0.9). The CNN model used for this research study was the “You Only Look Once” model. This model can detect and localize multiple objects or multiple instances of the same object in each image. It is a fully convolutional deep neural network; 24 convolutional layers followed by 2 fully connected layers. This model was implemented using the TensorFlow framework freely available from Google. Results: The created CNN model was able to detect and localize the malocclusions with an accuracy of 99.99%, precision of 99.79%, and a recall of 100%. Conclusions: The use of computational deep convolutional neural networks to identify and localize orthodontic problems from clinical images proved valid. The built AI engine accurately detected and localized malocclusion from different views of intra-oral clinical images.}
}
@article{CARLI2012119,
title = {Efficient algorithms for large scale linear system identification using stable spline estimators},
journal = {IFAC Proceedings Volumes},
volume = {45},
number = {16},
pages = {119-124},
year = {2012},
note = {16th IFAC Symposium on System Identification},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20120711-3-BE-2027.00394},
url = {https://www.sciencedirect.com/science/article/pii/S1474667015379386},
author = {Francesca P. Carli and Alessandro Chiuso and Gianluigi Pillonetto},
keywords = {Parametric prediction error methods, output error models, model complexity, marginal likelihood, kernel eigenfunctions},
abstract = {A new nonparametric approach for system identification has been recently proposed where, in place of postulating parametric classes of impulse responses, the estimation process starts from an infinite-dimensional space. In particular, the impulse response is seen as the realization of a zero-mean Gaussian process. Its covariance, the so called stable spline kernel, encodes information on system stability and depends on few hyperparameters estimated from data via marginal likelihood optimization. This approach has been proved to compare much favorably with classical parametric methods but, in data rich situations, a possible drawback may be represented by its computational complexity which scales with the cube of the number of available samples. In this work we design a new computational strategy which may reduce significantly the computational load required by the stable spline estimator, thus extending its practical applicability also to large-scale scenarios.}
}
@article{LOW2020e03083,
title = {Induction approach via P-Graph to rank clean technologies},
journal = {Heliyon},
volume = {6},
number = {1},
pages = {e03083},
year = {2020},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2019.e03083},
url = {https://www.sciencedirect.com/science/article/pii/S2405844019367428},
author = {C.X. Low and W.Y. Ng and Z.A. Putra and K.B. Aviso and M.A.B. Promentilla and R.R. Tan},
keywords = {Chemical engineering, Optimal selection, Simple additive weighting, Clean technologies, Induction, Decision analysis, P-Graph},
abstract = {Identification of appropriate clean technologies for industrial implementation requires systematic evaluation based on a set of criteria that normally reflect economic, technical, environmental and other aspects. Such multiple attribute decision-making (MADM) problems involve rating a finite set of alternatives with respect to multiple potentially conflicting criteria. Conventional MADM approaches often involve explicit trade-offs in between criteria based on the expert's or decision maker's priorities. In practice, many experts arrive at decisions based on their tacit knowledge. This paper presents a new induction approach, wherein the implicit preference rules that estimate the expert's thinking pathways can be induced. P-graph framework is applied to the induction approach as it adds the advantage of being able to determine both optimal and near-optimal solutions that best approximate the decision structure of an expert. The method elicits the knowledge of experts from their ranking of a small set of sample alternatives. Then, the information is processed to induce implicit rules which are subsequently used to rank new alternatives. Hence, the expert's preferences are approximated by the new rankings. The proposed induction approach is demonstrated in the case study on the ranking of Negative Emission Technologies (NETs) viability for industry implementation.}
}
@incollection{ALEKSANDER200599,
title = {Machine consciousness},
editor = {Steven Laureys},
series = {Progress in Brain Research},
publisher = {Elsevier},
volume = {150},
pages = {99-108},
year = {2005},
booktitle = {The Boundaries of Consciousness: Neurobiology and Neuropathology},
issn = {0079-6123},
doi = {https://doi.org/10.1016/S0079-6123(05)50008-6},
url = {https://www.sciencedirect.com/science/article/pii/S0079612305500086},
author = {Igor Aleksander},
abstract = {The work from several laboratories on the modeling of consciousness is reviewed. This ranges, on one hand, from purely functional models where behavior is important and leads to an attribution of consciousness to, on the other hand, material work closely derived from the information about the anatomy of the brain. At the functional end of the spectrum, applications are described specifically directed at a job-finding problem, where the person being served should not discern between being served by a conscious human or a machine. This employs an implementation of global workspace theories. At the material end, attempts at modeling attentional brain mechanisms, and basic biochemical processes in children are discussed. There are also general prescriptions for functional schemas that facilitate discussions for the presence of consciousness in computational systems and axiomatic structures that define necessary architectural features without which it would be difficult to represent sensations. Another distinction between these two approaches is whether one attempts to model phenomenology (material end) or not (functional end). The former is sometimes called “synthetic phenomenology.” The upshot of this chapter is that studying consciousness through the design of machines is likely to have two major outcomes. The first is to provide a wide-ranging computational language to express the concept of consciousness. The second is to suggest a wide-ranging set of computational methods for building competent machinery that benefits from the flexibility of conscious representations.}
}
@incollection{GRANGER1986137,
title = {The Computation Of Contingency In Classical Conditioning},
editor = {Gordon H. Bower},
series = {Psychology of Learning and Motivation},
publisher = {Academic Press},
volume = {20},
pages = {137-192},
year = {1986},
issn = {0079-7421},
doi = {https://doi.org/10.1016/S0079-7421(08)60018-3},
url = {https://www.sciencedirect.com/science/article/pii/S0079742108600183},
author = {Richard H. Granger and Jeffrey C. Schlimmer},
abstract = {Publisher Summary
This chapter discusses a unified framework, which encompasses the computations, algorithms, and neurobiological implementations underlying classical conditioning. It presents an extensive mathematical analysis of the constraints on classical conditioning—that is, the precise contingency conditions under which mammals may and may not learn a particular association between two events in a classical conditioning situation. In classical conditioning, an unconditional stimulus (US)—that is, a cue, which is inherently biologically salient to an animal (such as an electric shock), is repeatedly paired with a conditional stimulus (CS), a cue that initially has no special significance to the animal over repeated trials, the animal can learn that the CS is predictive of or associated with the US. This phenomenon of associative learning is subject to laws and constraints: An association is learned to some extent in some conditions and to a lesser extent in others.}
}
@article{ZHANG2025127717,
title = {CCMA: A framework for cascading cooperative multi-agent in autonomous driving merging using Large Language Models},
journal = {Expert Systems with Applications},
volume = {282},
pages = {127717},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.127717},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425013399},
author = {Miao Zhang and Zhenlong Fang and Tianyi Wang and Shuai Lu and Xueqian Wang and Tianyu Shi},
keywords = {Large Language Model, Autonomous driving, Reinforcement Learning, In-context learning, Multi-agent system, Cooperative merging},
abstract = {Traditional Reinforcement Learning (RL) suffers from challenges in replicating human-like behaviors, generalizing effectively in multi-agent scenarios, and overcoming inherent interpretability issues. These tasks become even more difficult when they require a deep understanding of the environment, coordination of agents’ intentions and driving styles across various scenarios, and the overall optimization of safety, efficiency, and comfort in dynamic environments. Recently, Large Language Model (LLM) enhanced methods have shown promise in improving generalization and interoperability. However, these approaches primarily focus on single-agent scenarios and often neglect the necessary coordination among multiple road users. Therefore, in this paper, we introduce the Cascading Cooperative Multi-agent (CCMA) framework, designed to address these challenges by enhancing human-like behaviors and fostering multi-level cooperation across diverse multi-agent driving tasks, ultimately improving both micro and macro-level performance in complex driving environments. Specifically, the CCMA framework integrates RL for individual interactions, a fine-tuned LLM for regional cooperation, a reward function for global optimization, and the Retrieval-augmented Generation mechanism to dynamically optimize decision-making across complex driving scenarios. Our experiments demonstrate that our CCMA method not only enhances human-like behaviors and interpretability, but also outperforms other state-of-the-art RL methods in multi-agent environments. These findings highlight the significant impact of cascading coordinated communication and dynamic functional alignment in advanced, human-like multi-agent autonomous driving environments. Our project page is https://miaorain.github.io/rainrun.github.io/.}
}
@incollection{YANG202333,
title = {Chapter 2 - Machine learning for solid mechanics},
editor = {Yuebing Zheng and Zilong Wu},
booktitle = {Intelligent Nanotechnology},
publisher = {Elsevier},
pages = {33-45},
year = {2023},
series = {Materials Today},
isbn = {978-0-323-85796-3},
doi = {https://doi.org/10.1016/B978-0-323-85796-3.00002-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780323857963000020},
author = {Charles Yang and Zhizhou Zhang and Grace X. Gu},
keywords = {Solid mechanics, Inverse design, Physics-informed deep learning, Graph neural networks},
abstract = {Solid mechanics is an important field responsible for the robust designs of humanity's greatest engineering accomplishments, from skyscrapers to airplanes to the space shuttle. A burst of new manufacturing techniques and novel next-generation materials is ushering in a new age of engineering revolving around sustainable development. In this chapter, we outline how artificial intelligence (AI) can help scientists and engineers manage the increasing complexity and computational requirements in solid mechanics fields. Two common problem-solving frameworks, forward and inverse design, as well as two promising new AI-based approaches, physics-informed deep learning and graph neural networks, are covered.}
}
@article{VIEIRA2020106268,
title = {Symmetry exploitation to reduce impedance evaluations in grounding grids},
journal = {International Journal of Electrical Power & Energy Systems},
volume = {123},
pages = {106268},
year = {2020},
issn = {0142-0615},
doi = {https://doi.org/10.1016/j.ijepes.2020.106268},
url = {https://www.sciencedirect.com/science/article/pii/S0142061519342188},
author = {Pedro H.N. Vieira and Rodolfo A.R. Moura and Marco Aurélio O. Schroeder and Antonio C.S. Lima},
keywords = {Electromagnetic analysis, Frequency response, Grounding, Method of moments, Numerical methods, Symmetry},
abstract = {One main concern on wideband evaluation of grounding systems is the high computational burden related to the determination of the impedance matrices. Traditionally, one has to divide any given conductor in a large number of segments which leads to a rather time consuming procedure. However, there are a number of geometrical symmetries that if exploited can significantly reduce the overall computational time. This work aims at investigating the adequacy of using some existing symmetries to reduce computer burden in the assessment of a wideband grounding system in models based on the Method of Moments. An algorithmic approach is proposed to extend the symmetry exploitation to arbitrarily oriented uniform rectangular grounding systems. Several topologies are used to assess the performance of the proposed approach. According to results, the proposed methodology can be more than 12 times faster than the traditional approach without loss of accuracy because it is not a numerical approximation.}
}
@article{FARISCO2024106714,
title = {Is artificial consciousness achievable? Lessons from the human brain},
journal = {Neural Networks},
volume = {180},
pages = {106714},
year = {2024},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2024.106714},
url = {https://www.sciencedirect.com/science/article/pii/S0893608024006385},
author = {Michele Farisco and Kathinka Evers and Jean-Pierre Changeux},
keywords = {Brain, Consciousness, Artificial intelligence, Neuromorphic computing, Robotics, Cognition, Neuroscience},
abstract = {We here analyse the question of developing artificial consciousness from an evolutionary perspective, taking the evolution of the human brain and its relation with consciousness as a reference model or as a benchmark. This kind of analysis reveals several structural and functional features of the human brain that appear to be key for reaching human-like complex conscious experience and that current research on Artificial Intelligence (AI) should take into account in its attempt to develop systems capable of human-like conscious processing. We argue that, even if AI is limited in its ability to emulate human consciousness for both intrinsic (i.e., structural and architectural) and extrinsic (i.e., related to the current stage of scientific and technological knowledge) reasons, taking inspiration from those characteristics of the brain that make human-like conscious processing possible and/or modulate it, is a potentially promising strategy towards developing conscious AI. Also, it cannot be theoretically excluded that AI research can develop partial or potentially alternative forms of consciousness that are qualitatively different from the human form, and that may be either more or less sophisticated depending on the perspectives. Therefore, we recommend neuroscience-inspired caution in talking about artificial consciousness: since the use of the same word “consciousness” for humans and AI becomes ambiguous and potentially misleading, we propose to clearly specify which level and/or type of consciousness AI research aims to develop, as well as what would be common versus differ in AI conscious processing compared to human conscious experience.}
}
@incollection{GINSBURGH2006947,
title = {Chapter 27 The Computation of Prices Indices},
editor = {Victor A. Ginsburg and David Throsby},
series = {Handbook of the Economics of Art and Culture},
publisher = {Elsevier},
volume = {1},
pages = {947-979},
year = {2006},
issn = {1574-0676},
doi = {https://doi.org/10.1016/S1574-0676(06)01027-1},
url = {https://www.sciencedirect.com/science/article/pii/S1574067606010271},
author = {Victor Ginsburgh and Jianping Mei and Michael Moses},
keywords = {prices indices, repeat sales, hedonic pricing, auctions},
abstract = {While there are no significant investment characteristics that inhibit art from being considered as an asset, a major hurdle has long been the lack of a systematic measure of its financial performance. Due to its heterogeneity (each piece is different) and its infrequency of trading (the exact same piece does not come to the market very often), the determination of changes in market value is difficult to ascertain. Two estimation methods are commonly used to construct indices. Repeat-sales regression (RSR) uses prices of individual objects traded at two distinct moments in time. If the characteristics of an object do not change (which is usually so for collectibles), the heterogeneity issue is bypassed. The basic idea of the hedonic regression (HR) method is to regress prices on various attributes of objects (dimensions, artist, subject matter, etc.) and to use the residuals of the regression which can be considered as “characteristic-free prices” to compute the price index. The chapter deals with the basics of hedonic and repeat-sales estimators, and tries to interpret in economic terms what both are trying to achieve. It also goes into some more technical details which may be useful for researchers who want to construct such indices, and gives some guidelines on how to go about collecting data, and the choice between RSR and HR that this induces. Both methods are compared using simulated returns, pointing to which method should be used given the data at hand.}
}
@article{PRATO201888,
title = {Considering built environment and spatial correlation in modeling pedestrian injury severity},
journal = {Traffic Injury Prevention},
volume = {19},
number = {1},
pages = {88-93},
year = {2018},
issn = {1538-9588},
doi = {https://doi.org/10.1080/15389588.2017.1329535},
url = {https://www.sciencedirect.com/science/article/pii/S1538958822003630},
author = {Carlo G. Prato and Sigal Kaplan and Alexandre Patrier and Thomas K. Rasmussen},
keywords = {Pedestrian crashes, injury severity models, built environment, spatial correlation},
abstract = {ABSTRACT
Objective: This study looks at mitigating and aggravating factors that are associated with the injury severity of pedestrians when they have crashes with another road user and overcomes existing limitations in the literature by focusing attention on the built environment and considering spatial correlation across crashes. Method: Reports for 6,539 pedestrian crashes occurred in Denmark between 2006 and 2015 were merged with geographic information system resources containing detailed information about the built environment and exposure at the crash locations. A linearized spatial logit model estimated the probability of pedestrians sustaining a severe or fatal injury conditional on the occurrence of a crash with another road user. Results: This study confirms previous findings about older pedestrians and intoxicated pedestrians being the most vulnerable road users and crashes with heavy vehicles and in roads with higher speed limits being related to the most severe outcomes. This study provides novel perspectives by showing positive spatial correlations of crashes with the same severity outcomes and emphasizing the role of the built environment in the proximity of the crash. Conclusions: This study emphasizes the need for thinking about traffic calming measures, illumination solutions, road maintenance programs, and speed limit reductions. Moreover, this study emphasizes the role of the built environment, because shopping areas, residential areas, and walking traffic density are positively related to a reduction in pedestrian injury severity. Often, these areas have in common a larger pedestrian mass that is more likely to make other road users more aware and attentive, whereas the same does not seem to apply to areas with lower pedestrian density.}
}
@article{LOU2023102236,
title = {A function-behavior mapping approach for product conceptual design inspired by memory mechanism},
journal = {Advanced Engineering Informatics},
volume = {58},
pages = {102236},
year = {2023},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2023.102236},
url = {https://www.sciencedirect.com/science/article/pii/S1474034623003646},
author = {Shanhe Lou and Yixiong Feng and Yicong Gao and Hao Zheng and Tao Peng and Jianrong Tan},
keywords = {Conceptual design, Situated function-behavior-structure, Memory mechanism, Reinforcement learning},
abstract = {Conceptual design is a pivotal stage for new product development that relies more on designers to solve open-ended and ill-defined problems. Situated function-behavior-structure ontology is an acknowledged method to facilitate conceptual design in a goal-oriented way. However, it depends on the subjective cognition abilities of designers, which are influenced by limited memory and reasoning capacities. Developing computer-aided methods grounded in this ontology holds significant promise in enhancing designers' cognitive abilities. This study delves into the function-behavior (F-B) mapping process. It explores the effect of working memory and long-term memory on design cognition and introduces a memory-inspired reinforcement learning framework for F-B mapping. The Markov decision process is then adopted to formalize F-B mapping while motivation-driven Q learning is employed by the design agent to learn knowledge from historical design cases. The learned state-action value matrix can be applied to guide the designer in selecting feasible behaviors for the specific function requirement. The proposed approach empowers design agents with self-learning and self-evolving capacities. A case study on the F-B mapping of a traction system is conducted to illustrate the feasibility and practicability of the proposed approach.}
}
@article{SRINIVAS199799,
title = {Strategic decision-making processes: network-based representation and stochastic simulation},
journal = {Decision Support Systems},
volume = {21},
number = {2},
pages = {99-110},
year = {1997},
note = {Special Issue: Expertise and Modeling Expert Decision Making},
issn = {0167-9236},
doi = {https://doi.org/10.1016/S0167-9236(97)00023-7},
url = {https://www.sciencedirect.com/science/article/pii/S0167923697000237},
author = {V. Srinivas and B. Shekar},
keywords = {Qualitative probabilistic networks, Stochastic simulation, Cognitive maps, Strategic thinking, Decision-making process, Network-based representation},
abstract = {Representation of decision-making in organizations is an intricate process. Qualitative Probabilistic Network (QPN)-based approach offers a scheme which is useful for representing processes involved in decision-making. This paper demonstrates the usefulness of QPN-based scheme with an illustrative case study. The focus of the case study is on understanding the strategic behavior of a key player in the Indian Automobile Industry. This is done by transforming Cognitive Maps developed into QPN-based formalisms and analyzing them. In addition to this, stochastic simulation experiment is performed on the QPN-based networks to generate hypothetical scenarios.}
}
@article{FENG2022127434,
title = {Parallel cooperation search algorithm and artificial intelligence method for streamflow time series forecasting},
journal = {Journal of Hydrology},
volume = {606},
pages = {127434},
year = {2022},
issn = {0022-1694},
doi = {https://doi.org/10.1016/j.jhydrol.2022.127434},
url = {https://www.sciencedirect.com/science/article/pii/S0022169422000099},
author = {Zhong-kai Feng and Peng-fei Shi and Tao Yang and Wen-jing Niu and Jian-zhong Zhou and Chun-tian Cheng},
keywords = {Hydrological time series forecasting, Artificial intelligence, Evolutionary computation, Parallel computing},
abstract = {Reliable streamflow prediction is an important productive information in the hydrology and water resources management fields. As used to forecast the nonlinear streamflow time series, the conventional artificial intelligence model may suffer from local convergence defect and fail to track the dynamic changes of the hydrological process when the model parameters and network structure are not well identified. Thus, this research develops a practical hydrological forecasting model based on parallel cooperation search algorithm (PCSA) and extreme learning machine (ELM), where the standard ELM method is chosen as the basic forecasting model, and then the PCSA method using several smaller and independent subswarms for parallel computation is used to determine satisfying input-hidden weights and hidden biases of the ELM model. The proposed model is used to forecast the nonlinear streamflow time series of several real-world hydrological stations in China. The results demonstrate that the proposed model outperforms the standard ELM model in various evaluation indicators. Thus, the key contributions of this study lie in two aspects: (1) for the first time, the parallel computing technique is developed to improve the global search ability and resources utilization efficiency of the emerging cooperation search algorithm; (2) an artificial intelligence model coupled with parallel evolutionary optimizer is proposed to improve the prediction accuracy of hydrological time series.}
}
@article{BAYER202380,
title = {The SPEAK study rationale and design: A linguistic corpus-based approach to understanding thought disorder},
journal = {Schizophrenia Research},
volume = {259},
pages = {80-87},
year = {2023},
note = {Language and Speech Analysis in Schizophrenia and Related Psychoses},
issn = {0920-9964},
doi = {https://doi.org/10.1016/j.schres.2022.12.048},
url = {https://www.sciencedirect.com/science/article/pii/S0920996422004959},
author = {J.M.M. Bayer and J. Spark and M. Krcmar and M. Formica and K. Gwyther and A. Srivastava and A. Selloni and M. Cotter and J. Hartmann and A. Polari and Z.R. Bilgrami and C. Sarac and A. Lu and Alison R. Yung and A. McGowan and P. McGorry and J.L. Shah and G.A. Cecchi and R. Mizrahi and B. Nelson and C.M. Corcoran},
keywords = {Thought disorder, Ultra/clinical high risk, Natural language processing, Psychosis, Latent semantic analysis, Part-of-speech-tagging},
abstract = {Aim
Psychotic symptoms are typically measured using clinical ratings, but more objective and sensitive metrics are needed. Hence, we will assess thought disorder using the Research Domain Criteria (RDoC) heuristic for language production, and its recommended paradigm of “linguistic corpus-based analyses of language output”. Positive thought disorder (e.g., tangentiality and derailment) can be assessed using word-embedding approaches that assess semantic coherence, whereas negative thought disorder (e.g., concreteness, poverty of speech) can be assessed using part-of-speech (POS) tagging to assess syntactic complexity. We aim to establish convergent validity of automated linguistic metrics with clinical ratings, assess normative demographic variance, determine cognitive and functional correlates, and replicate their predictive power for psychosis transition among at-risk youths.
Methods
This study will assess language production in 450 English-speaking individuals in Australia and Canada, who have recent onset psychosis, are at clinical high risk (CHR) for psychosis, or who are healthy volunteers, all well-characterized for cognition, function and symptoms. Speech will be elicited using open-ended interviews. Audio files will be transcribed and preprocessed for automated natural language processing (NLP) analyses of coherence and complexity. Data analyses include canonical correlation, multivariate linear regression with regularization, and machine-learning classification of group status and psychosis outcome.
Conclusions
This prospective study aims to characterize language disturbance across stages of psychosis using computational approaches, including psychometric properties, normative variance and clinical correlates, important for biomarker development. SPEAK will create a large archive of language data available to other investigators, a rich resource for the field.}
}
@incollection{HORRIGAN2004317,
title = {A Study in the Process of Planning, Designing and Executing a Survey Program: The BLS American Time-Use Survey},
series = {Contributions to Economic Analysis},
publisher = {Elsevier},
volume = {271},
pages = {317-350},
year = {2004},
booktitle = {The Economics of Time Use},
issn = {0573-8555},
doi = {https://doi.org/10.1016/S0573-8555(04)71012-3},
url = {https://www.sciencedirect.com/science/article/pii/S0573855504710123},
author = {Michael Horrigan and Diane Herz},
keywords = {US, time use, survey, American time-use survey},
abstract = {In this study, we describe the evolution of the American time-use survey (ATUS) from its inception as an issue of statistical policy interest in 1991 to its implementation in January 2003 as an ongoing monthly survey sponsored by the US Bureau of Labor Statistics. This 12-year process included four developmental phases. Each successive phase represented a deeper level of agency commitment and outside statistical support. The resulting reports referenced in the text reflect an evolution in our thinking on survey estimation objectives, units of measurement, universe frame and sampling plan, and data collection and coding protocols.}
}
@article{BANDARAGODA2019104424,
title = {Enabling Collaborative Numerical Modeling in Earth Sciences using Knowledge Infrastructure},
journal = {Environmental Modelling & Software},
volume = {120},
pages = {104424},
year = {2019},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2019.03.020},
url = {https://www.sciencedirect.com/science/article/pii/S1364815219301562},
author = {C. Bandaragoda and A. Castronova and E. Istanbulluoglu and R. Strauch and S.S. Nudurupati and J. Phuong and J.M. Adams and N.M. Gasparini and K. Barnhart and E.W.H. Hutton and D.E.J. Hobley and N.J. Lyons and G.E. Tucker and D.G. Tarboton and R. Idaszak and S. Wang},
keywords = {Cyberinfrastructure, Knowledge infrastructure, Reproducible modeling, Landlab, HydroShare, Earth science education},
abstract = {Knowledge infrastructure is an intellectual framework for creating, sharing, and distributing knowledge. In this paper, we use knowledge infrastructure to address common barriers to entry into numerical modeling in Earth sciences as demonstrated in three computational narratives: physical process modeling education, replicating published model results, and reusing published models to extend research. We outline six critical functional requirements: 1) workflows designed for new users; 2) community-supported collaborative web platform; 3) distributed data storage; 4) software environment; 5) personalized cloud-based high-performance computing platform; and 6) a standardized open source modeling framework. Our methods meet these functional requirements by providing three interactive computational narratives for hands-on, problem-based research using Landlab on HydroShare. Landlab is an open-source toolkit for building, coupling, and exploring two-dimensional numerical models. HydroShare is an online collaborative environment for the sharing of data and models. We describe the methods we are using to accelerate knowledge development by providing a suite of modular and interoperable process components that allows students, domain experts, collaborators, researchers, and sponsors to learn by exploring shared data and modeling resources. The system is designed to support uses on the continuum from fully-developed modeling applications to prototyping research software tools. Landlab notebooks are available for interactive computing on HydroShare at https://doi.org/10.4211/hs.fdc3a06e6ad842abacfa5b896df73a76 and for further development on Github at https://zenodo.org/badge/latestdoi/187289993.}
}
@article{LI2025103353,
title = {A multi-task engineering design intention recognition approach based on Vision Transformer and EEG data},
journal = {Advanced Engineering Informatics},
volume = {65},
pages = {103353},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103353},
url = {https://www.sciencedirect.com/science/article/pii/S1474034625002460},
author = {Mingrui Li and Zuoxu Wang and Fan Li and Jihong Liu},
keywords = {Design intention recognition, Engineering design, EEG, Vision Transformer},
abstract = {Engineering product design involves a variety of tasks and scenarios, including design modeling, design calculation, process planning, etc. When performing these design tasks, designers generate constantly shifting design intentions. Accurately recognizing these design intentions allows for a more thorough exploration of design processes from the perspective of cognition, facilitating the advancement of intelligent engineering design. Electroencephalogram (EEG) technology has emerged as an effective tool in recent years, which can provide direct insight into designers’ cognitive processes and intentions. However, the current application of EEG technology in engineering design faces difficulties in adapting to multi-task scenarios and rarely targets the design process directly. This study proposed a design intention recognition approach based on Vision Transformer (ViT) and EEG data applicable to multiple engineering design tasks. An image-like representation matrix is introduced to organize designers’ EEG data with the retention of its spatial and frequency features. Then, standard EEG data under different design intentions as well as the EEG data from real design processes is utilized to train and fine-tune a ViT-based design intention recognition model. An experiment workflow for collecting the two types of EEG data is also presented, along with detailed examples of three design tasks. The comparative experiment results and the case study demonstrates the feasibility of the proposed design intention recognition approach.}
}
@article{ASH1984412,
title = {Computations of cuspidal cohomology of congruence subgroups of SL(3, Z)},
journal = {Journal of Number Theory},
volume = {19},
number = {3},
pages = {412-436},
year = {1984},
issn = {0022-314X},
doi = {https://doi.org/10.1016/0022-314X(84)90081-7},
url = {https://www.sciencedirect.com/science/article/pii/0022314X84900817},
author = {Avner Ash and Daniel Grayson and Philip Green},
abstract = {Algorithms are presented which find a basis of the vector space of cuspidal cohomology of certain congruence subgroups of SL(3, Z) and which determine the action of the Hecke operators on this space. These algorithms were implemented on a computer. Four pairs of cuspidal classes were found with prime level less than 100. Tables are given of the eigenvalues of the first few Hecke operators on these classes.}
}
@article{ALONSO2025100509,
title = {A novel approach for job matching and skill recommendation using transformers and the O*NET database},
journal = {Big Data Research},
volume = {39},
pages = {100509},
year = {2025},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2025.100509},
url = {https://www.sciencedirect.com/science/article/pii/S2214579625000048},
author = {Rubén Alonso and Danilo Dessí and Antonello Meloni and Diego {Reforgiato Recupero}},
keywords = {Information extraction, Transformers, Online enrolling process, Natural language processing, Course recommendation},
abstract = {Today we have tons of information posted on the web every day regarding job supply and demand which has heavily affected the job market. The online enrolling process has thus become efficient for applicants as it allows them to present their resumes using the Internet and, as such, simultaneously to numerous organizations. Online systems such as Monster.com, OfferZen, and LinkedIn contain millions of job offers and resumes of potential candidates leaving to companies with the hard task to face an enormous amount of data to manage to select the most suitable applicant. The task of assessing the resumes of candidates and providing automatic recommendations on which one suits a particular position best has, therefore, become essential to speed up the hiring process. Similarly, it is important to help applicants to quickly find a job appropriate to their skills and provide recommendations about what they need to master to become eligible for certain jobs. Our approach lies in this context and proposes a new method to identify skills from candidates' resumes and match resumes with job descriptions. We employed the O*NET database entities related to different skills and abilities required by different jobs; moreover, we leveraged deep learning technologies to compute the semantic similarity between O*NET entities and part of text extracted from candidates' resumes. The ultimate goal is to identify the most suitable job for a certain resume according to the information there contained. We have defined two scenarios: i) given a resume, identify the top O*NET occupations with the highest match with the resume, ii) given a candidate's resume and a set of job descriptions, identify which one of the input jobs is the most suitable for the candidate. The evaluation that has been carried out indicates that the proposed approach outperforms the baselines in the two scenarios. Finally, we provide a use case for candidates where it is possible to recommend courses with the goal to fill certain skills and make them qualified for a certain job.}
}
@article{PIRES2024625,
title = {Selection of Naval Bases and Stations for submarines: a multimethodological approach},
journal = {Procedia Computer Science},
volume = {242},
pages = {625-632},
year = {2024},
note = {11th International Conference on Information Technology and Quantitative Management (ITQM 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.08.119},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924018386},
author = {Tullio Pires and Celio Manso {de Azevedo Junior} and Mateus Vanzetta and Marcos {dos Santos} and Carlos Francisco {Simões Gomes}},
keywords = {Submarines, Naval Base, Multicriteria, MPSI-MARA, SCA},
abstract = {With the PROSUB program, the Brazilian Navy (MB) has been renewing its feet of submarines. However, this is not a movement that is exclusively Brazilian. With the worsening of crises around the world, many countries are in the process of expanding their armed forces, and coastal nations, in particular, are paying significant attention to their submarine weapons. However, as it is not only convenient to acquire submarines but also to operate them, it is necessary to define from where they will do so. Given the above, this current work aims to present a framework with a multimethodological focus, that is, presenting a combined use of the problem structuring method (PSM) Strategic Choice Approach with the multicriteria method MPSI-MARA. As a result, the ordering of some points along the Brazilian coast, made non-specific, is presented as a suggestion for the implementation of new Submarine Bases and/or Naval Support Stations.}
}
@article{YUCEL2019352,
title = {Battling gender stereotypes: A user study of a code-learning game, “Code Combat,” with middle school children},
journal = {Computers in Human Behavior},
volume = {99},
pages = {352-365},
year = {2019},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2019.05.029},
url = {https://www.sciencedirect.com/science/article/pii/S0747563219302109},
author = {Yeliz Yücel and Kerem Rızvanoğlu},
keywords = {User experience (UX), Digital gender divide, Gender stereotypes, Stereotype threat, Games, Serious games},
abstract = {Abstract.
Gender has been consistently controlled as a variable in usability and playability tests. However, there is no consensus on whether and how gender differences should influence the design of digital environments. According to some research, digital environments may be unintentionally designed especially for males as a result of the existing gender biases which risks reproducing gender-polarized culture in a computational field. This study attempts to highlight that females are still being negatively affected by existing gender stereotypes and prescribed gender identities despite relatively equal access and use of computer technology. This qualitative study aims to provide insights about the first-time user experience in a home environment of 16 middle school children in Turkey (8 males - 8 females), aged between 11 and 14 years, with a code learning game named “Code Combat”. The analysis is supported with complementary quantitative findings. The present study investigates the participants' conceptualizations and opinions toward coding concept and this specific coding game. Further, it explores how existing gender stereotypes and gender biased expectations impact their behaviors and attitudes in the context of game experience. Our results indicated that perceived computer competence and perceived coding difficulty had important effects on the participants’ performance relatedly with their gender identity. According to our findings, there are important gender differences to be found in our 9 constructs, namely; perceived computer competence, perceived coding difficulty, identification, perceived game difficulty, perceived success, level of enjoyment, level of anxiety, the likelihood of playing it another time and the likelihood of trying new features.}
}
@article{BAUM1997195,
title = {A Bayesian approach to relevance in game playing},
journal = {Artificial Intelligence},
volume = {97},
number = {1},
pages = {195-242},
year = {1997},
note = {Relevance},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(97)00059-3},
url = {https://www.sciencedirect.com/science/article/pii/S0004370297000593},
author = {Eric B. Baum and Warren D. Smith},
keywords = {Relevance, Game tree search, Game theory, Computer game playing, Directed search, Utility guided search, Metareasoning, Computer chess, Computer Othello, Game trees, Graphical model, Decision theory, Utility, Bayesian model, Evaluation function, Rational search},
abstract = {The point of game tree search is to insulate oneself from errors in the evaluation function. The standard approach is to grow a full width tree as deep as time allows, and then value the tree as if the leaf evaluations were exact. The alpha-beta algorithm implements this with great computational efficiency. This approach has been effective in many games. Our approach is to form a Bayesian model of our uncertainty. We adopt an evaluation function that returns a probability distribution estimating the probability of various errors in valuing each position. These estimates are obtained by training from data. We thus use additional information at each leaf not available to the standard approach. We utilize this information in three ways: to evaluate which move is best after we are done expanding, to allocate additional thinking time to moves where additional time is most relevant to game outcome, and, perhaps most importantly, to expand the tree along the most relevant lines. Our measure of the relevance of expanding a given leaf provably approximates a measure of the impact of expanding the leaf on expected payoff, including the impact of the outcome of the leaf expansion on later expansion decisions. Our algorithms run (under reasonable assumptions) in time linear in the size of the final tree and hence except for a small constant factor, are as time efficient as alpha-beta. Our algorithm focuses on relevant lines, on which it can in principle grow a tree several times as deep as alpha-beta in a given amount of time. We have tested our approach on a variety of games, including Othello, Kalah, Warri, and others. Our probability independence approximations are seen to be significantly violated, but nonetheless our tree valuation scheme was found to play significantly better than minimax or the Probability Product rule when both competitors search the same tree. Our full search algorithm was found to outplay a highly ranked, directly comparable alpha-beta Othello program even when the alpha-beta program was given sizeable time odds, and also performed well against the three top Othello programs on the Internet Othello Server.}
}
@incollection{YANG2001291,
title = {Curved Beam Theories and Related Computational Aspects},
editor = {S. Valliappan and N. Khalili},
booktitle = {Computational Mechanics–New Frontiers for the New Millennium},
publisher = {Elsevier},
address = {Oxford},
pages = {291-298},
year = {2001},
isbn = {978-0-08-043981-5},
doi = {https://doi.org/10.1016/B978-0-08-043981-5.50046-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780080439815500468},
author = {Yeong-Bin Yang},
keywords = {Buckling, curved beam, curved beam element, joint equilibrium, stability, straight beam element},
abstract = {ABSTRACT
The theories of buckling for horizontal curved beams presented by Timoshenko, Vlasov, Yoo, and Yang and Kuo are first reviewed, with their key features identified. The previous argument concerning the incapability of straight beam elements to predict the buckling loads of curved beams in incorrect, due to overlook of the conditions of equilibrium for structural joints connecting non-aligned members in the deformed position, as implied by conventional finite element approaches. If such conditions are duly taken into account, then the straight beam elements derived, which are referred to as the semitangential elements, can be used as a reliable tool for predicting the buckling loads of curved beams. Moreover, by simulating a curved beam in the limit as an infinite number of infinitesimal elements, the theory for straight beams can be manipulated through use of the concept of transfer matrix to yield the ones for curved beams for the cases of uniform bending and uniform compression. It is in this sense that the theories of straight beams and curved beams are unified.}
}
@article{MWAPE2025343,
title = {Life cycle sustainability assessment of staple food processing: A double and dynamic materiality approach},
journal = {Sustainable Production and Consumption},
volume = {56},
pages = {343-363},
year = {2025},
issn = {2352-5509},
doi = {https://doi.org/10.1016/j.spc.2025.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S2352550925000764},
author = {Mwewa Chikonkolo Mwape and Aditya Parmar and Franz Roman and Naushad M. Emmambux and Yaovi Ouézou Azouma and Oliver Hensel},
keywords = {LCSA, Staple foods, Double materiality, Dynamic materiality, MEFA, Global warming potential (GWP), Python modeling, ESG},
abstract = {Globally, 70 % of people are fed through peasant food systems that are responsible for growing 50 % of the world's food calories on 30 % of the land. In the global south, particularly in Sub-Saharan Africa, small-scale farming serves as a crucial lifeline for the food and income needs of local populations. Yet, it remains underfunded and under-researched in the context of sustainable development. Even if the traditional Life Cycle Sustainability Assessment offers a holistic approach to evaluating the impacts of staple food processing across environmental, economic, and social dimensions, its inability to track dynamic materiality limits its application in evaluating future impacts. Therefore, this study aimed to provide a comprehensive Life Cycle Sustainability Assessment framework for staple food processing, using cassava to produce gari, a staple food for more than 300 million West Africans, as a case study. This framework integrates Material and Energy Flow Analysis techniques to trace resource use and emissions. The research incorporated Environmental, Social and Governance pillars; double materiality, evaluating both the direct and indirect impacts of processing activities, alongside dynamic materiality to capture evolving environmental, financial, and social factors through scenarios. Python computational modeling was used to perform these complex analyses, ensuring accuracy and adaptability. The findings highlight significant energy inefficiencies (6.67 kWh kg-1) coupled with a high Global Warming Potential (GWP) of 9.02 kgCO2eq kg-1 and production costs of $0.56 kg-1. The most significant opportunities for improvement were identified in optimizing energy consumption and transforming waste into biogas. The dynamic model revealed that integrating renewable energy sources could substantially reduce environmental impacts and increase the Net Profit Margin from 34.43 to 52.52 %, as proposed in the energy transition from woodfuel and gasoline to a Hybrid Solar and Biogas energy system. This study contributes to the growing body of literature on Life Cycle Sustainability Assessment by applying a comprehensive framework to staple food processing. The findings offer valuable insights into the environmental, social, and economic trade-offs in food processing systems, providing practical recommendations for improving sustainability throughout the food supply chain. Extended studies using these methods on other staples are highly recommended.}
}
@incollection{NI2016239,
title = {Chapter 17 - More Intelligent Models},
editor = {Daiheng Ni},
booktitle = {Traffic Flow Theory},
publisher = {Butterworth-Heinemann},
pages = {239-251},
year = {2016},
isbn = {978-0-12-804134-5},
doi = {https://doi.org/10.1016/B978-0-12-804134-5.00017-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128041345000179},
author = {Daiheng Ni},
keywords = {Car-following models, Psycho-physical model, Carsim model, Rule-based model, Neural network model},
abstract = {Along the lines of car-following models, single-regime models stand at one end and use one equation to handle all driving situations. Models can become increasingly intelligent if they include more and more equations to represent different regimes, such as start-up, speedup, free-flow, approaching, following, and stopping. Even more intelligent models can mimic the way of human thinking—for example, using rules and reasoning based on neural networks.}
}
@article{KHARE2022105028,
title = {A hybrid decision support system for automatic detection of Schizophrenia using EEG signals},
journal = {Computers in Biology and Medicine},
volume = {141},
pages = {105028},
year = {2022},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2021.105028},
url = {https://www.sciencedirect.com/science/article/pii/S0010482521008222},
author = {Smith K. Khare and Varun Bajaj},
keywords = {Schizophrenia, Electroencephalography, Optimization, Robust variational mode decomposition, Optimized extreme learning machine classifier},
abstract = {Background
Schizophrenia (SCZ) is a serious neurological condition in which people suffer with distorted perception of reality. SCZ may result in a combination of delusions, hallucinations, disordered thinking, and behavior. This causes permanent disability and hampers routine functioning. Trained neurologists use interviewing and visual inspection techniques for the detection and diagnosis of SCZ. These techniques are manual, time-consuming, subjective, and error-prone. Therefore, there is a need to develop an automatic model for SCZ classification. The aim of this study is to develop an automated SCZ classification model using electroencephalogram (EEG) signals. The EEG signals can capture the changes in neural dynamics of human cognition during SCZ.
Method
Based on the nature of the SCZ condition, the EEG signals must be examined. For accurate interpretation of EEG signals during SCZ, an automated model integrating a robust variational mode decomposition (RVMD) and an optimized extreme learning machine (OELM) classifier is developed. Traditional VMD suffers from noisy mode generation, mode duplication, under segmentation, and mode discarding. These problems are suppressed in RVMD by automating the selection of quadratic penalty factor (α) and a number of modes (L). The hyperparameters (HPM) of the OELM classifier are automatically selected to ensure maximum accuracy for each mode without overfitting or underfitting. For the selection of α and L in RVMD and HPM in the OELM classifier, a whale optimization algorithm is used. The root mean square error is minimized for RVMD and classification accuracy of each mode is maximized for the OELM classifier. The EEG signals of three conditions performing basic sensory tasks have been analyzed to detect SCZ.
Results
The Kruskal Wallis test is used to select different features extracted from the modes produced by RVMD. An OELM classifier is tested using a ten-fold cross-validation technique. An accuracy, precision, specificity, F-1 measure, sensitivity, and Cohen's Kappa of 92.93%, 93.94%, 91.06% 94.07%, 97.15%, and 85.32% are obtained.
Conclusion
The third mode's chaotic features helped to capture the significant changes that occurred during the SCZ state. The findings of the RVMD-OELM-based hybrid decision support system can help neuro-experts for the accurate identification of SCZ in real-time scenarios.}
}
@article{RICH2018110,
title = {Participatory systems approaches for urban and peri-urban agriculture planning: The role of system dynamics and spatial group model building},
journal = {Agricultural Systems},
volume = {160},
pages = {110-123},
year = {2018},
issn = {0308-521X},
doi = {https://doi.org/10.1016/j.agsy.2016.09.022},
url = {https://www.sciencedirect.com/science/article/pii/S0308521X16305959},
author = {Karl M. Rich and Magda Rich and Kanar Dizyee},
keywords = {Urban agriculture, System dynamics, Spatial group model building, Participatory processes, Planning, Christchurch},
abstract = {Urban agriculture has become an important research theme in recent years. Over the past decade, a number of different, diverse value chains have been established in the urban areas of developed and developing countries alike, with increasing convergence in their motivations related to food security and livelihoods development, particularly for poor and disadvantaged segments of society. However, for urban agriculture to be sustainable as a livelihoods and resilience strategy will require decision-support tools that allow planners and participants alike to jointly develop strategies and assess potential leverage points within urban food value chains. In this paper, we argue that system dynamics (SD) models combined with participatory approaches have important roles in bridging this gap, though these will need to be adapted to the spatial influences that exist in urban settings. We first review elements of urban agriculture and some of the policy challenges faced in this growing phenomenon. We follow this by motivating the role of SD models in the context of urban agriculture and note their potential utility in overlaying quantitative models of urban food value chains alongside their land-use characteristics, highlighting the dynamic feedbacks between intensive processes within changing urban food systems and extensive processes associated with land-use and planning. From this background, we introduce the concept of spatial group model building (SGMB), which adapts standard group model building concepts to account for both the spatial context of urban agriculture and enables a spatially sensitive, participatory approach to qualitative and quantitative model building. We provide a qualitative proof-of-concept of SGMB principles and techniques in the context of describing the setting and dynamic issues facing organic urban agriculture value chains in Christchurch, New Zealand. Our approach fills an important space between participatory GIS practices and the development of complex spatial system dynamics models, infusing systems thinking principles to participatory processes, while showing a way to enhance the future development of quantitative spatial system dynamics models more generally.}
}
@article{TSOTSOS2021305,
title = {On the control of attentional processes in vision},
journal = {Cortex},
volume = {137},
pages = {305-329},
year = {2021},
issn = {0010-9452},
doi = {https://doi.org/10.1016/j.cortex.2021.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S0010945221000150},
author = {John K. Tsotsos and Omar Abid and Iuliia Kotseruba and Markus D. Solbach},
keywords = {Vision, Attention, Control, Cognitive program, Selective tuning},
abstract = {The study of attentional processing in vision has a long and deep history. Recently, several papers have presented insightful perspectives into how the coordination of multiple attentional functions in the brain might occur. These begin with experimental observations and the authors propose structures, processes, and computations that might explain those observations. Here, we consider a perspective that past works have not, as a complementary approach to the experimentally-grounded ones. We approach the same problem as past authors but from the other end of the computational spectrum, from the problem nature, as Marr's Computational Level would prescribe. What problem must the brain solve when orchestrating attentional processes in order to successfully complete one of the myriad possible visuospatial tasks at which we as humans excel? The hope, of course, is for the approaches to eventually meet and thus form a complete theory, but this is likely not soon. We make the first steps towards this by addressing the necessity of attentional control, examining the breadth and computational difficulty of the visuospatial and attentional tasks seen in human behavior, and suggesting a sketch of how attentional control might arise in the brain. The key conclusions of this paper are that an executive controller is necessary for human attentional function in vision, and that there is a 'first principles' computational approach to its understanding that is complementary to the previous approaches that focus on modelling or learning from experimental observations directly.}
}
@article{LAFORCADE2010347,
title = {A Domain-Specific Modeling approach for supporting the specification of Visual Instructional Design Languages and the building of dedicated editors},
journal = {Journal of Visual Languages & Computing},
volume = {21},
number = {6},
pages = {347-358},
year = {2010},
note = {Special Issue on Visual Instructional Design Languages},
issn = {1045-926X},
doi = {https://doi.org/10.1016/j.jvlc.2010.08.008},
url = {https://www.sciencedirect.com/science/article/pii/S1045926X10000492},
author = {Pierre Laforcade},
keywords = {Visual Instructional Design Languages Domain Specific Modeling, Visual and executable models},
abstract = {This paper presents, illustrates and discusses theories and practices about the application of a domain-specific modeling (DSM) approach to facilitate the specification of Visual Instructional Design Languages (VIDLs) and the development of dedicated graphical editors. Although this approach still requires software engineering skills, it tackles the need of building VIDLs allowing both visual models for human-interpretation purposes (explicit designs, communication, thinking, etc.) and machine-readable notations for deployment or other instructional design activities. This article proposes a theoretical application and a categorization, based on a domain-oriented separation of concerns of instructional design. It also presents some practical illustrations from experiments of specific DSM tooling. Key lessons learned as well as observed obstacles and challenges to deal with are discussed in order to further develop such an approach.}
}
@incollection{PREISIG20121242,
title = {HAZOP - an automaton-inspired approach},
editor = {Ian David Lockhart Bogle and Michael Fairweather},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {30},
pages = {1242-1246},
year = {2012},
booktitle = {22nd European Symposium on Computer Aided Process Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-444-59520-1.50107-X},
url = {https://www.sciencedirect.com/science/article/pii/B978044459520150107X},
author = {Heinz A Preisig and Flavio Manenti},
keywords = {dynamic system, safety, HAZOP},
abstract = {If there exists a gradient pointing outwards a save-operation region, then there exists a possible path for the plant to cross out of the save-operation region. A method is introduced that allows the identification of such surface pieces in the phase space of the state variable for the analysed equipment. The idea is based on splitting the phase space into subspaces separated by the zero-dynamic component surface. The computation requires only root solving of the right-hand side of the dynamic equations, one at the time.}
}
@article{ABDALLAH2024102341,
title = {An evaluation of the use of air cooling to enhance photovoltaic performance},
journal = {Thermal Science and Engineering Progress},
volume = {47},
pages = {102341},
year = {2024},
issn = {2451-9049},
doi = {https://doi.org/10.1016/j.tsep.2023.102341},
url = {https://www.sciencedirect.com/science/article/pii/S2451904923006947},
author = {Ramez Abdallah and Tamer Haddad and Mohammad Zayed and Adel Juaidi and Tareq Salameh},
keywords = {Photovoltaic, ANSYS fluent, CFD, PV cooling, Heat sink},
abstract = {The rapid rise in global energy consumption and its consequences on climate change has made incorporating renewable energy sources like solar photovoltaics into the building envelope easier. However, in spite of extensive uses and significant technological advances, the lower solar panel efficiencies caused by high temperatures remain a significant barrier to the viability of deploying photovoltaic technology in regions with hot climates utilizing computational fluid dynamics (CFD). This research examines the cooling effectiveness of air-cooled photovoltaic (PV) under the climate of Nablus - Palestine. This study presents a numerical model designed to cool solar panels using various air-cooled channel configurations. Rectangular fins made of high thermal conductivity materials such as copper were used in this study. The parametric study was based on the changing baseplate thickness, fin spacing, height, and thickness through a stepwise optimization process to enhance the heat transfer mechanism. The results show that the optimum design of average volume temperatures for the PV cell models in air-cooled channel configurations with and without fins were 40.28 °C and 42.58 °C, respectively. The optimum design was obtained at 3, 110, 60, and 4 mm for baseplate thickness, fin spacing, height, and thickness, respectively. This optimum design was responsible for the average PV panel temperature drop by 1.6 %, 1.3 %, 5.9 %, and 6.2 % for baseplate thickness, fin spacing, height, and thickness, respectively. The optimum design of an air-cooled cooling channel for PV is an important insight provided by this work, and it may help in the future development of more effective and affordable cooling methods.}
}
@article{SELKER2005410,
title = {Fostering motivation and creativity for computer users},
journal = {International Journal of Human-Computer Studies},
volume = {63},
number = {4},
pages = {410-421},
year = {2005},
note = {Computer support for creativity},
issn = {1071-5819},
doi = {https://doi.org/10.1016/j.ijhcs.2005.04.005},
url = {https://www.sciencedirect.com/science/article/pii/S1071581905000443},
author = {Ted Selker},
keywords = {Communicate, Communication, Community, Computers, Creative, Creativity, Creativity-enhancing, Design, Engineering, Filter, Graphics, Human–computer, Idea, Interaction, Motivation, Product, Programming, Social, Support, Text, User interface},
abstract = {Creativity might be viewed as any process which results in a novel and useful product. People use computers for creative tasks; they flesh out ideas for text, graphics, engineering solutions, etc. Computer programming is an especially creative activity, but few tools for programming aid creativity. Computers can be designed to foster creativity as well. As a start, all computer programs should help users enumerate ideas, remember alternatives and support various ways to compare them. More sophisticated thinking aids could implement other successful techniques as well. Most computers are used in solitude; however, people depend on social supports for creativity. User scenarios can provide the important social support and gracious cues normally offered by collaborators that keep people motivated and help them consider alternatives. People also use computers to build community and to communicate. Computers should also support and filter these potentially creativity-enhancing communication acts. User-interface designers are so busy exposing features and fighting bugs that they might ignore their users’ needs for motivation and creativity support. This paper develops the notion that creativity and motivation enhancement can easily be aligned with the design of high-quality human–computer interaction. User interface toolkits and evaluations should include support for motivation and creativity-enhancing approaches.}
}
@article{ROSEGGER1992iii,
title = {Editorial},
journal = {Technovation},
volume = {12},
number = {1},
pages = {iii-iv},
year = {1992},
issn = {0166-4972},
doi = {https://doi.org/10.1016/0166-4972(92)90028-G},
url = {https://www.sciencedirect.com/science/article/pii/016649729290028G},
author = {Gerhard Rosegger},
abstract = {Alfred North Whitehead observed that “It is a profoundly erroneous truism, repeated by all copy-books and by eminent people when they are making speeches, that we should cultivate the habit of thinking what we are doing. The precise opposite is the case. Civilization advances by extending the number of important operations which we can perform without thinking about them.”}
}
@article{WARNIER201715,
title = {Distributed monitoring for the prevention of cascading failures in operational power grids},
journal = {International Journal of Critical Infrastructure Protection},
volume = {17},
pages = {15-27},
year = {2017},
issn = {1874-5482},
doi = {https://doi.org/10.1016/j.ijcip.2017.03.003},
url = {https://www.sciencedirect.com/science/article/pii/S1874548216300427},
author = {Martijn Warnier and Stefan Dulman and Yakup Koç and Eric Pauwels},
keywords = {Power Grids, Cascading Failures, Robustness, Real-Time Monitoring, Distributed Computation},
abstract = {Electrical power grids are vulnerable to cascading failures that can lead to large blackouts. The detection and prevention of cascading failures in power grids are important problems. Currently, grid operators mainly monitor the states (loading levels) of individual components in a power grid. The complex architecture of a power grid, with its many interdependencies, makes it difficult to aggregate the data provided by local components in a meaningful and timely manner. Indeed, monitoring the resilience of an operational power grid to cascading failures is a major challenge. This paper attempts to address this challenge. It presents a robustness metric based on the topology and operative state of a power grid to quantify the robustness of the grid. Also, it presents a distributed computation method with self-stabilizing properties that can be used for near real-time monitoring of grid robustness. The research thus provides insights into the resilience of a dynamic operational power grid to cascading failures during real-time in a manner that is both scalable and robust. Computations are pushed to the power grid network, making the results available at each node and enabling automated distributed control mechanisms to be implemented.}
}
@article{SMOLARCZYK2024100669,
title = {Let’s get them on board: Focus group discussions with adolescents on empowering leisure engagement in Fab Labs and makerspaces},
journal = {International Journal of Child-Computer Interaction},
volume = {41},
pages = {100669},
year = {2024},
issn = {2212-8689},
doi = {https://doi.org/10.1016/j.ijcci.2024.100669},
url = {https://www.sciencedirect.com/science/article/pii/S2212868924000370},
author = {Kathrin Smolarczyk and Marios Mouratidis and Sophie Uhing and Rolf Becker and Stephan Kröner},
keywords = {Maker activities, Leisure, Youth, Focus groups, Sustainability},
abstract = {Makerspaces and Fab Labs are growing in number all over the world, holding the potential to empower children and adolescents. They form an important pathway to provide young people with access to digital manufacturing technologies while fostering self-determination, collaboration, and creativity. We explore how the engagement in Fab Lab based leisure maker activities may be promoted, taking into account both the perspectives of adolescents and the potential of surrounding systems. For this, we conducted focus group discussions with N = 61 non-maker, adolescent girls and boys from 6th to 9th grade, to scrutinize hindering and promoting factors of their engagement in leisure maker activities, and to explore their preferences regarding the involvement of parents, teachers and peers while considering the ecological sustainability of the activities. A reflexive thematic analysis identified the hindering and promoting factors across different aspects of maker activities such as the purpose, location and setting, content, and learning processes. Implications for the promotion and design of maker activities, as well as implications for further research, are discussed.}
}
@article{REHDER201454,
title = {Independence and dependence in human causal reasoning},
journal = {Cognitive Psychology},
volume = {72},
pages = {54-107},
year = {2014},
issn = {0010-0285},
doi = {https://doi.org/10.1016/j.cogpsych.2014.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S0010028514000176},
author = {Bob Rehder},
keywords = {Causal reasoning, Causal inference, Causal Markov condition, Conditional independence, Screening off},
abstract = {Causal graphical models (CGMs) are a popular formalism used to model human causal reasoning and learning. The key property of CGMs is the causal Markov condition, which stipulates patterns of independence and dependence among causally related variables. Five experiments found that while adult’s causal inferences exhibited aspects of veridical causal reasoning, they also exhibited a small but tenacious tendency to violate the Markov condition. They also failed to exhibit robust discounting in which the presence of one cause as an explanation of an effect makes the presence of another less likely. Instead, subjects often reasoned “associatively,” that is, assumed that the presence of one variable implied the presence of other, causally related variables, even those that were (according to the Markov condition) conditionally independent. This tendency was unaffected by manipulations (e.g., response deadlines) known to influence fast and intuitive reasoning processes, suggesting that an associative response to a causal reasoning question is sometimes the product of careful and deliberate thinking. That about 60% of the erroneous associative inferences were made by about a quarter of the subjects suggests the presence of substantial individual differences in this tendency. There was also evidence that inferences were influenced by subjects’ assumptions about factors that disable causal relations and their use of a conjunctive reasoning strategy. Theories that strive to provide high fidelity accounts of human causal reasoning will need to relax the independence constraints imposed by CGMs.}
}
@article{LEVINSON2012167,
title = {Tools from evolutionary biology shed new light on the diversification of languages},
journal = {Trends in Cognitive Sciences},
volume = {16},
number = {3},
pages = {167-173},
year = {2012},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2012.01.007},
url = {https://www.sciencedirect.com/science/article/pii/S1364661312000290},
author = {Stephen C. Levinson and Russell D. Gray},
abstract = {Computational methods have revolutionized evolutionary biology. In this paper we explore the impact these methods are now having on our understanding of the forces that both affect the diversification of human languages and shape human cognition. We show how these methods can illuminate problems ranging from the nature of constraints on linguistic variation to the role that social processes play in determining the rate of linguistic change. Throughout the paper we argue that the cognitive sciences should move away from an idealized model of human cognition, to a more biologically realistic model where variation is central.}
}
@article{BOCK2020100960,
title = {On the semantics for spreadsheets with sheet-defined functions},
journal = {Journal of Computer Languages},
volume = {57},
pages = {100960},
year = {2020},
issn = {2590-1184},
doi = {https://doi.org/10.1016/j.cola.2020.100960},
url = {https://www.sciencedirect.com/science/article/pii/S2590118420300204},
author = {Alexander Asp Bock and Thomas Bøgholm and Peter Sestoft and Bent Thomsen and Lone Leth Thomsen},
keywords = {Spreadsheet, Semantics, Funcalc, Sheet-defined function, Recalculation},
abstract = {We give an operational semantics for the evaluation of spreadsheets, including sheet-defined and built-in numeric functions in the Funcalc spreadsheet platform. The semantics allows for different implementations and we discuss sheet-defined functions implemented using both interpretation and run-time code generation. The semantics specifies the expected result of a computation, also considering non-deterministic functions, independently of an evaluation mechanism. It can be extended to include the cost of formula evaluation for a cost analysis e.g. for use in parallelization of computations. An interesting future direction is to investigate experimentally how close our semantics is to that of major spreadsheet implementations.}
}
@article{LO2022111357,
title = {Architectural patterns for the design of federated learning systems},
journal = {Journal of Systems and Software},
volume = {191},
pages = {111357},
year = {2022},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2022.111357},
url = {https://www.sciencedirect.com/science/article/pii/S0164121222000899},
author = {Sin Kit Lo and Qinghua Lu and Liming Zhu and Hye-Young Paik and Xiwei Xu and Chen Wang},
keywords = {Federated learning, Pattern, Software architecture, Machine learning, Artificial intelligence},
abstract = {Federated learning has received fast-growing interests from academia and industry to tackle the challenges of data hungriness and privacy in machine learning. A federated learning system can be viewed as a large-scale distributed system with different components and stakeholders as numerous client devices participate in federated learning. Designing a federated learning system requires software system design thinking apart from the machine learning knowledge. Although much effort has been put into federated learning from the machine learning technique aspects, the software architecture design concerns in building federated learning systems have been largely ignored. Therefore, in this paper, we present a collection of architectural patterns to deal with the design challenges of federated learning systems. Architectural patterns present reusable solutions to a commonly occurring problem within a given context during software architecture design. The presented patterns are based on the results of a systematic literature review and include three client management patterns, four model management patterns, three model training patterns, four model aggregation patterns, and one configuration pattern. The patterns are associated to the particular state transitions in a federated learning model lifecycle, serving as a guidance for effective use of the patterns in the design of federated learning systems.}
}
@incollection{LUDLOW2025,
title = {Competence/Performance},
booktitle = {Reference Module in Social Sciences},
publisher = {Elsevier},
year = {2025},
isbn = {978-0-443-15785-1},
doi = {https://doi.org/10.1016/B978-0-323-95504-1.00557-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780323955041005573},
author = {Peter Ludlow},
keywords = {Linguistic competence, Linguistic performance, Cognizing rules, Knowing rules, Rule following, Natural logic, Norms},
abstract = {The competence/performance distinction has played a role in linguistic theorizing since 1965. The key idea is that one can distinguish between the grammar that an agent has and the performance errors that violate the posited rules or principles of the grammar. The first question to consider is what counts as a performance error and whether it can be defined in a way that avoids bailing out defective theories. The next question concerns what competence consists in—in what sense do we have linguistic rules or principles? One idea is that the rule plays a normative (guiding) role. Another idea is that we merely “cognize” or represent the rule. These lead to different conceptions of performance errors. We then turn to versions of the competence/performance distinction in other fields, including ethics and logic.}
}
@article{PEREZESCOBAR202423,
title = {Minimal logical teleology in artifacts and biology connects the two domains and frames mechanisms via epistemic circularity},
journal = {Studies in History and Philosophy of Science},
volume = {104},
pages = {23-37},
year = {2024},
issn = {0039-3681},
doi = {https://doi.org/10.1016/j.shpsa.2024.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S0039368124000104},
author = {José Antonio Pérez-Escobar},
keywords = {Minimal logical teleology, Analogies, Scientific explanation, Epistemic circularity, Scientific modelling, Cognitive neuroscience},
abstract = {The understanding of artifacts and biological phenomena has often influenced each other. This work argues that at the core of these epistemic bridges there are shared teleological notions and explanations manifested in analogies between artifacts and biological phenomena. To this end, I first propose a focus on the logical structure of minimal teleological explanations, which renders said epistemic bridges more evident than an ontological or metaphysical approach to teleology, and which can be used to describe scientific practices in different areas by virtue of formal generality and minimalism (section 2). Second, I show how this approach highlights some epistemic features shared by the understanding of artifacts and biological phenomena, like a specific kind of epistemic circularity, and how functional analogies between artifacts and biological phenomena translate such epistemic circularity from one domain to the other (section 3). Third, I conduct a case study on the scientific practice around the brain's “compass”, showing how the understanding of artifacts influences purpose ascription and measurement, and frames mechanisms in biology, especially in areas where purpose ascription is most difficult, like cognitive neuroscience (sections 4 and 5).}
}
@incollection{SYMMONDS20123,
title = {Chapter 1 - The Neurobiology of Preferences},
editor = {Raymond Dolan and Tali Sharot},
booktitle = {Neuroscience of Preference and Choice},
publisher = {Academic Press},
address = {San Diego},
pages = {3-31},
year = {2012},
isbn = {978-0-12-381431-9},
doi = {https://doi.org/10.1016/B978-0-12-381431-9.00001-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780123814319000012},
author = {Mkael Symmonds and Raymond J. Dolan},
keywords = {Preference, choice, neuroscience, neuroeconomics, decision-making, action, value},
abstract = {Publisher Summary
The neuroscience of choice and preference dates back to the nineteenth century, with the emergence of the idea of functional specialization as a fundamental organizational principle of the brain. The development of neuroimaging techniques—in particular, functional magnetic resonance imaging (fMRI)—has meant that questions related to choice and preference can now be addressed non-invasively in humans. There are important examples where choices do not accord with internal wants. An addict may perform an action in the present despite expressing a desire to avoid doing this very action on a prior occasion. A major conundrum when thinking about neurobiological mechanisms in decision-making is the fact that choices are often noisy or stochastic. A different network of regions in precuneus, left prefrontal, and temproparietal cortex reflected endogenous inequity aversion across subjects, illustrating that even within the context of a specific task, preferences for the same stimulus feature can be expressed in different regions and modulated in a distinct manner.}
}
@article{OYELADE2025127455,
title = {SMAR + NIE IdeaGen: A knowledge graph based node importance estimation with analogical reasoning on large language model for idea generation},
journal = {Expert Systems with Applications},
volume = {279},
pages = {127455},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.127455},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425010772},
author = {Olaide N. Oyelade and Hui Wang and Karen Rafferty},
keywords = {Knowledge graphs (KGs), Large language model (LLMs), Idea generation, Novelty, Analogical reasoning, Node importance estimation, Natural language processing (NLP), Isomorphic subgraphs},
abstract = {Idea generation describes a creative process involving reasoning over some knowledge to derive new information. Traditional approaches such as mind-map and brainstorming are limited and often fail due to lack of quality ideas and ineffective methods. The reasoning capability of large language models (LLMs) have been investigated for ideation tasks and have reported interesting performance. However, these models suffer from limited logical reasoning capability which hinders the use of structural and factual real-world knowledge in discovery of latent insight and predict possible outcome when applied to ideation. In addition, the possibility of LLMs regurgitating knowledge learnt from datasets might adversely impact the degree of novel ideas the models can generate. In this paper, a two-stage logical reasoning approach is applied to initiate the search for candidate idea pathways based on the knowledge graphs (KGs) to address the problem of reasoning, domain-specificity and novelty. The divergence stage this reasoning explores utilizes a new node importance estimation (NIE) technique over KGs to discover latent connections supporting idea generation. In the convergence stage of this reasoning, subgraph matching using analogical reasoning (SMAR) is applied to find matching patterns to describe a new idea. The use of SMAR + NIE and KGs helps to achieve an improvement in reasoning over KGs before transferring such reasoning to LLMs for translation of idea into natural language. To evaluate the degree of novelty of ideas generated, a relevance-to-novelty scoring metrics is proposed based on multiple premise entailment (MPE). We combined this metric with other popular metrics to evaluate the performance of SMAR + NIE on benchmark datasets, and as well on the quality of ideas generated. Findings from the study showed that this approach demonstrates competitive performance with mainstream LLMs in idea generation tasks.}
}
@incollection{MORGERA1986389,
title = {COMPUTATIONAL COMPLEXITY AND VLSI IMPLEMENTATION OF AN OPTIMAL FEATURE SELECTION STRATEGY††Work supported by Canada NSERC Grant AO912.},
editor = {Edzard S. GELSEMA and Laveen N. KANAL},
booktitle = {Pattern Recognition in Practice},
publisher = {Elsevier},
address = {Amsterdam},
pages = {389-400},
year = {1986},
isbn = {978-0-444-87877-9},
doi = {https://doi.org/10.1016/B978-0-444-87877-9.50036-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780444878779500364},
author = {Salvatore D. Morgera}
}
@article{CHAKRABORTY2013180,
title = {Secret image sharing using grayscale payload decomposition and irreversible image steganography},
journal = {Journal of Information Security and Applications},
volume = {18},
number = {4},
pages = {180-192},
year = {2013},
issn = {2214-2126},
doi = {https://doi.org/10.1016/j.istr.2013.02.006},
url = {https://www.sciencedirect.com/science/article/pii/S1363412713000162},
author = {Soumendu Chakraborty and Anand Singh Jalal and Charul Bhatnagar},
keywords = {DSF matrix, Error matrix, Sign matrix, Bit plane},
abstract = {To provide an added security level most of the existing reversible as well as irreversible image steganography schemes emphasize on encrypting the secret image (payload) before embedding it to the cover image. The complexity of encryption for a large payload where the embedding algorithm itself is complex may adversely affect the steganographic system. Schemes that can induce same level of distortion, as any standard encryption technique with lower computational complexity, can improve the performance of stego systems. In this paper, we propose a secure secret image sharing scheme, which bears minimal computational complexity. The proposed scheme, as a replacement for encryption, diversifies the payload into different matrices which are embedded into carrier image (cover image) using bit X-OR operation. A payload is a grayscale image which is divided into frequency matrix, error matrix, and sign matrix. The frequency matrix is scaled down using a mapping algorithm to produce Down Scaled Frequency (DSF) matrix. The DSF matrix, error matrix, and sign matrix are then embedded in different cover images using bit X-OR operation between the bit planes of the matrices and respective cover images. Analysis of the proposed scheme shows that it effectively camouflages the payload with minimum computation time.}
}
@article{LIETO20161,
title = {From human to artificial cognition and back: New perspectives on cognitively inspired AI systems},
journal = {Cognitive Systems Research},
volume = {39},
pages = {1-3},
year = {2016},
note = {From human to artificial cognition (and back): new perspectives of cognitively inspired AI systems},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2016.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S1389041716300183},
author = {Antonio Lieto and Daniele P. Radicioni},
keywords = {Cognitive systems, Artificial intelligence, Computational models of cognition, Epistemology of the artificial},
abstract = {We overview the main historical and technological elements characterising the rise, the fall and the recent renaissance of the cognitive approaches to Artificial Intelligence and provide some insights and suggestions about the future directions and challenges that, in our opinion, this discipline needs to face in the next years.}
}
@article{AIBINU2023100590,
title = {Solutions of fractional differential equations by using a blend of variational iteration method with Sumudu transform and application to price adjustment equations},
journal = {Partial Differential Equations in Applied Mathematics},
volume = {8},
pages = {100590},
year = {2023},
issn = {2666-8181},
doi = {https://doi.org/10.1016/j.padiff.2023.100590},
url = {https://www.sciencedirect.com/science/article/pii/S2666818123001031},
author = {M.O. Aibinu and S. Moyo},
keywords = {Sumudu transform, Caputo fractional derivative, Price adjustment, Model, Market equilibrium},
abstract = {The presence of delays in a mathematical model can improve its vitality and suitability in describing several phenomena. However, in the presence of delays, nonlinear fractional differential equations are more difficult to study. This paper presents the use of a blend of variational iteration method with Sumudu transform for solving delay differential equations with Caputo derivatives of fractional variable order. Moreover, the paper introduces delays into the price adjustment equations to propose new price adjustment models with more potential for vitality and suitability. The paper assigns suitable real values to the parameters for the graphical display and comparison of the obtained solutions. The paper presents the interactions that exist among the price, demand, supply and dependence of supply and demand on the price, which can be applied to estimate the equilibrium price.}
}
@article{JIANG2020556,
title = {Energy aware edge computing: A survey},
journal = {Computer Communications},
volume = {151},
pages = {556-580},
year = {2020},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2020.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S014036641930831X},
author = {Congfeng Jiang and Tiantian Fan and Honghao Gao and Weisong Shi and Liangkai Liu and Christophe Cérin and Jian Wan},
keywords = {Edge computing, Energy efficiency, Computing offloading, Benchmarking, Computation partitioning},
abstract = {Edge computing is an emerging paradigm for the increasing computing and networking demands from end devices to smart things. Edge computing allows the computation to be offloaded from the cloud data centers to the network edge and edge nodes for lower latency, security and privacy preservation. Although energy efficiency in cloud data centers has been broadly investigated, energy efficiency in edge computing is largely left uninvestigated due to the complicated interactions between edge devices, edge servers, and cloud data centers. In order to achieve energy efficiency in edge computing, a systematic review on energy efficiency of edge devices, edge servers, and cloud data centers is required. In this paper, we survey the state-of-the-art research work on energy-aware edge computing, and identify related research challenges and directions, including architecture, operating system, middleware, applications services, and computation offloading.}
}
@article{GAO201464,
title = {Unconscious processing modulates creative problem solving: Evidence from an electrophysiological study},
journal = {Consciousness and Cognition},
volume = {26},
pages = {64-73},
year = {2014},
issn = {1053-8100},
doi = {https://doi.org/10.1016/j.concog.2014.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S1053810014000464},
author = {Ying Gao and Hao Zhang},
keywords = {Unconscious processing, Divergent thinking, Creative problem solving, Creativity, Event-related potential},
abstract = {Previous behavioral studies have identified the significant role of subliminal cues in creative problem solving. However, neural mechanisms of such unconscious processing remain poorly understood. Here we utilized an event-related potential (ERP) approach and sandwich mask technique to investigate cerebral activities underlying the unconscious processing of cues in creative problem solving. College students were instructed to solve divergent problems under three different conditions (conscious cue, unconscious cue and no-cue conditions). Our data showed that creative problem solving can benefit from unconscious cues, although not as much as from conscious cues. More importantly, we found that there are crucial ERP components associated with unconscious processing of cues in solving divergent problems. Similar to the processing of conscious cues, processing unconscious cues in problem solving involves the semantic activation of unconscious cues (N280–340) in the right inferior parietal lobule (BA 40), new association formation (P350–450) in the right parahippocampal gyrus (BA 36), and mental representation transformation (P500–760) in the right superior temporal gyrus (BA 22). The present results suggest that creative problem solving can be modulated by unconscious processing of enlightening information that is weakly diffused in the semantic network beyond our conscious awareness.}
}
@article{BASOV2020101433,
title = {Socio-semantic and other dualities},
journal = {Poetics},
volume = {78},
pages = {101433},
year = {2020},
note = {Discourse, Meaning, and Networks: Advances in Socio-Semantic Analysis},
issn = {0304-422X},
doi = {https://doi.org/10.1016/j.poetic.2020.101433},
url = {https://www.sciencedirect.com/science/article/pii/S0304422X19304073},
author = {Nikita Basov and Ronald Breiger and Iina Hellsten},
keywords = {Social network, Semantic network, Socio-semantic network, Duality, Culture, Special Issue},
abstract = {The social and the cultural orders are dual – that is, they constitute each other. To understand either we need to account for both. Socio-semantic network analysis brings together the study of relations among actors (social networks), relations among elements of actors’ cultural structures (their semantic networks), and relations among these two orders of networks. In this introductory essay, we describe how the duality of the social and semantic networks that constitute each other, as well as other related dualities (including material / symbolic, micro / macro, computational / qualitative, in-presence contexts / online contexts, ‘Big’ data / ‘thick’ data), have evolved in recent decades to mold socio-semantic network analysis into its present form. In doing so, we delineate the current state of the art and the main features of socio-semantic network analysis as highlighted by the papers included in this Special Issue. These articles range from in-depth analysis of ‘thick’ data on small group interactions to automated analysis of ‘Big’ online data in contexts extending from Renaissance parliamentary discussions to cutting-edge global scientific fields of the 21st century. We conclude by delineating current problems of and future prospects for socio-semantic network analysis.}
}
@article{COVINGTON2016869,
title = {Expanding the Language Network: Direct Contributions from the Hippocampus},
journal = {Trends in Cognitive Sciences},
volume = {20},
number = {12},
pages = {869-870},
year = {2016},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2016.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S1364661316301759},
author = {Natalie V. Covington and Melissa C. Duff},
keywords = {hippocampus, language, memory, online processing, theta oscillations},
abstract = {New research suggests that the same hippocampal computations used in support of memory are also used for language processing, providing direct neurophysiological evidence of a shared neural mechanism for memory and language. This work expands classic memory and language models and represents a new opportunity for studying the memory–language interface.}
}
@article{VELOSO2023104997,
title = {Spatial synthesis for architectural design as an interactive simulation with multiple agents},
journal = {Automation in Construction},
volume = {154},
pages = {104997},
year = {2023},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2023.104997},
url = {https://www.sciencedirect.com/science/article/pii/S0926580523002571},
author = {Pedro Veloso and Ramesh Krishnamurti},
keywords = {Spatial synthesis, Interactive simulation, Artificial intelligence, Agent-based modeling, Multi-agent deep reinforcement learning},
abstract = {Motivated by reflection-in-action in architectural design, this article introduces a spatial synthesis artifact that relies on multi-agent reinforcement learning to address spatial goals with fine-grained control in a simulation. It relies on parameter sharing with proximal policy optimization and a parameterized reward function to train robust agent policies in random environments with random spatial problems. The agents are evaluated in three design cases: a house design with 12 agents in three sites, a museum with 18 agents in an interstitial urban site, and a speculative design of a housing complex with 96 agents on a large empty site. The policies performed well in all the cases and produced morphologically consistent solutions. However, in cases with a larger number of agents, the system largely benefited from a spring layout algorithm for the initialization. Future research will address more complex spatial synthesis problems and mechanisms for human-computer interaction.}
}
@article{KHACHATURIAN20253,
title = {Perspective on “Brain Network Disorders”},
journal = {Brain Network Disorders},
volume = {1},
number = {1},
pages = {3-6},
year = {2025},
issn = {3050-6239},
doi = {https://doi.org/10.1016/j.bnd.2024.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S3050623924000129},
author = {Zaven Khachaturian and Jean-Marie C. Boutellier and Jiri Damborsky and Ara S. Khachaturian}
}
@article{YANOVA201682,
title = {Relativistic Psychometrics in Subjective Scaling},
journal = {Procedia Computer Science},
volume = {102},
pages = {82-89},
year = {2016},
note = {12th International Conference on Application of Fuzzy Systems and Soft Computing, ICAFS 2016, 29-30 August 2016, Vienna, Austria},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2016.09.373},
url = {https://www.sciencedirect.com/science/article/pii/S1877050916325534},
author = {Natalia Yanova},
keywords = {mental representation, relativistic psychometrics, computational theory of perceptions, image understanding, significance, expert test system, psychosemiotics},
abstract = {The article announces the possibilities of semantic modeling in the development of feedback tools in social sciences. A new approach to the computational theory of perceptions (CTP) for analysis of mental object is proposed. The article demonstrates the implementation of relativistic psychometrics for the study of mental response (opinions, expectations and attitudes). The problem of image understanding and its significance is considered in combination of soft and hard computing. It is shown that the modeling of object (its coding and decoding in ‘mental map’) obeys the semiotic and mathematical logic. Computing with perceptions for the rules of mental representation proves their identity to the laws of conservation. The article demonstrates the versatility of the semiotic description of objects in Minkowski space. It also confirms by mathematical solution C. S. Peirce's metaphor, according to which the semiology of language is a truly universal algebra of relations.}
}
@article{PROKOPENKO2019134,
title = {Self-referential basis of undecidable dynamics: From the Liar paradox and the halting problem to the edge of chaos},
journal = {Physics of Life Reviews},
volume = {31},
pages = {134-156},
year = {2019},
note = {Physics of Mind},
issn = {1571-0645},
doi = {https://doi.org/10.1016/j.plrev.2018.12.003},
url = {https://www.sciencedirect.com/science/article/pii/S1571064519300077},
author = {Mikhail Prokopenko and Michael Harré and Joseph Lizier and Fabio Boschetti and Pavlos Peppas and Stuart Kauffman},
keywords = {Self-reference, Diagonalization, Undecidability, Incomputability, Program-data duality, Complexity},
abstract = {In this paper we explore several fundamental relations between formal systems, algorithms, and dynamical systems, focussing on the roles of undecidability, universality, diagonalization, and self-reference in each of these computational frameworks. Some of these interconnections are well-known, while some are clarified in this study as a result of a fine-grained comparison between recursive formal systems, Turing machines, and Cellular Automata (CAs). In particular, we elaborate on the diagonalization argument applied to distributed computation carried out by CAs, illustrating the key elements of Gödel's proof for CAs. The comparative analysis emphasizes three factors which underlie the capacity to generate undecidable dynamics within the examined computational frameworks: (i) the program-data duality; (ii) the potential to access an infinite computational medium; and (iii) the ability to implement negation. The considered adaptations of Gödel's proof distinguish between computational universality and undecidability, and show how the diagonalization argument exploits, on several levels, the self-referential basis of undecidability.}
}
@article{WANG2023109577,
title = {Study of the flow field of a new fishtail-type stirring impeller in a stirred tank},
journal = {Chemical Engineering and Processing - Process Intensification},
volume = {194},
pages = {109577},
year = {2023},
issn = {0255-2701},
doi = {https://doi.org/10.1016/j.cep.2023.109577},
url = {https://www.sciencedirect.com/science/article/pii/S0255270123003148},
author = {Zhaohui Wang and Deli Li and Quanjie Gao and Qianwen Yang and Xiao Xiong and Changzhi Jiang and Feng Zhang},
keywords = {Computational fluid dynamics, Power consumption, Particle image velocimetry, Impeller design, Blade inclination},
abstract = {Abstracts
In this study, a new fishtail impeller was introduced to improve the mixing of fluids in the stirred tank. The validity of the numerical model was first demonstrated by PIV experiments. Secondly, the CFD technique was used to analyze and predict the flow field characteristics in the stirred tank. Also, the effect of blade inclination on the mixing effect is analyzed. Finally, a comparative Analysis with the whale tail impeller is carried out to demonstrate the superiority of this research work. The results show that The results of the study showed that the power number of the fishtail impeller was reduced by 16.4 % compared to the RT impeller. The pumping efficiency of the fishtail impeller was increased by 25.52 %. The results also show that increasing the blade inclination increases the turbulent kinetic energy in the stirred tank. Comparative analysis with the whale-tail impeller reveals that the power number of the fish-tail impeller is reduced by 17.2 % and the pumping efficiency is increased by 19.97 %.}
}
@article{BURKE2024102382,
title = {A chance for models to show their quality: Stochastic process model-log dimensions},
journal = {Information Systems},
volume = {124},
pages = {102382},
year = {2024},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2024.102382},
url = {https://www.sciencedirect.com/science/article/pii/S0306437924000401},
author = {Adam T. Burke and Sander J.J. Leemans and Moe T. Wynn and Wil M.P. {van der Aalst} and Arthur H.M. {ter Hofstede}},
keywords = {Stochastic process mining, Process conformance, Stochastic Petri nets, Adhesion, Relevance, Simplicity},
abstract = {Process models describe the desired or observed behaviour of organisations. In stochastic process mining, computational analysis of trace data yields process models which describe process paths and their probability of execution. To understand the quality of these models, and to compare them, quantitative quality measures are used. This research investigates model comparison empirically, using stochastic process models built from real-life logs. The experimental design collects a large number of models generated randomly and using process discovery techniques. Twenty-five different metrics are taken on these models, using both existing process model metrics and new, exploratory ones. The results are analysed quantitatively, making particular use of principal component analysis. Based on this analysis, we suggest three stochastic process model dimensions: adhesion, relevance and simplicity. We also suggest possible metrics for these dimensions, and demonstrate their use on example models.}
}
@article{PIANTADOSI2012199,
title = {Bootstrapping in a language of thought: A formal model of numerical concept learning},
journal = {Cognition},
volume = {123},
number = {2},
pages = {199-217},
year = {2012},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2011.11.005},
url = {https://www.sciencedirect.com/science/article/pii/S0010027711002769},
author = {Steven T. Piantadosi and Joshua B. Tenenbaum and Noah D. Goodman},
keywords = {Number word learning, Bootstrapping, Cognitive development, Bayesian model, Language of thought, CP transition},
abstract = {In acquiring number words, children exhibit a qualitative leap in which they transition from understanding a few number words, to possessing a rich system of interrelated numerical concepts. We present a computational framework for understanding this inductive leap as the consequence of statistical inference over a sufficiently powerful representational system. We provide an implemented model that is powerful enough to learn number word meanings and other related conceptual systems from naturalistic data. The model shows that bootstrapping can be made computationally and philosophically well-founded as a theory of number learning. Our approach demonstrates how learners may combine core cognitive operations to build sophisticated representations during the course of development, and how this process explains observed developmental patterns in number word learning.}
}
@article{DELATORRE2014653,
title = {Monte Carlo advances and concentrated solar applications},
journal = {Solar Energy},
volume = {103},
pages = {653-681},
year = {2014},
issn = {0038-092X},
doi = {https://doi.org/10.1016/j.solener.2013.02.035},
url = {https://www.sciencedirect.com/science/article/pii/S0038092X13001448},
author = {J. Delatorre and G. Baud and J.J. Bézian and S. Blanco and C. Caliot and J.F. Cornet and C. Coustet and J. Dauchet and M. {El Hafi} and V. Eymet and R. Fournier and J. Gautrais and O. Gourmel and D. Joseph and N. Meilhac and A. Pajot and M. Paulin and P. Perez and B. Piaud and M. Roger and J. Rolland and F. Veynandt and S. Weitz},
keywords = {Monte Carlo algorithm, Concentrated solar energy, Solar energy flux density distribution, Solar concentrators design optimization, Sensitivity computation},
abstract = {The Monte Carlo method is partially reviewed with the objective of illustrating how some of the most recent methodological advances can benefit to concentrated solar research. This review puts forward the practical consequences of writing down and handling the integral formulation associated to each Monte Carlo algorithm. Starting with simple examples and up to the most complex multiple reflection, multiple scattering configurations, we try to argue that these formulations are very much accessible to the non specialist and that they allow a straightforward entry to sensitivity computations (for assistance in design optimization processes) and to convergence enhancement techniques involving subtle concepts such as control variate and zero variance. All illustration examples makePROMES - UPR CNRS 8521 - 7, rue du Four Solaire, 66120 Font Romeu Odeillo, France use of the public domain development environment EDStar (including advanced parallelized computer graphics libraries) and are meant to serve as start basis either for the upgrading of existing Monte Carlo codes, or for fast implementation of ad hoc codes when specific needs cannot be answered with standard concentrated solar codes (in particular as far as the new generation of solar receivers is concerned).}
}
@article{JIN201559,
title = {Significance and Challenges of Big Data Research},
journal = {Big Data Research},
volume = {2},
number = {2},
pages = {59-64},
year = {2015},
note = {Visions on Big Data},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2015.01.006},
url = {https://www.sciencedirect.com/science/article/pii/S2214579615000076},
author = {Xiaolong Jin and Benjamin W. Wah and Xueqi Cheng and Yuanzhuo Wang},
keywords = {Big data, Data complexity, Computational complexity, System complexity},
abstract = {In recent years, the rapid development of Internet, Internet of Things, and Cloud Computing have led to the explosive growth of data in almost every industry and business area. Big data has rapidly developed into a hot topic that attracts extensive attention from academia, industry, and governments around the world. In this position paper, we first briefly introduce the concept of big data, including its definition, features, and value. We then identify from different perspectives the significance and opportunities that big data brings to us. Next, we present representative big data initiatives all over the world. We describe the grand challenges (namely, data complexity, computational complexity, and system complexity), as well as possible solutions to address these challenges. Finally, we conclude the paper by presenting several suggestions on carrying out big data projects.}
}
@article{GAO2021107161,
title = {Optical waves/modes in a multicomponent inhomogeneous optical fiber via a three-coupled variable-coefficient nonlinear Schrödinger system},
journal = {Applied Mathematics Letters},
volume = {120},
pages = {107161},
year = {2021},
issn = {0893-9659},
doi = {https://doi.org/10.1016/j.aml.2021.107161},
url = {https://www.sciencedirect.com/science/article/pii/S089396592100080X},
author = {Xin-Yi Gao and Yong-Jiang Guo and Wen-Rui Shan},
keywords = {Optical waves/modes, Multicomponent inhomogeneous optical fiber, Symbolic computation, Three-coupled variable-coefficient nonlinear Schrödinger system, Similarity reduction, Bäcklund transformation with analytic solutions},
abstract = {Recent progress in optical fibers is impressive, while nonlinear Schrödinger-type models are seen in fiber optics and other fields (such as ferromagnetism, plasma physics, Bose–Einstein condensation and oceanography). Hereby, our symbolic computation on a three-coupled variable-coefficient nonlinear Schrödinger system is performed, for the picosecond-pulse attenuation/amplification in a multicomponent inhomogeneous optical fiber with diverse polarizations/frequencies. For the slowly-varying envelopes of optical modes, we obtain a similarity reduction, an auto-Bäcklund transformation and some analytic solutions, which rely on the optical-fiber variable coefficients, i.e., the fiber loss/gain, nonlinearity and group velocity dispersion. Relevant variable-coefficient constraints are presented. Our results might be of some use in the construction of logic gates, optical computing, soliton switching, design of fiber directional couplers, quantum information processing, soliton amplification in the wavelength division multiplexing systems, solitonic studies in the all-optical devices and birefringence fiber systems.}
}
@article{NAKHAEI2022116422,
title = {A novel framework for technical performance evaluation of water distribution networks based on the water-energy nexus concept},
journal = {Energy Conversion and Management},
volume = {273},
pages = {116422},
year = {2022},
issn = {0196-8904},
doi = {https://doi.org/10.1016/j.enconman.2022.116422},
url = {https://www.sciencedirect.com/science/article/pii/S0196890422012006},
author = {Mahdi Nakhaei and Mehran Akrami and Mohammad Gheibi and Pedro {Daniel Urbina Coronado} and Mostafa Hajiaghaei-Keshteli and Jürgen Mahlknecht},
keywords = {Water distribution network, Water energy nexus, EPANET, Design of experiments, Machine learning},
abstract = {Today energy recovery using Micro-Hydropowers (MHPs) in Water Distribution Networks (WDN) is a well-known approach for recycling the wasted energy in infrastructures as a sample of circular economy. Likewise, in this study for the first time a framework for evaluation of WDN for energy harvesting have been designed with the application of statistical optimization, simulation, and artificial intelligence concepts. In this study, after modelling a WDN in Mashhad, Iran, with Environmental Protection Agency Network Evaluation Tool (EPANET) software, the potential of energy recovery using MHP technology was optimized with the application of Design of Experiment (DOE) methods, including Taguchi and Response Surface Methodology (RSM) and then the model prediction ability was improved by Artificial Neural Network (ANN) technique. Results of this investigation revealed that the combination of Taguchi and RSM methods could successfully optimize the energy recovery potential with consideration of improving the hydraulic parameters of WDN. With the application of RSM and Taguchi, high potential positions for MHP placement are detected and analyzed based on a high-performance operational decision-making methodology. According to Artificial Intelligence (AI) computations, energy harvesting and hydraulic responses can be estimated with more than a 99 % correlation coefficient. Also, it shows that the soft-operator can be executed to control the features of MHPs in WDNs. The outputs of this research demonstrated that MHP harvested energy is more than 400KW for the run time of this study with consideration of hydraulic parameters.}
}
@article{SURYANARAYANA2024100495,
title = {Artificial Intelligence Enhanced Digital Learning for the Sustainability of Education Management System},
journal = {The Journal of High Technology Management Research},
volume = {35},
number = {2},
pages = {100495},
year = {2024},
issn = {1047-8310},
doi = {https://doi.org/10.1016/j.hitech.2024.100495},
url = {https://www.sciencedirect.com/science/article/pii/S104783102400004X},
author = {K.S. Suryanarayana and V.S. Prasad Kandi and G. Pavani and Akuthota Sankar Rao and Sandeep Rout and T. {Siva Rama Krishna}},
keywords = {Artificial intelligence, Digital education, Sustainability, Role of AI in education, Educational management},
abstract = {Maintenance schedules are scheduled ahead of time and automatically based on the continuous monitoring of the equipment by statistical methods, thanks to artificial intelligence-enabled digital transformation and the best fit model based on Machine Management Index in a pedagogical system. One of the most important aspects of universities is the widespread use of machine learning methods to evaluate students' progress. Machine learning approaches are designed to speed up the learning process without sacrificing accuracy. The dynamics of teaching and learning have shifted since the introduction of modern technological tools. The educational system as a whole has changed and developed over time. These days, people can get an education outside of the classroom as well, thanks to the proliferation of online courses and resources. Everyone's professional life begins with their education. By analyzing past data, artificial intelligence methods can resolve existing problems. When applied properly, artificial intelligence can be a highly efficient method for solving problems with a predictable and repeatable solution space. The learner's personality can be predicted based on a number of factors using machine learning approaches. This article examines how AI may improve digital learning in education management systems to sustain the education ecosystem. AI in education improves student results, learning experiences, and administrative processes. This study discusses AI applications in education management systems and associated problems and opportunities. We also explore ethical issues and the roadmap for using AI to improve education. Educational institutions can provide individualized curriculum for students based on their unique personalities and areas of interest. Institutions of higher learning can benefit greatly from this instrument for personality prediction by recommending a course of study that will better prepare students to enter the field of their choice and achieve professional success.}
}
@article{REMINGTON2018938,
title = {A Dynamical Systems Perspective on Flexible Motor Timing},
journal = {Trends in Cognitive Sciences},
volume = {22},
number = {10},
pages = {938-952},
year = {2018},
note = {Special Issue: Time in the Brain},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2018.07.010},
url = {https://www.sciencedirect.com/science/article/pii/S1364661318301724},
author = {Evan D. Remington and Seth W. Egger and Devika Narain and Jing Wang and Mehrdad Jazayeri},
keywords = {dynamical systems, flexible timing, sensorimotor control, learning, movement sequences, movement planning},
abstract = {A hallmark of higher brain function is the ability to rapidly and flexibly adjust behavioral responses based on internal and external cues. Here, we examine the computational principles that allow decisions and actions to unfold flexibly in time. We adopt a dynamical systems perspective and outline how temporal flexibility in such a system can be achieved through manipulations of inputs and initial conditions. We then review evidence from experiments in nonhuman primates that support this interpretation. Finally, we explore the broader utility and limitations of the dynamical systems perspective as a general framework for addressing open questions related to the temporal control of movements, as well as in the domains of learning and sequence generation.}
}
@article{YOUNG1987309,
title = {The metaphor machine: A database method for creativity support},
journal = {Decision Support Systems},
volume = {3},
number = {4},
pages = {309-317},
year = {1987},
issn = {0167-9236},
doi = {https://doi.org/10.1016/0167-9236(87)90102-3},
url = {https://www.sciencedirect.com/science/article/pii/0167923687901023},
author = {Lawrence F Young},
keywords = {Creativity Support Systems, Methaphor Generation by Computer, Database Methods for Metaphor Generation, Idea Processing Support, Computer Support of Metaphorical Thinking, Support of Creative Thinking, Qualitative Support Systems, A Relational Calculus for Metaphor Generation, Relational Database Methods for Metaphor Generation, Interactive Support Systems for Creativity, Right-Brained Support Systems, Relational Algebra for Metaphor Generation, Database Structures for Metaphor Generation, Computer Support of Divergent Thinking},
abstract = {This paper shows how a data base method can be applied to the automatic generation of metaphors. The utility of automatic metaphor generation is based on providing interactive support to creative human thinking processes. Such interactive support systems have been called Idea Processing systems, and are seen as special qualitative types of Decision Support Systems (DSS). They include functions to support metaphorical thinking as well as other modes of creative idea development. The paper presents brief backgrounds references on creativity and the relevance of metaphors, as well as to previous work in Idea Processing. It then presents a relational data base method for automatic metaphor generation. The method is described and illustrated, as well as shown in relational algebra and relational calculus notation. In conclusion, the paper indicates how the relational data base method presented can be operationalized through using existing data base software or by integration with a specialized interface for the particular application of metaphor generation.}
}