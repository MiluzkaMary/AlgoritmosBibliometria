@article{BRUNDAGE201532,
title = {Taking superintelligence seriously: Superintelligence: Paths, dangers, strategies by Nick Bostrom (Oxford University Press, 2014)},
journal = {Futures},
volume = {72},
pages = {32-35},
year = {2015},
note = {Confronting Future Catastrophic Threats To Humanity},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2015.07.009},
url = {https://www.sciencedirect.com/science/article/pii/S0016328715000932},
author = {Miles Brundage},
keywords = {Existential risk, Artificial intelligence, Superintelligence, Responsible innovation},
abstract = {A new book by Nick Bostrom, Superintelligence: Paths, Dangers, Strategies, is reviewed. Superintelligence explores the future of artificial intelligence and related technologies and the risks they may pose to human civilization. The book ably demonstrates the potential for serious thinking aimed at the long-term future. Bostrom succeeds in arguing that the development of superintelligent machines will, if not properly managed, create catastrophic risks to humanity. The book falls short in some respects, and some sections are more compelling and novel than others. Overall, however, Bostrom’s book succeeds in demolishing the “null hypothesis” according to which the possibility and risks of superintelligence can continue to be ignored, and is a must-read for those interested in the long-term future of humanity.}
}
@article{GAO20222707,
title = {Similarity reductions for a generalized (3+1)-dimensional variable-coefficient B-type Kadomtsev–Petviashvili equation in fluid dynamics},
journal = {Chinese Journal of Physics},
volume = {77},
pages = {2707-2712},
year = {2022},
issn = {0577-9073},
doi = {https://doi.org/10.1016/j.cjph.2022.04.014},
url = {https://www.sciencedirect.com/science/article/pii/S0577907322001228},
author = {Xin-Yi Gao and Yong-Jiang Guo and Wen-Rui Shan},
keywords = {Fluid dynamics, Generalized (3+1)-dimensional variable-coefficient B-type Kadomtsev–Petviashvili equation, Similarity reductions, Symbolic computation},
abstract = {Rather intriguing, the paper Chin. J. Phys. 73 (2021) 600-612 has studied a (3+1)-dimensional B-type Kadomtsev–Petviashvili equation in fluid dynamics, while fluid dynamics has a wide range of applications, including those for geophysics, mechanical engineering, civil engineering, chemical engineering, astrophysics and biology. In this paper, taking into consideration certain nonlinear waves in fluid dynamics, we investigate a generalized variable-coefficient version of the aforementioned equation. Making use of symbolic computation, with respect to the amplitude or elevation of the relevant wave, we construct out two sets of the similarity reductions, which rely on the variable coefficients in the generalized equation.}
}
@article{LOMBARDI2022100601,
title = {Understanding emerging patterns and dynamics through the lenses of the cyber-physical universe},
journal = {Patterns},
volume = {3},
number = {11},
pages = {100601},
year = {2022},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2022.100601},
url = {https://www.sciencedirect.com/science/article/pii/S2666389922002264},
author = {Mauro Lombardi and Simone Vannuccini},
keywords = {cyber-physical universe, ubiquitous computing, information technology, artificial intelligence, decision making},
abstract = {Summary
The complex interaction among contemporary techno- and socio-economic processes has set the stage for the emergence of a cyber-physical universe, the novel landscape in which agents behave and interact, and which is centered on the fundamental role played by information and computation at all levels. In this paper, we weave into a single analysis the different threads that lead to (and characterize) the cyber-physical universe and outline a map of its building blocks and the complex dynamics at work in the new environment. The resulting description is used to assess how decision-making processes should evolve in order to be able to address the opportunities and challenges of the current era of deep and extended changes. The analysis offers an encompassing interpretative grid to understand and unpack patterns in the contemporary socio-technical systems that experience a fundamental informational turn; this can inform new research trajectories and help open up new areas for scientific inquiry.}
}
@article{DEY2016177,
title = {A probabilistic approach to diagnose faults of air handling units in buildings},
journal = {Energy and Buildings},
volume = {130},
pages = {177-187},
year = {2016},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2016.08.017},
url = {https://www.sciencedirect.com/science/article/pii/S0378778816306958},
author = {Debashis Dey and Bing Dong},
keywords = {Air Handling Unit, Bayesian belief network, APAR rules, Fault detection and diagnosis},
abstract = {Air handling unit (AHU) is one of the most extensively used equipment in large commercial buildings. This device is typically customized and lacks quality system integration which can result in hardwire failures and control errors. Air handling unit Performance Assessment Rules (APAR) is a fault detection tool that uses a set of expert rules derived from mass and energy balances to detect faults in air handling units. APAR is computationally simple enough that it can be embedded in commercial building automation and control systems and relies only upon sensor data and control signals that are commonly available in these systems. Although APAR has advantages over other methods, for example no training data required and easy to implement commercially, most of the time it is unable to provide the root diagnosis of the faults. For instance, a fault on temperature sensor could be bias, drifting bias, inappropriate location, or complete failure. In addition a fault in mixing box can be return and/or outdoor damper leak or stuck. In addition, when multiple rules are satisfied, the list of faults increases. There is no proper way to have the correct diagnosis for rule based fault detection system. To overcome this limitation, we proposed Bayesian Belief Network (BBN) as a diagnostic tool. BBN can be used to simulate diagnostic thinking of FDD experts through a probabilistic way. In this study we developed a new way to detect and diagnose faults in AHU through combining APAR rules and Bayesian Belief network. Bayesian Belief Network is used as a decision support tool for rule based expert system. BBN is highly capable to prioritize faults when multiple rules are satisfied simultaneously. Also it can get information from previous AHU operating conditions and maintenance records to provide proper diagnosis. The proposed model is validated with real time measured data of a campus building. The results show that BBN correctly prioritize faults that are verified by manual investigation.}
}
@article{ZHOU2022100001,
title = {Science in One Health: A new journal with a new approach},
journal = {Science in One Health},
volume = {1},
pages = {100001},
year = {2022},
issn = {2949-7043},
doi = {https://doi.org/10.1016/j.soh.2022.100001},
url = {https://www.sciencedirect.com/science/article/pii/S2949704322000014},
author = {Xiao-Nong Zhou and Marcel Tanner},
keywords = {One Health, Human health, Animal health, Ecosystem health, Research and implementation science},
abstract = {One Health recognizes the close links and interdependence among human health, animal health and environmental health. With the pandemic of COVID-19 and the risk of many emerging or reemerging infectious diseases of zoonotic nature as well as the spreading antimicrobial resistance, One Health has become one of top concerns globally, as it entails the essential global public health challenges from antimicrobial resistance over zoonoses, to climate change, food security and societal well-being. Research priorities in One Health include the study on interactions of human-animal-plants-nature ecology interface, systems thinking, integrated surveillance and response systems, and the overall One Health governance as part of the global health and sustainability governance. The now launched journal, Science in One Health, aims to be a resource platform that disseminates scientific evidence, knowledge, and tools on the One Health approaches and respective possible socio-ecological interventions. Thus, aims at providing fruitful exchanges of information and experience among researchers, and decision makers as well as public health actors.}
}
@article{MATINFAR2025100304,
title = {Unmasking the brain in cocaine use disorder: A deep learning approach with graph convolutional networks and principal component analysis},
journal = {Next Research},
volume = {2},
number = {2},
pages = {100304},
year = {2025},
issn = {3050-4759},
doi = {https://doi.org/10.1016/j.nexres.2025.100304},
url = {https://www.sciencedirect.com/science/article/pii/S3050475925001757},
author = {Mohammad-Mehdi Matinfar and Mozafar Bag-Mohammdi and Mojtaba Karami},
keywords = {Deep learning, Cocaine use disorder, Graph conventional network, Principal component analysis, fMRI analysis},
abstract = {The SUDMEX CONN dataset, comprising extensive brain imaging data and demographic information, serves as a valuable resource for exploring the neurobiological mechanisms underlying Cocaine Use Disorder (CUD). In this study, we employed Graph Convolutional Networks (GCNs) to diagnose CUD using resting-state functional magnetic resonance imaging (rs-fMRI) data. Kernel Principal Component Analysis (PCA) was utilized for dimensionality reduction and enhancing computational efficiency. We construct brain graphs from the augmented rs-fMRI images and use GCN for the classification task. The simulation results demonstrate that the GCN model, trained on the SUDMEX CONN dataset, can accurately distinguish between CUD patients and healthy controls (HC) based on brain connectivity patterns. Our approach provides valuable insights into the underlying neurobiological mechanisms of CUD and highlights the potential of innovative tools for understanding and treating this chronic disorder. Ablation studies confirmed kernel PCA is highly efficient in reducing the dataset dimensionality and accelerating the simulation time. The simulation results revealed that the combined approach of GCN and kernel PCA could achieve an impressive accuracy of 98.25 % in diagnosing CUD cases. Our implementation codes are available at https://github.com/MehdiMatinfar/GPCA.}
}
@article{KNYAZEV201817,
title = {Resting state connectivity mediates the relationship between collectivism and social cognition},
journal = {International Journal of Psychophysiology},
volume = {123},
pages = {17-24},
year = {2018},
issn = {0167-8760},
doi = {https://doi.org/10.1016/j.ijpsycho.2017.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S0167876017305470},
author = {Gennady G. Knyazev and Alexander N. Savostyanov and Andrey V. Bocharov and Ekaterina A. Merkulova},
keywords = {Collectivism, Social cognition, Medial prefrontal cortex, Connectivity, Mediation analysis},
abstract = {Humans are intrinsically social beings and it is natural that self-processing is associated with social cognition. The degree to which the self is perceived as a part of social environment is modulated by cultural stereotypes, such as collectivism and individualism. Here, we tested the hypothesis that individuals who endorse collectivist values would spontaneously think more about their relationships with other people and this association would be mediated by connectivity between the medial prefrontal cortex (MPFC) and the rest of the brain. Connectivity was evaluated based on resting state EEG data using the recently developed methods, which combine beamformer spatial filtering with seed based connectivity estimation. The formal mediation analysis revealed that collectivism is associated with an enhanced connectivity of MPFC with a set of cortical regions that are frequently co-activated in moral reasoning, empathy, and theory of mind tasks and with diminished connectivity with the precuneus\posterior cingulate cortex, which is involved in self-centered cognition. The relationship between collectivism and social cognition was mediated by MPFC connectivity with the left middle temporal gyrus implying that in participants with collectivistic attitude, thinking about relationships with other people may be associated with semantic memory retrieval and reasoning on moral issues and others' intentions.}
}
@article{COOK201895,
title = {An investigation of an undergraduate student’s reasoning with zero-divisors and the zero-product property},
journal = {The Journal of Mathematical Behavior},
volume = {49},
pages = {95-115},
year = {2018},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2017.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S0732312317301748},
author = {John Paul Cook},
keywords = {Abstract algebra, Zero-product property, Zero-divisors, Equation solving, Student thinking, Realistic Mathematics Education},
abstract = {The zero-product property (ZPP), often stated as ‘if ab = 0, then a = 0 or b = 0,’ is an important concept in secondary algebra (as a tool for solving equations) and abstract algebra (as a property of integral domains). This study analyzes results from a teaching experiment to investigate how an undergraduate mathematics major might intuitively reason with zero-divisors and the ZPP. There are two primary findings. First, a procedurally embodied view of equation solving might preclude students’ attention to the algebraic properties (including the ZPP) that justify the equivalence of two equations. Second, students might not carefully attend to zero-divisors because they are employing the converse of the ZPP instead of the ZPP itself. These findings advance a hypothesis about why students might view abstract algebra as a different subject than school algebra and also affirm the utility of the student-centered theoretical perspective that guided the instructional design and analysis of student activity.}
}
@article{EVANS2003454,
title = {In two minds: dual-process accounts of reasoning},
journal = {Trends in Cognitive Sciences},
volume = {7},
number = {10},
pages = {454-459},
year = {2003},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2003.08.012},
url = {https://www.sciencedirect.com/science/article/pii/S1364661303002250},
author = {Jonathan St.B.T. Evans},
abstract = {Researchers in thinking and reasoning have proposed recently that there are two distinct cognitive systems underlying reasoning. System 1 is old in evolutionary terms and shared with other animals: it comprises a set of autonomous subsystems that include both innate input modules and domain-specific knowledge acquired by a domain-general learning mechanism. System 2 is evolutionarily recent and distinctively human: it permits abstract reasoning and hypothetical thinking, but is constrained by working memory capacity and correlated with measures of general intelligence. These theories essentially posit two minds in one brain with a range of experimental psychological evidence showing that the two systems compete for control of our inferences and actions.}
}
@article{VONRICHTHOFEN2018573,
title = {The ‘Urban Elements’ method for teaching parametric urban design to professionals},
journal = {Frontiers of Architectural Research},
volume = {7},
number = {4},
pages = {573-587},
year = {2018},
issn = {2095-2635},
doi = {https://doi.org/10.1016/j.foar.2018.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S209526351830044X},
author = {Aurel {von Richthofen} and Katja Knecht and Yufan Miao and Reinhard König},
keywords = {Urban design education, Parametric urban design, Singapore, Urban Elements},
abstract = {The article proposes a method for teaching advanced urban design to working professionals in Singapore. The article aims to expand the discourse on parametric urban design education by introducing ‘Urban Elements’ as conceptual urban design instruments with an inherent rule-based logic, which can help to bridge gaps in teaching parametric urban design thinking. As case study we present a course developed for and delivered to the Urban Redevelopment Authority (URA) in Singapore in 2017 by the Future Cities Laboratory at the Singapore-ETH Centre. The article reports on the pedagogical method, course results and course feedback. The main difficulties of teaching professionals in parametric urban design are described and possible reasons and improvements are discussed. The results show that participants using the ‘Urban Elements’ method successfully linked theoretical input to urban design problems, applied evidence-based urban design strategies to these problems, and developed parametric definitions to explore the solution spaces of these urban design challenges. The teaching methodology presented opens up a new research field for urban design pedagogy at the intersection of explicating urban design intent, integrating multidisciplinary knowledge and exploring new software driven tools.}
}
@article{LAVALLEY2024108825,
title = {Transdiagnostic failure to adapt interoceptive precision estimates across affective, substance use, and eating disorders: A replication and extension of previous results},
journal = {Biological Psychology},
volume = {191},
pages = {108825},
year = {2024},
issn = {0301-0511},
doi = {https://doi.org/10.1016/j.biopsycho.2024.108825},
url = {https://www.sciencedirect.com/science/article/pii/S030105112400084X},
author = {Claire A. Lavalley and Navid Hakimi and Samuel Taylor and Rayus Kuplicki and Katherine L. Forthman and Jennifer L. Stewart and Martin P. Paulus and Sahib S. Khalsa and Ryan Smith},
keywords = {Interoception, Depression, Anxiety, Substance use, Eating disorders, Precision, Priors, Bayesian perception, Computational modeling},
abstract = {Recent Bayesian theories of interoception suggest that perception of bodily states rests upon a precision-weighted integration of afferent signals and prior beliefs. In a previous study, we fit a computational model of perception to behavior on a heartbeat tapping task to test whether aberrant precision-weighting could explain misestimation of cardiac states in psychopathology. We found that, during an interoceptive perturbation designed to amplify afferent signal precision (inspiratory breath-holding), healthy individuals increased the precision-weighting assigned to ascending cardiac signals (relative to resting conditions), while individuals with anxiety, depression, substance use disorders, and/or eating disorders did not. In this pre-registered study, we aimed to replicate and extend our prior findings in a new transdiagnostic patient sample (N = 285) similar to the one in the original study. As expected, patients in this new sample were also unable to adjust beliefs about the precision of cardiac signals – preventing the ability to accurately perceive changes in their cardiac state. Follow-up analyses combining samples from the previous and current study (N = 719) also afforded power to identify group differences between narrower diagnostic categories, and to examine predictive accuracy when logistic regression models were trained on one sample and tested on the other. With this confirmatory evidence in place, future studies should examine the utility of interoceptive precision measures in predicting treatment outcomes and test whether these computational mechanisms might represent novel therapeutic targets.}
}
@article{EVANS2022281,
title = {The explainability paradox: Challenges for xAI in digital pathology},
journal = {Future Generation Computer Systems},
volume = {133},
pages = {281-296},
year = {2022},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2022.03.009},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X22000838},
author = {Theodore Evans and Carl Orge Retzlaff and Christian Geißler and Michaela Kargl and Markus Plass and Heimo Müller and Tim-Rasmus Kiehl and Norman Zerbe and Andreas Holzinger},
keywords = {Explainable AI, Digital pathology, Usability, Trust, Artificial intelligence},
abstract = {The increasing prevalence of digitised workflows in diagnostic pathology opens the door to life-saving applications of artificial intelligence (AI). Explainability is identified as a critical component for the safety, approval and acceptance of AI systems for clinical use. Despite the cross-disciplinary challenge of building explainable AI (xAI), very few application- and user-centric studies in this domain have been carried out. We conducted the first mixed-methods study of user interaction with samples of state-of-the-art AI explainability techniques for digital pathology. This study reveals challenging dilemmas faced by developers of xAI solutions for medicine and proposes empirically-backed principles for their safer and more effective design.}
}
@article{NG2024100090,
title = {Using cospaces in augmented reality digital story creation: A thematic analysis},
journal = {Computers & Education: X Reality},
volume = {5},
pages = {100090},
year = {2024},
issn = {2949-6780},
doi = {https://doi.org/10.1016/j.cexr.2024.100090},
url = {https://www.sciencedirect.com/science/article/pii/S2949678024000400},
author = {Davy Tsz Kit Ng and Wan Yee Winsy Lai and Morris Siu-yung Jong and Chi Wui Ng},
keywords = {Digital storytelling, CoSpaces, Online community, Augmented reality, Language learning},
abstract = {With the digital affordances of augmented reality (AR) technologies, research has shown their value for contextualized, interactive and collaborative language learning through supporting real-world immersion. In recent years, CoSpaces has been a popular AR learning tool with an extensive library of 3D models and constructive gadgets, as well as a visual programming platform. With this tool, students can create projects of digital stories by building personalized AR artifacts, scenes, and storylines, and then share their projects in a dynamic and global community of children. This study examined the characteristics of 39 selected CoSpaces’ open projects via thematic analysis and categorization into five learning contexts: (1) art, history, culture and design, (2) STEM, (3) classroom English and everyday communication, (4) fairy tale/literature, and (5) campus tour. Furthermore, this study identified six language learning competencies derived from digital story creation: (1) discovering knowledge, (2) connecting to prior experience and knowledge, (3) conducting research, (4) problem-solving, (5) expressing and creating digitally, as well as (6) presenting, appreciating and evaluating. Digital literacy refers to the ability to use technology to find, evaluate, create, and communicate information. In addition, three major types of digital literacy skills necessary for AR digital storytelling processes have been identified, encompassing digital creativity, technoligcal proficiency, and research skills. Our results contribute to discovering educational values in developing digital language competency through AR digital story creation. Recommendations are offered for future research and for educators to design appropriate AR learning experiences.}
}
@article{20213341,
title = {Tim Behrens},
journal = {Neuron},
volume = {109},
number = {21},
pages = {3341-3343},
year = {2021},
issn = {0896-6273},
doi = {https://doi.org/10.1016/j.neuron.2021.09.047},
url = {https://www.sciencedirect.com/science/article/pii/S0896627321007200},
abstract = {Summary
Tim Behrens discusses with Neuron creative ways to facilitate virtual meetings, the multiple ways that the pandemic has affected different people, and his advice for the younger generation of neuroscientists in general and computational scientists in particular.}
}
@article{PAMPLONA2020100189,
title = {An overview of air delay: A case study of the Brazilian scenario},
journal = {Transportation Research Interdisciplinary Perspectives},
volume = {7},
pages = {100189},
year = {2020},
issn = {2590-1982},
doi = {https://doi.org/10.1016/j.trip.2020.100189},
url = {https://www.sciencedirect.com/science/article/pii/S2590198220301007},
author = {Daniel Alberto Pamplona and Claudio Jorge Pinto Alves},
keywords = {Air delay, Air traffic flow, Problem-structuring method, Value-focused thinking},
abstract = {Delay is a key point in air transportation activity. As a performance metric, it affects common policy concerns. Delay impacts passenger satisfaction and imposes costs. The complexity that sets in for the air traffic manager is how to mitigate delay, especially in an environment with several stakeholders. The present article applied a problem-structuring method (PSM), named value-focused thinking (VFT), to structure the problem of the air traffic flow management arrival delay. The inflexibility of incorporating a flight operator's specific needs is considered one of the reasons for the limited success of air traffic flow management (ATFM) programs. PSM allows participants to clarify their dilemmas, converge on a mutually liable problem, or agree to the proposed solutions and compromise on what partially solves the issue. The problem is that most papers focus only on the applied solution for air delay mitigation. Before implementing operational research techniques, we investigated the nature and characteristics of air delay. Results showed that there were several stakeholders with distinctive requirements for their business and many of their objectives are interconnected. The use of VFT provided an objective map that can be used as a guide for future solutions.}
}
@article{RASANAN2024857,
title = {Beyond discrete-choice options},
journal = {Trends in Cognitive Sciences},
volume = {28},
number = {9},
pages = {857-870},
year = {2024},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2024.07.004},
url = {https://www.sciencedirect.com/science/article/pii/S136466132400175X},
author = {Amir Hosein Hadian Rasanan and Nathan J. Evans and Laura Fontanesi and Catherine Manning and Cynthia Huang-Pollock and Dora Matzke and Andrew Heathcote and Jörg Rieskamp and Maarten Speekenbrink and Michael J. Frank and Stefano Palminteri and Christopher G. Lucas and Jerome R. Busemeyer and Roger Ratcliff and Jamal Amani Rad},
abstract = {While decision theories have evolved over the past five decades, their focus has largely been on choices among a limited number of discrete options, even though many real-world situations have a continuous-option space. Recently, theories have attempted to address decisions with continuous-option spaces, and several computational models have been proposed within the sequential sampling framework to explain how we make a decision in continuous-option space. This article aims to review the main attempts to understand decisions on continuous-option spaces, give an overview of applications of these types of decisions, and present puzzles to be addressed by future developments.}
}
@article{TALANOV201641,
title = {Emotional simulations and depression diagnostics},
journal = {Biologically Inspired Cognitive Architectures},
volume = {18},
pages = {41-50},
year = {2016},
issn = {2212-683X},
doi = {https://doi.org/10.1016/j.bica.2016.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S2212683X16300676},
author = {Max Talanov and Jordi Vallverdú and Bin Hu and Philip Moore and Alexander Toschev and Diana Shatunova and Anzhela Maganova and Denis Sedlenko and Alexey Leukhin},
keywords = {Dopamine, Serotonin, Fear, Artificial intelligence, Simulation, Rat brain, Affective computing, Emotion modelling, Neuromodulation},
abstract = {In this work we propose the following hypothesis: the neuromodulatory mechanisms that control the emotional states of mammals can be translated and re-implemented in a computer by controlling the computational performance of a hosted computational system. In our specific implementation, we represent the simulation of the ‘fear-like’ state based on the three dimensional neuromodulatory model of affects, in this paper ‘affects’ refer to the basic emotional inborn states, inherited from works of Hugo Lövheim. Whilst dopamine controls attention, serotonin is the key for inhibition, and fear is a elicitator for inhibitory and protective processes. This inhibition can promote [in a cognitive system] to blocking behaviour which can be labelled as ’depression’. Therefore, our interest is how to reimplement biomimetically both action-regulators without the computational system to resulting in a ‘failed’ scenario. We have simulated 1000ms of the dopamine system using NEST Neural Simulation Tool with the rat brain as the model. The results of the simulation experiments are reported with an evaluation to demonstrate the correctness of our hypothesis.}
}
@article{KANCHANATAWAN2018168,
title = {Affective symptoms in schizophrenia are strongly associated with neurocognitive deficits indicating disorders in executive functions, visual memory, attention and social cognition},
journal = {Progress in Neuro-Psychopharmacology and Biological Psychiatry},
volume = {80},
pages = {168-176},
year = {2018},
note = {Peripheral markers of inflammation, oxidative & nitrosative stress pathways and memory functions as a new target of pharmacotherapy in depression},
issn = {0278-5846},
doi = {https://doi.org/10.1016/j.pnpbp.2017.06.031},
url = {https://www.sciencedirect.com/science/article/pii/S027858461730129X},
author = {Buranee Kanchanatawan and Supaksorn Thika and George Anderson and Piotr Galecki and Michael Maes},
keywords = {Major depression, Bipolar, Anxiety, Schizophrenia, CANTAB, Cognition},
abstract = {The aim of this study was to assess the neurocognitive correlates of affective symptoms in schizophrenia. Towards this end, 40 healthy controls and 80 schizophrenia patients were investigated with six tests of the Cambridge Neuropsychological Test Automated Battery (CANTAB), assessing spatial working memory, paired-association learning, one touch stocking, rapid visual information (RVP), emotional recognition test and intra/extradimensional set shifting. The Hamilton Depression (HDRS) and Anxiety (HAMA) Rating Scales and the Calgary Depression Scale for Schizophrenia (CDSS) as well as the Positive and Negative Syndrome Scale (PANSS) were also used. There were highly significant associations between all 6 CANTAB tests and HDRS, HAMA and CDSS (except RVP) scores. The most significant items associating with neurocognitive impairments in schizophrenia were self-depreciation (CDSS), fatigue, psychomotor retardation and agitation, psychic and somatic anxiety (HDRS), fears, cognitive symptoms, somatic-muscular, genito-urinary and autonomic symptoms and anxious behavior (HAMA). The selected HDRS and HAMA symptoms indicate fatigue, fears, anxiety, agitation, retardation, somatization and subjective cognitive complaints (SCC) and are therefore labeled “FAARS”. Up to 28.8% of the variance in the 6 CANTAB measurements was explained by FAARS, which are better predictors of neurocognitive impairments than the PANSS negative subscale score. Neurocognitive deficits in schizophrenia are best predicted by FAARS combined with difficulties in abstract thinking. In conclusion, depression and anxiety symptoms accompanying the negative and positive symptoms of schizophrenia are associated with neurocognitive deficits indicating disorders in executive functions, attention, visual memory, and social cognition. Neurocognitive deficits in schizophrenia reflect difficulties in abstract thinking and FAARS, including subjective cognitive complaints.}
}
@article{HARWOOD201610,
title = {Locking up passwords – for good},
journal = {Network Security},
volume = {2016},
number = {4},
pages = {10-13},
year = {2016},
issn = {1353-4858},
doi = {https://doi.org/10.1016/S1353-4858(16)30037-X},
url = {https://www.sciencedirect.com/science/article/pii/S135348581630037X},
author = {Will Harwood},
abstract = {It's clear that bulk identity thefts – that is, the mass stealing of passwords or other personally identifiable information (PII) – are among the most harmful types of cyber-attack faced by businesses. They're a huge problem, not only in terms of the damage each attack causes, but also the volume of attacks overall. A cursory glance over the business headlines for the past few years announces huge password or PII thefts from organisations ranging from Sony PlayStation to eBay and Facebook to JP Morgan. We were barely a week into 2016 when it was revealed that email passwords for up to 320,000 users had been stolen from Time Warner. Bulk identity thefts are among the most harmful types of cyber-attack faced by businesses today and part of the problem is that businesses, security firms and cyber-criminals all share the same playing field. Thinking beyond standard computing architectures is the only solution to the ongoing arms race between hackers and security vendors. In a battle against cyber-criminality, in which businesses are always playing catch-up, this is a way of getting on the front foot and beginning to operate in a world beyond the attackers' reach, says Dr Will Harwood of Silicon:SAFE.}
}
@article{MIHAI20221082,
title = {Multimodal emotion detection from multiple data streams for improved decision making},
journal = {Procedia Computer Science},
volume = {214},
pages = {1082-1089},
year = {2022},
note = {9th International Conference on Information Technology and Quantitative Management},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.11.281},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922019937},
author = {Neghina Mihai and Matei Alexandru and Zamfirescu Bala-Constantin},
keywords = {emotion detection, sensor fusion, multimodal, affect},
abstract = {Recent neurological studies shows that emotions are tightly connected to the thinking and cognitive actions, being part of the decision-making process. Considering this, having a way to help decision making processes based on current emotion of the user or to consider the potential emotional impact if a decision is made, would be beneficial. This paper introduces a novel method for fusing multiple emotional signals, using a weighted average, where each weight value adapts to real time conditions, based on signal type, presence, and quality. In the context of a training station for manual operation, we implemented and tested separately several emotion detection methods, each based on a different signal acquired from audio, video, and galvanic skin response data streams. The final goal is to include the proposed method together with state of the art emotion detection machine learning algorithms as part of the digital twin training station for manual operation.}
}
@article{SHIVERSMCNAIR201836,
title = {User-Centered Design In and Beyond the Classroom: Toward an Accountable Practice},
journal = {Computers and Composition},
volume = {49},
pages = {36-47},
year = {2018},
note = {User-Centered Design and Usability in the Composition Classroom},
issn = {8755-4615},
doi = {https://doi.org/10.1016/j.compcom.2018.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S8755461518300379},
author = {Ann Shivers-McNair and Joy Phillips and Alyse Campbell and Hanh H. Mai and Alice Yan and John Forrest Macy and James Wenlock and Savannah Fry and Yishan Guan},
keywords = {user-centered design, user experience, usability testing, design thinking},
abstract = {The authors, an instructor and students, describe our practice of user-centered design on three levels: in the design and structure of an advanced undergraduate course in which we all participated, in student projects designed during the course, and in our reflections on the course presented here. We argue that principles of user-centered design can and should be more than course concepts and assignments; they can be core practices of the course that hold both students and teachers accountable for the impacts of their rhetorical choices. We offer a model for other teacher-scholars looking to involve students in the design of their courses and in writing together about their work.}
}
@article{SPARAPANI2023102186,
title = {Factors associated with classroom participation in preschool through third grade learners on the autism spectrum},
journal = {Research in Autism Spectrum Disorders},
volume = {105},
pages = {102186},
year = {2023},
issn = {1750-9467},
doi = {https://doi.org/10.1016/j.rasd.2023.102186},
url = {https://www.sciencedirect.com/science/article/pii/S1750946723000867},
author = {Nicole Sparapani and Nancy Tseng and Laurel Towers and Sandy Birkeneder and Sana Karimi and Cameron J. Alexander and Johanna Vega Garcia and Taffeta Wood and Amanda Dimachkie Nunnally},
keywords = {Autism, Instructional opportunities, Mathematical tasks, Teacher language, Active engagement, Spontaneous communication},
abstract = {Background
Access to mathematics instruction that involves opportunities for critical thinking and procedural fluency promotes mathematics learning. Studies have outlined effective strategies for teaching mathematics to children on the autism spectrum, however, the focus of these interventions often represent a narrow set of mathematical skills and concepts centered on procedural learning without linking ideas to underlying concepts.
Methods
This study utilized classroom video observations to evaluate the variability in and nature of mathematical learning opportunities presented to 76 autistic students within 49 preschool–3rd grade general and special education learning contexts. We examined teacher instructional practices and student participation across 109 mathematical tasks within larger mathematics lessons.
Results
Students were most often presented with mathematical tasks that required low-level cognitive demand, such as tasks focusing on rote memorization and practicing predetermined steps to solve basic algorithms. Furthermore, the nature of the mathematical task was linked with the language that teachers used, and this in turn, was associated with students’ participation within the learning opportunity.
Conclusions
Our findings indicate that features of talk within specific types of mathematical tasks, including math-related talk and responsive language, were associated with increased student active engagement and spontaneous communication. The knowledge gained from this study contributes to the development of optimized instructional practices for school-aged children on the autism spectrum—information that could be used to prepare both preservice and in-service teachers.}
}
@article{YANG2024109519,
title = {Global optimization strategy of prosumer data center system operation based on multi-agent deep reinforcement learning},
journal = {Journal of Building Engineering},
volume = {91},
pages = {109519},
year = {2024},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2024.109519},
url = {https://www.sciencedirect.com/science/article/pii/S2352710224010878},
author = {Dongfang Yang and Xiaoyuan Wang and Rendong Shen and Yang Li and Lei Gu and Ruifan Zheng and Jun Zhao and Xue Tian},
keywords = {Data center system, Global cooperative optimization, D3QN, VDN},
abstract = {The escalating issues of high energy consumption and carbon emissions in data centers (DCs) necessitate the optimization of system operations. However, early optimization strategies were overly simplistic and lacked automated updating and iterative capabilities. With the evolution of artificial intelligence (AI), researchers have applied deep reinforcement learning (DRL) algorithms to system operations. However, the optimization focus has been limited to the internal systems, lacking global optimization. In this paper, a global optimization control strategy based on the Dueling double-deep Q network (D3QN) and value decomposition network (VDN) algorithms is proposed to make the DCs system operate more closely with the upstream, midstream, and downstream. By adjusting battery charging/discharging capacity, computational workload, and waste heat utilization heating temperature global synergistic optimization is achieved. Compared with without optimization, renewable energy waste, operation cost, total electricity consumption, and grid electricity consumption are reduced by 18.37%, 9.78%, 4.01%, and 29.74%, respectively. Additionally, a detailed comparison between non-algorithmic optimization and algorithmic optimization is provided, offering valuable insights for substantial energy savings and emissions reduction in DCs. The results demonstrate the importance of fully exploring the interactive potential between upstream energy supply, midstream computational workload, and downstream waste heat recovery to achieve synergistic global optimization of “computing power", “thermal energy" and “electrical energy" for the sustainable and green development of DCs or other prosumer buildings.}
}
@article{WAHLHEIM2025380,
title = {Memory updating and the structure of event representations},
journal = {Trends in Cognitive Sciences},
volume = {29},
number = {4},
pages = {380-392},
year = {2025},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2024.11.008},
url = {https://www.sciencedirect.com/science/article/pii/S1364661324003152},
author = {Christopher N. Wahlheim and Jeffrey M. Zacks},
keywords = {memory updating, prediction error, interference, event cognition, pattern separation, hippocampus},
abstract = {People form memories of specific events and use those memories to make predictions about similar new experiences. Living in a dynamic environment presents a challenge: How does one represent valid prior events in memory while encoding new experiences when things change? There is evidence for two seemingly contradictory classes of mechanism: One differentiates outdated event features by making them less similar or less accessible than updated event features. The other integrates updated features of new events with outdated memories, and the relationship between them, into a structured representation. Integrative encoding may occur when changed events trigger inaccurate predictions based on remembered prior events. We propose that this promotes subsequent recollection of events and their order, enabling adaptation to environmental changes.}
}
@article{MOEBEHRENS2013e201304003,
title = {THE BIOLOGICAL MICROPROCESSOR, OR HOW TO BUILD A COMPUTER WITH BIOLOGICAL PARTS},
journal = {Computational and Structural Biotechnology Journal},
volume = {7},
number = {8},
pages = {e201304003},
year = {2013},
issn = {2001-0370},
doi = {https://doi.org/10.5936/csbj.201304003},
url = {https://www.sciencedirect.com/science/article/pii/S200103701460026X},
author = {Gerd HG Moe-Behrens},
abstract = {Systemics, a revolutionary paradigm shift in scientific thinking, with applications in systems biology, and synthetic biology, have led to the idea of using silicon computers and their engineering principles as a blueprint for the engineering of a similar machine made from biological parts. Here we describe these building blocks and how they can be assembled to a general purpose computer system, a biological microprocessor. Such a system consists of biological parts building an input / output device, an arithmetic logic unit, a control unit, memory, and wires (busses) to interconnect these components. A biocomputer can be used to monitor and control a biological system.}
}
@article{BARNES2021,
title = {Gene Expression and Data Analysis Pipeline Using Cancer BioPortal in the Classroom},
journal = {Journal of Microbiology & Biology Education},
volume = {22},
number = {1},
year = {2021},
issn = {1935-7877},
doi = {https://doi.org/10.1128/jmbe.v22i1.2315},
url = {https://www.sciencedirect.com/science/article/pii/S1935787721000277},
author = {Chassidy N. Barnes and Blake P. Johnson and Stefanie W. Leacock and Ruben M. Ceballos and Lori L. Hensley and Nathan S. Reyna},
abstract = {At institutions with an emphasis on authentic research experiences as an integral part of the biology curriculum, COVID created a huge challenge for course instructors whose learning objectives were designed for such experiences. Moving such laboratory experiences online when remote learning became necessary has resulted in a new model for CUREs that utilizes free online databases to provide not only a novel research experience for students, but also the opportunity to engage in big data analysis.
ABSTRACT
At institutions with an emphasis on authentic research experiences as an integral part of the biology curriculum, COVID created a huge challenge for course instructors whose learning objectives were designed for such experiences. Moving such laboratory experiences online when remote learning became necessary has resulted in a new model for CUREs that utilizes free online databases to provide not only a novel research experience for students, but also the opportunity to engage in big data analysis. Cancer BioPortal (cBioPortal) is an open-access collective cancer research resource for storing and exploring clinical, genomic, proteomic, and transcriptomic data. cBioPortal eliminates the computational barrier of interpreting complex genomic data by providing easily understandable visualization that can be interpreted and translated into relevant biological insights. Because no prior computational knowledge is required, cBioPortal is an ideal educational tool for either in-person or distance learning environments. We developed a pedagogical approach, video tutorials, and data analysis workflows centered on using cBioPortal. Pedagogically, students develop an initial research outline that is continually updated and graded throughout the project. Progress during the project or course is assessed by a series of student presentations that are 5 to 15 min in length and are aimed at explaining the approach used in data acquisition, interpretation of the data, and relevance to the initial hypothesis. While cancer-specific, this analysis platform appeals to a wide range of classes and student interests. Further, the project has been successfully done both as an independent research experience and as part of a virtual class-based research project.}
}
@article{MILDNER2019763,
title = {Spontaneous Thought as an Unconstrained Memory Process},
journal = {Trends in Neurosciences},
volume = {42},
number = {11},
pages = {763-777},
year = {2019},
issn = {0166-2236},
doi = {https://doi.org/10.1016/j.tins.2019.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S0166223619301626},
author = {Judith N. Mildner and Diana I. Tamir},
keywords = {spontaneous thought, memory, computational model, mind wandering, default network},
abstract = {The stream of thought can flow freely, without much guidance from attention or cognitive control. What determines what we think about from one moment to the next? Spontaneous thought shares many commonalities with memory processes. We use insights from computational models of memory to explain how the stream of thought flows through the landscape of memory. In this framework of spontaneous thought, semantic memory scaffolds episodic memory to form the content of thought, and drifting context modulated by one's current state – both internal and external – constrains the area of memory to explore. This conceptualization of spontaneous thought can help to answer outstanding questions such as: what is the function of spontaneous thought, and how does the mind select what to think about?}
}
@article{MENG201851,
title = {Conducting highly principled data science: A statistician’s job and joy},
journal = {Statistics & Probability Letters},
volume = {136},
pages = {51-57},
year = {2018},
note = {The role of Statistics in the era of big data},
issn = {0167-7152},
doi = {https://doi.org/10.1016/j.spl.2018.02.053},
url = {https://www.sciencedirect.com/science/article/pii/S0167715218300981},
author = {Xiao-Li Meng},
keywords = {Astrostatistics, Computational efficiency, Principled corner cutting, Scientific justification},
abstract = {Highly Principled Data Science insists on methodologies that are: (1) scientifically justified; (2) statistically principled; and (3) computationally efficient. An astrostatistics collaboration, together with some reminiscences, illustrates the increased roles statisticians can and should play to ensure this trio, and to advance the science of data along the way.}
}
@article{VEGA2008255,
title = {The catwalk task: Reflections and synthesis: Part 2},
journal = {The Journal of Mathematical Behavior},
volume = {27},
number = {4},
pages = {255-263},
year = {2008},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2009.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S0732312309000042},
author = {Emiliano Vega and Shawn Hicks},
keywords = {Modeling, Representation, Teacher learning, Task design},
abstract = {In this article we recount our experiences with a series of encounters with the catwalk task and reflect on the professional growth that these opportunities afforded. First, we individually reflect on our own mathematical work on the catwalk task. Second, we reflect on our experiences working with a group of community college students on the catwalk task and our interpretations of their mathematical thinking. In so doing we also detail a number of innovative and novel student-generated representations of the catwalk photos. Finally, we each individually reflect on the entire experience with the catwalk problem, as mathematics learners, as teachers, and as professionals.}
}
@incollection{STEEDMAN2011925,
title = {21 - Temporality},
editor = {Johan {van Benthem} and Alice {ter Meulen}},
booktitle = {Handbook of Logic and Language (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {London},
pages = {925-969},
year = {2011},
isbn = {978-0-444-53726-3},
doi = {https://doi.org/10.1016/B978-0-444-53726-3.00021-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780444537263000219},
author = {Mark Steedman},
keywords = {tense, aspect, natural language semantics, computational semantics, temporal semantics, aktionsarten, causality, evidentiality},
abstract = {Publisher Summary
In thinking about the logical and computational semantics of temporal categories in natural languages, issues of temporal ontology, or metaphysics, must be distinguished from issues of temporal relation. The first thing to observe about the temporal ontology implicit in natural languages is that it is not purely temporal. To take a simple example, the English perfect, when predicated of an event like losing a watch, says that some contextually retrievable consequences of the event in question hold at the time under discussion. Thus, conjoining such a perfect with a further clause denying those consequences is infelicitous. The claim that the semantics depends directly on the conceptual representation of action and contingency suggests that this semantics might be universal, despite considerable differences in its syntactic and morphological encoding across languages. The work described in this chapter suggests that such differences across languages are superficial. Ironically, the English tense/aspect system seems to be based on semantic primitives remarkably like those, which Whorf ascribed to Hopi. Matters of temporal sequence and temporal locality seem to be quite secondary to matters of perspective and contingency. This observation in turn suggests that the semantics of tense and aspect is profoundly shaped by concerns with goals, actions, and consequences, and that temporality in the narrow sense of the term is merely one facet of this system among many.}
}
@article{ROBINSON20231189,
title = {Opportunities and challenges for microbiomics in ecosystem restoration},
journal = {Trends in Ecology & Evolution},
volume = {38},
number = {12},
pages = {1189-1202},
year = {2023},
issn = {0169-5347},
doi = {https://doi.org/10.1016/j.tree.2023.07.009},
url = {https://www.sciencedirect.com/science/article/pii/S0169534723002112},
author = {Jake M. Robinson and Riley Hodgson and Siegfried L. Krauss and Craig Liddicoat and Ashish A. Malik and Belinda C. Martin and Jakki J. Mohr and David Moreno-Mateos and Miriam Muñoz-Rojas and Shawn D. Peddle and Martin F. Breed},
keywords = {ecosystem restoration, microbiome, microbiomics, metagenomics, restoration ecology, innovation},
abstract = {Microbiomics is the science of characterizing microbial community structure, function, and dynamics. It has great potential to advance our understanding of plant–soil–microbe processes and interaction networks which can be applied to improve ecosystem restoration. However, microbiomics may be perceived as complex and the technology is not accessible to all. The opportunities of microbiomics in restoration ecology are considerable, but so are the practical challenges. Applying microbiomics in restoration must move beyond compositional assessments to incorporate tools to study the complexity of ecosystem recovery. Advances in metaomic tools provide unprecedented possibilities to aid restoration interventions. Moreover, complementary non-omic applications, such as microbial inoculants and biopriming, have the potential to improve restoration objectives by enhancing the establishment and health of vegetation communities.}
}
@incollection{KERN202469,
title = {Chapter 5 - The turbinates—an overview},
editor = {Eugene Barton Kern and Oren Friedman},
booktitle = {Empty Nose Syndrome},
publisher = {Elsevier},
pages = {69-96},
year = {2024},
isbn = {978-0-443-10715-3},
doi = {https://doi.org/10.1016/B978-0-443-10715-3.00005-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780443107153000056},
author = {Eugene Barton Kern and Oren Friedman},
keywords = {Acetylcholine, secondary atrophic rhinitis, autonomic nervous system, turbinate anatomy, middle turbinate anatomy, inferior turbinate anatomy, capacitance vessels (sinusoids), “diffusor function,” functional residual capacity of the nose (FRCn), “,” hypertrophy (increase in cell ), hyperplasia (increase in cell ), nasal cycle, nasal obstruction, on-urgical urbinate eduction djunctive rocedure (n-sTRAP), out-fracture (lateralization), squamous metaplasia, submucous resection, ozaena, “resistor function,” total inferior turbinectomy, turbinates, turbinectomy, turbinoplasty, acoustic rhinometry, and rhinomanometry},
abstract = {This chapter presents an overview of the turbinates. To the best of our knowledge, it was a New Yorker, William M. Jarvis, MD, who in 1882 described three cases of utilizing a snare to affect a partial turbinectomy. At the dawn of the 20th century, most surgeons were promoting total inferior turbinectomy not only for nasal airway obstruction but astoundingly also for hearing impairment and tinnitus. The turbinate enlargement or “hypertrophy” is neither the cause nor a complication of hearing loss. Fortunately, and for the most part, dazed blunders and egregious errors in thinking by esteemed experts, for the most part, have remedied itself through scientific studies, since the late 19th century. This chapter traces the more than a century long history of turbinate thinking and surgery offering both sides of the turbinate debate in their “own words.” All the various procedures used to reduce the inferior turbinate are presented. To be as fair minded as possible, numerous authors are quoted, spanning more than one hundred years; some observed and reported the serious adverse effects of aggressive turbinate surgery, pleading for a conservative approach to inferior turbinate surgical intervention, while others claimed that turbinectomy was without any serious sequelae which is challenged by the facts.}
}
@article{LI2024e40037,
title = {The application and impact of artificial intelligence technology in graphic design: A critical interpretive synthesis},
journal = {Heliyon},
volume = {10},
number = {21},
pages = {e40037},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e40037},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024160689},
author = {Hong Li and Tao Xue and Aijia Zhang and Xuexing Luo and Lingqi Kong and Guanghui Huang},
keywords = {Atificial intelligence, Graphic design, Machine learning, Computer vision, Visual communication design, Systematic review},
abstract = {In the field of graphic design, the application of Artificial Intelligence (AI) is reshaping the design process. This study employs the Critical Interpretive Synthesis (CIS) approach to explore the impacts and challenges of AI on graphic design. Through a comprehensive review of 33 papers, this research reveals four research paradigms of AI in graphic design: Artificial Intelligence Driven Design Automation and Generation (AIDAG), Artificial Intelligence Assisted Graphic Design and Image Processing (AGDIP), Artificial Intelligence in Art and Creative Design Processes (AACDP), and Artificial Intelligence Enhanced Visual Attention and Emotional Response Modeling (AVERM). These paradigms demonstrate the multidimensional role of AI in design, ranging from automation to emotional interaction. The findings suggest that AI serves a dual role as both a design tool and a medium for innovation. AI not only enhances the automation and efficiency of the design process but also fosters designers' creative thinking and understanding of users' emotional needs. This study also proposes a path for the application of the four paradigms in the graphic design process, providing effective design ideas for future design workflows.}
}
@article{FAHIMI2024,
title = {Improving the Efficiency of Inferences From Hybrid Samples for Effective Health Surveillance Surveys: Comprehensive Review of Quantitative Methods},
journal = {JMIR Public Health and Surveillance},
volume = {10},
year = {2024},
issn = {2369-2960},
doi = {https://doi.org/10.2196/48186},
url = {https://www.sciencedirect.com/science/article/pii/S2369296024000188},
author = {Mansour Fahimi and Elizabeth C Hair and Elizabeth K Do and Jennifer M Kreslake and Xiaolu Yan and Elisa Chan and Frances M Barlas and Abigail Giles and Larry Osborn},
keywords = {hybrid samples, composite estimation, optimal composition factor, unequal weighting effect, composite weighting, weighting, surveillance, sample survey, data collection, risk factor},
abstract = {Background
Increasingly, survey researchers rely on hybrid samples to improve coverage and increase the number of respondents by combining independent samples. For instance, it is possible to combine 2 probability samples with one relying on telephone and another on mail. More commonly, however, researchers are now supplementing probability samples with those from online panels that are less costly. Setting aside ad hoc approaches that are void of rigor, traditionally, the method of composite estimation has been used to blend results from different sample surveys. This means individual point estimates from different surveys are pooled together, 1 estimate at a time. Given that for a typical study many estimates must be produced, this piecemeal approach is computationally burdensome and subject to the inferential limitations of the individual surveys that are used in this process.
Objective
In this paper, we will provide a comprehensive review of the traditional method of composite estimation. Subsequently, the method of composite weighting is introduced, which is significantly more efficient, both computationally and inferentially when pooling data from multiple surveys. With the growing interest in hybrid sampling alternatives, we hope to offer an accessible methodology for improving the efficiency of inferences from such sample surveys without sacrificing rigor.
Methods
Specifically, we will illustrate why the many ad hoc procedures for blending survey data from multiple surveys are void of scientific integrity and subject to misleading inferences. Moreover, we will demonstrate how the traditional approach of composite estimation fails to offer a pragmatic and scalable solution in practice. By relying on theoretical and empirical justifications, in contrast, we will show how our proposed methodology of composite weighting is both scientifically sound and inferentially and computationally superior to the old method of composite estimation.
Results
Using data from 3 large surveys that have relied on hybrid samples composed of probability-based and supplemental sample components from online panels, we illustrate that our proposed method of composite weighting is superior to the traditional method of composite estimation in 2 distinct ways. Computationally, it is vastly less demanding and hence more accessible for practitioners. Inferentially, it produces more efficient estimates with higher levels of external validity when pooling data from multiple surveys.
Conclusions
The new realities of the digital age have brought about a number of resilient challenges for survey researchers, which in turn have exposed some of the inefficiencies associated with the traditional methods this community has relied upon for decades. The resilience of such challenges suggests that piecemeal approaches that may have limited applicability or restricted accessibility will prove to be inadequate and transient. It is from this perspective that our proposed method of composite weighting has aimed to introduce a durable and accessible solution for hybrid sample surveys.}
}
@article{BOSCH2017,
title = {Graduate Biomedical Science Education Needs a New Philosophy},
journal = {mBio},
volume = {8},
number = {6},
year = {2017},
issn = {2150-7511},
doi = {https://doi.org/10.1128/mbio.01539-17},
url = {https://www.sciencedirect.com/science/article/pii/S2161212917003111},
author = {Gundula Bosch and Arturo Casadevall},
keywords = {Ph.D., education, graduate},
abstract = {ABSTRACT
There is a growing realization that graduate education in the biomedical sciences is successful at teaching students how to conduct research but falls short in preparing them for a diverse job market, communicating with the public, and remaining versatile scientists throughout their careers. Major problems with graduate level education today include overspecialization in a narrow area of science without a proper grounding in essential critical thinking skills. Shortcomings in education may also contribute to some of the problems of the biomedical sciences, such as poor reproducibility, shoddy literature, and the rise in retracted publications. The challenge is to modify graduate programs such that they continue to generate individuals capable of conducting deep research while at the same time producing more broadly trained scientists without lengthening the time to a degree. Here we describe our first experiences at Johns Hopkins and propose a manifesto for reforming graduate science education.}
}
@article{MIDGLEY2019181,
title = {Anticipatory practice and the making of surplus food},
journal = {Geoforum},
volume = {99},
pages = {181-189},
year = {2019},
issn = {0016-7185},
doi = {https://doi.org/10.1016/j.geoforum.2018.09.013},
url = {https://www.sciencedirect.com/science/article/pii/S0016718518302720},
author = {Jane L. Midgley},
keywords = {Surplus food, Anticipation, Market devices, Redistribution, United Kingdom},
abstract = {This paper explores the practices that have evolved between a global food retailer and a leading charitable surplus food redistributor to enable the utilization of surplus food in community and charitable meal settings in the UK. I argue that to understand surplus food and its potential futures (consumed or wasted), closer engagement with anticipatory thinking is needed. Drawing on interview data with key stakeholders and observations of the food industry redistribution process the paper explores the anticipatory actions taken by different actors as they attempt to manage the possible futures of foods that become categorized as surplus. The paper shows how different market devices are used to manage market concerns about surplus food and work to assure its future consumption. The devices focus on managing the risks of the food becoming unsafe and the associated legal liabilities. The market concerns, as expressions of anticipatory thinking, inform a series of anticipatory practices throughout the redistribution process to enable all actors, and especially the Retailer, to trust in the process. The paper concludes by noting how reliant the redistribution process is on anticipatory practices, especially pre-emption and improvisation to make the process workable, but also how these work to contain the various concerns within market arrangements. The paper highlights the importance of anticipation as a theoretical basis for exploring surplus food and the concept of surplus more widely.}
}
@article{DELLANNA2022105064,
title = {Evolving Fuzzy logic Systems for creative personalized Socially Assistive Robots},
journal = {Engineering Applications of Artificial Intelligence},
volume = {114},
pages = {105064},
year = {2022},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2022.105064},
url = {https://www.sciencedirect.com/science/article/pii/S0952197622002251},
author = {Davide Dell’Anna and Anahita Jamshidnejad},
keywords = {Evolving Fuzzy logic Systems, Personalized Socially Assistive Robots, Robot creativity},
abstract = {Socially Assistive Robots (SARs) are increasingly used in dementia and elderly care. In order to provide effective assistance, SARs need to be personalized to individual patients and account for stimulating their divergent thinking in creative ways. Rule-based fuzzy logic systems provide effective methods for automated decision-making of SARs. However, expanding and modifying the rules of fuzzy logic systems to account for the evolving needs, preferences, and medical conditions of patients can be tedious and costly. In this paper, we introduce EFS4SAR, a novel Evolving Fuzzy logic System for Socially Assistive Robots that supports autonomous evolution of the fuzzy rules that steer the behavior of the SAR. EFS4SAR combines traditional rule-based fuzzy logic systems with evolutionary algorithms, which model the process of evolution in nature and have shown to result in creative behaviors. We evaluate EFS4SAR via computer simulations on both synthetic and real-world data. The results show that the fuzzy rules evolved over time are not only personalized with respect to the personal preferences and therapeutic needs of the patients, but they also meet the following criteria for creativity of SARs: originality and effectiveness of the therapeutic tasks proposed to the patients. Compared to existing evolving fuzzy systems, EFS4SAR achieves similar effectiveness with higher degree of originality.}
}
@article{ENE2016973,
title = {A genetic algorithm for minimizing energy consumption in warehouses},
journal = {Energy},
volume = {114},
pages = {973-980},
year = {2016},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2016.08.045},
url = {https://www.sciencedirect.com/science/article/pii/S0360544216311586},
author = {Seval Ene and İlker Küçükoğlu and Aslı Aksoy and Nursel Öztürk},
keywords = {Genetic algorithm, Green supply chain, Minimization of energy consumption, Warehouse management},
abstract = {Green supply chain management is generally defined as integration of green thinking and environmental issues into the whole supply chain operations like product design, manufacturing process, warehousing, distribution etc. Within this context green principles should be adopted in warehouse management to minimize negative impact on the environment. In warehouse operations, picking must be analyzed attentively which is widely studied in literature for minimizing service time levels because of its close relation to the higher costs. The efficiency of picking in warehouses mainly depends on storage assignment policy that directly affects picking performance in warehouses. In this paper, picking operation in warehouses is studied to minimize energy consumption with proper storage policy other than service time. Genetic algorithm (GA) is proposed to solve the problem and numerical examples are presented to demonstrate the performance of the GA. Results show that, the GA gives efficient solutions to the problem.}
}
@article{MEEKINGS2025101262,
title = {What's the point of talking? Auditory targets and communicative goals},
journal = {Journal of Neurolinguistics},
volume = {75},
pages = {101262},
year = {2025},
issn = {0911-6044},
doi = {https://doi.org/10.1016/j.jneuroling.2025.101262},
url = {https://www.sciencedirect.com/science/article/pii/S0911604425000181},
author = {Sophie Meekings and Sophie K. Scott},
abstract = {Human speech production is a complex action requiring minute control over the articulators and sensitivity to the surrounding environment. Computational and empirical work has attempted to identify the specific neural mechanisms and cognitive processes that allow us to reliably produce speech sounds. This work has established that humans can use their perception of the auditory and somatosensory consequences of their actions to guide subsequent speech movements. However, speech predominantly takes place in a communicative context, and this context is also known to modulate the way that people speak: human voices are highly flexible. In this paper, we try to unite the traditional motor control conception of internally defined acoustic and somatosensory goals with linguistic research showing that talkers respond and entrain to their conversational partners. We provide an overview of the theoretical and empirical work surrounding the use of sensory feedback monitoring in speech production and discuss practical constraints that have limited more naturalistic investigations into dyadic interaction. To conclude, we argue that the variability of results seen in the speech motor control literature reflects a more complex underlying neural architecture, and an overarching communicative goal that supersedes specific phonetic targets.}
}
@article{EARL2019303,
title = {Elusive optima: A process tracing analysis of procedural rationality in mobile phone connection plan choices},
journal = {Journal of Economic Behavior & Organization},
volume = {161},
pages = {303-322},
year = {2019},
issn = {0167-2681},
doi = {https://doi.org/10.1016/j.jebo.2019.03.021},
url = {https://www.sciencedirect.com/science/article/pii/S0167268119300988},
author = {Peter E. Earl and Lana Friesen and Christopher Shadforth},
keywords = {Consumer capabilities, Choice overload, Procedural rationality, Process tracing},
abstract = {This paper reports an experiment in which subjects were rewarded on the basis of how close they came to finding the cheapest mobile phone plan to serve a particular usage remit by searching freely in the Internet. During the task, subjects were required to ‘think aloud’ and recordings were made of what they said and what they did on their computer screens. Analysis of the screen-capture movie recordings revealed major shortfalls in procedural rationality, including poor strategic thinking about how to deal with choice overload, poor conceptual understanding of mobile phone plans and pricing systems, as well as cognitive and calculation errors. Our novel method leads to a very different policy focus from that implied by viewing the problem in terms of excess information per se and irrationality as driven by innate heuristics and biases.}
}
@article{LI2022126546,
title = {Dynamic forecasting performance and liquidity evaluation of financial market by Econophysics and Bayesian methods},
journal = {Physica A: Statistical Mechanics and its Applications},
volume = {588},
pages = {126546},
year = {2022},
issn = {0378-4371},
doi = {https://doi.org/10.1016/j.physa.2021.126546},
url = {https://www.sciencedirect.com/science/article/pii/S0378437121008190},
author = {Jiang-Cheng Li and Chen Tao and Hai-Feng Li},
keywords = {Econophysics, Agent-based model, Liquidity risk assessment, Machine learning thinking, Microcosmic evolution models},
abstract = {In a complex financial system, what is the forecasting performance of macro and micro evolution models of Econophysics on asset prices? For this problem, from the perspective of machine learning, we study the dynamic forecasting and liquidity assessment of financial markets, based on econophysics and Bayesian methods. We establish eight dynamic prediction methods, based on our proposed likelihood estimation and Bayesian estimation methods of macro and micro evolution models of econophysics. Combined machine learning thinking and real data, we empirically study and simulate the out-of-sample dynamic forecasting analysis of eight proposed methods and compare with the benchmark GARCH model. A variety of loss functions, superior predictive ability test (SPA), Akaike and Bayesian information criterion (AIC and BIC) methods are introduced to further evaluate the forecasting performance of our proposed methods. The research of out of sample prediction shows that (1) the method of the simplified stochastic model with Bayesian method for only sample return has the best forecasting performance; (2) the method of the stochastic model with Bayesian method for only return samples has the worst forecasting performance. For the liquidity assessment problem, there is a strong correlation between the trading probability evaluated by the proposed eight methods and the real turnover rate, and an increase of liquidity is correspond to the increase of asset risk. In other words, it suggests that all proposed methods can well evaluate market liquidity.}
}
@article{WHITE200337,
title = {Promoting productive mathematical classroom discourse with diverse students},
journal = {The Journal of Mathematical Behavior},
volume = {22},
number = {1},
pages = {37-53},
year = {2003},
issn = {0732-3123},
doi = {https://doi.org/10.1016/S0732-3123(03)00003-8},
url = {https://www.sciencedirect.com/science/article/pii/S0732312303000038},
author = {Dorothy Y. White},
keywords = {Classroom discourse, Questioning techniques, Equity/diversity, Elementary mathematics teaching},
abstract = {Productive mathematical classroom discourse allows students to concentrate on sense making and reasoning; it allows teachers to reflect on students’ understanding and to stimulate mathematical thinking. The focus of the paper is to describe, through classroom vignettes of two teachers, the importance of including all students in classroom discourse and its influence on students’ mathematical thinking. Each classroom vignette illustrates one of four themes that emerged from the classroom discourse: (a) valuing students’ ideas, (b) exploring students’ answers, (c) incorporating students’ background knowledge, and (d) encouraging student-to-student communication. Recommendations for further research on classroom discourse in diverse settings are offered.}
}
@article{HUANG201724,
title = {Energy and carbon performance evaluation for buildings and urban precincts: review and a new modelling concept},
journal = {Journal of Cleaner Production},
volume = {163},
pages = {24-35},
year = {2017},
note = {Achieving Low/no Fossil-carbon Economies based upon the Essential Transformations to Support them},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2015.12.008},
url = {https://www.sciencedirect.com/science/article/pii/S0959652615018235},
author = {Bin Huang and Ke Xing and Stephen Pullen},
keywords = {Buildings, Integrated modelling, Life cycle energy, Systems thinking, Urban precincts},
abstract = {With the accelerating pace of urbanisation around the world, the planning, development and operation of buildings and precincts have become increasingly important with respect to energy use and the associated carbon footprint of the modern built environment. Over recent decades, much effort, both in research and in practice, has been devoted to building construction and urban planning for the improvement of energy efficiency and greenhouse gas emissions. However, the accuracy of modelling and evaluation of energy and carbon performance for buildings and urban precincts remains limited, affected by inadequate energy intensity data and highly integrated building systems, as well as the complex interactions between buildings and the urban eco-system. This paper presents a critical review of current measures and models for representing and assessing life cycle energy as well as associated emissions profiles at both the building and the precinct levels. It also identifies influential factors and explores interactions among buildings, surrounding environment and user behaviours at the urban precinct level by taking a systems perspective. Based on such a review, this study maps out some key challenges for integrating energy and carbon metrics, and finally proposes a precinct-level system boundary definition and an integrated model following systems thinking. The proposed model can facilitate a critical thinking approach about the evaluations of global energy and emissions, and support the quantification of energy consumption and associated emissions for building precinct systems.}
}
@article{SMYE2022105015,
title = {Interdisciplinary approaches to metastasis},
journal = {iScience},
volume = {25},
number = {9},
pages = {105015},
year = {2022},
issn = {2589-0042},
doi = {https://doi.org/10.1016/j.isci.2022.105015},
url = {https://www.sciencedirect.com/science/article/pii/S2589004222012871},
author = {Stephen W. Smye and Robert A. Gatenby},
abstract = {Summary
Interdisciplinary research is making a significant contribution to understanding metastasis - one of the grand challenges in cancer research. Examples drawn from apparently unconnected areas of physics, and described at a recent workshop on metastasis, illustrate the value of interdiscplinary thinking.}
}
@article{YOON2021100865,
title = {United States and South Korean citizens’ interpretation and assessment of COVID-19 quantitative data},
journal = {The Journal of Mathematical Behavior},
volume = {62},
pages = {100865},
year = {2021},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2021.100865},
url = {https://www.sciencedirect.com/science/article/pii/S0732312321000262},
author = {Hyunkyoung Yoon and Cameron O’Neill Byerley and Surani Joshua and Kevin Moore and Min Sook Park and Stacy Musgrave and Laura Valaas and James Drimalla},
keywords = {COVID-19, Graphs, Representations of quantitative data, Rate of change, Exponential growth},
abstract = {We investigate United States and South Korean citizens’ mathematical schemes and how these schemes supported or hindered their attempts to assess the severity of COVID-19. We selected web and media-based COVID-19 data representations that we hypothesized citizens would interpret differently depending on their mathematical schemes. We included items that we conjectured would be easier or more difficult to interpret with schemes that prior research had reported were more or less productive, respectively. We used the representations during clinical interviews with 25 United States and seven South Korean citizens. We illustrate that citizens’ mathematical schemes (as well as their beliefs) impacted how they assessed the severity of COVID-19. We present vignettes of citizens’ schemes that inhibited interpreting representations of COVID-19 in ways compatible with the displayed quantitative data, schemes that aided them in assessing the severity of COVID-19, and beliefs about the reliability of scientific data that overrode their mathematical conclusions.}
}
@incollection{LEACH202221,
title = {Chapter 2 - AI and the limits of human creativity in urban planning and design},
editor = {Imdat As and Prithwish Basu and Pratap Talwar},
booktitle = {Artificial Intelligence in Urban Planning and Design},
publisher = {Elsevier},
pages = {21-37},
year = {2022},
isbn = {978-0-12-823941-4},
doi = {https://doi.org/10.1016/B978-0-12-823941-4.00013-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128239414000135},
author = {Neil Leach},
keywords = {AlphaGo, AI, Strategy, Urban planning, Creativity, Perception},
abstract = {What can architects learn from AlphaGo? This chapter explores the lessons to be learnt from the famous match where AlphaGo, a machine-learning system developed by DeepMind, beat leading Korean Go player, Lee Sedol. It explores the ramifications of this victory on a series of different levels, from the global impact of the match on research into AI to the impact on Xkool Technologies and Spacemaker AI, two architectural start-ups developing AI systems for architecture and urban planning. It makes a particular comparison between the operations of AlphaGo and the strategic thinking of urban planning, arguing that AI now puts the future of urban planners—and possibly also architects—at risk. It then goes on to appraise the famous Move 37 made by AlphaGo in Game 2 of this match. It argues that, despite appearances, this move was not actually creative. Finally, it explores how we might view human creativity in the light of comments made about AlphaGo. The chapter concludes by speculating whether the ultimate lesson of AlphaGo is that creativity is simply a question of “perceived creativity.”}
}
@article{CAO2024101244,
title = {Explanatory models in neuroscience, Part 1: Taking mechanistic abstraction seriously},
journal = {Cognitive Systems Research},
volume = {87},
pages = {101244},
year = {2024},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2024.101244},
url = {https://www.sciencedirect.com/science/article/pii/S138904172400038X},
author = {Rosa Cao and Daniel Yamins},
keywords = {Mechanism, Models, Explanation, Constraints, Similarity, Mapping, Abstraction, Functional abstraction, Neural networks, Computation, Philosophy, Vision, Constraint, Prediction, Transform, Levels of explanation, Mechanistic explanation, Neuroscience, Understanding},
abstract = {Despite the recent success of neural network models in mimicking animal performance on various tasks, critics worry that these models fail to illuminate brain function. We take it that a central approach to explanation in systems neuroscience is that of mechanistic modeling, where understanding the system requires us to characterize its parts, organization, and activities, and how those give rise to behaviors of interest. However, it remains controversial what it takes for a model to be mechanistic, and whether computational models such as neural networks qualify as explanatory on this approach. We argue that certain kinds of neural network models are actually good examples of mechanistic models, when an appropriate notion of mechanistic mapping is deployed. Building on existing work on model-to-mechanism mapping (3M), we describe criteria delineating such a notion, which we call 3M++. These criteria require us, first, to identify an abstract level of description that is still detailed enough to be “runnable”, and then, to construct model-to-brain mappings using the same principles as those employed for brain-to-brain mapping across individuals. Perhaps surprisingly, the abstractions required are just those already in use in experimental neuroscience and deployed in the construction of more familiar computational models — just as the principles of inter-brain mappings are very much in the spirit of those already employed in the collection and analysis of data across animals. In a companion paper, we address the relationship between optimization and intelligibility, in the context of functional evolutionary explanations. Taken together, mechanistic interpretations of computational models and the dependencies between form and function illuminated by optimization processes can help us to understand why brain systems are built they way they are.}
}
@article{LOWENSTEIN20191237,
title = {Visual perception, cognition, and error in dermatologic diagnosis: Diagnosis and error},
journal = {Journal of the American Academy of Dermatology},
volume = {81},
number = {6},
pages = {1237-1245},
year = {2019},
issn = {0190-9622},
doi = {https://doi.org/10.1016/j.jaad.2018.12.072},
url = {https://www.sciencedirect.com/science/article/pii/S0190962219303251},
author = {Eve J. Lowenstein and Richard Sidlow and Christine J. Ko},
keywords = {cognitive error, diagnostic error, heuristic, metacognition, patient safety, visual intelligence},
abstract = {Diagnostic error in dermatology is a large practice gap that has received little attention. Diagnosis in dermatology relies heavily on a heuristic approach that is responsible for our perception of clinical findings. To improve our diagnostic accuracy, a better understanding of the strengths and limitations of heuristics (cognitive shortcuts) used in dermatology is essential. Numerous methods have been proposed to improve diagnostic accuracy, including brain training, reducing cognitive load, and getting feedback and second opinions. Becoming comfortable with the uncertainty intrinsic to medicine is essential. Ultimately, the practice of metacognition, or thinking about how we think, can offer corrective insights to improve accuracy in diagnosis.}
}
@article{ZUO2010268,
title = {Integrating performance-based design in beginning interior design education: an interactive dialog between the built environment and its context},
journal = {Design Studies},
volume = {31},
number = {3},
pages = {268-287},
year = {2010},
issn = {0142-694X},
doi = {https://doi.org/10.1016/j.destud.2009.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X09000969},
author = {Qun Zuo and Wesley Leonard and Eileen E. MaloneBeach},
keywords = {performance-based design, interior design, design education, computer aided design, design process},
abstract = {This paper presents a new paradigm in interior design education in which building performance simulation was employed for decision making and design generation. Digital technology was intermixed with conventional paper-based media in the design process to explore formal, spatial and passive solar energy solutions. The intention of the study was to re-discover the value of computers in assisting design thinking and improving effective learning. The results indicated the Performance-Based Design approach resulted in an early awareness of sustainable energy for beginning interior design students. Further, it enhanced understanding of the mutual relationship between interior and exterior and between the built and natural environment. This paper acknowledged the achievements as well as limitations and future directions for the integration of Performance-Based Design into interior design curriculum.}
}
@incollection{NIE2019205,
title = {Two-Stage Land Use Optimization for A Food-Energy-Water Nexus System: A Case Study In Texas Edwards Region},
editor = {Salvador Garcia Muñoz and Carl D. Laird and Matthew J. Realff},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {47},
pages = {205-210},
year = {2019},
booktitle = {Proceedings of the 9th International Conference on Foundations of Computer-Aided Process Design},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-12-818597-1.50033-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128185971500333},
author = {Yaling Nie and Styliani Avraamidou and Xin Xiao and Efstratios N. Pistikopoulos and Jie Li},
keywords = {Land use optimization, Food-Energy-Water Nexus, multi-period planning},
abstract = {Efficient land use planning and scheduling in Food-Energy-Water Nexus (FEW-N) related systems is a complicated decision-making problem with resource competitions and conflicting objectives. Systematic thinking based on FEW-N is a necessity for modeling and optimization of the systems. However, challenges arise in making decisions while encountering conflicting objectives, multi-scale and multi-period problems, and multiple stakeholders. To address these challenges, we developed a generic optimization-based land allocation approach, which provides i) a composite FEW-N metric to help solve the multi-objective optimization problem and carry out assessments, and ii) a two-stage decomposition strategy to solve the multi-scale and multi-period planning and scheduling problem. The developed strategy was applied in a case study within the Texas Edwards Region. Computational results indicate that the approach can provide a comprehensive FEW-N metric to select strategies for optimal land allocation and limit stresses in the FEW-N, and achieve trade-off solutions for the multi-scale and multi-period FEW land use systems.}
}
@article{USKOKOVIC2023e15015,
title = {Natural sciences and chess: A romantic relationship missing from higher education curricula},
journal = {Heliyon},
volume = {9},
number = {4},
pages = {e15015},
year = {2023},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2023.e15015},
url = {https://www.sciencedirect.com/science/article/pii/S2405844023022223},
author = {Vuk Uskoković},
keywords = {Chemistry, Chess, Creativity, Culture, Education, Instruction, Science},
abstract = {Chess is a game that delicately weaves analytical thinking around artistic experience, yet recent conversions of STEM (Science-Technology-Engineering-Mathematics) to STEAM (Science-Technology-Engineering-Art-Mathematics) have omitted adding chess as an elementary coursework to K-12 and higher education curricula. Chess, as per arguments presented in this essay, can be considered as a language and a tool for furthering the development of artistic skills among scientists and analytical, pattern-recognition skills among artists. It can also serve as a missing link between science and art in STEAM curricula thanks to its finding itself halfway between the two. A handful of analogies are drawn here from chess, illustrated sporadically with positions from real-life chess games and converted to lessons in creativity for students in natural sciences. The discussion centered around these analogies is reinforced by a literature review of studies conducted over the past 80 years to assess the effect of exposing students to lessons in chess on their learning in distant domains. Overall, great benefits can emerge from complementing science education with chess and it is hoped that chess will become an integral part of basic education in primary schools and universities worldwide in the near future.}
}
@article{TRAYVICK2024116109,
title = {Speech and language patterns in autism: Towards natural language processing as a research and clinical tool},
journal = {Psychiatry Research},
volume = {340},
pages = {116109},
year = {2024},
issn = {0165-1781},
doi = {https://doi.org/10.1016/j.psychres.2024.116109},
url = {https://www.sciencedirect.com/science/article/pii/S0165178124003949},
author = {Jadyn Trayvick and Sarah B. Barkley and Alessia McGowan and Agrima Srivastava and Arabella W. Peters and Guillermo A. Cecchi and Jennifer H. Foss-Feig and Cheryl M. Corcoran},
keywords = {Autism, Speech, Language, Natural language processing, Automated speech analysis, Acoustics, Computational phenotyping},
abstract = {Speech and language differences have long been described as important characteristics of autism spectrum disorder (ASD). Linguistic abnormalities range from prosodic differences in pitch, intensity, and rate of speech, to language idiosyncrasies and difficulties with pragmatics and reciprocal conversation. Heterogeneity of findings and a reliance on qualitative, subjective ratings, however, limit a full understanding of linguistic phenotypes in autism. This review summarizes evidence of both speech and language differences in ASD. We also describe recent advances in linguistic research, aided by automated methods and software like natural language processing (NLP) and speech analytic software. Such approaches allow for objective, quantitative measurement of speech and language patterns that may be more tractable and unbiased. Future research integrating both speech and language features and capturing “natural language” samples may yield a more comprehensive understanding of language differences in autism, offering potential implications for diagnosis, intervention, and research.}
}
@article{KHAN2021104263,
title = {A novel hybrid gravitational search particle swarm optimization algorithm},
journal = {Engineering Applications of Artificial Intelligence},
volume = {102},
pages = {104263},
year = {2021},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2021.104263},
url = {https://www.sciencedirect.com/science/article/pii/S095219762100110X},
author = {Talha Ali Khan and Sai Ho Ling},
keywords = {PSO, GSA, Hybrid, DNA computation},
abstract = {Particle Swarm Optimization (PSO) algorithm is a member of the swarm computational family and widely used for solving nonlinear optimization problems. But, it tends to suffer from premature stagnation, trapped in the local minimum and loses exploration capability as the iteration progresses. On the contrary, Gravitational Search Algorithm (GSA) is proficient for searching global optimum, however, its drawback is its slow searching speed in the final phase. To overcome these problems in this paper a novel Hybrid Gravitational Search Particle Swarm Optimization Algorithm (HGSPSO) is presented. The key concept behind the proposed method is to merge the local search ability of GSA with the capability for social thinking (gbest) of PSO. To examine the effectiveness of these methods in solving the abovementioned issues of slow convergence rate and trapping in local minima five standard and some modern CEC benchmark functions are used to ensure the efficacy of the presented method. Additionally, a DNA sequence problem is also solved to confirm the proficiency of the proposed method. Different parameters such as Hairpin, Continuity, H-measure, and Similarity are employed as objective functions. A hierarchal approach was used to solve this multi-objective problem where a single objective function is first obtained through a weighted sum method and the results were then empirically validated. The proposed algorithm has demonstrated an extraordinary performance per solution stability and convergence.}
}
@article{DAS2023100065,
title = {Informatics on a social view and need of ethical interventions for wellbeing via interference of artificial intelligence},
journal = {Telematics and Informatics Reports},
volume = {11},
pages = {100065},
year = {2023},
issn = {2772-5030},
doi = {https://doi.org/10.1016/j.teler.2023.100065},
url = {https://www.sciencedirect.com/science/article/pii/S2772503023000257},
author = {Kabita Das and Manaswini Pattanaik and Smitimayee Basantia and Radhashyam Mishra and Debashreemayee Das and Kanhucharan Sahoo and Biswaranjan Paital},
keywords = {Artificial intelligence, Ethical enquiry, Ethics in technology, Human conduct, Moral value, Social cognition, Human intelligence},
abstract = {The main focus of this paper was to discuss and appraise the attribution of intelligence and value judgement on Artificial Intelligence (AI) and its regulated use in society. Humans are tool-making creatures and AI is used for civilization via tools. During the time of pre-civilization, tools were simple in the form of crude construction, using hand skills but at present, the achievements are the substitution of machinery to relieve/replace human intellect. AI is the scientific technique of bringing learning, adaptation, and self-organization of machines. It encompasses various concepts and methods, deployed by researchers in many diverse fields of computation and cognition. This is the computational mode of a brain, based on artificial neural networks. The usefulness of AI ethically, initiates a big question i.e. if the human mind is not self-sufficient for any work without harming the moral sentiment of others then, how can people believe in a computational model of the mind, is a machine, morally responsible for any good or bad action. We highlight issues on the use of AI in the replacement of the human mind asking what is the value of humans in this age of AI? Can AI reciprocate and respect human values better than human beings? Can AI replace human intelligence? In the case of ethical enquiry, it is rather a herculean task to consider a machine's action to be moral or immoral, after all, it is just a machinery action devoid of any moral quality.}
}
@article{URECH2022101731,
title = {A simulation-based design framework to iteratively analyze and shape urban landscapes using point cloud modeling},
journal = {Computers, Environment and Urban Systems},
volume = {91},
pages = {101731},
year = {2022},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2021.101731},
url = {https://www.sciencedirect.com/science/article/pii/S0198971521001381},
author = {Philipp R.W. Urech and Muhammad Omer Mughal and Carlos Bartesaghi-Koc},
keywords = {Point-cloud modeling, Computational fluid dynamics, Laser-scanned data, Urban landscape design, Design performance},
abstract = {The topic of this paper evolves on the discourse of digital modeling in landscape design. Current design methods stagger to address physical forms and dynamics present in the environment. This status quo limits possibilities to integrate scientific evidence when developing spatial and aesthetic configurations in urban landscapes. Remote sensing technology such as laser scanning measures physical forms to reproduce them as geo-specific digital 3D models, while dynamic simulation is widely used to predict how scenarios will perform under given conditions. However, there is still a need for a holistic design process that is capable of integrating both the measured physical forms and physical dynamics. This paper presents a novel framework using point cloud modeling to shape design scenarios that are iteratively evaluated for their performance. The proposed framework is demonstrated through a case study in Singapore. New spatial configurations are tested for the site through an iterative and comparative analysis of the design performance. The case study exposes (1) a site-specific design approach by iteratively modeling a laser-scanned point cloud model, (2) a workflow to convert the geometric data from the point cloud models into voxels and meshes, (3) an integration of computational fluid dynamics (CFD) simulation during design development as per-point attributes, and (4) a comparison of the configurations to identify best performing scenarios. This design framework can support city managers, planners, urban and landscape designers to better inform their decision-making process by relying on accurate scientific feedback. By guiding the design process with the consideration of the built environment as a complex adaptive system, it will be possible to improve how open spaces and ecosystem services perform in cities, and to design landscapes that can mitigate dynamic events such as urban heat islands.}
}
@article{KROGER2013189,
title = {An ERP study of passive creative conceptual expansion using a modified alternate uses task},
journal = {Brain Research},
volume = {1527},
pages = {189-198},
year = {2013},
issn = {0006-8993},
doi = {https://doi.org/10.1016/j.brainres.2013.07.007},
url = {https://www.sciencedirect.com/science/article/pii/S0006899313009566},
author = {Sören Kröger and Barbara Rutter and Holger Hill and Sabine Windmann and Christiane Hermann and Anna Abraham},
keywords = {Creativity, ERP, N400, Conceptual expansion, Alternate uses task, Divergent thinking, Semantic cognition},
abstract = {A novel ERP paradigm was employed to investigate conceptual expansion, a central component of creative thinking. Participants were presented with word pairs, consisting of everyday objects and uses for these objects, which had to be judged based on the two defining criteria of creative products: unusualness and appropriateness. Three subject-determined trial types resulted from this judgement: high unusual and low appropriate (nonsensical uses), low unusual and high appropriate (common uses), and high unusual and high appropriate (creative uses). Word pairs of the creative uses type are held to passively induce conceptual expansion. The N400 component was not specifically modulated by conceptual expansion but was, instead, generally responsive as a function of unusualness or novelty of the stimuli (nonsense=creative>common). Explorative analyses in a later time window (500–900ms) revealed that ERP activity in this phase indexes appropriateness (nonsense>creative=common). In the discussion of these findings with reference to the literature on semantic cognition, both components are proposed as indexing processes relevant to conceptual expansion as they are selectively involved in the encoding and integration of a newly established semantic connection between two previously unrelated concepts.}
}
@article{WHITACRE2020100816,
title = {The roles of tools and models in a prospective elementary teachers’ developing understanding of multidigit multiplication},
journal = {The Journal of Mathematical Behavior},
volume = {60},
pages = {100816},
year = {2020},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2020.100816},
url = {https://www.sciencedirect.com/science/article/pii/S0732312320300808},
author = {Ian Whitacre and Chepina Rumsey},
keywords = {Prospective teachers, Mental computation, Multiplication, Tools, Models},
abstract = {It is important for prospective elementary teachers to understand multidigit multiplication deeply; however, the development of such understanding presents challenges. We document the development of a prospective elementary teacher’s reasoning about multidigit multiplication during a Number and Operations course. We present evidence of profound progress in Valerie’s understanding of multidigit multiplication, and we highlight the roles of particular tools and models in her developing reasoning. In this way, we contribute an illuminating case study that can inform the work of mathematics teacher educators. We discuss specific instructional implications that derive from this case.}
}
@article{HASELI2023184,
title = {HECON: Weight assessment of the product loyalty criteria considering the customer decision's halo effect using the convolutional neural networks},
journal = {Information Sciences},
volume = {623},
pages = {184-205},
year = {2023},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2022.12.027},
url = {https://www.sciencedirect.com/science/article/pii/S0020025522015201},
author = {Gholamreza Haseli and Ramin Ranjbarzadeh and Mostafa Hajiaghaei-Keshteli and Saeid {Jafarzadeh Ghoushchi} and Aliakbar Hasani and Muhammet Deveci and Weiping Ding},
keywords = {Customer loyalty, Deep learning, Convolutional neural networks, Multi-criteria decision-making, Halo effect},
abstract = {The economic pressures and increasing competition in markets have led to the CEOs of companies being forced to make the right strategic decisions in the development of products for selling the right products to the right customers. To achieve this goal, companies need to know which criteria of their products lead to customer loyalty to that product. In the past, various methods have been introduced to obtain the importance (weight) of criteria that use the opinions of experts or customers. There is a halo effect in human decisions that leads to biases in evaluating the criteria by influencing human emotions. This study introduces a new method for weight assessment of the product loyalty criteria by considering the customer's decisions halo effect using the convolutional neural network (CNN) called the halo effect using the convolutional neural networks (HECON) method. In the HECON method to consider the halo effect of the customer decisions, a CNN model is proposed as the deep learning pipeline to obtain more accurate weights of the criteria. The HECON method to obtain the weight of the criteria and identify criteria that lead to product loyalty needs to collect the feedback of a large number of customers based on the net promoter score (NPS) scale. The innovation of the HECON method is to obtain the effect level of each product criterion on selection and loyalty to the product through the feedback of a large number of customers by considering the halo effect on the customers' thinking. To date, the analyzing methods have often not been able to identify the halo effect in evaluating the reasons for customer loyalty to the product. The halo effect indicates sometimes some of the product criteria secretly affect the customers' opinions that require deep neural networks to analyze them. By using the deep CNN model of the HECON method to evaluate product criteria for understanding customer behavior, companies will be able to identify customers' behavior and develop their products exactly following the customer's desires. To evaluate the performance and demonstrate the applicability of the HECON method, presented two case studies. It is presented that there are challenging differences between the results of the HECON method with the other methods because the HECON method considers the halo effect on the customer decisions and demonstrates better performance.}
}
@article{HUANG2012250,
title = {The effectiveness of using procedural scaffoldings in a paper-plus-smartphone collaborative learning context},
journal = {Computers & Education},
volume = {59},
number = {2},
pages = {250-259},
year = {2012},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2012.01.015},
url = {https://www.sciencedirect.com/science/article/pii/S0360131512000310},
author = {Hui-Wen Huang and Chih-Wei Wu and Nian-Shing Chen},
keywords = {Cooperative/collaborative learning, Improving classroom teaching, Teaching/learning strategies},
abstract = {The purpose of this study was to evaluate the effectiveness of using procedural scaffoldings in fostering students’ group discourse levels and learning outcomes in a paper-plus-smartphone collaborative learning context. All participants used built-in camera smartphones to learn new knowledge by scanning Quick Response (QR) codes, a type of two-dimensional barcode, embedded in paper-based learning materials in this study. Sixty undergraduate and graduate students enrolled at a four-year university in southern Taiwan participated in this study. Participants were randomly assigned into two different groups, using procedural scaffoldings learning and non-procedural scaffoldings learning. The learning unit about the Long Tail, an important concept used in products sales, was the learning task that participants were expected to complete. During the experiment, pretest–posttest and the completed group worksheets were used to collect data. The researchers applied content analyses, chi-square test, t-test, and ANCOVA to answer research questions. The findings indicated that participants in the experimental group using procedural scaffoldings achieved better learning outcomes than their counterparts in the control group in terms of group discourse levels, group learning, and individual learning.}
}
@article{KAMPPINEN1998481,
title = {Evolution and culture: the Darwinian view on infosphere},
journal = {Futures},
volume = {30},
number = {5},
pages = {481-484},
year = {1998},
issn = {0016-3287},
doi = {https://doi.org/10.1016/S0016-3287(98)00050-0},
url = {https://www.sciencedirect.com/science/article/pii/S0016328798000500},
author = {Matti Kamppinen},
abstract = {This essay looks at the idea that human culture is an evolving system, a complex entity that undergoes evolutionary processes. This idea can also be expressed as follows: the cultural infosphere has the same mode of operation as the organic biosphere. There are three parts to the essay: it begins with some highlights from the history of evolutionary thinking; second, it explains the mechanisms of cultural selection; and third, it discusses the vision of the future provided by evolutionary thinking. The kind of evolutionary thinking focused upon is one that takes Charles Darwin seriously. The depth, reach and relevance of Darwinian thinking has been aptly exposed by Daniel C. Dennett,[1]and this essay assesses its worth in futures research.}
}
@article{PENG202484,
title = {Multi-perspective thought navigation for source-free entity linking},
journal = {Pattern Recognition Letters},
volume = {178},
pages = {84-90},
year = {2024},
issn = {0167-8655},
doi = {https://doi.org/10.1016/j.patrec.2023.12.020},
url = {https://www.sciencedirect.com/science/article/pii/S0167865523003677},
author = {Bohua Peng and Wei He and Bin Chen and Aline Villavicencio and Chengfu Wu},
keywords = {Information retrieval, Question generation, Entity linking, Chain-of-thought reasoning},
abstract = {Neural entity-linking models excel at bridging the lexical gap of multiple facets of facts, such as entity-related claims or evidence documents. Despite advancements in self-supervised learning and pretrained language models, challenges persist in entity linking, particularly in interpretability and transferability. Moreover, these models need many aligned documents to adapt to emerging entities, which may not be available due to data scarcity. In this work, we propose a novel Demonstrative Self-TrAining fRamework (D-STAR) that leverages multi-perspective thought navigation. D-STAR iteratively optimizes a question generator and an entity retriever by navigating thoughts on a dynamic graph reasoning across multiple perspectives for question generation. The generated question–answer pairs, along with hard negatives shared in the graph, enable adaptation with minimal computational overhead. Additionally, we introduce a new task, source-free entity linking, focusing on unsupervised transfer learning without direct access to original domain data. To demonstrate the feasibility of this task, we provide a generated question–answering dataset, FandomWiki, for novel entities. Our experiments show that D-STAR significantly improves baselines on SciFact, Zeshel, and FandomWiki.}
}
@article{ENDICOTT2003403,
title = {Moral reasoning, intercultural development, and multicultural experiences: relations and cognitive underpinnings},
journal = {International Journal of Intercultural Relations},
volume = {27},
number = {4},
pages = {403-419},
year = {2003},
note = {Special Training Issue},
issn = {0147-1767},
doi = {https://doi.org/10.1016/S0147-1767(03)00030-0},
url = {https://www.sciencedirect.com/science/article/pii/S0147176703000300},
author = {Leilani Endicott and Tonia Bock and Darcia Narvaez},
keywords = {Moral development, Intercultural development, Flexible thinking, Cognitive complexity, Multicultural experience, Schema theory},
abstract = {The relation between moral reasoning and intercultural sensitivity is discussed. We hypothesize that multicultural experiences are related to both types of development, describe the cognitive processes through which multicultural experiences theoretically facilitate development, and present empirical data supporting the association. Though the underlying developmental constructs were initially conceptualized as stage theories, we borrow from cognitive science and contemporary theories of human learning (Derry, 1996) to think of moral and intercultural development in terms of increasing sociocognitive flexibility. Intercultural and moral development share the common element of a critical shift from rigid to flexible thinking. In moral reasoning, this is characterized by the shift from conventional to post-conventional thinking. In intercultural development, a similar movement occurs between the ethnocentric and ethnorelative orientations of intercultural sensitivity. In order to test our hypothesis, college students (n=70) took measures of intercultural development (Intercultural Development Inventory), moral judgment (Defining Issues Test), and multicultural experience (Multicultural Experience Questionnaire). The results indicate that moral judgment and intercultural development are significantly related to one another. Both are related to multicultural experiences, particularly depth of the experiences, as opposed to breadth.}
}
@article{GAO2023103794,
title = {Developing virtual acoustic terrain for Urban Air Mobility trajectory planning},
journal = {Transportation Research Part D: Transport and Environment},
volume = {120},
pages = {103794},
year = {2023},
issn = {1361-9209},
doi = {https://doi.org/10.1016/j.trd.2023.103794},
url = {https://www.sciencedirect.com/science/article/pii/S1361920923001918},
author = {Zhenyu Gao and Alex Porcayo and John-Paul Clarke},
keywords = {Urban Air Mobility, Sustainable aviation, Noise modeling, Trajectory planning, Optimization},
abstract = {Urban Air Mobility (UAM) is a transformative concept that must operate harmoniously within the constraints imposed by societal impacts. Noise-aware flight trajectory planning can address UAM’s community noise concerns. However, the traditional trajectory optimization paradigm requires repetitive computations of a flight’s noise footprints in complex urban environments and is computationally expensive. In this work, we propose virtual acoustic terrain, a novel concept to enable an efficient trajectory optimization paradigm. By applying acoustic ray tracing and the principle of reciprocity in a complex urban environment, we convert different noise constraints into 3D exclusion zones which UAM operations should avoid to maintain limited noise impact. It combines with the physical urban terrain to define an acceptable fly zone for non-repetitive noise-aware trajectory optimization. This framework provides a new angle to future urban area airspace management and can also accommodate other forms of societal constraints.}
}
@incollection{MACHINMASTROMATTEO2025376,
title = {Literacy of the Future},
editor = {David Baker and Lucy Ellis},
booktitle = {Encyclopedia of Libraries, Librarianship, and Information Science (First Edition)},
publisher = {Academic Press},
edition = {First Edition},
address = {Oxford},
pages = {376-387},
year = {2025},
isbn = {978-0-323-95690-1},
doi = {https://doi.org/10.1016/B978-0-323-95689-5.00197-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780323956895001978},
author = {Juan D. Machin-Mastromatteo},
keywords = {Adaptation, Collaboration, Critical engagement, Democratic engagement, Digital literacy, Educational integration, Ethical dimensions, Futures Literacy, Information literacy, Lifelong learning, Media literacy, Multiliteracies, Programming skills, Social participation, Technological advancements},
abstract = {This entry summarizes the development of the literacy concepts most commonly associated with LIS, namely information literacy, digital literacy, and media literacy, which frame a synthesis of the future perspectives of these and other literacies that have been proposed in the literature.11An alphabetical and non-exhaustive list could include: academic literacy, artificial intelligence or algorithmic literacy, civic literacy, context literacy, data literacy, emotional literacy, financial literacy, focus literacy, futures literacies, game literacy, graphic literacy, health literacy, literacy education, legal literacy, media literacy, multiliteracies, new literacies, new media literacies, navigation literacy, numerical literacy, participatory/participation literacy, personal literacy, psycho-literacy, scientific literacy, search engine literacy, skepticism literacy, statistical literacy, transliteracy, and visual literacy or visuacy. Note: not all of these are covered in this entry for space limitations. These future perspectives are organized in nine sections: the educational implications of literacy, information literacy, digital literacy, literacy education, multiliteracies and holistic perspectives, media literacy, futures literacy, algorithmic literacy and artificial intelligence implications, and other literacies. The purpose of this entry is to offer a brief overview and commentary on the types of literacies that we need to be aware of and competent in for the near future. As these future trends are derived from the specialized literature, they include some already occurring considerations. However, they might become more salient topics in the upcoming years, and they might entail many different implications for the future of LIS professionals, libraries, and even for education in general.}
}
@incollection{MILLER2017103,
title = {6 - Graduate and postgraduate education at a crossroads},
editor = {Susan M. Miller and Walter H. Moos and Barbara H. Munk and Stephen A. Munk},
booktitle = {Managing the Drug Discovery Process},
publisher = {Woodhead Publishing},
address = {Boston},
pages = {103-128},
year = {2017},
isbn = {978-0-08-100625-2},
doi = {https://doi.org/10.1016/B978-0-08-100625-2.00006-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780081006252000064},
author = {Susan M. Miller and Walter H. Moos and Barbara H. Munk and Stephen A. Munk},
keywords = {Academia, Career, Critical thinking, Diversity, Education, Graduate school, Immigration, Industry, Jobs, Learn by doing, Medicinal chemistry, Online education, Organic chemistry, Postdoctoral, Postgraduate, Master's degree, Doctorate.},
abstract = {In this chapter we introduce the proverbial crossroads we have reached in graduate and postgraduate education and jobs. Many factors are at play, including an explosion of information, available now, at your fingertips, a move away from memorization toward critical thinking, the importance of learning by doing, and what has been called “the gathering storm.” Core drug discovery disciplines are discussed, such as medicinal and organic chemistry, especially in the context of academia–industry symbiosis. Challenges in making sure we continue to assemble the best and the brightest to tackle important biomedical problems are considered. Finally, we scratch the surface of how to navigate employers, employment, and careers.}
}
@article{CASTANEDA2023102391,
title = {A simulation-based approach for assessing the innovation barriers in the manufacturing firms},
journal = {Technology in Society},
volume = {75},
pages = {102391},
year = {2023},
issn = {0160-791X},
doi = {https://doi.org/10.1016/j.techsoc.2023.102391},
url = {https://www.sciencedirect.com/science/article/pii/S0160791X23001963},
author = {Monica Castaneda and Milton M. Herrera and Alberto Méndez-Morales},
keywords = {Product innovation, Process innovation, Manufacturing sector, System dynamics, Barriers to innovation, Innovation policy},
abstract = {One of the most important challenges organisations’ faces to innovate is dealing with different types of barriers. Particularly, the case of manufacturing firms confronts several barriers, such as demand uncertainty, product imitation, lack of employees, scarcity of government funding, absence of internal and external financing. This paper aims to provide new insights regarding to the innovation barriers faced by the manufacturing firms. To do this, we implemented a computational model for analysing the barriers to innovation in the Colombian case. In this model, product and processes innovation are studied. It was concluded that for the innovation of process, the highly important barrier is the shortcoming of internal financing, while for the innovation of product is the lack of employees. Results show that the government expenditure is scarce compared to private and external investment.}
}
@incollection{MARKOVA2015443,
title = {Representations, Social Psychology of},
editor = {James D. Wright},
booktitle = {International Encyclopedia of the Social & Behavioral Sciences (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {443-449},
year = {2015},
isbn = {978-0-08-097087-5},
doi = {https://doi.org/10.1016/B978-0-08-097086-8.24084-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780080970868240841},
author = {Ivana Marková},
keywords = {Anchoring, Cognitive polyphasia, Common sense, Communication, Dialogicality, Ego–Alter–Object, Ethics, Figurative scheme, Imagination, Interactional epistemology, Intervention strategies, Language, Objectification, Social representations, Themata},
abstract = {The theory of social representations studies the formation and transformation of meanings and activities of complex social phenomena like health and illness, political problems or environmental issues in and through language and communication, history and culture. There are two mutually interdependent meanings of social representations. The first meaning concerns the theory of social representations as an interactional theory of knowledge. It refers to networks of concepts and figurative schemes that are generated in and through tradition, common sense, daily knowledge, and communication; these are shared by particular groups and communities. The main features of this theory are the Ego–Alter–Object, the field, the interdependence of asymmetries and symmetries, ethics, figurative scheme, and cognitive polyphasia. Second, social representations refer to concrete social phenomena and to forms of apprehending and creating social realities in and through communication, experience, social practices, and interventions. Human thinking is characterized by the capacity to make distinctions and understand phenomena as dyadic antinomies or themata. Thematization of dyadic antinomies is linked with anchoring and objectification, through which social representations are formed and transformed.}
}
@article{WANG1996579,
title = {The IDS model of intelligent design system},
journal = {Computers & Structures},
volume = {61},
number = {3},
pages = {579-586},
year = {1996},
issn = {0045-7949},
doi = {https://doi.org/10.1016/0045-7949(96)00054-5},
url = {https://www.sciencedirect.com/science/article/pii/0045794996000545},
author = {Xiaotong Wang},
abstract = {Existing models of intelligent design system nowadays are generally logic-based, which solve only simple and small-scale design problems. In the author's opinion, these models which concentrate only on far-fetched use of logical inference and abstract knowledge deviate from designer's thinking and decision process; the crux of the deviation is the lack of imitating thinking with mental imagery ability. Considering the nature of design problems and imitating rational thinking with alternate use of pattern association and symbolic operation, a new intelligent design system (IDS) model and its implementation techniques are presented. Imitation of thinking with mental imagery which is also called pattern association in the IDS model is considered by applying artificial neural network (ANN) techniques. The pattern association in the IDS model imitates the rule of human thinking, “comprehending by analogy”, to some extent. Because of the robustness of the pattern-type knowledge used in pattern association, IDS provides a practical way in producing a design scheme using incomplete and/or undeterminate input data, which is very difficult to achieve in general expert design systems. According to the IDS model, an intelligent structural layout design system of wing (ISDW) is developed. ISDW realizes mapping from key parameters of design requirements and the environment of the wing to the layout design of wing structure in not only graphic form, but also in readable data form. After getting a layout of wing structure, the user will modify it interactively by Auto-CAD, and then return to the ISDW environment to produce FEM meshes by an intelligent meshing interface in order to do the preliminary static and dynamic structural analysis. The design schemes created by the system proved to be proper and usable, and this concludes that IDS model is practicable and practical.}
}
@article{FUKAI2021145,
title = {Neural mechanisms for learning hierarchical structures of information},
journal = {Current Opinion in Neurobiology},
volume = {70},
pages = {145-153},
year = {2021},
note = {Computational Neuroscience},
issn = {0959-4388},
doi = {https://doi.org/10.1016/j.conb.2021.10.011},
url = {https://www.sciencedirect.com/science/article/pii/S0959438821001252},
author = {Tomoki Fukai and Toshitake Asabuki and Tatsuya Haga},
abstract = {Spatial and temporal information from the environment is often hierarchically organized, so is our knowledge formed about the environment. Identifying the meaningful segments embedded in hierarchically structured information is crucial for cognitive functions, including visual, auditory, motor, memory, and language processing. Segmentation enables the grasping of the links between isolated entities, offering the basis for reasoning and thinking. Importantly, the brain learns such segmentation without external instructions. Here, we review the underlying computational mechanisms implemented at the single-cell and network levels. The network-level mechanism has an interesting similarity to machine-learning methods for graph segmentation. The brain possibly implements methods for the analysis of the hierarchical structures of the environment at multiple levels of its processing hierarchy.}
}
@incollection{HALFORD2020327,
title = {Cognitive Developmental Theories☆},
editor = {Janette B. Benson},
booktitle = {Encyclopedia of Infant and Early Childhood Development (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {327-336},
year = {2020},
isbn = {978-0-12-816511-9},
doi = {https://doi.org/10.1016/B978-0-12-809324-5.05787-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128093245057874},
author = {G.S. Halford},
keywords = {Analogy, Cognitive complexity, Conceptual chunking, Dynamic systems, Information processing, Mental models, Neural net, Object permanence, Relational knowledge, Symbolic processes, Theory of mind, Working memory},
abstract = {Theories of cognitive development are reviewed, beginning with pioneering theories by Piaget and Vygotsky. Neo-Piagetian theories which integrated Piagetian theory with other conceptions of cognition were developed by McLaughlin, Pascual-Leone, Case, Fischer, and Chapman. Complexity theories propose that children become capable of dealing with more complex relations as they develop. Information processing theories, neural net theories, dynamic systems theories, and theories of reasoning processes all provide models of the reasoning processes employed by children at different ages. Microgenetic analysis methods are used to study the processes of transition from one level of thinking to the next. Conceptual coherence is achieved by categorizing cognitive processes according to their core properties.}
}
@article{WILKINSON2013394,
title = {The past and the future of business marketing theory},
journal = {Industrial Marketing Management},
volume = {42},
number = {3},
pages = {394-404},
year = {2013},
note = {Theoretical Perspectives in Industrial Marketing Management},
issn = {0019-8501},
doi = {https://doi.org/10.1016/j.indmarman.2013.02.007},
url = {https://www.sciencedirect.com/science/article/pii/S0019850113000266},
author = {Ian F. Wilkinson and Louise C. Young},
keywords = {Complex adaptive systems, Business relations and networks, Dynamics and evolution, Agent based models, Mechanisms},
abstract = {A complex systems approach to understanding and modelling business marketing systems is described. The focus is on the dynamics and evolution of such systems and the processes and mechanisms driving this, rather than the more usual comparative static, variables based statistical models. Order emerges in a self-organising, bottom up way from the local or micro actions and interactions of those involved. We describe the development of our thinking regarding this approach and its main features, including the development of agent based simulation models and the identification and modelling of underlying mechanisms and processes. We conclude by discussing the implications of this approach for business marketing theory and research.}
}
@article{OXMAN1994141,
title = {Precedents in design: a computational model for the organization of precedent knowledge},
journal = {Design Studies},
volume = {15},
number = {2},
pages = {141-157},
year = {1994},
issn = {0142-694X},
doi = {https://doi.org/10.1016/0142-694X(94)90021-3},
url = {https://www.sciencedirect.com/science/article/pii/0142694X94900213},
author = {Rivka E Oxman},
keywords = {case-based reasoning, design precedents, memory organization},
abstract = {A computational model for the organization of design precedent knowledge is developed. The model is composed of distinct chunks of knowledge called design stories. A formalism for the design story is proposed which represents the linkage between design issue, concept and form in designs. Stories are structured in memory according to a semantic network. The lexicon of the semantic network acts as a memory index. The memory structure and indexing system are demonstrated to enhance search and to support cross-contextual browsing and exploration in the precedent library. The approach is demonstrated in a pilot design aid system in the task domain of early conceptual design in architecture.}
}
@article{BRESSANELLI2024142512,
title = {Are digital servitization-based Circular Economy business models sustainable? A systemic what-if simulation model},
journal = {Journal of Cleaner Production},
volume = {458},
pages = {142512},
year = {2024},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2024.142512},
url = {https://www.sciencedirect.com/science/article/pii/S0959652624019607},
author = {Gianmarco Bressanelli and Nicola Saccani and Marco Perona},
keywords = {Circular economy, Digital servitization, Sustainability impact assessment, Electrical and electronics equipment, Life cycle thinking, Systemic perspective},
abstract = {Manufacturing companies are struggling with the implementation of Circular Economy, especially due to the uncertainty regarding its potential sustainability benefits. In particular, and despite digital servitization is advocated by several studies as a way to achieve environmental gains, circular business models based on digital servitization are not always sustainable due to burden shifting and unexpected consequences which are difficult to assess before implementation. This is particularly relevant for the Electrical and Electronics Equipment industry, which suffers structural weaknesses such as the dependance on critical raw materials and an increasing waste generation. However, literature lacks models and tools able to address the complexity inherent in the systemic micro-macro perspective envisioned by Circular Economy, while studies that quantitatively assess the sustainability impacts and trade-offs of digital servitization-based circular scenarios are limited. This article aims to develop a better understanding of how the sustainability impacts of circular and servitized scenarios can be assessed and quantified at the economic, environmental, and social level, adopting a systemic perspective through the development of a what-if simulation model. The model is implemented in a spreadsheet tool and applied to a digital servitization-based Circular Economy scenario inspired by the case of a company offering long-lasting, high-efficient washing machines as-a-service. Results show that digital servitization can actually lead to a win-win-win situation with net positive effects to the environment, the society, and the economy. This result is based on the joint application of product design for digitalization and life extension, pay-per-use business models, and product reuse. These results are robust within a significant range of key parameters values. Practitioners and policymakers may use the model to support the evaluation of different circular and servitized scenarios before implementation.}
}
@article{BELLO2025101316,
title = {Self-control on the path toward artificial moral agency},
journal = {Cognitive Systems Research},
volume = {89},
pages = {101316},
year = {2025},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2024.101316},
url = {https://www.sciencedirect.com/science/article/pii/S1389041724001104},
author = {Paul Bello and Will Bridewell},
keywords = {Self-control, Attention, Cognitive architecture},
abstract = {The ability of agents to commit to their plans and see them through is a core concept in the philosophy of action (Bratman, 1987, Holton, 2009) and is considered to be a defining feature of having an intention. Seeing plans through in the face of highly compelling opportunities for action that are incompatible with our current commitments requires self-control. In this review paper, we draw upon ancient and modern literature on self-control along with contemporary ideas about the cognitive architecture supporting intentional action to argue that any computational account of moral agency must include an approach to self-control. In addition, we extract and develop a list of necessary features of the phenomena against which individual modeling efforts can be compared. The ARCADIA cognitive system will be discussed in light of this list of features and used to demonstrate both success and failure in a highly simplified self-control dilemma. Finally, we end by discussing a path toward more functionally complete models of agency and control, along with offering perfunctory thoughts on some of the more conceptually challenging issues to address in the future.}
}
@article{YIM2014144,
title = {A development of a quantitative situation awareness measurement tool: Computational Representation of Situation Awareness with Graphical Expressions (CoRSAGE)},
journal = {Annals of Nuclear Energy},
volume = {65},
pages = {144-157},
year = {2014},
issn = {0306-4549},
doi = {https://doi.org/10.1016/j.anucene.2013.10.029},
url = {https://www.sciencedirect.com/science/article/pii/S0306454913005598},
author = {Ho Bin Yim and Seung Min Lee and Poong Hyun Seong},
keywords = {Quantitative measure, Situation awareness, Graphical expression, NPP MCR operators},
abstract = {Operator performance measures are used for multiple purposes, such as control room design, human system interface (HSI) evaluation, training, and so on. Performance measures are often focused on results; however, especially for a training purpose – at least in a nuclear industry, more detailed descriptions about processes are required. Situation awareness (SA) measurements have directly/indirectly played as a complimentary measure and provided descriptive insights on how to improve performance of operators for the next training. Unfortunately, most of the well-developed SA measurement techniques, such as Situation Awareness Global Assessment Technique (SAGAT) need an expert opinion which sometimes troubles easy spread of measurement’s application or usage. A quantitative SA measurement tool named Computational Representation of Situation Awareness with Graphical Expressions (CoRSAGE) is introduced to resolve some of these concerns. CoRSAGE is based on production rules to represent a human operator’s cognitive process of problem solving, and Bayesian inference to quantify it. Petri Net concept is also used for graphical expressions of SA flow. Three components – inference transition, volatile/non-volatile memory tokens – were newly developed to achieve required functions. Training data of a Loss of Coolant Accident (LOCA) scenario for an emergency condition and an earthquake scenario for an abnormal condition by real plant operators were used to validate the tool. The validation result showed that CoRSAGE performed a reasonable match to other performance results.}
}
@article{BRENT19961,
title = {Advances in the computational study of language acquisition},
journal = {Cognition},
volume = {61},
number = {1},
pages = {1-38},
year = {1996},
note = {Compositional Language Acquisition},
issn = {0010-0277},
doi = {https://doi.org/10.1016/S0010-0277(96)00779-2},
url = {https://www.sciencedirect.com/science/article/pii/S0010027796007792},
author = {Michael R. Brent},
abstract = {This paper provides a tutorial introduction to computational studies of how children learn their native languages. Its aim is to make recent advances accessible to the broader research community, and to place them in the context of current theoretical issues. The first section locates computational studies and behavioral studies within a common theoretical framework. The next two sections review two papers that appear in this volume: one on learning the meanings of words and one on learning the sounds of words. The following section highlights an idea which emerges independently in these two papers and which I have dubbed autonomous bootstrapping. Classical bootstrapping hypotheses propose that children begin to get a toe-hold in a particular linguistic domain, such as syntax, by exploiting information from another domain, such as semantics. Autonomous bootstrapping complements the cross-domain acquisition strategies of classical bootstrapping with strategies that apply within a single domain. Autonomous bootstrapping strategies work by representing partial and/or uncertain linguistic knowledge and using it to analyze the input. The next two sections review two more more contributions to this special issue: one on learning word meanings via selectional preferences and one on algorithms for setting grammatical parameters. The final section suggests directions for future research.}
}
@article{HUANG2022108818,
title = {Hippocampus-heuristic character recognition network for zero-shot learning in Chinese character recognition},
journal = {Pattern Recognition},
volume = {130},
pages = {108818},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108818},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002990},
author = {Guanjie Huang and Xiangyu Luo and Shaowei Wang and Tianlong Gu and Kaile Su},
keywords = {Chinese character recognition, Hippocampus thinking, Radical analysis, Zero-shot learning, Label embedding},
abstract = {The recognition of Chinese characters has always been a challenging task due to their huge variety and complex structures. The current radical-based methods fail to recognize Chinese characters without learning all of their radicals in the training stage. To this end, we propose a novel Hippocampus-heuristic Character Recognition Network (HCRN), which can recognize unseen Chinese characters only by training part of radicals. More specifically, the network architecture of HCRN is a new pseudo-siamese network designed by us, which can learn features from pairs of input samples and use them to predict unseen characters. The experimental results on the recognition of printed and handwritten characters show that HCRN is robust and effective on zero/few-shot learning tasks. For the printed characters, the mean accuracy of HCRN outperforms the state-of-the-art approach by 23.93% on recognizing unseen characters. For the handwritten characters, HCRN improves the mean accuracy by 11.25% on recognizing unseen characters.}
}
@article{SCHAEFER198897,
title = {A history of ab initio computational quantum chemistry: 1950–1960},
journal = {Tetrahedron Computer Methodology},
volume = {1},
number = {2},
pages = {97-102},
year = {1988},
issn = {0898-5529},
doi = {https://doi.org/10.1016/0898-5529(88)90014-0},
url = {https://www.sciencedirect.com/science/article/pii/0898552988900140},
author = {Henry F. Schaefer},
keywords = {Quantum chemistry, Ab initio, Electronic structure theory, Molecular quantum mechanics, Computations},
abstract = {Although ab initio computational quantum chemistry produced virtually no predictions of chemical interest during the 1950's, an important foundation for future work was laid during this decade. Much of this fundamental computational research was carried out in the laboratories of Frank Boys in Cambridge (England) and Clemens Roothaan and Robert Mulliken in Chicago. Other senior contributors to ab initio chemical theory during this period include Klaus Ruedenberg, Robert Parr, John Pople, Robert Nesbet, Harrison Shull, Per-Olov Löwdin, Isaiah Shavitt, Albert Matsen, Douglas McLean, and Bernard Ransil.}
}
@article{SFARD20121,
title = {Introduction: Developing mathematical discourse—Some insights from communicational research},
journal = {International Journal of Educational Research},
volume = {51-52},
pages = {1-9},
year = {2012},
note = {Developing mathematical discourse–Some insights from communicational research},
issn = {0883-0355},
doi = {https://doi.org/10.1016/j.ijer.2011.12.013},
url = {https://www.sciencedirect.com/science/article/pii/S0883035511001327},
author = {Anna Sfard},
keywords = {Mathematics, Discourse, Learning, Development, Cognition, Emotions, Interactions},
abstract = {Quite diverse in their foci and specific themes, the seven articles collected in this special issue are unified by their common conceptual framework. Grounded in the premise that thinking can be usefully defined as self-communicating and that mathematics can thus be viewed as a discourse, the communicational framework provides a unified set of conceptual tools with which to investigate cognitive, affective and social aspects of mathematics learning. The communicational tools are employed by the authors as they investigate diverse aspects of mathematical discourse and explore its development in the classroom and beyond. The seven studies combine together to produce a set of insights, some of which go against widespread beliefs about teaching and learning mathematics.}
}
@article{BOUDIN2016448,
title = {Opinion dynamics: Kinetic modelling with mass media, application to the Scottish independence referendum},
journal = {Physica A: Statistical Mechanics and its Applications},
volume = {444},
pages = {448-457},
year = {2016},
issn = {0378-4371},
doi = {https://doi.org/10.1016/j.physa.2015.10.014},
url = {https://www.sciencedirect.com/science/article/pii/S0378437115008602},
author = {Laurent Boudin and Francesco Salvarani},
keywords = {Opinion formation, Mass media, Kinetic equations},
abstract = {We consider a kinetic model describing some mechanisms of opinion formation in the framework of referendums, where the individuals, who can interact between themselves and modify their opinion by means of spontaneous self-thinking, are moreover under the influence of mass media. We study, at the numerical level, both the transient and the asymptotic regimes. In particular, we point out that a plurality of media, with different orientations, is a key ingredient to allow pluralism and prevent consensus. The forecasts of the model are compared to some surveys related to the Scottish independence referendum of 2014.}
}
@article{GALBUSERA2022103109,
title = {Game-based training in critical infrastructure protection and resilience},
journal = {International Journal of Disaster Risk Reduction},
volume = {78},
pages = {103109},
year = {2022},
issn = {2212-4209},
doi = {https://doi.org/10.1016/j.ijdrr.2022.103109},
url = {https://www.sciencedirect.com/science/article/pii/S2212420922003284},
author = {Luca Galbusera and Monica Cardarilli and Marina {Gómez Lara} and Georgios Giannopoulos},
keywords = {Critical infrastructure, Resilience, Preparedness, Training, Exercises, Serious games, Gamification},
abstract = {Several institutions worldwide are reflecting on the relevance of training and exercises to critical infrastructure protection and resilience. This is witnessed, for instance, by Council Directive 2008/114/EC in the EU and the Homeland Security Exercise and Evaluation Program in the US. Contributing to the research actions in the field, the present article discusses methodological approaches, tools, techniques, and technologies relevant to this domain. In particular, we report on a recent training initiative elaborated by the authors and involving a game-based, modelling-and-simulation-backed, computer-assisted exercise for critical infrastructure expert audiences. This was developed taking advantage of JRC's Geospatial Risk and Resilience Assessment Platform (GRRASP) and critical infrastructure analysis methodologies integrated therein. The overarching objective was to enhance system thinking and raise awareness of resilience aspects while familiarizing participants with specific analysis tools and scientific models.}
}
@article{STREVENS202192,
title = {Permissible idealizations for the purpose of prediction},
journal = {Studies in History and Philosophy of Science Part A},
volume = {85},
pages = {92-100},
year = {2021},
issn = {0039-3681},
doi = {https://doi.org/10.1016/j.shpsa.2020.09.009},
url = {https://www.sciencedirect.com/science/article/pii/S0039368120301813},
author = {Michael Strevens},
keywords = {Prediction, Idealization, Modeling, Difference-making, Causal relevance},
abstract = {Every model leaves out or distorts some factors that are causally connected to its target phenomenon—the phenomenon that it seeks to predict or explain. If we want to make predictions, and we want to base decisions on those predictions, what is it safe to omit or to simplify, and what ought a causal model to describe fully and correctly? A schematic answer: the factors that matter are those that make a difference to the target phenomenon. There are several ways to understand differencemaking. This paper advances a view as to which is the most relevant to the forecaster and the decision-maker. It turns out that the right notion of differencemaking for thinking about idealization in prediction is also the right notion for thinking about idealization in explanation; this suggests a carefully circumscribed version of Hempel’s famous thesis that there is a symmetry between explanation and prediction.}
}
@article{MOSKOWITZ200387,
title = {The intertwining of psychophysics and sensory analysis: historical perspectives and future opportunities—a personal view},
journal = {Food Quality and Preference},
volume = {14},
number = {2},
pages = {87-98},
year = {2003},
issn = {0950-3293},
doi = {https://doi.org/10.1016/S0950-3293(02)00072-1},
url = {https://www.sciencedirect.com/science/article/pii/S0950329302000721},
author = {Howard R. Moskowitz},
keywords = {History, Psychology, Psychophysics},
abstract = {From today’s point of view, psychophysics and sensory analysis appear conjoined, at least from the vantage point of sensory analysis. This paper shows how psychophysical thinking has not only entered sensory analysis, but also shaped some of the ways that modern day sensory analysts conceptualize their problems and go about solving them. The paper also shows how this was not always the case. The rapprochement of the two fields has only gradually developed as sensory analysis has come to accept psychophysical thinking. The paper concludes by listing a series of trends that may bring the two fields even closer in the future.}
}
@article{MININA2022104684,
title = {Neuron quantum computers and a way to unification of science: A compendium of Efim Liberman's scientific work},
journal = {Biosystems},
volume = {217},
pages = {104684},
year = {2022},
issn = {0303-2647},
doi = {https://doi.org/10.1016/j.biosystems.2022.104684},
url = {https://www.sciencedirect.com/science/article/pii/S0303264722000727},
author = {Svetlana V. Minina and Nikita E. Shklovskiy-Kordi},
keywords = {Efim liberman, cAMP, Biological computation, Biophysics, Chaimatics, Quantum biology, Unity of science, Quantum computation, Molecular cell computer, Quantum regulator},
abstract = {In 1972, Efim Liberman, a Soviet biophysicist, pioneered a brand-new approach to studying the operation of the brain, the live cell and the human mind by publishing a paper titled “Cell as a molecular computer” (1972). In this paper, Liberman posited that a consecutive/parallel stochastic molecular computer (MCC) controls a living cell. An MCC operates with molecule-words (DNA, RNA, proteins) according to the program recorded in DNA and RNA. Computational operations are implemented by molecular operators acting as enzymes. An MCC is present in each live cell. A neuron cell MCC can be involved in solving tasks for the entire organism. Neuron MCC investigation was started with studying an impact of an intracellular injection of cyclic AMP on electric activity of a neuron. Cyclic nucleotides were considered as input words for an MCC, which are generated inside a neuron as a result of synaptic activity. This led Efim Liberman to the idea that, in order to solve complex physical problems, which are encountered by a neuron and require rapid solutions, the molecular computer adjusts the operation of the quantum molecular regulator, which uses the “computational environment” of the cytoskeleton and quantum properties of the elementary hypersound quasiparticles for completing mathematical operations for the minimum price of action. Efim Liberman suggested that the human self-consciousness is a quantum computer of even a higher level and designated it as an extreme quantum regulator. In order to describe such systems, he suggested to join biology, physics and mathematics into a unified science, and formulated its four fundamental principles. Results of Efim Liberman’s theoretical and experimental studies on the topic of biological computation are summarized in this review.}
}
@article{GARAS2024100885,
title = {A data analytics case study analyzing IRS SOI migration data using no code, low code technologies},
journal = {Journal of Accounting Education},
volume = {66},
pages = {100885},
year = {2024},
issn = {0748-5751},
doi = {https://doi.org/10.1016/j.jaccedu.2024.100885},
url = {https://www.sciencedirect.com/science/article/pii/S0748575124000010},
author = {Samy Garas and Susan L. Wright},
keywords = {Robotic process automation, UiPath, Alteryx, Tableau, Data automation, Data analytics, Data visualizations, Regional migration, Government planning, Business planning},
abstract = {Organizations generate and accumulate vast amounts of structured and unstructured data that have value for formulating and supporting strategic decisions. The advancement of no-code and low-code software has enabled the use of this data to provide significant data insights and business intelligence by employing multiple forms of data analytics. The imperative to cultivate a robust and proficient group of individuals with expertise in data analytics has led to a substantial increase in the number of educational programs focused on data science and analytics. Accounting educators can capitalize on these trends by integrating data analytics and software skills into the accounting curriculum. This case offers essential materials to aid in the development of the curriculum to support accounting and analytics educators. This case serves many objectives by providing a professional setting in which you take on the role of junior data analyst, offering necessary context and motivation for completing the tasks. The case allows you to analyze extensive data sets obtained from the IRS Statistics of Income (SOI) website in order to investigate migration patterns based on state, year, age, and income categories. UiPath-robotic process automation (RPA), Alteryx-based data analysis, and Tableau-based data visualization tools are employed to extract, generate, and present descriptive statistics and to conduct a simple times series analysis. These insights are highly valuable to decision makers in business and government organizations. You are encouraged to engage in critical thinking and to consider the potential impacts of migratory patterns on choices made by firm executives and public policy makers. Migration patterns have a significant impact on firm management decisions, influencing either to expand or reduce current operations and indicating the availability and expansion of new talent pools. Migration patterns have a significant impact on the decision made by public policy makers, particularly in relation to public utilities, infrastructure, and other services and benefits. You analyze temporal data to deduce the influence of changes in the tax code and shifts in the economy. You gain expertise in managing large data sets, exploring features of analytics software, and creating compelling visualizations to effectively communicate important discoveries. Instructors and students are given comprehensive instructions and videos to facilitate the efficient application of these technologies.}
}
@incollection{SUGHRUE2024151,
title = {Chapter 6 - Reimagining neurocognitive functions as emergent phenomena: What resting state is really showing us},
editor = {Michael E. Sughrue and Jacky T. Yeung and Nicholas B. Dadario},
booktitle = {Connectomic Medicine},
publisher = {Academic Press},
pages = {151-157},
year = {2024},
isbn = {978-0-443-19089-6},
doi = {https://doi.org/10.1016/B978-0-443-19089-6.00008-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780443190896000082},
author = {Michael E. Sughrue and Jacky T. Yeung and Nicholas B. Dadario},
keywords = {Brain hub, Brain landscape, Network control theory, Neurocognitive function, Resting-state fMRI, Structural connectome},
abstract = {In this chapter, we introduce a new way of thinking about neurocognitive functioning and related dysfunction. We discuss how structural wiring patterns, global rhythms in deep structures, and electrochemical gain from neurotransmitters play a key role in the internal dynamics of what the brain is doing. Importantly, together, these elements dictate how the brain can or cannot obtain different brain states. Simultaneously, disruption in intrinsic structures and internal dynamics alters the energetic landscape causing some brain states to become more favorable or less favorable. Importantly, we go on to describe how landscapes arise from structural connectomes, and how these connections can dictate spontaneous behavioral patterns and tendencies in normal as well as pathologic states, such as a depressed patient being stuck in a self-ruminating and negative state. Resting-state fMRI also provides a keyhole into these processes as the entire set of the structural connectome creates the patterns of functional connectivity seen in resting-state brain activity.}
}
@incollection{BROWN201589,
title = {Space, Linguistic Expression of},
editor = {James D. Wright},
booktitle = {International Encyclopedia of the Social & Behavioral Sciences (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {89-93},
year = {2015},
isbn = {978-0-08-097087-5},
doi = {https://doi.org/10.1016/B978-0-08-097086-8.57017-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780080970868570172},
author = {Penelope Brown},
keywords = {Adpositions, Language and cognition, Language universals, Locative constructions, Motion verbs, Space, Spatial frames of reference, Topological language},
abstract = {Spatial cognition is central to human thinking, and spatial language is thus an important area of study, as it may reveal fundamental properties of human thought. Recent research has shown that spatial language is much more divergent across languages than had previously been thought, suggesting significant cultural patterning of spatial conceptualization. This article reviews spatial language cross-linguistically, sets out a typological framework for the language of space, and considers the relationship of spatial language to spatial cognition, in the context of extensive linguistic diversity in the spatial domain.}
}
@article{LEE2021101596,
title = {Measuring Mohr social capital},
journal = {Poetics},
volume = {88},
pages = {101596},
year = {2021},
note = {Measure Mohr Culture},
issn = {0304-422X},
doi = {https://doi.org/10.1016/j.poetic.2021.101596},
url = {https://www.sciencedirect.com/science/article/pii/S0304422X21000863},
author = {Monica Lee and Amaç Herdağdelen and Minsu Park and John Levi Martin},
abstract = {We here bring together two different traditions of thinking about social capital. One, the Tocquevillian, looks to associations and group memberships as the core of social capital. The other, the Colemanian, looks to interpersonal networks as the core of social capital. We argue that the most common way of articulating how humans use these types of relationships in different ways—the distinction between “bridging” and “bonding” social capital—is epistemically unstable. What might be possible, however, is to use the insights developed by Ronald Burt regarding tie non-redundancy to study associational social capital. We do this by drawing on the insights of the approach consistently adopted and developed by John Mohr, which emphasizes duality and diversity, to develop measures of group affiliation-based social capital. We accordingly, for both Tocquevillian and Colemanian social capital, distinguish measures that focus on the mass of social capital from those that focus on its diversity. To illustrate, we use de-identified data from 77 Million U.S. Facebook Groups users to measure their degree of all resulting types of social capital. We show that our understanding of who has the most social capital varies greatly by whether we are considering Tocquevillian or Colemanian capital, and whether we are focusing on mass or diversity.}
}
@article{COUVELAS2020326,
title = {Bioclimatic building design theory and application},
journal = {Procedia Manufacturing},
volume = {44},
pages = {326-333},
year = {2020},
note = {The 1st International Conference on Optimization-Driven Architectural Design (OPTARCH 2019)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.02.238},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920308258},
author = {Agnes Couvelas},
keywords = {Modern architecture, Cultural heritage, Sustainable architecture design, Bioclimatic Performance Optimization, Inter-locality},
abstract = {Ecological thinking is the recognition of the dialectic unity between natural and man-made environment, the respect to what exists around us, and the concomitant “openness” toward others. Here, I present examples from my own work to describe a number of passive bioclimatic approaches focused on the above principles. First, the use of the wind as an expressive element in building design, including the enhancement of air flow in the interior space, the moderation of wind and sand accumulation, the moderation of the sound carried by prevailing winds, and the conversion of the wind into a means of protection against its own force. Second, the use of adaptive building envelopes and shading systems to achieve control of natural light, ventilation and temperature of the inner space through their own transformability, surface openings and materials, including planting as a building material; in a sense, treating buildings as living organisms. Three of these examples have been included in the H2020-MSCA-RISE OptArch project, in which I am scientifically responsible for the work package WP5 entitled “Improvement of bioclimatic design through optimization of performance”.}
}
@article{SCHWARZ201359,
title = {Business wargaming for teaching strategy making},
journal = {Futures},
volume = {51},
pages = {59-66},
year = {2013},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2013.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S0016328713000864},
author = {Jan Oliver Schwarz},
keywords = {Business wargaming, Teaching, Simulation, Management education, Strategy making, Strategic thinking},
abstract = {An increasingly complex and dynamic business environment requires new approaches to teaching strategy to management students. Business wargaming, a dynamic strategic simulation, is discussed as a management simulation which can respond to the contemporary challenges in management education. Reflecting on the practical use of business wargaming in the classroom, it is described how such simulations prepare management students for making strategic decisions in complex and dynamic environments characterised by high uncertainty concerning the future.}
}
@article{LIN20162176,
title = {New statistical analysis in marketing research with fuzzy data},
journal = {Journal of Business Research},
volume = {69},
number = {6},
pages = {2176-2181},
year = {2016},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2015.12.026},
url = {https://www.sciencedirect.com/science/article/pii/S0148296315006517},
author = {Hsin-Cheng Lin and Chen-Song Wang and Juei Chao Chen and Berlin Wu},
keywords = {Decision making, Fuzzy statistics, Fuzzy data, Marketing research},
abstract = {This research proposes new statistical methods for marketing research and decision making. The study employs a soft computing technique and a new statistical tool to evaluate people's thinking. Because the classical measurement system has difficulties in dealing with the non-real valued information, the study aims to find an appropriate measurement system to overcome this problem. The main idea is to decompose the data into a two-dimensional type, centroid and its length (area). The two-dimensional questionnaires this study proposes help reaching market information.}
}
@article{KOPPAKA2024,
title = {Mechanism and Selectivity of Bi(V)-Aryl Oxyfunctionalization in Trifluoroacetic Acid Solvents},
journal = {Organometallics},
year = {2024},
issn = {0276-7333},
doi = {https://doi.org/10.1021/acs.organomet.4c00319},
url = {https://www.sciencedirect.com/science/article/pii/S0276733324003509},
author = {Anjaneyulu Koppaka and Dongdong Yang and Sanaz Mohammadzadeh Koumleh and Burjor Captain and Roy A. Periana and Daniel H. Ess},
abstract = {The oxidative functionalization of aromatic sp2 C–H bonds to C–O bonds is a difficult transformation. For main-group metals, the oxyfunctionalization step of a metal-aryl bond is generally slow and potentially problematic if carried out in a relatively strong acid solvent where protonation could prevent oxyfunctionalization. In this work, we experimentally and computationally analyzed the oxyfunctionalization reaction of (Ph)3BiV(TFA)2 (TFA = trifluoroacetate) in a trifluoroacetic acid (TFAH) solvent. Experiments showed a single oxyfunctionalization product phenyl TFA (PhTFA) and two equivalents of benzene. Explicit/continuum solvent density functional theory calculations revealed that a direct intramolecular reductive functionalization pathway is lower in energy than radical or ionic pathways, and surprisingly from (Ph)3BiV(TFA)2, the reductive functionalization pathway is potentially competitive with protonation. In contrast, for (Ph)2BiV(TFA)3 oxyfunctionalization is significantly lower in energy than protonation. For BiIII-phenyl intermediates, redox neutral protonation is significantly lower in energy than a second functionalization. We also examined the oxyfunctionalization versus protonation of BiV-phenyl complexes with a coordinated biphenyl ligand and a coordinated biphenyl sulfone ligand, which both resulted in oxyfunctionalization. For the biphenyl ligand complex, a protonation-first mechanism is proposed, while for the biphenyl sulfone ligand, an oxyfunctionalization first mechanism is consistent with both calculations and experiments.
}
}
@article{SATTARI2021104981,
title = {Application of Bayesian network and artificial intelligence to reduce accident/incident rates in oil & gas companies},
journal = {Safety Science},
volume = {133},
pages = {104981},
year = {2021},
issn = {0925-7535},
doi = {https://doi.org/10.1016/j.ssci.2020.104981},
url = {https://www.sciencedirect.com/science/article/pii/S0925753520303787},
author = {Fereshteh Sattari and Renato Macciotta and Daniel Kurian and Lianne Lefsrud},
keywords = {Artificial intelligence, Bayesian network, Machine learning, Keyword analysis, Incident data, Process safety management, Latent causes},
abstract = {Process safety management (PSM) is a framework that demonstrates a company’s commitment to process safety, a better understanding of hazards and risks, a comprehensive assessment and management of risks, and enhanced learning from experience to improve overall safety and operational performance. Companies often use an incident data reporting system to execute PSM. While companies keep incident data in thousands of reports, rarely do they glean full value in learning from these to prevent and reduce future incidents. To overcome this challenge, this research applied machine learning and keyword analysis to label and classify 8199 incident reports from an oil and gas company into nine groups identified in the latest version of PSM guidelines published by the Center for Chemical Process Safety (CCPS). To converge on an optimal solution, two different Bayesian network techniques (Tabu and hill climbing) were applied. Both methods resulted in the same map, showing that the Total Number of Incidents has the maximum dependency (50%) on Asset Integrity & Reliability; this means focusing resources on this aspect could reduce the total number of incidents by half. Cross correlation analysis (CCA) was also applied, which validated and confirmed this result. This analysis identifies which measures enhance the company’s safety management strategy to reduce these latent causes, but also supports critical thinking, enhanced communication, and learning culture to improve organizational safety.}
}
@article{HAJELA20021,
title = {Soft computing in multidisciplinary aerospace design—new directions for research},
journal = {Progress in Aerospace Sciences},
volume = {38},
number = {1},
pages = {1-21},
year = {2002},
issn = {0376-0421},
doi = {https://doi.org/10.1016/S0376-0421(01)00015-X},
url = {https://www.sciencedirect.com/science/article/pii/S037604210100015X},
author = {Prabhat Hajela},
abstract = {There has been increased activity in the study of methods for multidisciplinary analysis and design. This field of research has been a busy one over the past decade, driven by advances in computational methods and significant new developments in computer hardware. There is a concern, however, that while new computers will derive their computational speed through parallel processing, current algorithmic procedures that have roots in serial thinking are poor candidates for use on such machines—a paradigm shift is required! Among new advances in computational methods, soft computing techniques have enjoyed a remarkable period of development and growth. Of these, methods of neural computing, evolutionary search, and fuzzy logic have been the most extensively explored in problems of multidisciplinary analysis and design. The paper will summarize important accomplishments to-date, of neurocomputing, fuzzy logic, and evolutionary search, including immune network modeling, in the field of multidisciplinary aerospace design.}
}
@article{1995462,
title = {95/06537 Historical rates of atmospheric Pb deposition using 210Pb dated peat cores: Corroboration, computation, and interpretation},
journal = {Fuel and Energy Abstracts},
volume = {36},
number = {6},
pages = {462},
year = {1995},
issn = {0140-6701},
doi = {https://doi.org/10.1016/0140-6701(95)98112-5},
url = {https://www.sciencedirect.com/science/article/pii/0140670195981125}
}
@article{GORMONG20231988,
title = {Neighboring Group Effects on the Rates of Cleavage of Si–O–Si-Containing Compounds},
journal = {The Journal of Organic Chemistry},
volume = {88},
number = {4},
pages = {1988-1995},
year = {2023},
issn = {0022-3263},
doi = {https://doi.org/10.1021/acs.joc.2c02126},
url = {https://www.sciencedirect.com/science/article/pii/S002232632300107X},
author = {Ethan A. Gormong and Dorian S. Sneddon and Theresa M. Reineke and Thomas R. Hoye},
abstract = {ABSTRACT
The presence of a nearby tethered functional group (G, G = tertiary amide or amine) can significantly impact the rate of cleavage of an Si–O bond. We report here an in situ1H NMR spectroscopic investigation of the relative rates of cleavage of model substrates containing two different Si–O substructures, namely alkoxydisiloxanes [GRO–Si­(Me2)–O–SiMe3] and carbodisiloxanes [GR–Si­(Me2)–O–SiMe3]. The trends in the relative rates (which slowed with increasing chain length, with a notable exception) of alkoxydisiloxane hydrolyses were probed via computation. The results correlated well with the experimental data. In contrast to the hydrolysis of the alkoxydisiloxanes, the carbodisiloxanes were not fully hydrolyzed, but rather formed an equilibrium mixture of starting asymmetric disiloxane, two silanols, and a new symmetrical disiloxane. We also uncovered a facile siloxy-metathesis reaction of an incoming silanol with the carbodisiloxane substrate [e.g., Me2NR–Si­(Me2)–O–SiMe3 + HOSiEt3 ⇋ Me2NR–Si­(Me2)–O–SiEt3 + HOSiMe3] facilitated by the pendant dimethylamino group, a process that was also probed by computation.}
}
@incollection{SUGHRUE2024205,
title = {Chapter 12 - Connectomic approaches to neurosurgical planning},
editor = {Michael E. Sughrue and Jacky T. Yeung and Nicholas B. Dadario},
booktitle = {Connectomic Medicine},
publisher = {Academic Press},
pages = {205-214},
year = {2024},
isbn = {978-0-443-19089-6},
doi = {https://doi.org/10.1016/B978-0-443-19089-6.00011-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780443190896000112},
author = {Michael E. Sughrue and Jacky T. Yeung and Nicholas B. Dadario},
keywords = {Brain tumor surgery, Cerebral cortex, Cognitive deficits, fMRI, Graph theory, Neuro-Oncology, Onco-functional balance},
abstract = {In this chapter, we introduce how connectomics can provide an improved understanding of the structural and functional organization of the human brain which can be applied for intracerebral brain surgery. In particular, such connectomic thinking expands our ability to improve patient functional outcomes after surgery beyond mere motor and language functions by also considering the anatomy responsible for complex cognitive functions. We introduce the concept of “disconnection surgery,” where the surgical decisions when removing a tumor can be thought of a series of specific cuts that we plan to perform on the periphery of the tumor such that we can disconnect the tumor from the surrounding networks. Connectomics allows us to define the risks associated with specific tumors and surgical decisions, which can subsequently guide the operation but also tailor preoperative patient discussion. Novel mathematical concepts from the field of network neuroscience on graph theory are also introduced so as to better define truly eloquent brain regions on an individualized basis.}
}
@incollection{MILLER2023203,
title = {Chapter 7 - The calculated uncertainty of scientific discovery: From Maths to Deep Maths},
editor = {Steven G. Krantz and Arni S.R. {Srinivasa Rao} and C.R. Rao},
series = {Handbook of Statistics},
publisher = {Elsevier},
volume = {49},
pages = {203-226},
year = {2023},
booktitle = {Artificial Intelligence},
issn = {0169-7161},
doi = {https://doi.org/10.1016/bs.host.2023.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S016971612300024X},
author = {D. Douglas Miller},
keywords = {Mathematics, Philosophy, Statistics, Null hypothesis, Artificial intelligence, Data dimensionality, Machine learning, Algorithms, Deep learning, Stochastic gradient descent, Model optimization, Bias, Neural networks, Backpropagation, Large language models, Model generalizability},
abstract = {Throughout history, diverse Maths have underpinned numerous important natural and physical science discoveries. In their initial development and application, these Maths were often incompletely or imperfectly understood, with constants and “fudge factors” needed to account for statistical uncertainties to advance a scientific discipline. Some polymaths have acted as philosophers in support of new ways of thinking, based on their novel discoveries about the natural and physical world. Deep Maths integral to artificial intelligence (AI), machine learning and deep learning (DL), are also subject to human imperfections (i.e., computational errors, operator assumptions) and stochastic uncertainties (i.e., modeling biases, convergence optimizers). Mathematicians and domain experts can collaborate to increase AI model accuracy by improving training data quality (i.e., curating, reducing dimensionality), mitigating human and machine biases, and understanding data contexts prior to query. Since the advent of DL and through the design of multilayered feedforward neural networks then large language models, scientists have applied advanced AI computing capabilities to push the limits of this technology trend. Recently, AI's capacity to uncover newly modeled insights has been hyped beyond the proven limits of DL model accuracy. History has witnessed the acceptance of new knowledge (primarily by peers) based on the accuracy and/or reproducibility of empirical observations and on varied interpretations of mathematical proofs. Societal enthusiasm for science or technology insertion is often limited by the general public's understanding of the underlying Maths and Deep Maths, and related human fears and concerns of displacement (i.e., lost jobs, ecological impact, less privacy, etc.). Today's proponents of societal progress based on new discoveries and technologies are motivated by a range of influences (i.e., humanity, control, security, profit, etc.), creating additional uncertainties that can deflect initial scientific enthusiasm and/or delay widespread adoption.}
}
@article{BREIT2025102621,
title = {Mathematics achievement and learner characteristics: A systematic review of meta-analyses},
journal = {Learning and Individual Differences},
volume = {118},
pages = {102621},
year = {2025},
issn = {1041-6080},
doi = {https://doi.org/10.1016/j.lindif.2024.102621},
url = {https://www.sciencedirect.com/science/article/pii/S1041608024002140},
author = {Moritz Breit and Michael Schneider and Franzis Preckel},
keywords = {Mathematics achievement, meta-analysis, Systematic review, Math talent, Predictors},
abstract = {Learners' individual differences in mathematics achievement are associated with individual differences in psychological characteristics. A number of meta-analyses have quantified the strengths of these correlations. However, these findings are scattered across different strands of the literature. The present systematic review aims to integrate these strands by providing an overview of meta-analyses of psychological correlates of mathematics achievement. We conducted a systematic literature search and included 30 meta-analyses, reporting correlations between mathematics achievement and 66 variables based on 13,853 effect sizes and an estimated 4,658,717 participants. The correlations are rank-ordered by size and complemented with information about the meta-analyses, their inclusion criteria, and methods. The results show strong associations of mathematics achievement with verbal skills and abilities, prior knowledge, intelligence, creativity, math-specific skills, math self-concept, self-regulation, meta-cognition, and executive functions. Relatively weaker relations were observed for emotional intelligence, achievement goals, academic emotions, and the Big Five personality traits.}
}
@article{GLASSMEYER2021100873,
title = {Identifying and supporting teachers’ robust understanding of proportional reasoning},
journal = {The Journal of Mathematical Behavior},
volume = {62},
pages = {100873},
year = {2021},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2021.100873},
url = {https://www.sciencedirect.com/science/article/pii/S0732312321000341},
author = {David Glassmeyer and Aaron Brakoniecki and Julie M. Amador},
keywords = {Content knowledge, Knowledge resource, Proportional reasoning, Proportions, Ratio, Teachers},
abstract = {This case study uses the Framework for Teachers’ Robust Understanding of Proportional Reasoning for Teaching (Weiland et al., 2020) to characterize how 51 mathematics teachers solved a comparison proportional problem. We found 50 of the 51 teachers productively drew upon four knowledge resources: (1) proportional situation, (2) ratios as part: part or part: whole, (3) unit rates, and (4) ratio as measure. This study details these and teachers’ less commonly used knowledge resources, as well as counterproductive statements related to the knowledge resources. We analyze the structure of the comparison proportion problem and suggest why teachers drew on particular knowledge resources. Lastly, we highlight how counterproductive statements highlight areas of focus for mathematics teacher educators and extends the operationalizing of the robust proportional reasoning framework for mathematics education researchers.}
}