@article{LEHRER200039,
title = {Developing Model-Based Reasoning in Mathematics and Science},
journal = {Journal of Applied Developmental Psychology},
volume = {21},
number = {1},
pages = {39-48},
year = {2000},
issn = {0193-3973},
doi = {https://doi.org/10.1016/S0193-3973(99)00049-0},
url = {https://www.sciencedirect.com/science/article/pii/S0193397399000490},
author = {Richard Lehrer and Leona Schauble},
abstract = {It is essential to base instruction on a foundation of understanding of children's thinking, but it is equally important to adopt the longer-term view that is needed to stretch these early competencies into forms of thinking that are complex, multifaceted, and subject to development over years, rather than weeks or months. We pursue this topic through our studies of model-based reasoning. We have identified four forms of models and related modeling practices that show promise for developing model-based reasoning. Models have the fortuitous feature of making forms of student reasoning public and inspectable—not only among the community of modelers, but also to teachers. Modeling provides feedback about student thinking that can guide teaching decisions, an important dividend for improving professional practice.}
}
@incollection{PERKINS2002187,
title = {Standard logic as a model of reasoning: The empirical critique},
editor = {Dov M. Gabbay and Ralph H. Johnson and Hans Jürgen Ohlbach and John Woods},
series = {Studies in Logic and Practical Reasoning},
publisher = {Elsevier},
volume = {1},
pages = {187-223},
year = {2002},
booktitle = {Handbook of the Logic of Argument and Inference},
issn = {1570-2464},
doi = {https://doi.org/10.1016/S1570-2464(02)80007-6},
url = {https://www.sciencedirect.com/science/article/pii/S1570246402800076},
author = {David N. Perkins},
abstract = {Publisher Summary
This chapter describes standard logic as a model of reasoning. The notion of formal logic has figured centrally in conceptions of human reasoning, rationality, and adaptiveness. The chapter reviews the evidence, appraises its weight, and offers a summative judgment of the place of logic in human thinking. "Standard logic," includes the canons of formal deduction, the special case of disconfirming hypotheses by finding counterevidence for their implications, and also the principles of probabilistic and statistical inference developed by mathematicians over the past couple of hundred years. It also examines deliberate or reflexive reasoning. The chapter argues that standard logic or subsets of it can be implemented in quite different ways and that human cognition incorporates more than one implementation. In addition, almost all the research on the role of standard logic in human thinking concerns deliberate rather than reflexive reasoning. Accordingly, the present analysis focuses on deliberate reasoning and the place of standard logic in it.}
}
@article{SOSA201656,
title = {Visual divergence in humans and computers},
journal = {Design Studies},
volume = {42},
pages = {56-85},
year = {2016},
issn = {0142-694X},
doi = {https://doi.org/10.1016/j.destud.2015.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X15000836},
author = {Ricardo Sosa and Nicolas Rojas and John S. Gero and Qinqi Xu},
keywords = {creativity, sketching, computer models, solution space},
abstract = {Studies of design creativity have underlined the importance of divergent reasoning and visual reasoning in idea generation. Connecting these two key design skills, this paper presents a model of divergent visual reasoning for the study of creativity. A visual divergence task called ShapeStorm is demonstrated for the study of creative ideation that can be applied to humans as well as computational systems. The model is examined in a study with human subjects, a computational stochastic generator, and a geometrical analysis of the solution space. The main significance of this task is that it offers a straightforward means to define a simple design task that can be used across research studies. Several scenarios for the application of ShapeStorm for the study of creativity are advanced.}
}
@incollection{2007229,
title = {Chapter 6 - Computational Methods for Optimal Filtering of Stochastic Signals},
editor = {A. Torokhti and P. Howlett},
series = {Mathematics in Science and Engineering},
publisher = {Elsevier},
volume = {212},
pages = {229-290},
year = {2007},
booktitle = {Computational Methods for Modelling of Nonlinear Systems},
issn = {0076-5392},
doi = {https://doi.org/10.1016/S0076-5392(07)80049-X},
url = {https://www.sciencedirect.com/science/article/pii/S007653920780049X}
}
@article{VARGASCARPINTERO2025120104,
title = {Development of an integrated multi-criteria framework to assess the implementation potential of biobased value chains and webs with a territorial approach},
journal = {Industrial Crops and Products},
volume = {223},
pages = {120104},
year = {2025},
issn = {0926-6690},
doi = {https://doi.org/10.1016/j.indcrop.2024.120104},
url = {https://www.sciencedirect.com/science/article/pii/S0926669024020818},
author = {Ricardo Vargas-Carpintero},
keywords = {Biobased value chain, Biobased value web, Biorefinery, Territorial bioeconomy system, Multi-criteria assessment, Land-based bioeconomy},
abstract = {Biobased value chains and webs (BVCW) encompass value adding activities and actors from biomass production, its processing into biobased products for manifold sectors, until their commercialization and use. BVCW are part of territorial bioeconomy systems and are shaped by contextual settings. The design and development of BVCW involve strategic decisions towards their sustainable implementation. Throughout the design and development of BVCW, the adoption of an integral approach that links technical aspects of biomass-to-product pathways with non-technical aspects and context factors is necessary to increase the BVCW implementation potential. Accordingly, an active incorporation of the territorial context of BVCW in the design process is required. In view of these requirements, in this study an integrated, multi-criteria framework is developed to assess the implementation potential in BVCW design. For this purpose, key elements from existing biorefinery and biomass supply chain design methodologies are identified and integrated in a multi-criteria framework that allows the consideration of both an internal and external perspective of the BVCW in relation to the context. The conceptualized framework serves as an evaluation approach to check the implementability of biomass-to-product pathways BVCW configurations in form of by means of a multi-criteria catalogue. The set of criteria integrates relevant aspects for the design and development of BVCW from land-based biomass (e.g. crops and crop residues). It entails key criteria related to the functionality of the biomass-to-product pathway in technical-economic terms and the surrounding biophysical, social and economic context. The further operationalization of the multi-criteria catalogue by means of an indicator-based assessment could enable the prioritization and selection of BVCW configurations with best implementation potential. In this way, the framework provides a practical approach for decision-makers, local actors and researchers involved in the design and development of BVCW tailored to the territorial context.}
}
@article{KESBERG2021110458,
title = {Personal values as motivational basis of psychological essentialism: An exploration of the value profile underlying essentialist beliefs},
journal = {Personality and Individual Differences},
volume = {171},
pages = {110458},
year = {2021},
issn = {0191-8869},
doi = {https://doi.org/10.1016/j.paid.2020.110458},
url = {https://www.sciencedirect.com/science/article/pii/S0191886920306498},
author = {Rebekka Kesberg and Johannes Keller},
keywords = {Essentialist beliefs, Human values, Psychological essentialism},
abstract = {Essentialist lay-theories can reflect a belief in genetic, social, and metaphysical determinism. These three types of essentialist beliefs are similar as they can be linked to a set of motives and each of those beliefs is related to stereotyping and prejudice. Nevertheless, the available evidence indicates that the three types of essentialist thinking are largely unrelated and it is unclear why some individuals endorse one type of essentialism and reject another. Examining the association between the endorsement of essentialist beliefs and personal values, our results based on N = 348 respondents indicate that specific value profiles build the motivational basis for specific essentialist beliefs. Specifically, conservation values are associated with belief in genetic and metaphysical determinism, while self-transcendence and self-enhancement are associated with belief in social determinism.}
}
@article{YUAN2025112659,
title = {Experimental and numerical studies on elastic vibrations of a thin spinning gear with a novel tooth contact model},
journal = {Mechanical Systems and Signal Processing},
volume = {231},
pages = {112659},
year = {2025},
issn = {0888-3270},
doi = {https://doi.org/10.1016/j.ymssp.2025.112659},
url = {https://www.sciencedirect.com/science/article/pii/S0888327025003607},
author = {Yunbo Yuan and Chenxin Wang and Kedar S. Vaidya and Guang Zhao and Donghua Wang and Robert G. Parker},
keywords = {Experiment, Natural frequency splitting, Deformable gear, Harmonic finite element model, Tooth contact model, Correlation},
abstract = {This work experimentally and numerically investigates elastic vibrations of a spinning spur gear with thin web and rim. Experimental measurements of radial and axial vibrations of the gear rim in a downward speed sweep test demonstrate splitting of three pairs of natural frequencies with respect to speed. Resonances are observed at intersections between the natural frequency lines and mesh frequency harmonic lines. Different from conventional thinking that spur gears have negligible out-of-plane vibrations, the tested spur gear has significant axial vibrations comparable to or even larger than radial vibrations. The experimental results, including the natural frequency splitting behaviors, specific natural frequencies, slopes of the split natural frequencies, and modal behaviors, correlate well to those from a harmonic finite element model. Vibration modes corresponding to the three pairs of split natural frequencies are standing-wave modes at zero speed (i.e., without gyroscopic effect), and they transition to traveling-wave modes for nonzero speeds (i.e., with gyroscopic effects). The measured resonances qualitatively match those from dynamic simulations of a harmonic finite element model with time-varying mesh stiffness excitation that considers distributed contact along the tooth facewidth and changing contact locations along the tooth profile. Both of these effects can result in significant changes (increases in most cases) of the resonant amplitudes for the elastic modes. Inclusion of these two effects in the mesh excitation is necessary for elastically deformable gears.}
}
@article{GILBERT2018278,
title = {Decoding intentions of self and others from fMRI activity patterns},
journal = {NeuroImage},
volume = {172},
pages = {278-290},
year = {2018},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2017.12.090},
url = {https://www.sciencedirect.com/science/article/pii/S105381191731114X},
author = {Sam J. Gilbert and Hoki Fung},
keywords = {Intentions, MVPA, fMRI},
abstract = {Previous studies using multi-voxel pattern analysis have decoded the content of participants' delayed intentions from patterns of fMRI data. Here we investigate whether this technique can be used to decode not only participants' own intentions, but also their representation of the intentions held by other people. In other words: if Sam is thinking about Hoki, can we decode the content of Hoki's intention by scanning Sam's brain? We additionally distinguished two components of intentions: action-plans versus goals, and included novel control analyses that allowed us to distinguish intending an outcome from simply expecting it to occur or simulating its consequences. Regions of frontal, parietal, and occipital cortex contained patterns from which it was possible to decode intentions of both self and other. Furthermore, crossclasification between self and other was possible, suggesting overlap between the two. Control analyses suggested that these results reflected visuo-spatial processes by which intentions were generated in our paradigm, rather than anything special about intentions per se. There was no evidence for any representation of intentions as mental states distinct from visuospatial processes involved in generating their content and/or simulating their outcomes. These findings suggest that the brain activity patterns decoded in intention-decoding fMRI studies may reflect domain-general processes rather than being intention-specific.}
}
@article{KASHYAPKASHYAP2021395,
title = {The universal language: mathematics or music?},
journal = {Journal for Multicultural Education},
volume = {15},
number = {4},
pages = {395-415},
year = {2021},
issn = {2053-535X},
doi = {https://doi.org/10.1108/JME-05-2021-0064},
url = {https://www.sciencedirect.com/science/article/pii/S2053535X21000197},
author = {RaviRavi KashyapKashyap},
keywords = {Mathematics, Multicultural, Music, Education policy, Artistic encoding of knowledge, Universal language},
abstract = {Purpose
Music could be a challenger for mathematics and a potential candidate for the title “The Universal Language.” This paper aims to discuss the primary objectives of engaging with music, including the therapeutic benefits. Similarities, between mathematics and music and how studying one might enhance one’s abilities of the other are pointed out.
Design/methodology/approach
A formal definition for a universal language is given. A qualitative approach, supplemented with rigorous reasoning, is adopted. The narrative relies on the author’s experiences, teaching mathematical concepts and musical interactions, with students from several countries. A vast amount of literature is reviewed and the corresponding findings are connected toward the arguments made.
Findings
The paper demonstrates that one day, once we understand both mathematics and music better, we might see both of them as the same language. Until then, it is essential to supplement mathematics with music. The educational implications, for all fields, are to ensure that the future creators of knowledge are equally adept at both music and mathematics. The wider policy connotations are to create a blueprint for a society with a vibrant musical and artistic environment.
Originality/value
This study illuminates new ways of thinking about music and mathematics. The possibility that many seemingly complex entities (including our universe, virtual computer worlds, mathematical operations, etc.), are made up of combinations of much simpler building blocks is hinted at. Familiarity with any intricate element of life, without getting flustered, is bound to produce remarkable results in other such endeavors.}
}
@article{BIALEK19901227,
title = {Temporal filtering in retinal bipolar cells. Elements of an optimal computation?},
journal = {Biophysical Journal},
volume = {58},
number = {5},
pages = {1227-1233},
year = {1990},
issn = {0006-3495},
doi = {https://doi.org/10.1016/S0006-3495(90)82463-2},
url = {https://www.sciencedirect.com/science/article/pii/S0006349590824632},
author = {W. Bialek and W.G. Owen},
abstract = {Recent experiments indicate that the dark-adapted vertebrate visual system can count photons with a reliability limited by dark noise in the rod photoreceptors themselves. This suggests that subsequent layers of the retina, responsible for signal processing, add little if any excess noise and extract all the available information. Given the signal and noise characteristics of the photoreceptors, what is the structure of such an optimal processor? We show that optimal estimates of time-varying light intensity can be accomplished by a two-stage filter, and we suggest that the first stage should be identified with the filtering which occurs at the first anatomical stage in retinal signal processing, signal transfer from the rod photoreceptor to the bipolar cell. This leads to parameter-free predictions of the bipolar cell response, which are in excellent agreement with experiments comparing rod and bipolar cell dynamics in the same retina. As far as we know this is the first case in which the computationally significant dynamics of a neuron could be predicted rather than modeled.}
}
@article{CUI2024101074,
title = {AI-enhanced collective intelligence},
journal = {Patterns},
volume = {5},
number = {11},
pages = {101074},
year = {2024},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2024.101074},
url = {https://www.sciencedirect.com/science/article/pii/S2666389924002332},
author = {Hao Cui and Taha Yasseri},
keywords = {AI, collective intelligence, hybrid intelligence, multi-agent systems, human-machine networks, human-machine intelligence},
abstract = {Summary
Current societal challenges exceed the capacity of humans operating either alone or collectively. As AI evolves, its role within human collectives will vary from an assistive tool to a participatory member. Humans and AI possess complementary capabilities that, together, can surpass the collective intelligence of either humans or AI in isolation. However, the interactions in human-AI systems are inherently complex, involving intricate processes and interdependencies. This review incorporates perspectives from complex network science to conceptualize a multilayer representation of human-AI collective intelligence, comprising cognition, physical, and information layers. Within this multilayer network, humans and AI agents exhibit varying characteristics; humans differ in diversity from surface-level to deep-level attributes, while AI agents range in degrees of functionality and anthropomorphism. We explore how agents’ diversity and interactions influence the system’s collective intelligence and analyze real-world instances of AI-enhanced collective intelligence. We conclude by considering potential challenges and future developments in this field.}
}
@article{LEUKHIN2018166,
title = {Bio-plausible simulation of three monoamine systems to replicate emotional phenomena in a machine},
journal = {Biologically Inspired Cognitive Architectures},
volume = {26},
pages = {166-173},
year = {2018},
issn = {2212-683X},
doi = {https://doi.org/10.1016/j.bica.2018.10.007},
url = {https://www.sciencedirect.com/science/article/pii/S2212683X1830152X},
author = {Alexey Leukhin and Max Talanov and Jordi Vallverdú and Fail Gafarov},
keywords = {Affective computing, Affective computation, Spiking neural networks, Bio-inspired cognitive architecture},
abstract = {In this paper we present the validation of the three-dimensional model of emotions by Hugo Lövheim the “cube of emotion” via neurosimulation in the NEST. We also present the extension of original “cube of emotion” with the bridge to computational processes parameters. The neurosimulation is done via re-implementation of DA, 5-HT and NA subsystems of a rat brain to replicate 8 basic psycho-emotional states according to the “cube of emotion”. Results of neurosimulations indicate the incremental influence of DA and NA over computational resources of a psycho-emotional state while 5-HT decreases the computational resources used to calculate a psycho-emotional state. This way we indicate the feasibility of the bio-plausible re-implementation of psycho-emotional states in a computational system. This approach could be useful extension of decision making and load balancing components of modern artificial agents as well as intelligent robotic systems.}
}
@article{TONKS2021102036,
title = {How situational competence beliefs and task value relate to inference strategies and comprehension during reading},
journal = {Learning and Individual Differences},
volume = {90},
pages = {102036},
year = {2021},
issn = {1041-6080},
doi = {https://doi.org/10.1016/j.lindif.2021.102036},
url = {https://www.sciencedirect.com/science/article/pii/S104160802100073X},
author = {Stephen M. Tonks and Joseph P. Magliano and John Schwartz and Ryan D. Kopatich},
keywords = {Reading motivation, Inference generation, Reading comprehension, College students},
abstract = {In two studies, we explored the associations among situational reading-related competence beliefs and task value, inference strategies, comprehension during reading, and foundational skills in college age students. In Study 1, 93 participants from a community college completed assessments of comprehension and two types of inference strategies (elaboration and bridging), each immediately followed by a survey of their competence beliefs and task value regarding the task. Results showed that competence beliefs and task value related positively to reading comprehension. In addition, task value was positively associated with both elaborating and bridging inferences, and competence beliefs correlated positively with bridging inferences. In Study 2, we investigated these associations further in a group of 418 students studying at three different colleges. Participants completed the same assessments for competence beliefs, task value, and inference strategies, as well as assessments of comprehension and foundational reading skills. Study 2 analyses revealed that foundational reading skills were a strong predictor of both types of inferencing and also comprehension. Further, when controlling for foundational reading skills, task value predicted elaboration and bridging inferences, whereas competence beliefs did not predict inferencing, but were trending as a predictor of comprehension. Finally, we created a path model to explore mediational effects, and found that task value positively predicted comprehension performance through increased elaborations while thinking aloud.}
}
@article{ZHOU2025121668,
title = {Sparse loss-aware ternarization for neural networks},
journal = {Information Sciences},
volume = {693},
pages = {121668},
year = {2025},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2024.121668},
url = {https://www.sciencedirect.com/science/article/pii/S0020025524015822},
author = {Ruizhi Zhou and Lingfeng Niu and Dachuan Xu},
keywords = {Machine learning, Ternary neural networks, Loss-aware quantization, Sparse regularization, ADMM},
abstract = {Deep neural networks (DNNs) have shown great success in machine learning tasks and widely used in many fields. However, the substantial computational and storage requirements inherent to DNNs are usually high, which poses challenges for deploying deep learning models on resource-limited devices and hindering further applications. To address this issue, the lightweight nature of neural networks has garnered significant attention, and quantization has become one of the most popular approaches to compress DNNs. In this paper, we introduce a sparse loss-aware ternarization (SLT) model for training ternary neural networks, which encodes the floating-point parameters into {−1,0,1}. Specifically, we abstract the ternarization process as an optimization problem with discrete constraints, and then modify it by applying sparse regularization to identify insignificant weights. To deal with the challenges brought by the discreteness of the model, we decouple discrete constraints from the objective function and design a new algorithm based on the Alternating Direction Method of Multipliers (ADMM). Extensive experiments are conducted on public datasets with popular network architectures. Comparisons with several state-of-the-art baselines demonstrate that SLT always attains comparable accuracy while having better compression performance.}
}
@article{LIU2022121968,
title = {Comparison of coal-to-ethanol product separation strategies},
journal = {Separation and Purification Technology},
volume = {301},
pages = {121968},
year = {2022},
issn = {1383-5866},
doi = {https://doi.org/10.1016/j.seppur.2022.121968},
url = {https://www.sciencedirect.com/science/article/pii/S1383586622015234},
author = {Daoyan Liu and Hao Lyu and Jiahao Wang and Chengtian Cui and Jinsheng Sun},
keywords = {Coal-to-ethanol, Separation strategy, Differential evolution algorithm, Parallel computation, Heat integration},
abstract = {Given China's energy structure and the limitations of bioethanol, the coal-to-ethanol (CTE) pathway, from dimethyl ether to ethanol (DMTE) via carbonylation and hydrogenation, is highly anticipated. Ethanol, methanol, methyl acetate, and ethyl acetate are the crude hydrogenation products that need to be purified, requiring at least an eight-column scheme. However, the optimization of the existing separation strategy with ethanol as the priority is unfavorable in the following aspects: it is usually plagued by tedious rules of thumb and, due to the large scale of the process, is prone to falling into local minima; pre-designed heat integration inevitably neglects the interaction of parameter optimization and heat integration; reports on alternative feasible distillation sequences are scarce in publications, let alone comparisons amongst these counterparts. Therefore, four viable separation strategies are proposed in this paper to compare with this faulted separation strategy. A self-adapting dynamic differential evolution (SADDE) algorithm, which is accelerated by parallel computation, is used to search for optimal column parameters of all the configuration options and facilitates simultaneous heat integration structure synthesis. Two strategies stand out after 3000 generations of evolution. Splitting methanol outperforms in specific steam consumption (SSC) of ethanol (1.8177), much better than the benchmark (2.4840), and splitting ethyl acetate with ethyl acetate priority has the most competitive total annual cost (TAC), 23.98% lower than the benchmark. In summary, this paper provides a reference for optimizing complex distillation systems like CTE product separation, or more specifically, the DMTE route, before the appearance of the most suitable separation strategy in demand. Furthermore, it will also serve for the CTE superstructure to further explore the optimal distillation sequence.}
}
@article{LI20234116,
title = {A call for caution in the era of AI-accelerated materials science},
journal = {Matter},
volume = {6},
number = {12},
pages = {4116-4117},
year = {2023},
issn = {2590-2385},
doi = {https://doi.org/10.1016/j.matt.2023.10.027},
url = {https://www.sciencedirect.com/science/article/pii/S2590238523005283},
author = {Kangming Li and Edward Kim and Yao Fehlis and Daniel Persaud and Brian DeCost and Michael Greenwood and Jason Hattrick-Simpers},
abstract = {It is safe to state that the field of matter has successfully entered the fourth paradigm, where machine learning and artificial intelligence (AI) are universally seen as useful, if not truly intelligent. AI’s utilization is near-ubiquitous from the prediction of novel materials to reducing computational overhead for material simulations; its value has been demonstrated time and again by both theorists and experimentalists. There is, however, a worrying trend toward large datasets and overparameterized models being all we need to accelerate science through accurate and robust machine learning systems.}
}
@article{CHILMON2020106870,
title = {Modelling and simulation considerations for an end-to-end supply chain system},
journal = {Computers & Industrial Engineering},
volume = {150},
pages = {106870},
year = {2020},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2020.106870},
url = {https://www.sciencedirect.com/science/article/pii/S0360835220305659},
author = {Barbara Chilmon and Nicoleta S. Tipi},
keywords = {Simulation, End-to-end supply chain, Systematic literature review},
abstract = {The efforts of this review paper are twofold: to provide an insightful examination of various contributions to knowledge surrounding simulation methods within an end-to-end supply chain and to guide research agenda by indicating generic elements required to model such systems using simulation. The authors examined 255 publications from 21 peer-reviewed journals in the field of an end-to-end supply chain and simulation using a systematic literature review approach. Each publication was thoroughly reviewed to capture best practices and key characteristics relative to simulation modelling techniques used in the context of complex end-to-end supply chain systems. This allowed for identification of generic elements required to model such systems, which were grouped into Structural, Computational and System Organization pillars. This research contributes to the body of knowledge by defining generic aspects of simulation modelling techniques used to study properties and attributes of complex end-to-end supply chains. The paper advances the theoretical understanding of the simulation methods used and applicability of simulation methodology in modelling end-to-end supply chain systems. The research presents the key findings from the use of simulation in modelling end-to-end supply chains and the main ways in which this modelling technique has informed research and practise.}
}
@article{MUNEEPEERAKUL2012123,
title = {The effect of scaling and connection on the sustainability of a socio-economic resource system},
journal = {Ecological Economics},
volume = {77},
pages = {123-128},
year = {2012},
issn = {0921-8009},
doi = {https://doi.org/10.1016/j.ecolecon.2012.02.017},
url = {https://www.sciencedirect.com/science/article/pii/S092180091200081X},
author = {Rachata Muneepeerakul and Murad R. Qubbaj},
keywords = {Sustainability, Scaling, Connection, Bifurcation, Population dynamics},
abstract = {Policy makers dealing with complex systems oftentimes rely on “linear thinking.” This is understandable due to the ease and convenience offered by the simplicity of such conceptualization. Although this line of thinking may help facilitate decision making processes, it is only as defensible as the degree at which the system under consideration behaves linearly. Recent work shows that diverse properties of cities exhibit power-law relationships with population size. Such relationships may invalidate the reliance on linear thinking. Furthermore, in the era of globalization, resources and people move virtually freely through bounds of any confines used to define a system. We incorporate into a simple resource-population model the power-law scaling behavior and the influence of import and immigration, and investigate their effects on sustainable growth of communities. We explore through bifurcation analysis the different scenarios of how an unsustainable system could be sustained. Import can be effective if: the import exceeds a critical level and a critical mass of people populates the system. In contrast, increasing immigration alone can rescue the intrinsically unsustainable system, both directly through people entering the system and indirectly by increasing its harvesting ability, although critical values exist that cause the population to sharply rise or shrink.}
}
@article{ZANUY2006330,
title = {Computational Study of the Fibril Organization of Polyglutamine Repeats Reveals a Common Motif Identified in β-Helices},
journal = {Journal of Molecular Biology},
volume = {358},
number = {1},
pages = {330-345},
year = {2006},
issn = {0022-2836},
doi = {https://doi.org/10.1016/j.jmb.2006.01.070},
url = {https://www.sciencedirect.com/science/article/pii/S0022283606001112},
author = {David Zanuy and Kannan Gunasekaran and Arthur M. Lesk and Ruth Nussinov},
keywords = {protofibril conformation, polyglutamine repeats, β-helices, structural analysis, huntingtin protein},
abstract = {The formation of fibril aggregates by long polyglutamine sequences is assumed to play a major role in neurodegenerative diseases such as Huntington. Here, we model peptides rich in glutamine, through a series of molecular dynamics simulations. Starting from a rigid nanotube-like conformation, we have obtained a new conformational template that shares structural features of a tubular helix and of a β-helix conformational organization. Our new model can be described as a super-helical arrangement of flat β-sheet segments linked by planar turns or bends. Interestingly, our comprehensive analysis of the Protein Data Bank reveals that this is a common motif in β-helices (termed β-bend), although it has not been identified so far. The motif is based on the alternation of β-sheet and helical conformation as the protein sequence is followed from the N to the C termini (β-αR-β-polyPro-β). We further identify this motif in the ssNMR structure of the protofibril of the amyloidogenic peptide Aβ1-40. The recurrence of the β-bend suggests a general mode of connecting long parallel β-sheet segments that would allow the growth of partially ordered fibril structures. The design allows the peptide backbone to change direction with a minimal loss of main chain hydrogen bonds. The identification of a coherent organization beyond that of the β-sheet segments in different folds rich in parallel β-sheets suggests a higher degree of ordered structure in protein fibrils, in agreement with their low solubility and dense molecular packing.}
}
@article{THAGARD1986301,
title = {Parallel computation and the mind-body problem},
journal = {Cognitive Science},
volume = {10},
number = {3},
pages = {301-318},
year = {1986},
issn = {0364-0213},
doi = {https://doi.org/10.1016/S0364-0213(86)80020-9},
url = {https://www.sciencedirect.com/science/article/pii/S0364021386800209},
author = {Paul Thagard},
abstract = {The position in the philosophy of mind called functionalism claims that mental states are to be understood in terms of their functional relationships to other mental states, not in terms of their material instantiation in any particular kind of hardware. But the argument that material instantiation is irrelevant to functional relationships is computationally naive. This paper uses recent work on parallel computation to argue that software and hardware are much more intertwined than the functionalists allow. Parallelism offers qualitative as well as quantitative advantages, leading to different styles of programming as well as increased speed. Hence hardware may well matter to the mental: only by further empirical investigations of the relation between the mind and brain and between artificial intelligence software and underlying hardware will we be able to achieve a defensible solution to the mind-body problem. The major disadvantage of parallel systems is the need to coordinate their subprocesses, but recent proposals that consciousness provides a serial control for parallel computation are implausible.}
}
@article{STEFIK1989241,
title = {Computation and cognition: Toward a foundation of cognitive science: Z.W. Pylyshyn, (MIT Press, Cambridge, MA, 1986); 292 pages, $33.75 (hardcover), $9.95 (paperback)},
journal = {Artificial Intelligence},
volume = {38},
number = {2},
pages = {241-247},
year = {1989},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(89)90061-1},
url = {https://www.sciencedirect.com/science/article/pii/0004370289900611},
author = {Mark Stefik}
}
@article{PADGETT1994185,
title = {Computational intelligence standards: motivation, current activities and progress},
journal = {Computer Standards & Interfaces},
volume = {16},
number = {3},
pages = {185-203},
year = {1994},
issn = {0920-5489},
doi = {https://doi.org/10.1016/0920-5489(94)90011-6},
url = {https://www.sciencedirect.com/science/article/pii/0920548994900116},
author = {Mary Lou Padgett and Walter J Karplus and Steve Deiss and Robert Shelton},
keywords = {Terminology, Artificial neural networks, Specification, Virtual reality},
abstract = {Computational Intelligence is an emerging technology of keen interest to the developers of computer standards and interfaces. Coherent communications among the diverse set of users of computational AI is necessary for the protection of all parties and can help further the serious development of artificial neural networks, fuzzy systems, evolutionary programming and virtual reality. Current activities of the IEEE Neural Networks Council Standards Committee encompass all these areas, emphasizing the development of glossaries and symbologies, performance measures and interface standards for these interrelated fields. Progress toward these goals is described in this paper.}
}
@article{CASTROSCHEZ201465,
title = {Experience applying language processing techniques to develop educational software that allow active learning methodologies by advising students},
journal = {Journal of Network and Computer Applications},
volume = {41},
pages = {65-79},
year = {2014},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2013.10.011},
url = {https://www.sciencedirect.com/science/article/pii/S1084804513002166},
author = {J.J. Castro-Schez and M.A. Redondo and F. Jurado and J. Albusac},
keywords = {Environment for active learning, Formal languages techniques, Automatic assessment},
abstract = {This paper is focused on those systems that allow students to build their own knowledge by providing them with feedback regarding their actions while performing a problem based learning activity or while making changes to problem statements, so that a higher order thinking skill can be achieved. This feedback is the consequence of an automatic assessment. Particularly, we propose a method that makes use of Language Processor techniques for developing these kinds of systems. This method could be applied in subjects in which problem statements and solutions can be formalized by mean of a formal language and the problems can be solved in an algorithmic way. The method has been used to develop a number of tools that are partially described in this paper. Thus, we show that our approach is applicable in addressing the development of the aforementioned systems. One of these tools (a virtual laboratory for language processing) has been in use for several years in order to support home assignments. The data collected for these years are presented and analyzed in this paper. The results of the analysis confirm that this tool is effective in facilitating the achievement of learning outcomes.}
}
@article{UBAN2021480,
title = {An emotion and cognitive based analysis of mental health disorders from social media data},
journal = {Future Generation Computer Systems},
volume = {124},
pages = {480-494},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2021.05.032},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X21001825},
author = {Ana-Sabina Uban and Berta Chulvi and Paolo Rosso},
keywords = {Mental health disorders, Early risk prediction, Emotions, Cognitive styles, Deep learning, Social media},
abstract = {Mental disorders can severely affect quality of life, constitute a major predictive factor of suicide, and are usually underdiagnosed and undertreated. Early detection of signs of mental health problems is particularly important, since unattended, they can be life-threatening. This is why a deep understanding of the complex manifestations of mental disorder development is important. We present a study of mental disorders in social media, from different perspectives. We are interested in understanding whether monitoring language in social media could help with early detection of mental disorders, using computational methods. We developed deep learning models to learn linguistic markers of disorders, at different levels of the language (content, style, emotions), and further try to interpret the behavior of our models for a deeper understanding of mental disorder signs. We complement our prediction models with computational analyses grounded in theories from psychology related to cognitive styles and emotions, in order to understand to what extent it is possible to connect cognitive styles with the communication of emotions over time. The final goal is to distinguish between users diagnosed with a mental disorder and healthy users, in order to assist clinicians in diagnosing patients. We consider three different mental disorders, which we analyze separately and comparatively: depression, anorexia, and self-harm tendencies.}
}
@article{MEICHENBAUM1969101,
title = {The effects of instructions and reinforcement on thinking and language behavior of schizophrenics},
journal = {Behaviour Research and Therapy},
volume = {7},
number = {1},
pages = {101-114},
year = {1969},
issn = {0005-7967},
doi = {https://doi.org/10.1016/0005-7967(69)90054-0},
url = {https://www.sciencedirect.com/science/article/pii/0005796769900540},
author = {Donald H. Meichenbaum},
abstract = {Six experimental groups and 2 control groups (N=48) were used to investigate the relative effectiveness of prolonged training of schizophrenics with contingent social and token reinforcement on (a) the level of abstraction as measured on a proverbs task, (b) the percentage of “sick talk” (% ST) emitted in a structured interview, (c) both verbal response classes of proverb abstraction and % ST. Prior to treatment, schizophrenic Ss compared with 20 nonpsychiatric hospitalized medical patients were significantly inferior on the proverbs task and emitted five times more ST in a structured interview. The results indicated that the experimental treatments were effective in decreasing % ST and increasing abstraction to proverbs with token reinforcement being most effective. Evidence for response and stimulus generalization was obtained.}
}
@article{CAMARGO2022496,
title = {Existence, Hypotheses and Categories in Knowledge Representation},
journal = {Procedia Computer Science},
volume = {213},
pages = {496-503},
year = {2022},
note = {2022 Annual International Conference on Brain-Inspired Cognitive Architectures for Artificial Intelligence: The 13th Annual Meeting of the BICA Society},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.11.096},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922017872},
author = {Eduardo Camargo and Eduardo Yuji Sakabe and Ricardo Gudwin},
keywords = {Knowledge Representation, Cognitive Architecture, Cognitive Semiotics, Artificial Intelligent Agent},
abstract = {Cognitive architectures employ different means for knowledge representation. In this work, we describe how the Cognitive Systems Toolkit (CST), a toolkit for the construction of cognitive architectures addresses the issue of knowledge representation, by introducing the notion of a computational idea, as being an abstract and generic building block for representing multiple different pieces of knowledge. We particularly address how computational ideas can be used to represent both facts that really happened at an environment and just hypothesis that are not to be considered as being a part of existence, explaining how these are instances of general categories. At the end, we provide different examples to illustrate the subtle differences that are possible to be represented using this knowledge representation scheme.}
}
@article{DUBEY2020118,
title = {Understanding exploration in humans and machines by formalizing the function of curiosity},
journal = {Current Opinion in Behavioral Sciences},
volume = {35},
pages = {118-124},
year = {2020},
note = {Curiosity (Explore vs Exploit)},
issn = {2352-1546},
doi = {https://doi.org/10.1016/j.cobeha.2020.07.008},
url = {https://www.sciencedirect.com/science/article/pii/S2352154620301108},
author = {Rachit Dubey and Thomas L Griffiths},
abstract = {Recent work in machine learning has demonstrated the benefits of providing artificial agents with a sense of curiosity—a form of intrinsic reward that supports exploration. Two strategies have emerged for defining these rewards: favoring novelty and pursuing prediction errors. Psychological theories of curiosity have also emphasized these two factors. We show how these two literatures can be connected by understanding the function of curiosity, which requires thinking about the abstract computational problem that both humans and machines face as they explore their world.}
}
@article{PIETARINEN2025105410,
title = {Synechism 2.0: Contours of a new theory of continuity in bioengineering},
journal = {BioSystems},
volume = {250},
pages = {105410},
year = {2025},
issn = {0303-2647},
doi = {https://doi.org/10.1016/j.biosystems.2025.105410},
url = {https://www.sciencedirect.com/science/article/pii/S0303264725000206},
author = {Ahti-Veikko Pietarinen and Vera Shumilina},
keywords = {Charles S. Peirce, Synechism, Collective agency, Synthetic intelligence, Michael E. Levin, Bioengineering, Bioelectricity},
abstract = {The methodological principle of synechism, the all-pervading continuity first proposed by Charles Peirce in 1892, is reinvigorated in the present paper to prompt a comprehensive reevaluation of the integrated concepts of life, machines, agency, and intelligence. The evidence comes from the intersections of synthetic bioengineering, developmental biology, and cognitive and computational sciences. As a regulative principle, synechism, “that continuity governs the whole domain of experience in every element of it”, has been shown to infiltrate fundamental issues of contemporary biology, including cognition in different substrates, embodied agency, collectives (swarm and nested), intelligence on multiple scales, and developmental bioelectricity in morphogenesis. In the present paper, we make explicit modern biology's turn to this fundamental feature of science in its rejection of conceptual binaries, preference for collectives over individuals, quantitative over qualitative, and multiscale applicability of the emerging hypotheses about the integration of the first principles of the diversity of life. Specifically, synechism presents itself as the bedrock for research encompassing biological machines, chimaeras, organoids, and Xenobots. We then review a synechistic framework that embeds functionalist, information-theoretic, pragmaticist and inferentialist approaches to springboard to continuum-driven biosystemic behaviour.}
}
@article{1995146,
title = {95/02164 Sequential pressure-based Navier-Stokes algorithms on SIMD computers: Computational issues},
journal = {Fuel and Energy Abstracts},
volume = {36},
number = {2},
pages = {146},
year = {1995},
issn = {0140-6701},
doi = {https://doi.org/10.1016/0140-6701(95)93829-X},
url = {https://www.sciencedirect.com/science/article/pii/014067019593829X}
}
@article{ZIA2022108066,
title = {SoFTNet: A concept-controlled deep learning architecture for interpretable image classification},
journal = {Knowledge-Based Systems},
volume = {240},
pages = {108066},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2021.108066},
url = {https://www.sciencedirect.com/science/article/pii/S095070512101145X},
author = {Tehseen Zia and Nauman Bashir and Mirza Ahsan Ullah and Shakeeb Murtaza},
keywords = {Interpretability, Concepts, KNN, Explanation satisfaction},
abstract = {Interpreting deep learning (DL)-based computer vision models is challenging due to the complexity of internal representations. Most recent techniques for rendering DL learning outcomes interpretable operate on low-level features rather than high-level concepts. Methods that explicitly incorporate high-level concepts do so through a determination of the relevancy of user-defined concepts or else concepts extracted directly from the data. However, they do not leverage the potential of concepts to explain model predictions. To overcome this challenge, we introduce a novel DL architecture – the Slow/Fast Thinking Network (SoFTNet) – enabling users to define/control high-level features and utilize them to perform image classification predicatively. We draw inspiration from the dual-process theory of human thought processes, decoupling low-level, fast & non-transparent processing from high-level, slow & transparent processing. SoFTNet hence uses a shallow convolutional neural network for low-level processing in conjunction with a memory network for high-level concept-based reasoning. We conduct experiments on the CUB-200-2011 and STL-10 datasets and also present a novel concept-based deep K-nearest neighbor approach for baseline comparisons. Our experiments show that SoFTNet achieves comparable performance to state-of-art non-interpretable models and outperforms comparable interpretative methods.}
}
@article{SCHINCKUS20094415,
title = {Economic uncertainty and econophysics},
journal = {Physica A: Statistical Mechanics and its Applications},
volume = {388},
number = {20},
pages = {4415-4423},
year = {2009},
issn = {0378-4371},
doi = {https://doi.org/10.1016/j.physa.2009.07.008},
url = {https://www.sciencedirect.com/science/article/pii/S0378437109005494},
author = {Christophe Schinckus},
keywords = {Econophysics, Uncertainty, Economics, Keynes, Knight, Hayek},
abstract = {The objective of this paper is to provide a methodological link between econophysics and economics. I will study a key notion of both fields: uncertainty and the ways of thinking about it developed by the two disciplines. After having presented the main economic theories of uncertainty (provided by Knight, Keynes and Hayek), I show how this notion is paradoxically excluded from the economic field. In economics, uncertainty is totally reduced by an a priori Gaussian framework—in contrast to econophysics, which does not use a priori models because it works directly on data. Uncertainty is then not shaped by a specific model, and is partially and temporally reduced as models improve. This way of thinking about uncertainty has echoes in the economic literature. By presenting econophysics as a Knightian method, and a complementary approach to a Hayekian framework, this paper shows that econophysics can be methodologically justified from an economic point of view.}
}
@article{SEYMOUR2020117212,
title = {Hierarchical models of pain: Inference, information-seeking, and adaptive control.},
journal = {NeuroImage},
volume = {222},
pages = {117212},
year = {2020},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2020.117212},
url = {https://www.sciencedirect.com/science/article/pii/S1053811920306984},
author = {Ben Seymour and Flavia Mancini},
keywords = {Pain, Nociception, Information theory, Reinforcement learning, Optimal control, Predictive coding, Epistemic value, Free energy principle, Endogenous modulation},
abstract = {Computational models of pain consider how the brain processes nociceptive information and allow mapping neural circuits and networks to cognition and behaviour. To date, they have generally have assumed two largely independent processes: perceptual inference, typically modelled as an approximate Bayesian process, and action control, typically modelled as a reinforcement learning process. However, inference and control are intertwined in complex ways, challenging the clarity of this distinction. Here, we consider how they may comprise a parallel hierarchical architecture that combines inference, information-seeking, and adaptive value-based control. This sheds light on the complex neural architecture of the pain system, and takes us closer to understanding from where pain ’arises’ in the brain.}
}
@article{KUNZE2024249,
title = {Bioinspired approaches for resource-efficient material flow in production – an innovative actuator concept for peristaltic-based transport},
journal = {Procedia CIRP},
volume = {125},
pages = {249-254},
year = {2024},
note = {CIRP BioM 2024},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2024.08.043},
url = {https://www.sciencedirect.com/science/article/pii/S2212827124003974},
author = {Henriette Kunze and Marcel Lorenz},
keywords = {tensegrity, biotensegrity, assembly technologies},
abstract = {In automated material flow, in a wide variety of areas, the primary goal is usually to handle a wide spectrum of components as time- and cost-efficiently as possible. In view of the current and future challenges in industrial production, it is becoming apparent that ecological requirements are becoming increasingly important in automation solutions. For example, in form of resource efficiency, transformability and material efficiency. In this context, especially materials handling technology is subject of various optimization approaches, as no value is added to the part handled. The question: "How does material flow occur in nature?" thus offers biologically inspired approaches to thinking about transport in the industrial sector. This paper first presents a selection of concepts or existing mechanisms that are adaptable in materials- handling technology and have been developed based on a biological model. In the second part of this paper, a new concept is presented that is modeled on peristalsis as a transport mechanism. The approach presented here uses tensegrity-structures for assembly, which are characterized by their high material efficiency and flexibility. The transport movement is achieved by peristaltic typical contraction or relaxation of the respective structure parts.}
}
@article{BAICANG2025129857,
title = {Multi-modal information fusion for multi-task end-to-end behavior prediction in autonomous driving},
journal = {Neurocomputing},
volume = {634},
pages = {129857},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2025.129857},
url = {https://www.sciencedirect.com/science/article/pii/S0925231225005296},
author = {Guo Baicang and Liu Hao and Yang Xiao and Cao Yuan and Jin Lisheng and Wang Yinlin},
keywords = {Autonomous driving, Multi-modal fusion, Vehicle behavior prediction, End-to-End, Attention mechanism},
abstract = {Behavior prediction in autonomous driving is increasingly achieved through end-to-end frameworks that predict vehicle states from multi-modal information, streamlining decision-making and enhancing robustness in time-varying road conditions. This study proposes a novel multi-modal information fusion-based, multi-task end-to-end model that integrates RGB images, depth maps, and semantic segmentation data, enhancing situational awareness and predictive precision. Utilizing a Vision Transformer (ViT) for comprehensive spatial feature extraction and a Residual-CNN-BiGRU structure for capturing temporal dependencies, the model fuses spatiotemporal features to predict vehicle speed and steering angle with high precision. Through comparative, ablation, and generalization tests on the Udacity and self-collected datasets, the proposed model achieves steering angle prediction errors of MSE 0.012 rad, RMSE 0.109 rad, and MAE 0.074 rad, and speed prediction errors of MSE 0.321 km/h, RMSE 0.567 km/h, and MAE 0.373 km/h, outperforming existing driving behavior prediction models. Key contributions of this study include the development of a channel difference attention mechanism and advanced spatiotemporal feature fusion techniques, which improve predictive accuracy and robustness. These methods effectively balance computational efficiency and predictive performance, contributing to practical advancements in driving behavior prediction.}
}
@article{YUZGEC2025113169,
title = {Accelerated opposition learning based chaotic single candidate optimization algorithm: A new alternative to population-based heuristics},
journal = {Knowledge-Based Systems},
volume = {314},
pages = {113169},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.113169},
url = {https://www.sciencedirect.com/science/article/pii/S0950705125002163},
author = {Ugur Yuzgec},
keywords = {Opposition learning, Chaotic, Single candidate, Engineering design, Benchmark},
abstract = {This study considers the Single Candidate Optimizer (SCO) as an alternative to population-based heuristics, that is faster than them. Although the SCO algorithm is a fast single-candidate-based heuristic, it has certain limitations. To overcome these limitations and enhance the search performance of SCO, several solutions were proposed in this study. First, owing to the single-candidate nature of the SCO, the initial solution position can play a critical role. To compensate for this, an accelerated opposition-learning mechanism was integrated into the SCO. In addition, instead of the equation that is active when the number of unsuccessful improvement attempts is reached in the SCO structure, a mutation operator including chaotic functions (Levy, Gauss, and Cauchy) has been incorporated into the algorithm. Again, equations based on new approaches were added to the SCO algorithm to update the position of the candidate solution during the exploration and exploitation phases. Finally, the standard boundary value control mechanism is replaced with a more effective one. The algorithm developed in this study is named Accelerated Opposition Learning based Chaotic Single Candidate Optimizer (AccOppCSCO), inspired by the accelerated opposition learning mechanism and the mutation operator involving chaotic behaviors. The search capability of the proposed AccOppCSCO algorithm was first analyzed using four different methods: convergence, search history, trajectory, and computational complexity. The effectiveness of the mechanisms used in the AccOppCSCO algorithm for four different two-dimensional benchmark problems from the IEEE Congress on Evolutionary Computation 2014 (CEC2014) package was demonstrated. Subsequently, the performance of the proposed AccOppCSCO algorithm was evaluated on the CEC2014 and IEEE Congress on Evolutionary Computation 2020 (CEC2020) benchmark problems with different dimensions. The results show that the AccOppCSCO algorithm works effectively in the CEC2014 and CEC2020 test sets and offers better optimization results than SCO. The AccOppCSCO algorithm ranked first in the overall evaluation of the 30-dimensional CEC2014 comparison results with State of the Art (SOTA) heuristics from the literature. Finally, for ten different engineering design problems, the AccOppCSCO algorithm was analyzed and compared with the original SCO and other SOTA heuristics. The results show that AccOppCSCO is effective for engineering design problems. This emphasizes that the algorithm can work effectively on a wide range of problems and can be used in various applications. The source code of the AccOppCSCO algorithm for the CEC2014 benchmark suite is publicly available at https://github.com/uguryuzgec/AccOppCSCO.}
}
@article{BESHKOV2024109370,
title = {Topological structure of population activity in mouse visual cortex encodes densely sampled stimulus rotations},
journal = {iScience},
volume = {27},
number = {4},
pages = {109370},
year = {2024},
issn = {2589-0042},
doi = {https://doi.org/10.1016/j.isci.2024.109370},
url = {https://www.sciencedirect.com/science/article/pii/S2589004224005911},
author = {Kosio Beshkov and Marianne Fyhn and Torkel Hafting and Gaute T. Einevoll},
keywords = {Neuroscience, Sensory neuroscience, Cognitive neuroscience},
abstract = {Summary
The primary visual cortex is one of the most well understood regions supporting the processing involved in sensory computation. Following the popularization of high-density neural recordings, it has been observed that the activity of large neural populations is often constrained to low dimensional manifolds. In this work, we quantify the structure of such neural manifolds in the visual cortex. We do this by analyzing publicly available two-photon optical recordings of mouse primary visual cortex in response to visual stimuli with a densely sampled rotation angle. Using a geodesic metric along with persistent homology, we discover that population activity in response to such stimuli generates a circular manifold, encoding the angle of rotation. Furthermore, we observe that this circular manifold is expressed differently in subpopulations of neurons with differing orientation and direction selectivity. Finally, we discuss some of the obstacles to reliably retrieving the truthful topology generated by a neural population.}
}
@article{SPENCE2022100433,
title = {Gastrophysics: Getting creative with pairing flavours},
journal = {International Journal of Gastronomy and Food Science},
volume = {27},
pages = {100433},
year = {2022},
issn = {1878-450X},
doi = {https://doi.org/10.1016/j.ijgfs.2021.100433},
url = {https://www.sciencedirect.com/science/article/pii/S1878450X21001323},
author = {Charles Spence},
keywords = {Food pairing, Flavour pairing hypothesis, Sonic seasoning, Computational gastronomy, Data engineering, Gastrophysics},
abstract = {Traditionally, in the West, the decision about which flavours to pair in a tasting experience has been as much the personal choice of the chef or, more likely, the sommelier, as anything else. However, the last couple of decades have seen a rapid growth of research interest in the pairing of flavours. Nowadays, one can find examples of people pairing everything from beer with food, tea with cheese and chocolate, etc. As interest in the marketing potential of flavour pairing has risen, along with the growing public fascination in the topic, scientists have become increasingly interested in trying to understand the principles (both cognitive/intellectual and perceptual) underlying the successful pairing of flavours. In this narrative review, the relative strengths and weaknesses of the chemical, computational (gastronomy), and perceptual approaches to pairing flavours are highlighted. Thereafter, I show how the various principles of pairing (both perceptual and cognitive/intellectual) can be extended beyond the domain of pairing flavour with flavour to consider the rapidly growing are of sonic seasoning. The latter term refers to those situations in which specific pieces of music or soundscapes are matched, or paired, with particular tastes/flavours based on the crossmodal correspondences. The review ends by considering the future development of pairings flavours, and assessing novel means of establishing connections between flavours and other sensations.}
}
@article{COON1995787,
title = {Generalized block-tridiagonal matrix orderings for parallel computation in process flowsheeting},
journal = {Computers & Chemical Engineering},
volume = {19},
number = {6},
pages = {787-805},
year = {1995},
note = {Applications of Parallel Computing},
issn = {0098-1354},
doi = {https://doi.org/10.1016/0098-1354(94)00081-6},
url = {https://www.sciencedirect.com/science/article/pii/0098135494000816},
author = {A.B. Coon and M.A. Stadtherr},
abstract = {A new graph partitioning algorithm for use on structurally unsymmetric systems is presented. Unlike other partitioning algorithms that have been used to provide reorderings for structurally symmetric matrices, this algorithm employs a bipartite graph model, and hence, can be used to consider unsymmetric permutations of structurally unsymmetric matrices. It is shown that the algorithm can be used in identifying coarse-granular, balanced tasks in the direct solution of flowsheeting matrices by parallel techniques based on generalized block-tridiagonal and nested-block-tridiagonal matrix structures. It is also shown that such reorderings can be obtained inexpensively, in worst-case running times that increase linearly with the order of the matrix.}
}
@article{XU2025101288,
title = {Multi-criteria feature selection on maritime emission abatement alternatives},
journal = {Research in Transportation Business & Management},
volume = {59},
pages = {101288},
year = {2025},
issn = {2210-5395},
doi = {https://doi.org/10.1016/j.rtbm.2025.101288},
url = {https://www.sciencedirect.com/science/article/pii/S2210539525000033},
author = {Kaiqi Xu and Mario P. Brito and Patrick Beullens},
keywords = {Analytic hierarchy process, Multi-criteria decision making, Emission reduction, Technology selection, Sustainability port},
abstract = {To comply with MARPOL Annex VI, stakeholders face multi-criteria decision-making in technology selection. This study provides an Analytic Hierarchy Process (AHP)-based method to support stakeholders in selecting emission abatement technology aligned with their business demands, taking into account a range of sustainability criteria. The analysis reveals that there is no one-size-fits-all solution to technology selection. Low-sulfur fuel oil and LNG are preferable alternative fuels for large-size commercial (long-sea shipping) vessels due to their better capacity storage savings, while a dual-fuel engine offers flexibility in fuel changeover. Electrification offers zero-emission performance, lower noise levels, and peak energy solutions benefiting cruise ships and short-distance or harbor boats, but tugboats need greener diesel to meet performance criteria. From a policy perspective, our model provides insights into the effects of green transition processes in Norway and Singapore on stakeholders' decisions with respect to port infrastructure and land transport at the portside.}
}
@article{ALEXIOU2009623,
title = {Exploring the neurological basis of design cognition using brain imaging: some preliminary results},
journal = {Design Studies},
volume = {30},
number = {6},
pages = {623-647},
year = {2009},
issn = {0142-694X},
doi = {https://doi.org/10.1016/j.destud.2009.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X09000313},
author = {K. Alexiou and T. Zamenopoulos and J.H. Johnson and S.J. Gilbert},
keywords = {design cognition, problem solving, design problems, research methods, cognitive neuroscience},
abstract = {The paper presents a pilot interdisciplinary research study carried out as a step towards understanding the neurological basis of design thinking. The study involved functional magnetic resonance imaging (fMRI) of volunteers while performing design and problem-solving tasks. The findings suggest that design and problem solving involve distinct cognitive functions associated with distinct brain networks. The paper introduces the methodology, presents the findings, and discusses the potential role of brain imaging in design research.}
}
@article{KAMARI2017330,
title = {Sustainability focused decision-making in building renovation},
journal = {International Journal of Sustainable Built Environment},
volume = {6},
number = {2},
pages = {330-350},
year = {2017},
issn = {2212-6090},
doi = {https://doi.org/10.1016/j.ijsbe.2017.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S221260901730064X},
author = {Aliakbar Kamari and Rossella Corrao and Poul Henning Kirkegaard},
keywords = {Sustainability, Building renovation, Decision support, Knowledge management, Soft Systems Methodology (SSM), Value Focused Thinking (VFT)},
abstract = {An overview of recent research related to building renovation has revealed that efforts to date do not address sustainability issues comprehensively. The question then arises in regard to the holistic sustainability objectives within building renovation context. In order to deal with this question, the research adopts a multi-dimensional approach involving literature review, exploration of existing assessment methods and methodologies, individual and focus group interviews, and application of Soft Systems Methodologies (SSM) with Value Focused Thinking (VFT). In doing so, appropriate data about sustainability objectives have been collected and structured, and subsequently verified using a Delphi study. A sustainability framework was developed in cooperation with University of Palermo and Aarhus University to audit, develop and assess building renovation performance, and support decision-making during the project’s lifecycle. The paper represents the results of research aiming at addressing sustainability of the entire renovation effort including new categories, criteria, and indicators. The developed framework can be applied during different project stages and to assist in the consideration of the sustainability issues through support of decision-making and communication with relevant stakeholders. Early in a project, it can be used to identify key performance criteria, and later to evaluate/compare the pros and cons of alternative retrofitting solutions either during the design stage or upon the project completion. According to the procedure of the consensus-based process for the development of an effective sustainability decision-making framework which was employed in this study, the outcome can also be considered as an outset step intended for the establishment of a Decision Support Systems (DSS) and assessment tool suited to building renovation context.}
}
@article{BARNES2021,
title = {Gene Expression and Data Analysis Pipeline Using Cancer BioPortal in the Classroom},
journal = {Journal of Microbiology & Biology Education},
volume = {22},
number = {1},
year = {2021},
issn = {1935-7877},
doi = {https://doi.org/10.1128/jmbe.v22i1.2313},
url = {https://www.sciencedirect.com/science/article/pii/S1935787721000721},
author = {Chassidy N. Barnes and Blake P. Johnson and Stefanie W. Leacock and Ruben M. Ceballos and Lori L. Hensley and Nathan S. Reyna},
abstract = {At institutions with an emphasis on authentic research experiences as an integral part of the biology curriculum, COVID created a huge challenge for course instructors whose learning objectives were designed for such experiences. Moving such laboratory experiences online when remote learning became necessary has resulted in a new model for CUREs that utilizes free online databases to provide not only a novel research experience for students, but also the opportunity to engage in big data analysis.
ABSTRACT
At institutions with an emphasis on authentic research experiences as an integral part of the biology curriculum, COVID created a huge challenge for course instructors whose learning objectives were designed for such experiences. Moving such laboratory experiences online when remote learning became necessary has resulted in a new model for CUREs that utilizes free online databases to provide not only a novel research experience for students, but also the opportunity to engage in big data analysis. Cancer BioPortal (cBioPortal) is an open-access collective cancer research resource for storing and exploring clinical, genomic, proteomic, and transcriptomic data. cBioPortal eliminates the computational barrier of interpreting complex genomic data by providing easily understandable visualization that can be interpreted and translated into relevant biological insights. Because no prior computational knowledge is required, cBioPortal is an ideal educational tool for either in-person or distance learning environments. We developed a pedagogical approach, video tutorials, and data analysis workflows centered on using cBioPortal. Pedagogically, students develop an initial research outline that is continually updated and graded throughout the project. Progress during the project or course is assessed by a series of student presentations that are 5 to 15 min in length and are aimed at explaining the approach used in data acquisition, interpretation of the data, and relevance to the initial hypothesis. While cancer-specific, this analysis platform appeals to a wide range of classes and student interests. Further, the project has been successfully done both as an independent research experience and as part of a virtual class-based research project.}
}
@article{DONALDSON1995301,
title = {Building object-oriented systems: An introduction from concepts to implementation in c++: R. E. Callan, Computational Mechanics Publications, Southampton, UK, 1994. ISBN 1-85312-340-4. 304 pp. £47.00},
journal = {Artificial Intelligence in Engineering},
volume = {9},
number = {4},
pages = {301},
year = {1995},
note = {Selected Papers from the 1994 Japan/Korea Joint Conference on Expert Systems},
issn = {0954-1810},
doi = {https://doi.org/10.1016/0954-1810(95)90016-0},
url = {https://www.sciencedirect.com/science/article/pii/0954181095900160},
author = {Iain Donaldson}
}
@article{GORDON2018273,
title = {Healthier Choices in School Cafeterias: A Systematic Review of Cafeteria Interventions},
journal = {The Journal of Pediatrics},
volume = {203},
pages = {273-279.e2},
year = {2018},
issn = {0022-3476},
doi = {https://doi.org/10.1016/j.jpeds.2018.07.031},
url = {https://www.sciencedirect.com/science/article/pii/S0022347618309363},
author = {Katelyn Gordon and Linda Dynan and Robert Siegel},
keywords = {school cafeteria, behavioral economics, childhood obesity, food selection},
abstract = {Objective
To describe school cafeteria interventions in terms of a behavioral economics scheme and to assess which system is more likely to be effective in improving food selection or consumption.
Study design
With this systematic review, we categorize cafeteria interventions using the behavioral economics theory of Kahneman into system 1 (fast and intuitive thinking) and system 2 (slow and cognitively demanding) or mixed (having elements of system 1 and system 2). Pertinent studies were identified from review of the literature of interventions performed in school and cafeteria settings in children grades K-12 within the past 5 years (2012-2017) at time of search.
Results
In all, 48 of 978 studies met inclusion criteria. By defining success as a 30% improvement in a desired outcome or statistically significant reduction in body mass index, 89% of system 1, 67% of mixed (had both system 1 and 2 elements), and only 33% of system 2 interventions were successful.
Conclusions
This review found successful system 1 type school cafeteria interventions to be more common than system 2 type interventions and system 2 type interventions are less effective than system 1.}
}
@article{MOLNAR20152667,
title = {Three Dimensional Applications in Teaching and Learning Processes},
journal = {Procedia - Social and Behavioral Sciences},
volume = {191},
pages = {2667-2673},
year = {2015},
note = {The Proceedings of 6th World Conference on educational Sciences},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2015.04.600},
url = {https://www.sciencedirect.com/science/article/pii/S1877042815028608},
author = {György Molnár and András Benedek},
keywords = {ICT, 3D interactive system, new learning potential, Leonar3Do ;},
abstract = {In the world of today's information society the torrent of information we are dailyfaced with has to be appropriately transformed and translated in order to yield representationswe are somehow capable of understanding. By extending 2D representations to three-dimensional ones, pictorialcontents become more lifelike, getting closer to practice, creating the basis for a new view ofpictorial thinking, giving rise to the emergence to a very effective method of dealing withinformation overload. To depict three-dimensionalreality onto a two-dimensional plane of course constitutes an age-old scientific problem, theprincipal aim of the technique sought after being the exact representation.We also present a general review of Hungarian and international experienceson ICT application and its environment that comply with current practice.}
}
@article{MOSTERT202448,
title = {The Shortfalls of Mental Health Compartment Models: A Call to Improve Mental Health Investment Cases in Developing Countries},
journal = {Value in Health Regional Issues},
volume = {41},
pages = {48-53},
year = {2024},
issn = {2212-1099},
doi = {https://doi.org/10.1016/j.vhri.2023.11.012},
url = {https://www.sciencedirect.com/science/article/pii/S2212109923001449},
author = {Cyprian M. Mostert and Andrew Aballa and Linda Khakali and Willie Njoroge and Jasmit Shah and Samim Hasham and Zul Merali and Lukoye Atwoli},
keywords = {developing countries, investment cases, mental health compartment model},
abstract = {Objectives
There are irregularities in investment cases generated by the Mental Health Compartment Model. We discuss these irregularities and highlight the costing techniques that may be introduced to improve mental health investment cases.
Methods
This analysis uses data from the World Bank, the World Health Organization Mental Health Compartment Model, the United Nations Development Program, the Kenya Ministry of Health, and Statistics from the Kenyan National Commission of Human Rights.
Results
We demonstrate that the Mental Health Compartment Model produces irrelevant outcomes that are not helpful for clinical settings. The model inflated the productivity gains generated from mental health investment. In some cases, the model underestimated the economic costs of mental health. Such limitation renders the investment cases poor in providing valuable intervention points from the perspectives of both the users and the providers.
Conclusions
There is a need for further calibration and validation of the investment case outcomes. The current estimated results cannot be used to guide service provision, research, and mental health programming comprehensively.}
}
@article{HAAS2024110900,
title = {Models vetted against prediction error and parameter sensitivity standards can credibly evaluate ecosystem management options},
journal = {Ecological Modelling},
volume = {498},
pages = {110900},
year = {2024},
issn = {0304-3800},
doi = {https://doi.org/10.1016/j.ecolmodel.2024.110900},
url = {https://www.sciencedirect.com/science/article/pii/S0304380024002886},
author = {Timothy C. Haas},
keywords = {Model vetting, Model credibility, Ecosystem management, Parameter sensitivity, Robust statistical estimators, High performance computing},
abstract = {A new standard for assessing model credibility is developed. This standard consists of parameter estimation, prediction error assessment, and a parameter sensitivity analysis that is driven by outside individuals who are skeptical of the model’s credibility (hereafter, skeptics). Ecological/environmental models that have a one-step-ahead prediction error rate that is better than naive forecasting — and are not excessively sensitive to small changes in their parameter values are said here to be vetted. A procedure is described that can perform this assessment on any model being evaluated for possible participation in an ecosystem management decision. Uncertainty surrounding the model’s ability to predict future values of its output variables and in the estimates of all of its parameters should be part of any effort to vett a model. The vetting procedure described herein, Prediction Error Rate-Deterministic Sensitivity Analysis (PER-DSA), incorporates these two aspects of model uncertainty. DSA in particular, requires participation by skeptics and is the reason why a successful DSA gives a model sufficient credibility to have a voice in ecosystem management decision making. But these models need to be stochastic and represent the mechanistic processes of the system being modeled. For such models, performing a PER-DSA can be computationally expensive. A cluster computing algorithm to speed-up these computations is described as one way to answer this challenge. This new standard is illustrated through a PER-DSA of a population dynamics model of South African rhinoceros (Ceratotherium simum simum).}
}
@article{SENANAYAKE2024104705,
title = {Agent-based simulation for pedestrian evacuation: A systematic literature review},
journal = {International Journal of Disaster Risk Reduction},
volume = {111},
pages = {104705},
year = {2024},
issn = {2212-4209},
doi = {https://doi.org/10.1016/j.ijdrr.2024.104705},
url = {https://www.sciencedirect.com/science/article/pii/S2212420924004679},
author = {Gayani P.D.P. Senanayake and Minh Kieu and Yang Zou and Kim Dirks},
keywords = {Pedestrian behaviour modelling, Agent-based modelling, Behavioural decision-making, Emergency evacuation},
abstract = {Agent-based models (ABMs) offer promise for realistically simulating human behaviours and interactions during emergency evacuations. This review aims to systematically assess the state of the art in ABM-based evacuation modelling with respect to methodologies, validation practices, and the associated challenges over the past decade. The review critically examines 134 studies from 2013 to 2023 that have applied ABMs for pedestrian evacuation simulation to synthesise current capabilities, limitations, and advancement pathways. Findings identify persistent challenges related to modeller bias, computational complexity, data scarcity for calibration and validation, and the predominance of simplistic rule-based decision-making models, while promise exists with the adoption of flexible behavioural frameworks, high-performance computing architectures, machine learning techniques for adaptive agent behaviours and surrogate modelling, and evolutionary computation methods for transparent rule generation. The findings underscore the importance of interdisciplinary collaboration among behavioural scientists, modellers, and emergency planners to enhance the realism and reliability of ABMs. By providing a critical synthesis of the state-of-the-art and proposing future research directions, this review aims to accelerate the development and application of ABMs that can meaningfully enhance the safety and resilience of communities facing emergencies.}
}
@article{CHAUHAN2023107757,
title = {Personalized optimal room temperature and illuminance for maximizing occupant's mental task performance using physiological data},
journal = {Journal of Building Engineering},
volume = {78},
pages = {107757},
year = {2023},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2023.107757},
url = {https://www.sciencedirect.com/science/article/pii/S235271022301937X},
author = {Hardik Chauhan and Youjin Jang and Surakshya Pradhan and Hyosoo Moon},
keywords = {Indoor environment quality, Physiological response, Occupant performance, Machine learning, Particle swarm optimization},
abstract = {Indoor room temperature and illuminance level are critical factors of indoor environment quality (IEQ), affecting human mental task performance. These effects are reflected in their physiological responses such as heart rate, electrodermal activity, and skin temperature. Occupants' individual preferences, sensitivity, and physiological responses to different combinations of room temperature and illuminance level can differ among individuals. Despite previous studies investigating the individual and combined effects of different IEQ parameters, the limited research on the cross-modal relationship between room temperature and illuminance level and its impact on mental task performance highlights its significance. Moreover, to achieve personalized insights, it is essential to incorporate individual physiological responses, and this necessitates the development of an optimization model to comprehensively examine their impact. To address these issues, this study proposes a personalized model that optimizes room temperature and illuminance levels to enhance mental task performance using occupants' physiological data. Having the random forest algorithm, this study first predicted mental task performance, which includes four mental abilities such as attention, perception, working memory, and thinking ability using the occupant's physiological data. Then, the particle swarm optimization algorithm was employed to optimize room temperature and illuminance level to maximize the predicted mental task performance. The results of the proposed model align with observed values of room temperature and illuminance level during experiments, validating the adoption of a personalized approach. The findings contribute to future insights and guidelines for the design and management of indoor environments to maximize occupants' performance.}
}
@incollection{STAUFFER2006i,
title = {Biology, Sociology, Geology by Computational Physicists},
editor = {D. Stauffer and S. Moss {de Oliveira} and P.M.C. {de Oliveira} and J.S. Sá Martins},
series = {Monograph Series on Nonlinear Science and Complexity},
publisher = {Elsevier},
volume = {1},
pages = {i-276},
year = {2006},
booktitle = {Biology, Sociology, Geology by Computational Physicists},
issn = {1574-6917},
doi = {https://doi.org/10.1016/S1574-6917(05)01001-9},
url = {https://www.sciencedirect.com/science/article/pii/S1574691705010019},
author = {D. Stauffer and S. Moss {de Oliveira} and P.M.C. {de Oliveira} and J.S. Sá Martins}
}
@article{YECKEL19971379,
title = {Parallel computation of incompressible flows in materials processing: Numerical experiments in diagonal preconditioning},
journal = {Parallel Computing},
volume = {23},
number = {9},
pages = {1379-1400},
year = {1997},
note = {Parallel computing methods in applied fluid mechanics},
issn = {0167-8191},
doi = {https://doi.org/10.1016/S0167-8191(97)00059-8},
url = {https://www.sciencedirect.com/science/article/pii/S0167819197000598},
author = {Andrew Yeckel and Jeffrey J. Derby},
keywords = {Incompressible flow, Finite element method, Preconditioning, Iterative solution, Linear systems},
abstract = {Massively parallel computing is enabling dramatic advances in the simulation of three-dimensional flows in materials processing systems. This study focuses on the efficiency and robustness of parallel algorithms applied to such systems. Specifically, various diagonal preconditioning schemes are tested for the iterative solution of the linear equations arising from Newton's method applied to finite element discretizations. Two finite element discretizations are considered — the classical Galerkin and the Galerkin/least-squares method. Results show that the choice of preconditioning method can greatly influence the rate of convergence, but that no type worked uniformly well in all cases.}
}
@article{PARR2025105984,
title = {Inferring when to move},
journal = {Neuroscience & Biobehavioral Reviews},
volume = {169},
pages = {105984},
year = {2025},
issn = {0149-7634},
doi = {https://doi.org/10.1016/j.neubiorev.2024.105984},
url = {https://www.sciencedirect.com/science/article/pii/S0149763424004536},
author = {Thomas Parr and Ashwini Oswal and Sanjay G. Manohar},
keywords = {Computational neuroscience, Generative, Bayesian, Active inference, Movement, Dynamical systems, Parkinson’s disease},
abstract = {Most of our movement consists of sequences of discrete actions at regular intervals—including speech, walking, playing music, or even chewing. Despite this, few models of the motor system address how the brain determines the interval at which to trigger actions. This paper offers a theoretical analysis of the problem of timing movements. We consider a scenario in which we must align an alternating movement with a regular external (auditory) stimulus. We assume that our brains employ generative world models that include internal clocks of various speeds. These allow us to associate a temporally regular sensory input with an internal clock, and actions with parts of that clock cycle. We treat this as process of inferring which clock best explains sensory input. This offers a way in which temporally discrete choices might emerge from a continuous process. This is not straightforward, particularly if each of those choices unfolds during a time that has a (possibly unknown) duration. We develop a route for translation to neurology, in the context of Parkinson’s disease—a disorder that characteristically slows down movements. The effects are often elicited in clinic by alternating movements. We find that it is possible to reproduce behavioural and electrophysiological features associated with parkinsonism by disrupting specific parameters—that determine the priors for inferences made by the brain. We observe three core features of Parkinson’s disease: amplitude decrement, festination, and breakdown of repetitive movements. Our simulations provide a mechanistic interpretation of how pathology and therapeutics might influence behaviour and neural activity.}
}
@article{SCHIFERL1997249,
title = {Evolution of plastic anisotropy for high-strain-rate computations},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {143},
number = {3},
pages = {249-270},
year = {1997},
issn = {0045-7825},
doi = {https://doi.org/10.1016/S0045-7825(96)01159-0},
url = {https://www.sciencedirect.com/science/article/pii/S0045782596011590},
author = {Sheila K. Schiferl and Paul J. Maudlin},
abstract = {A model for anisotropic material strength, and for changes in the anisotropy due to plastic strain, is described. This model has been developed for use in high-rate, explicit, Lagrangian multidimensional continuum-mechanics codes. The model handles anisotropies, in single-phase materials, in particular the anisotropies due to crystallographic texture—preferred orientations of the single-crystal grains. Textural anisotropies, and the changes in these anisotropies, depend overwhelmingly on the crystal structure of the material and on the deformation history. The changes, particularly for complex deformations, are not amenable to simple analytical forms. To handle this problem, the material model described here includes a texture code, or micromechanical calculation, coupled to a continuum code. The texture code updates grain orientations as a function of tensor plastic strain, and calculates the yield strength in different directions. A yield function is fitted to these yield ‘points’. For each computational cell in the continuum simulation, the texture code tracks a particular set of grain orientations. The orientations will change due to the tensor strain history, and the yield function will change accordingly. Hence, the continuum code supplies a tensor strain to the texture code, and the texture code supplies an updated yield function to the continuum code. Since significant texture changes require relatively large strains—typically, a few percent or more—the texture code is not called very often, and the increase in computer time is not excessive. The model was implemented, using a finite-element continuum code and a texture code specialized for hexagonal-close-packed crystal structures. The results for several uniaxial stress problems and an explosive-forming problem are shown.}
}
@article{NARIMANI202441,
title = {Intelligent Control for Aerospace Engineers: A Novel Educational Framework},
journal = {IFAC-PapersOnLine},
volume = {58},
number = {16},
pages = {41-46},
year = {2024},
note = {2nd IFAC Workshop on Aerospace Control Education - WACE 2024},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2024.08.459},
url = {https://www.sciencedirect.com/science/article/pii/S240589632401228X},
author = {Mohammad Narimani and Seyyed Ali Emami and Paolo Castaldi},
keywords = {Aerospace control education, Intelligent control systems, Neural networks, Reinforcement learning, Model-based control, Adaptive control, Expert systems},
abstract = {The integration of intelligent control techniques into aerospace engineering education remains a challenge. This paper presents a novel approach for teaching intelligent control specifically designed for aerospace engineers, bridging the gap between theoretical foundations and practical applications. The proposed framework encompasses a comprehensive curriculum covering model-based and model-free approaches, leveraging neural networks, reinforcement learning, and other computational intelligence techniques. It emphasizes hands-on experiences through simulation-based exercises, hardware-in-the-loop experiments, and design projects tailored to different aerospace vehicle categories, including multi-rotor UAVs, helicopters, fixed-wing aircraft, and Hypersonic Flight Vehicles. The framework also addresses assessment methods, industry collaborations, and case studies to enhance student learning outcomes.}
}
@article{FATAHI2016272,
title = {A fuzzy cognitive map model to calculate a user's desirability based on personality in e-learning environments},
journal = {Computers in Human Behavior},
volume = {63},
pages = {272-281},
year = {2016},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2016.05.041},
url = {https://www.sciencedirect.com/science/article/pii/S0747563216303685},
author = {Somayeh Fatahi and Hadi Moradi},
keywords = {Personality, Emotion, User's status, Desirability, E-learning},
abstract = {The recent research in artificial intelligence shows an increasing interest in the modeling of human behavior factors such as personality, mood, and emotion for developing human-friendly systems. That is why there is an interest in developing models and algorithms to determine a human's emotions while interacting with a system to improve the quality of the interaction. In this paper, we propose a computational model to calculate a user's desirability based on personality in e-learning environments. The desirability is one of the most important variables in determining a user's emotions. The model receives several e-learning environmental events and predicts the desirability of the events based on the user's personality and his/her goals. The proposed model has been evaluated in a simulated and real e-learning environment. The results show that the model formulates the relationship between personality and emotions with high accuracy.}
}
@article{XIN2025111135,
title = {Environmental system dynamics: Current development and applications},
journal = {Ecological Modelling},
volume = {506},
pages = {111135},
year = {2025},
issn = {0304-3800},
doi = {https://doi.org/10.1016/j.ecolmodel.2025.111135},
url = {https://www.sciencedirect.com/science/article/pii/S0304380025001206},
author = {Sihui Xin and Zhouyuan Li and Junsong Nong and Jiaxin Wu and Xinwei Zou and Ruijin Wu and Shikui Dong and Rongling Wu and Shaopeng Wang},
keywords = {System dynamics, Stock-flow analysis, Causal loop feedback, Modeling applications, SD community development},
abstract = {System Dynamics (SD) has emerged as a fundamental and powerful modeling methodology for understanding and simulating the behavior of both natural and social-economic systems. This review explores the historical development, key concepts, and broad applications of SD as a fundamental modeling approach. Tracing its evolution from the its inception since 1960s, the paper outlines the four major phases of SD's growth, from its initial conceptualization, through its methodological refinements, to its widespread application in ecosystems, and socio-economic systems in recent over the half century. The review highlights the development of SD communities across the globe, including the North American school, European clusters, and the growing body of work in China, and the progress in the global collaboration. We discuss and organize the foundational paired concepts of SD, including stock-flow relationships, the structure-behavior paradigm, the cause-effect process, and the loop-feedback paradigm. We summarize the SD modeling workflow protocol in the four stages, as conceptualization, visualization, quantification, and verification. It demonstrates good practices of SD modeling in various ecological contexts, spanning population, community, landscape, and macro-ecosystem levels, while emphasizing the method's adaptability and capacity for spatial modeling. Building on an extensive literature review and bibliometric analysis, the paper synthesizes key progress in SD modeling while offering insights into future perspectives and potential advancements. It concludes by reflecting on SD's ability to address multi-scale, multi-dimensional challenges and its compatibility with emerging novel approaches. Our goal is to bridge SD with contemporary ecological modeling practices by systematically reviewing the theoretical and practical advances of SD. This review provides insights for scholars and practitioners seeking to embed SD approach to the environmental and ecological systems simulation.}
}
@article{AYOUGH2025111141,
title = {Modeling workers rotation in divisional seru production systems},
journal = {Computers & Industrial Engineering},
volume = {205},
pages = {111141},
year = {2025},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2025.111141},
url = {https://www.sciencedirect.com/science/article/pii/S0360835225002876},
author = {Ashkan Ayough and Fatameh Sadeghi Nouri and Behrooz Khorshidvand and Farbod Farhadi},
keywords = {Divisional  production system, Job rotation scheduling, Workforce adaptability, Invasive weed optimization},
abstract = {This study proposes a mathematical model for the job rotation problem in the Divisional seru Production System (DSPS) and develops an efficient solution algorithm. DSPS, a transitional phase toward a fully realized seru system, enhances flexibility and workforce adaptability in volatile manufacturing environments. A non-linear programming model optimizes maximum flow time in job rotation scheduling. Small-scale instances are solved using GAMS, while the Invasive Weed Optimization (IWO) meta-heuristic handles medium- and large-scale cases. Results show that IWO significantly outperforms GAMS in computation time while maintaining solution accuracy, with differences in objective values under 5% for most cases. Additionally, in 62.5% of cases, the number of assigned workers is fewer than the initial number of workers provided for each problem. Randomly generated test instances validate the model and algorithm, confirming their effectiveness in reducing flow time and workforce requirements. Post-optimal trials indicate that the number of rotation periods can be adjusted to minimize the flow time. It was discussed that when the number of rotation periods is optimized to minimize the flow time, imbalances among cells are also minimized. This study fills a gap in the literature and provides new insights for optimizing DSPS.}
}
@incollection{TSOTSOS1993261,
title = {The Role of Computational Complexity in Perceptual Theory},
editor = {Sergio C. Masin},
series = {Advances in Psychology},
publisher = {North-Holland},
volume = {99},
pages = {261-296},
year = {1993},
booktitle = {Foundations of Perceptual Theory},
issn = {0166-4115},
doi = {https://doi.org/10.1016/S0166-4115(08)62776-4},
url = {https://www.sciencedirect.com/science/article/pii/S0166411508627764},
author = {John K. Tsotsos},
abstract = {The validity of perceptual theories cannot be considered only in terms of how well the explanations fit experimental observations. Rather, it is argued that sufficient consideration must also be given to the physical realizability of the explanation. Experimental scientists attempt to explain their data and not just describe it, in essence, providing an algorithm whose behavior leads to the observed data. Thus, computational plausibility is not only an appropriate but a necessary consideration. One dimension of plausibility is satisfaction of the constraints imposed by the computational complexity of the problem, the resources available for the solution of the problem, and the specific algorithm proposed. It is shown that such constraints play critical roles in the explanations of perception, intelligent behavior, and evolution.}
}
@incollection{FAVERO2023509,
title = {Chapter 25 - Raster objects},
editor = {Luiz Paulo Fávero and Patrícia Belfiore and Rafael {de Freitas Souza}},
booktitle = {Data Science, Analytics and Machine Learning with R},
publisher = {Academic Press},
pages = {509-519},
year = {2023},
isbn = {978-0-12-824271-1},
doi = {https://doi.org/10.1016/B978-0-12-824271-1.00011-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128242711000111},
author = {Luiz Paulo Fávero and Patrícia Belfiore and Rafael {de Freitas Souza}},
keywords = {Spatial analysis, Maps, Raster objects, R},
abstract = {At the end of this chapter, you will be able to:•Understand what a raster object is;•Load and use raster objects;•Combine raster objects with shapefiles;•Manipulate and cut out raster objects;•Use the raster objects in such a way as to demand less computational time for the execution of tasks;•View raster objects in R language.}
}
@incollection{HUTCHINS20012068,
title = {Cognition, Distributed},
editor = {Neil J. Smelser and Paul B. Baltes},
booktitle = {International Encyclopedia of the Social & Behavioral Sciences},
publisher = {Pergamon},
address = {Oxford},
pages = {2068-2072},
year = {2001},
isbn = {978-0-08-043076-8},
doi = {https://doi.org/10.1016/B0-08-043076-7/01636-3},
url = {https://www.sciencedirect.com/science/article/pii/B0080430767016363},
author = {E. Hutchins},
abstract = {Distributed cognition is a framework for thinking about cognition which seeks to understand how the cognitive properties of aggregates emerge from the interactions of component parts. It can be applied to cognitive systems at many levels of complexity, from areas of an individual brain to communities of interacting persons. Distributed cognition is sometimes construed as a special kind of cognition that occurs when people are in interaction with one another or with material artifacts. This is only partly correct. Rather than being a kind of cognition, distributed cognition is a manner of thinking about cognition that permits one to examine the relationships between what is in the mind and the world the mind is in. When applied to groups of persons, distributed cognition provides a language for cognitive processes that are distributed across the members of a social group, between people and their material environments, and through time. It attempts to use an understanding of the social, cultural, and material context of cognitive practices to constrain models of cognitive processes within and among individual minds.}
}
@article{KUGURAKOVA2016217,
title = {Neurobiological Plausibility as Part of Criteria for Highly Realistic Cognitive Architectures},
journal = {Procedia Computer Science},
volume = {88},
pages = {217-223},
year = {2016},
note = {7th Annual International Conference on Biologically Inspired Cognitive Architectures, BICA 2016, held July 16 to July 19, 2016 in New York City, NY, USA},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2016.07.428},
url = {https://www.sciencedirect.com/science/article/pii/S1877050916316842},
author = {Vlada Kugurakova and Maxim Talanov and Denis Ivanov},
keywords = {Lövheim cube, cognitive architectures, neurobiological realism},
abstract = {In this paper we analyze neurobiologically inspired approaches to implement emotions in computational systems. We propose the criteria for realistic cognitive architectures and analyze current architectures using aforementioned criteria. The analysis indicated several interesting architectures H-CogAff, BICA that inspired us to start the development of our own based on biological realistic approaches.}
}
@incollection{WANDELL2025360,
title = {Visual processing},
editor = {Jordan Henry Grafman},
booktitle = {Encyclopedia of the Human Brain (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {360-381},
year = {2025},
isbn = {978-0-12-820481-8},
doi = {https://doi.org/10.1016/B978-0-12-820480-1.00116-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128204801001169},
author = {Brian A. Wandell and Jonathan Winawer},
keywords = {Physiological optics, Retinal circuits, Eye movements, Lateral geniculate nucleus, V1, Visual cortex, Functional specialization, Neural signaling, Visual field maps, Retinotopy, Receptive fields, Sparse representations, Asynchronous representation, Redundancy, Bayesian inference},
abstract = {The human visual system is a network of neural components that combine to create our perception of the world and guide our behavior. Deciphering the computational principles of this system is an important scientific challenge. We review measurements of these components, from the retinal encoding to cortical circuitry, and from molecules to circuits, focusing on measurements that are relevant to visual processing. We then delve into principles proposed to explain how this diverse collection of visual components enables us to interpret our surroundings.}
}
@article{RASTEIRO2009e9,
title = {LABVIRTUAL—A virtual platform to teach chemical processes},
journal = {Education for Chemical Engineers},
volume = {4},
number = {1},
pages = {e9-e19},
year = {2009},
issn = {1749-7728},
doi = {https://doi.org/10.1016/j.ece.2009.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S1749772809000025},
author = {M.G. Rasteiro and L. Ferreira and J. Teixeira and F.P. Bernardo and M.G. Carvalho and A. Ferreira and R.Q. Ferreira and F. Garcia and C.M.S.G. Baptista and N. Oliveira and M. Quina and L. Santos and P.A. Saraiva and A. Mendes and F. Magalhães and A.S. Almeida and J. Granjo and M. Ascenso and R.M. Bastos and R. Borges},
keywords = {Chemical processes, E-learning, Virtual laboratories, Computational platform},
abstract = {The need to develop the capacity for autonomous and critical thinking in students and introduce practical approaches that complement the scientific background, have been acting as driving-forces that motivate engineering educators to develop new teaching methodologies. The Chemical Engineering Departments of both the Universities of Coimbra and Porto have been experimenting in this area and addressing these concerns. Recently, they have been engaged in a broader project, involving a large group of academics with complementary competencies. This project is aimed at developing a virtual platform directed towards the learning of Chemical Processes with a wide scope. From the functional point of view the platform is organized into four main areas: Chemical Engineering, Chemical Processes, Virtual Experiments and Simulators. The Chemical Processes area is further divided into four different sections: Unit Operations and Separations, Chemical Reaction, Process Systems Engineering and Biological Processes. These sections include simulators, applications and case studies to better understand the chemical/biochemical processes. The Virtual Experiments area considers both the laboratory visualization of the basic phenomena related to the processes in the other four sections, and the remote monitoring of laboratory experiments. This platform, constructed around a dynamic Web Portal, allows discussion forums and is also aimed at sharing experiences with other schools. This paper describes the different subjects included in the web platform, as well as the simulation strategies and the web methodologies used for its construction, and also presents examples of application in the classroom.}
}
@article{HICKMAN1995153,
title = {Advanced computational methods for spatial information extraction},
journal = {Computers & Geosciences},
volume = {21},
number = {1},
pages = {153-173},
year = {1995},
issn = {0098-3004},
doi = {https://doi.org/10.1016/0098-3004(94)00063-Z},
url = {https://www.sciencedirect.com/science/article/pii/009830049400063Z},
author = {Betty L. Hickman and Michael P. Bishop and Michael V. Rescigno},
keywords = {Spatial feature extraction, Texture features, Parallel processing, Spatial task partitioning},
abstract = {A variety of mathematical approaches for spatial information extraction using digitized aerial photography and satellite imagery have been developed and implemented on serial computers. However, because of data volume and scale, the computational demands of spatial analysis procedures frequently exceed the capacity of available serial processing technologies. One way of addressing this problem is through parallel processing in which the power of multiple computing units can be used on a single problem. In this study we investigate the utility of parallel processing for spatial feature extraction. Our testing in the situation of texture feature extraction using a cooccurrence matrix indicates that dramatic reductions in execution time are possible—an image that required about 34 min to process using one processor was solved in under 2 min using nineteen processors. The availability of additional processors could result in smaller execution times. This speedup potential is a critical element in future studies focusing on more complex spatial analysis procedures.}
}
@article{NAKAKOJI2000451,
title = {Computational support for collective creativity},
journal = {Knowledge-Based Systems},
volume = {13},
number = {7},
pages = {451-458},
year = {2000},
issn = {0950-7051},
doi = {https://doi.org/10.1016/S0950-7051(00)00069-1},
url = {https://www.sciencedirect.com/science/article/pii/S0950705100000691},
author = {K Nakakoji and Y Yamamoto and M Ohira},
keywords = {Computer support for collective creativity, Human–computer interaction, Visual images in creative insight, Knowledge-based approaches, Visualization},
abstract = {The goal of our research is to develop computer systems that support designers’ collective creativity; such systems support individual creative aspects in design through the use of representations created by others in the community. We have developed two systems, IAM-eMMa and EVIDII, that both aim at supporting designers in finding visual images that would be useful for their creative design task. IAM-eMMa uses knowledge-based rules, which are constructed by other designers, to retrieve images related to a design task, and infers the underlying “rationale” when a designer chooses one of the images. EVIDII allows designers to associate affective words and images, and then shows several visual representations of the relationships among designers, images and words. By observing designers interacting with the two systems, we have identified that systems for supporting collective creativity need to be based on design knowledge that: (1) is contextualized; (2) is respectable and trustful; and (3) enables “appropriation” of a design task.}
}
@article{BUEHLER20081101,
title = {Theoretical and computational hierarchical nanomechanics of protein materials: Deformation and fracture},
journal = {Progress in Materials Science},
volume = {53},
number = {8},
pages = {1101-1241},
year = {2008},
issn = {0079-6425},
doi = {https://doi.org/10.1016/j.pmatsci.2008.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S0079642508000510},
author = {Markus J. Buehler and Sinan Keten and Theodor Ackbarow},
abstract = {Proteins constitute the building blocks of biological materials such as tendon, bone, skin, spider silk or cells. An important trait of these materials is that they display highly characteristic hierarchical structures, across multiple scales, from nano to macro. Protein materials are intriguing examples of materials that balance multiple tasks, representing some of the most sustainable material solutions that integrate structure and function. Here we review progress in understanding the deformation and fracture mechanisms of hierarchical protein materials by using a materials science approach to develop structure-process-property relations, an effort defined as materiomics. Deformation processes begin with an erratic motion of individual atoms around flaws or defects that quickly evolve into formation of macroscopic fractures as chemical bonds rupture rapidly, eventually compromising the integrity of the structure or the biological system leading to failure. The combination of large-scale atomistic simulation, multi-scale modeling methods, theoretical analyses combined with experimental validation provides a powerful approach in studying deformation and failure phenomena in protein materials. Here we review studies focused on the molecular origin of deformation and fracture processes of three types of protein materials. The review includes studies of collagen – Nature’s super-glue; beta-sheet rich protein structures as found in spider silk – a natural fiber that can reach the strength of a steel cable; as well as intermediate filaments – a class of alpha-helix based structural proteins responsible for the mechanical integrity of eukaryotic cells. The article concludes with a discussion of the significance of universally found structural patterns such as the staggered collagen fibril architecture or the alpha-helical protein motif.}
}
@article{TURNER2025989,
title = {Old Strategies, New Environments: Reinforcement Learning on Social Media},
journal = {Biological Psychiatry},
volume = {97},
number = {10},
pages = {989-1001},
year = {2025},
note = {Social Homeostasis: A New Framework for Preventing and Treating Psychiatric Conditions},
issn = {0006-3223},
doi = {https://doi.org/10.1016/j.biopsych.2024.12.012},
url = {https://www.sciencedirect.com/science/article/pii/S0006322324018201},
author = {Georgia Turner and Amanda M. Ferguson and Tanay Katiyar and Stefano Palminteri and Amy Orben},
keywords = {Development, Mental health, Reinforcement learning, Reward learning, Social media, Social reward},
abstract = {The rise of social media has profoundly altered the social world, introducing new behaviors that can satisfy our social needs. However, it is not yet known whether human social strategies, which are well adapted to the offline world we developed in, operate as effectively within this new social environment. Here, we describe how the computational framework of reinforcement learning (RL) can help us to precisely frame this problem and diagnose where behavior-environment mismatches emerge. The RL framework describes a process by which an agent can learn to maximize their long-term reward. RL, which has proven to be successful in characterizing human social behavior, consists of 3 stages: updating expected reward, valuating expected reward by integrating subjective costs such as effort, and selecting an action. Specific social media affordances, such as the quantifiability of social feedback, may interact with the RL process at each of these stages. In some cases, affordances can exploit RL biases that are beneficial offline by violating the environmental conditions under which such biases are optimal, such as when algorithmic personalization of content interacts with confirmation bias. Characterizing the impact of specific aspects of social media through this lens can improve our understanding of how digital environments shape human behavior. Ultimately, this formal framework could help address pressing open questions about social media use, including its changing role across human development and its impact on outcomes such as mental health.}
}
@article{SNEDDON20252898,
title = {Rapid (≤25 °C) cycloisomerization of anhydride-tethered triynes to benzynes – origin of a remarkable anhydride linker-induced rate enhancement††Electronic supplementary information (ESI) available. CCDC 2353613. For ESI and crystallographic data in CIF or other electronic format see DOI: https://doi.org/10.1039/d4sc07232d},
journal = {Chemical Science},
volume = {16},
number = {6},
pages = {2898-2906},
year = {2025},
issn = {2041-6520},
doi = {https://doi.org/10.1039/d4sc07232d},
url = {https://www.sciencedirect.com/science/article/pii/S2041652025000392},
author = {Dorian S. Sneddon and Paul V. Kevorkian and Thomas R. Hoye},
abstract = {The hexadehydro-Diels–Alder (HDDA) reaction is a cycloisomerization between a conjugated diyne and a tethered diynophile that generates ortho-benzyne derivatives. Considerable fundamental understanding of aryne reactivity has resulted from this body of research. The multi-yne cycloisomerization substrate is typically pre-formed and the (rate-limiting) closure of this diyne/diynophile pair to produce the isomeric benzyne generally requires thermal input, often requiring reaction temperatures of >100 °C and times of 16–48 h to achieve near-full conversion. We report here that diynoic acids can be dimerized and that the resulting substrate, having a 3-atom anhydride linker (i.e., OCOCO), then undergoes HDDA cyclization within minutes at or below room temperature. This allows for the novel in situ assembly and cyclization of HDDA benzyne precursors in an operationally simple protocol. Experimental kinetic data along with DFT computations are used to identify the source of this surprisingly huge rate acceleration afforded by the anhydride linker: >107 faster than the analogous multi-yne having, instead, a CH2OCH2 ether linker.}
}
@article{SHU2025111052,
title = {Optimal power flow in hybrid AC-DC systems considering N-k security constraints in the preventive-corrective control stage},
journal = {Electric Power Systems Research},
volume = {238},
pages = {111052},
year = {2025},
issn = {0378-7796},
doi = {https://doi.org/10.1016/j.epsr.2024.111052},
url = {https://www.sciencedirect.com/science/article/pii/S0378779624009349},
author = {Hongchun Shu and Hongfang Zhao and Mengli Liao},
keywords = {Flexible DC transmission, AC-DC hybrid system, -, Safety constraints, Optimal power flow},
abstract = {The optimal power flow methods for AC-DC systems containing VSC-HVDC generally only consider the economy during normal operation, overlooking the distribution of line transmission power in fault conditions. As a result, lines that continue to operate after a fault may experience overloading or operate at full capacity. Thus, a method for optimal power flow calculation is proposed that incorporates N-k security constraints in the preventive-corrective control stage for secure and economic operation of hybrid AC-DC systems. This method ensures that the line transmission power in the system meets the limits in the normal, short-term fault, and long-term fault states. In addition to the optimal power flow in the normal state, the method incorporates the system's imbalance as an indicator to evaluate system resilience. It combines this indicator with the economic, network loss, and performance metrics of the system, forming a two-stage bi-level multi-objective optimization model. Furthermore, to address the curse of dimensionality in anticipating system fault sets, a method for generating the anticipated fault set using non-sequential Monte Carlo simulation is proposed, along with a fault scenario search approach based on robust thinking to identify the most severe faults. Finally, the traditional IEEE 30-bus system was improved, and simulation verification was conducted using examples of an AC/DC system with a three-terminal DC network and a wind-solar-storage hybrid AC/DC system with a three-terminal DC network. The simulation results indicate that the proposed optimal power flow method considering the preventive-corrective control stage with N-k security constraints can effectively enhance system resilience. Furthermore, it improves the economic efficiency while ensuring the secure operation of the system.}
}
@article{BRENTDANIEL2023105630,
title = {Extremely rapid, Lagrangian modeling of 2D flooding: A rivulet-based approach},
journal = {Environmental Modelling & Software},
volume = {161},
pages = {105630},
year = {2023},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2023.105630},
url = {https://www.sciencedirect.com/science/article/pii/S1364815223000166},
author = {W. {Brent Daniel} and Corinne Roth and Xue Li and Cindy Rakowski and Tim McPherson and David Judi},
keywords = {Modeling, Fluid flow, Flood modeling, Hydrodynamic model, Rivulet, Lagrangian},
abstract = {Estimates of potential flood inundation areas and depths are critical to informing the preparedness, response, and investment decisions of many government agencies and private sector organizations, especially under a changing climate. The standard modeling approaches, however, are often either computationally intensive or constrained in their accuracy or applicability. A novel, rivulet-based, 2D model of flooding is described in this article that is 10,000 to 10 million times less computationally complex than the full solution of the shallow water equations, yet achieves inundation area hit rates of between 0.8 and 0.9, relative absolute mean errors of 10%–20% across a wide range of flow depths, and comparable accuracy at forecasting empirical high-water marks. This combination of accuracy and efficiency will significantly enhance real-time depth estimates during flood events, support detailed sensitivity analyses, and allow for the generation of large ensembles to enable complex uncertainty analyses.}
}
@article{SELOTE2025108302,
title = {A knowledge graph approach to drug repurposing for Alzheimer’s, Parkinson’s and Glioma using drug–disease–gene associations},
journal = {Computational Biology and Chemistry},
volume = {115},
pages = {108302},
year = {2025},
issn = {1476-9271},
doi = {https://doi.org/10.1016/j.compbiolchem.2024.108302},
url = {https://www.sciencedirect.com/science/article/pii/S1476927124002901},
author = {Ruchira Selote and Richa Makhijani},
keywords = {Knowledge graph, Drug repurposing, Node embeddings, Biased random walk, Feature learning, Cosine similarity},
abstract = {Drug Repurposing gives us facility to find the new uses of previously developed drugs rather than developing new drugs from start. Particularly during pandemic, drug repurposing caught much attention to provide new applications of the previously approved drugs. In our research, we provide a novel method for drug repurposing based on feature learning process from drug–disease–gene network. In our research, we aimed at finding drug candidates which can be repurposed under neurodegenerative diseases and glioma. We collected association data between drugs, diseases and genes from public resources and primarily examined the data related to Alzheimer’s, Parkinson’s and Glioma diseases. We created a Knowledge Graph using neo4j by integrating all these datasets and applied scalable feature learning algorithm known as node2vec to create node embeddings. These embeddings were later used to predict the unknown associations between disease and their candidate drugs by finding cosine similarity between disease and drug nodes embedding. We obtained a definitive set of candidate drugs for repurposing. These results were validated from the literature and CodReS online tool to rank the candidate drugs. Additionally, we verified the status of candidate drugs from pharmaceutical knowledge databases to confirm their significance.}
}
@article{YANG2020119,
title = {A multilevel neighborhood sequential decision approach of three-way granular computing},
journal = {Information Sciences},
volume = {538},
pages = {119-141},
year = {2020},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2020.05.060},
url = {https://www.sciencedirect.com/science/article/pii/S0020025520304734},
author = {Xin Yang and Tianrui Li and Dun Liu and Hamido Fujita},
keywords = {Three-way granular computing, Sequential three-way decision, Neighborhood, Multilevel},
abstract = {The fusion of three-way decision and granular computing provides powerful ideas and methods to understand and solve the problems of cognitive science by thinking and information processing in threes. As a typical representation of three-way granular computing, sequential three-way decision focuses on making a multiple stages of decisions by a sequence of trisecting-acting-outcome (TAO) models. To construct more general granules, levels, and hierarchies, we investigate an integrative multi-granularity approach to sequential three-way decision in a neighborhood system by the evolution mechanism of data and parameters. We employ the γ-cut similarity neighborhood relation based on Gaussian kernel function to the hierarchical granulation of universe. Subsequently, we propose the multilevel neighborhood granular structures by the combinations of horizontal granularity and vertical granularity, and discuss the monotonicity of level measurements associated with the uncertainty of decision. Based on such a neighborhood structured approach, a multilevel framework of sequential three-way decision is examined from coarser to finer concerning the granularity of neighborhood information. Finally, we report a series of experiments to demonstrate the performance of proposed models and algorithms.}
}
@article{SUN2024103771,
title = {Supply chain planning with free trade zone and uncertain demand},
journal = {Transportation Research Part E: Logistics and Transportation Review},
volume = {192},
pages = {103771},
year = {2024},
issn = {1366-5545},
doi = {https://doi.org/10.1016/j.tre.2024.103771},
url = {https://www.sciencedirect.com/science/article/pii/S1366554524003624},
author = {Haoying Sun and Manoj Vanajakumari and Chelliah Sriskandarajah and Subodha Kumar},
keywords = {Supply chain management, Robust optimization, Dynamic lot sizing},
abstract = {Our research is inspired by the subcontracting problem at a major oil field services company in North America. The company’s supply chain consists of suppliers bringing raw materials to a Free Trade Zone (FTZ). The FTZ receives raw materials in full containers from various suppliers, and then the company ships them to various plants (e.g. oil excavation sites) frequently via subcontractors. This allows the company to focus on managing only the inbound transportation and inventory at the FTZ. The demand for each raw material is stochastic. We derive an algorithm running at polynomial time for the stochastic programming formulation and perform μ− regret Robust Optimization to handle the demand uncertainty. We also use a Sample Average Approximation method to alleviate the high computational requirement of the robust optimization model. The modeling approach demonstrated by this paper not only meets the needs of this specific company and industry but also can be applied to other industries with similar supply chain structures.}
}
@article{GUPTA19941,
title = {On the principles of fuzzy neural networks},
journal = {Fuzzy Sets and Systems},
volume = {61},
number = {1},
pages = {1-18},
year = {1994},
issn = {0165-0114},
doi = {https://doi.org/10.1016/0165-0114(94)90279-8},
url = {https://www.sciencedirect.com/science/article/pii/0165011494902798},
author = {M.M. Gupta and D.H. Rao},
keywords = {Fuzzy logic, neural networks, fuzzy neural networks, confluence operation, synpatic and somatic operations},
abstract = {Over the last decade or so, significant advances have been made in two distinct technological areas: fuzzy logic and computational neutral networks. The theory of fuzzy logic provides a mathematical framework to capture the uncertainties associated with human cognitive processes, such as thinking and reasoning. Also, it provides a mathematical morphology to emulate certain perceptual and linguistic attributes associated with human cognition. On the other hand, the computational neural network paradigms have evolved in the process of understanding the incredible learning and adaptive features of neuronal mechanisms inherent in certain biological species. Computational neural networks replicate, on a small scale, some of the computational operations observed in biological learning and adaptation. The integration of these two fields, fuzzy logic and neural networks; has given birth to an emerging technological field — the fuzzy neural networks. The fuzzy neural networks have the potential to capture the benefits of the two fascinating fields, fuzzy logic and neural networks, into a single capsule. The intent of this tutorial paper is to describe the basic notions of biological and computational neuronal morphologies, and to describe the principles and architectures of fuzzy neural networks. Towards this goal, we develop a fuzzy neural architecture based upon the notion of T-norm and T-conorm connectives. An error-based learning scheme is described for this neural structure.}
}
@article{DECOUGNY1994157,
title = {Load balancing for the parallel adaptive solution of partial differential equations},
journal = {Applied Numerical Mathematics},
volume = {16},
number = {1},
pages = {157-182},
year = {1994},
issn = {0168-9274},
doi = {https://doi.org/10.1016/0168-9274(94)00039-5},
url = {https://www.sciencedirect.com/science/article/pii/0168927494000395},
author = {H.L. deCougny and K.D. Devine and J.E. Flaherty and R.M. Loy and C. Özturan and M.S. Shephard},
abstract = {An adaptive technique for a partial differential system automatically adjusts a computational mesh or varies the order of a numerical procedure with a goal of obtaining a solution satisfying prescribed accuracy criteria in an optimal fashion. Processor load imbalances will, therefore, be introduced at adaptive enrichment steps during the course of a parallel computation. We develop and describe three procedures for retaining and restoring load balance that have low unit cost and are appropriate for use in an adaptive solution environment. Tiling balances load by using local optimality criteria within overlapping processor neighborhoods. Elemental data are migrated between processors within the same neighborhoods to restore balance. Tiling is restricted to uniform two-dimensional meshes and provides limited control of communications volume by priority-based element selection criteria. These shortcomings can potentially be overcome by creating a dynamic partition graph connecting processors and their neighboring regions. After coloring the edges of the graph, elemental data are iteratively transferred between processors by pairwise exchange to permit a more global migration. Octree decomposition of a spatial domain is a successful three-dimensional mesh generation strategy. The octree structure facilities a rapid load balancing procedure by performing tree traversals that (i) appraise subtree costs and (ii) partition spatial regions accordingly. Computational results are reported for two- and three-dimensional problems using nCUBE/2 hypercube, MasPar MP-2, and Thinking Machines CM-5 computers.}
}
@article{FOWLER20245,
title = {Will variants of uncertain significance still exist in 2030?},
journal = {The American Journal of Human Genetics},
volume = {111},
number = {1},
pages = {5-10},
year = {2024},
issn = {0002-9297},
doi = {https://doi.org/10.1016/j.ajhg.2023.11.005},
url = {https://www.sciencedirect.com/science/article/pii/S0002929723004007},
author = {Douglas M. Fowler and Heidi L. Rehm},
abstract = {Summary
In 2020, the National Human Genome Research Institute (NHGRI) made ten “bold predictions,” including that “the clinical relevance of all encountered genomic variants will be readily predictable, rendering the diagnostic designation ‘variant of uncertain significance (VUS)’ obsolete.” We discuss the prospects for this prediction, arguing that many, if not most, VUS in coding regions will be resolved by 2030. We outline a confluence of recent changes making this possible, especially advances in the standards for variant classification that better leverage diverse types of evidence, improvements in computational variant effect predictor performance, scalable multiplexed assays of variant effect capable of saturating the genome, and data-sharing efforts that will maximize the information gained from each new individual sequenced and variant interpreted. We suggest that clinicians and researchers can realize a future where VUSs have largely been eliminated, in line with the NHGRI’s bold prediction. The length of time taken to reach this future, and thus whether we are able to achieve the goal of largely eliminating VUSs by 2030, is largely a consequence of the choices made now and in the next few years. We believe that investing in eliminating VUSs is worthwhile, since their predominance remains one of the biggest challenges to precision genomic medicine.}
}
@article{ELLIOT2022112418,
title = {An expanded framing of ecosystem services is needed for a sustainable urban future},
journal = {Renewable and Sustainable Energy Reviews},
volume = {162},
pages = {112418},
year = {2022},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2022.112418},
url = {https://www.sciencedirect.com/science/article/pii/S1364032122003264},
author = {T. Elliot and J.A. Torres-Matallana and B. Goldstein and J. {Babí Almenar} and E. Gómez-Baggethun and V. Proença and B. Rugani},
keywords = {Urban ecosystem services, Urban metabolism, Life cycle thinking, Land cover change, System dynamics modelling, Urban land teleconnections},
abstract = {Urban activities are an important driver of ecosystem services decline. Sustainable urbanisation necessitates anticipating and mitigating these negative socio-ecological impacts, both within and beyond city boundaries. There is a lack of scalable, dynamic models of changes to ecosystems wrought by urban processes. We developed a system dynamics model, ESTIMUM, to predict locations, types, and magnitude of changes in ecosystem services. We tested the model in Lisbon (Portugal) under four specific urban development scenarios – a base case scenario and three local sustainability-driven scenarios – to the year 2050. Our results show that urban sustainability policies focused on reducing impacts within Lisbon can be undermined by increased impacts in the extended regions that supply resources to the city. In particular, carbon sequestration from urban greening pales in comparison to growing greenhouse gases from the consumption of food, energy and construction materials. We also find that policies targeted at these extended environmental impacts can be much more effective than those with a limited focus on the urban form. For example, dietary shifts could support positive changes outside that city to increase global climate regulation by 54% compared to a mere 1% increase through intensive urban greening. This highlights the urgent need for a reframing of urban sustainability in policy and scholarly circles from city-centric focus towards an expanded multi-scalar conceptualisation of urban sustainability that accounts for urban impacts beyond the city boundaries.}
}
@article{HUANG2025135963,
title = {Dynamic performance optimization of a floating offshore wind turbine based on fractal-inspired design principles},
journal = {Energy},
volume = {324},
pages = {135963},
year = {2025},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2025.135963},
url = {https://www.sciencedirect.com/science/article/pii/S0360544225016056},
author = {Haoda Huang and Qingsong Liu and Musa Bashir and Sean Malkeson and Chun Li and Minnan Yue and Weipao Miao and Jin Wang},
keywords = {Floating offshore wind turbine, Leaf-vein structure, Computational fluid dynamics, Fractal dimension, Fully coupled dynamic response},
abstract = {As the development of onshore and fixed offshore wind turbines approaches saturation, floating offshore wind turbines (FOWTs) are increasingly gaining attention due to their ability to operate in deeper waters and harness more stable wind resources. However, the dynamic responses of FOWTs are amplified significantly under the complex sea conditions, posing challenges to the overall system stability. This study proposes a novel semi-submersible platform featuring fractal structure inspired by Victoria Amazonica as solutions to the overall system stability of FOWTs. The computational fluid dynamics method, integrated with dynamic fluid-body interaction and volume of fluid wave model, is used to examine the aero, hydro, and mooring dynamics of the FOWT. A parametric model of the fractal structure with different branch levels is constructed by recursive method. Firstly, the hydrodynamic performance of the novel platforms with multi-level branch structures is examined under single wave conditions. The results show that vortices in fractal structures present higher velocity gradients and greater viscous dissipation, thereby effectively absorbing wave energy. The stability of the platform improves progressively as the branch levels increase. Subsequently, the dynamic responses of the full-configuration FOWT mounted on the platform with 8-level fractal structure (8LFS-FOWT) are further evaluated under wind-wave coupling conditions. The results reveal that 8LFS-FOWT achieves superior hydrodynamic performance with the most notable improvement in pitch amplitude of 25.22 % decrease. This enhancement also brings a 12.75 % reduction in the standard deviation of power output, forming positive feedback to ensure safe and stable operation of the system. The findings provide a valuable reference for promoting the innovative platform design of FOWTs.}
}
@article{PIOLOPEZ2023103585,
title = {Morphoceuticals: Perspectives for discovery of drugs targeting anatomical control mechanisms in regenerative medicine, cancer and aging},
journal = {Drug Discovery Today},
volume = {28},
number = {6},
pages = {103585},
year = {2023},
issn = {1359-6446},
doi = {https://doi.org/10.1016/j.drudis.2023.103585},
url = {https://www.sciencedirect.com/science/article/pii/S1359644623001010},
author = {Léo Pio-Lopez and Michael Levin},
keywords = {Biomedicine, Drug discovery, Morphogenesis},
abstract = {Morphoceuticals are a new class of interventions that target the setpoints of anatomical homeostasis for efficient, modular control of growth and form. Here, we focus on a subclass: electroceuticals, which specifically target the cellular bioelectrical interface. Cellular collectives in all tissues form bioelectrical networks via ion channels and gap junctions that process morphogenetic information, controlling gene expression and allowing cell networks to adaptively and dynamically control growth and pattern formation. Recent progress in understanding this physiological control system, including predictive computational models, suggests that targeting bioelectrical interfaces can control embryogenesis and maintain shape against injury, senescence and tumorigenesis. We propose a roadmap for drug discovery focused on manipulating endogenous bioelectric signaling for regenerative medicine, cancer suppression and antiaging therapeutics.}
}
@article{CHATRABHUJ2024101045,
title = {Design of an iterative method for environmental-sustainable development: Integrating bioinspired computing techniques},
journal = {Environmental Development},
volume = {51},
pages = {101045},
year = {2024},
issn = {2211-4645},
doi = {https://doi.org/10.1016/j.envdev.2024.101045},
url = {https://www.sciencedirect.com/science/article/pii/S2211464524000836},
author = { Chatrabhuj and Kundan Meshram},
keywords = {Sustainable development, Bioinspired computing, Hybrid algorithms, Agent-based modelling, High-performance computing},
abstract = {The need for sustainable development has grown in response to global environmental, social, and economic challenges. Conventional computational methods frequently struggle to address the complex nature of the Sustainable Development Goals (SDGs), lacking the ability to balance global search with local optimization and failing to prioritize goals related to sustainability. To address these restrictions, this work introduces the Integrated Bioinspired Computing Model for Sustainable Development (IBCMSD). By combining Genetic Algorithms (GAs), Artificial Neural Networks (ANNs), and Ant Colony Optimization (ACO), a cohesive hybrid model is developed that improves exploration and exploitation, balance for increased efficiency, and solution quality. It is implemented on High-Performance Computing (HPC) clusters to ensure scalability and resilience when dealing with complicated optimization challenges. Furthermore, using a multidisciplinary co-design method completes the model with multiple views, increasing its relevance and applicability in real-world circumstances. IBCMSD makes a significant contribution to computational sustainability by leveraging bioinspired computing, potentially enabling informed decision-making and SDG accomplishment across multiple domains.}
}
@article{LI2024112467,
title = {Study on correlation between perioperative cognitive function and nutritional status in elderly patients with gastric cancer},
journal = {Experimental Gerontology},
volume = {193},
pages = {112467},
year = {2024},
issn = {0531-5565},
doi = {https://doi.org/10.1016/j.exger.2024.112467},
url = {https://www.sciencedirect.com/science/article/pii/S0531556524001098},
author = {Rong Li and Yuping Liu and Yingtao Meng and Xianlin Qu and Meimei Shang and Lihui Yang and Jie Chai},
keywords = {Elderly, Gastric cancer, Perioperative period, Cognitive function, Nutritional status, Correlation, Analysis},
abstract = {Objective: To investigate the cognitive function and nutritional status of elderly patients with gastric cancer during perioperative period, and to analyze their correlation. Methods: Aged patients undergoing gastric cancer surgery in The Affiliated Cancer Hospital of Shandong First Medical University from March to October 2021 were selected as the subjects of this study. The monitoring data of cognitive function and nutritional status were retrospectively analyzed from 1 to 3 days before surgery, 1 and 3 days after surgery, 7 days after surgery (before discharge) and 30 days after surgery to analyze the correlation between cognitive function and nutritional status in elderly patients with gastric cancer. Results: the incidence of mild cognitive impairment in elderly patients with gastric cancer was 52.43 %, the visual space of the two groups' (mild cognitive impairment) ability of execution, name, attention, language, abstract thinking, delayed memory and cognitive function scores were lower than 1 set of directional force (cognitive function in normal group), statistically significant difference (P < 0.05). The nutritional status of elderly patients with gastric cancer was lower than that of healthy elderly group at the same period (P < 0.05). The scores of visual spatial executive function, name, attention, delayed memory, orientation and total score of cognitive function in elderly gastric cancer patients were positively correlated with nutritional status (P < 0.05). Conclusions: The cognitive function and nutritional status of elderly patients with gastric cancer are both in a low state during treatment and a higher level of cognitive function can help patients maintain a more correct nutritional cognition, and the nutritional status of patients will be relatively better. There is a positive correlation between cognitive function and nutritional status in elderly patients with gastric cancer, which should be paid attention to in the treatment.}
}
@incollection{KUMAR2025185,
title = {Chapter 9 - Future prospective of neuromorphic computing in artificial intelligence: A review, methods, and challenges},
editor = {Harish Garg and Jyotir {Moy Chatterjee} and R. Sujatha and Shatrughan Modi},
booktitle = {Primer to Neuromorphic Computing},
publisher = {Academic Press},
pages = {185-197},
year = {2025},
isbn = {978-0-443-21480-6},
doi = {https://doi.org/10.1016/B978-0-443-21480-6.00008-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780443214806000080},
author = {Vivek Kumar and Kapil Joshi and Rajiv Kumar and Minakshi Memoria and Ashulekha Gupta and F. Ajesh},
keywords = {Neuromorphic computing, Artificial intelligence, Deep learning, Machine learning, Human brain modeling},
abstract = {Neuromorphic computing in the area of artificial intelligence (AI) offers the appeal of human brain modeling. In the Fourth Industrial Revolution era, AI is among the most advanced scientific knowledge that can integrate human behavior and intelligence into machines. Even though neuromorphic computing has been around since the 1980s, it is still a relatively new field. In the last 10 years, in particular, there has been a significant amount of study and the advancement of AI. The next stage of AI is thought to be Neuromorphic Computing. The development of neuromorphic computing technology will be crucial. The most potent computational device in existence, the human brain has long served as an inspiration for AI. This study discusses neuromorphic computing, a new form of sophisticated computing that draws inspiration from brain intelligence. The objective of this paper is to give a summary of the present status of AI and neuromorphic computing to express a viewpoint on the potential and challenges that lie ahead for the main applications of neuromorphic computing. We discuss the prospects for further development of these systems and highlight features of neuromorphic computing that are promising for the field's future.}
}
@incollection{FROEMER2025234,
title = {Belief updates, learning and adaptive decision making},
editor = {Jordan Henry Grafman},
booktitle = {Encyclopedia of the Human Brain (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {234-251},
year = {2025},
isbn = {978-0-12-820481-8},
doi = {https://doi.org/10.1016/B978-0-12-820480-1.00059-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128204801000590},
author = {Romy Froemer and Matthew R. Nassar},
keywords = {Reinforcement learning, Reward, Value, Action, Dopamine, Belief updating, Sequential sampling, Attention, Confidence, Context, Experience, Goal-directed behavior, Cost-benefit decision-making},
abstract = {People make decisions every day and the outcomes of those decisions often lead them to change their beliefs and in some cases shape their future behavior. How does the brain decide which meal to order at a restaurant, and how does it learn from the experience of eating that meal? Here we review work from neuroscience, psychology and economics that shapes our understanding of how the brain makes decisions and learns through experience. We focus on computational mechanisms that can explain core phenomena in learning and decision making as well as how such mechanisms are implemented in the brain. Our review highlights both the considerable progress made in the last decades elucidating mechanisms of learning and decision making as well as the vast territory of open questions that remain to be answered.}
}
@article{WOLLMANN2019278,
title = {Proposal for a model to hierarchize strategic decisions according to criteria of value innovation, sustainability and budgetary constraint},
journal = {Journal of Cleaner Production},
volume = {231},
pages = {278-289},
year = {2019},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2019.05.190},
url = {https://www.sciencedirect.com/science/article/pii/S095965261931724X},
author = {Dewey Wollmann and Ubiratã Tortato},
keywords = {Complex adaptive systems, Analytic network process, BOCR analysis, Linear programming},
abstract = {Organizations need management models, which will enable their executives to develop systemic thinking. In addition, executives should keep in mind that: some decision-making processes should be shared; impose political influence according to their preferences; value innovation strategies may be present; it is essential to consider the environmental, economic and social dimensions of sustainability. In this context, this study describes a model to hierarchize strategic decisions according to criteria innovation value, sustainability and budgetary constraint, developed according to the methodology proposed by Mitroff et al. (1974). In addition to hierarchizing, the model allows identifying the degree of importance of each of the strategic decisions in the performance indicators defined as evaluation criteria and sub-criteria. In the conceptualization phase, the model is influenced by concepts that describe complex adaptive systems. Next, the Analytic Network Process with Benefits, Opportunities, Costs and Risks Analysis and Linear Programming techniques are used in order to define the mathematical structure that operationalizes the model. The use of a hypothetical example demonstrates the capacity of the model proposed in this work to support the decision-making process of an organization in the selection of its decision alternatives. Thus, the model can help the academic and business communities concerned with the progress of sustainable societies, insofar as it subsidizes decision-making for the development and implementation of new products and processes related to cleaner production.}
}
@article{GOECKE2020101470,
title = {Testing competing claims about overclaiming},
journal = {Intelligence},
volume = {81},
pages = {101470},
year = {2020},
issn = {0160-2896},
doi = {https://doi.org/10.1016/j.intell.2020.101470},
url = {https://www.sciencedirect.com/science/article/pii/S0160289620300489},
author = {B. Goecke and S. Weiss and D. Steger and U. Schroeders and O. Wilhelm},
keywords = {Overclaiming, Declarative knowledge, Self-reported knowledge, Creativity, Intelligence, Faking},
abstract = {Overclaiming has been described as people's tendency to overestimate their cognitive abilities in general and their knowledge in particular. We discuss four different perspectives on the phenomenon of overclaiming that have been proposed in the research literature: Overclaiming as a result of a) self-enhancement tendencies, b) as a cognitive bias (e.g., hindsight bias, memory bias), c) as proxy for cognitive abilities, and d) as sign of creative engagement. Moreover, we discuss two different scoring methods for an OCQ (signal detection theory vs. familiarity ratings). To distinguish between the different viewpoints of what overclaiming is, we juxtaposed overclaiming, as indicated by claiming familiarity with non-existent terms, with fluid and crystallized intelligence, self-reported knowledge, creativity, faking ability, and personality. Overclaiming was measured with a newly comprised overclaiming questionnaire. Results of several latent variable analyses based upon a multivariate study with 298 participants were: First, overclaiming is neither predicted by honesty-humility nor faking ability and therefore reflects something different than mere self-enhancement tendencies. Second, overclaiming is not predicted by crystallized intelligence, but is highly predictive of self-reported knowledge and, thus, not suitable as an index or a proxy for cognitive abilities. Finally, overclaiming is neither related to divergent thinking and originality, and only moderately predicted by self-reported openness creativity from the HEXACO which means that overclaiming does not reflect creative ability. In sum, our results favor an interpretation of overclaiming as a phenomenon that requires more than self-enhancement motivation, in contrast to the claim that was initially proposed in the literature.}
}
@article{MILLNER2020704,
title = {Advancing the Understanding of Suicide: The Need for Formal Theory and Rigorous Descriptive Research},
journal = {Trends in Cognitive Sciences},
volume = {24},
number = {9},
pages = {704-716},
year = {2020},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2020.06.007},
url = {https://www.sciencedirect.com/science/article/pii/S1364661320301480},
author = {Alexander J. Millner and Donald J. Robinaugh and Matthew K. Nock},
keywords = {suicide, suicide theory, formal models},
abstract = {Suicide is a leading cause of death worldwide and perhaps the most puzzling and devastating of all human behaviors. Suicide research has primarily been guided by verbal theories containing vague constructs and poorly specified relationships. We propose two fundamental changes required to move toward a mechanistic understanding of suicide. First, we must formalize theories of suicide, expressing them as mathematical or computational models. Second, we must conduct rigorous descriptive research, prioritizing direct observation and precise measurement of suicidal thoughts and behaviors and of the factors posited to cause them. Together, theory formalization and rigorous descriptive research will facilitate abductive theory construction and strong theory testing, thereby improving the understanding and prevention of suicide and related behaviors.}
}
@article{NAGOEV2020615,
title = {Model of the reasoning process in a multiagent cognitive system},
journal = {Procedia Computer Science},
volume = {169},
pages = {615-619},
year = {2020},
note = {Postproceedings of the 10th Annual International Conference on Biologically Inspired Cognitive Architectures, BICA 2019 (Tenth Annual Meeting of the BICA Society), held August 15-19, 2019 in Seattle, Washington, USA},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.02.202},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920303252},
author = {Zalimhan Nagoev and Inna Pshenokova and Murat Anchekov},
keywords = {Multi-Agent Systems, Neurocognitive Architecture, Simulation Model, Artificial Intelligence Systems, Reasoning Models},
abstract = {A model of the reasoning process in a multiagent cognitive system for the synthesis of intelligent solutions of the problem is presented. The approach based on the computational abstraction of multi-agent neurocognitive systems that illustrates architectural conformity to self-organizing neurocognitive networks of the brain. The model represents the process of reasoning in the form of cognitive blocks that synthesize intelligent solutions and allow the user to effectively solve the tasks.}
}
@article{ADRIAENSEN2023106294,
title = {Systems-theoretic interdependence analysis in robot-assisted warehouse management},
journal = {Safety Science},
volume = {168},
pages = {106294},
year = {2023},
issn = {0925-7535},
doi = {https://doi.org/10.1016/j.ssci.2023.106294},
url = {https://www.sciencedirect.com/science/article/pii/S0925753523002369},
author = {Arie Adriaensen and Liliane Pintelon and Francesco Costantino and Giulio {Di Gravio} and Riccardo Patriarca},
keywords = {FRAM, Human-machine interaction, Industry 4.0, Industry 5.0, Cobots},
abstract = {The safe and efficient application of collaborative robots requires an understanding of actual work practices transformation, emerging from the adoption of new technological instruments. Functional systems-thinking is largely absent in literature about collaborative robot applications. In this context, this study proposes a framework that combines two safety analysis methods, being the Functional Resonance Analysis Method and Interdependence Analysis. Both safety and efficiency are examined by selected case study highlights to gain an in-depth understanding of human operators’ role as the central driver of human–machine (eco)systems in a warehouse distribution system, in which warehouse robot assistance is provided. Whereas the Functional Resonance Analysis Method first maps the work system interactions as a whole, Interdependence Analysis is subsequently applied to investigate individual inter-agent exchanges by the principles of Observability, Predictability, and Directability as a core principle for goal coordination between multiple agents, including warehouse robot agents. The case study examples reveal the combined effects of the working system environment and the robot application but also demonstrate possible operational solutions to deal with socio-technical complexity.}
}
@article{KNOWLTON2012373,
title = {A neurocomputational system for relational reasoning},
journal = {Trends in Cognitive Sciences},
volume = {16},
number = {7},
pages = {373-381},
year = {2012},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2012.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S1364661312001283},
author = {Barbara J. Knowlton and Robert G. Morrison and John E. Hummel and Keith J. Holyoak},
abstract = {The representation and manipulation of structured relations is central to human reasoning. Recent work in computational modeling and neuroscience has set the stage for developing more detailed neurocomputational models of these abilities. Several key neural findings appear to dovetail with computational constraints derived from a model of analogical processing, ‘Learning and Inference with Schemas and Analogies’ (LISA). These include evidence that (i) coherent oscillatory activity in the gamma and theta bands enables long-distance communication between the prefrontal cortex and posterior brain regions where information is stored; (ii) neurons in prefrontal cortex can rapidly learn to represent abstract concepts; (iii) a rostral-caudal abstraction gradient exists in the PFC; and (iv) the inferior frontal gyrus exerts inhibitory control over task-irrelevant information.}
}
@article{OFFENHUBER2023264,
title = {Reconsidering Representation in College Design Curricula},
journal = {She Ji: The Journal of Design, Economics, and Innovation},
volume = {9},
number = {2},
pages = {264-282},
year = {2023},
note = {The Future of Design Education: Rethinking Design Education for the 21st Century},
issn = {2405-8726},
doi = {https://doi.org/10.1016/j.sheji.2023.04.005},
url = {https://www.sciencedirect.com/science/article/pii/S2405872623000394},
author = {Dietmar Offenhuber and Joy Mountford},
keywords = {Representation, Data, Models, Maps, Visualization, Sensory modalities},
abstract = {The Future of Design Education working group on representation addressed the roles of data, maps, models, and interfaces as a continuum from representation to action. The article traces historical ideas of representation grounded by a linguistic paradigm to more recent approaches based on performance, embodiment, and sensory modalities other than vision. Discussions include the use of representations in the design process. Designers are able to use traditional forms of representation in the design of artifacts, such as sketches. These forms of representation are not sufficient for the design of systems. System design requires models that allow stakeholders to negotiate their view of a situation and design teams to iterate how things might work. Core ideas in the working group recommendations address issues of, substitution, formal rules, motivation, context dependency, materiality, provisionality, latency, performance, externalization, facilitation and negotiation, mediation, and measurement and evaluation. Discussions address the socio-political implications of representation and the expanding role of computing and data that call for a systems view.}
}
@article{BATTISTELLI2022,
title = {Online Strategies To Improve Quantitative Skills in Microbiology Laboratory Classes},
journal = {Journal of Microbiology & Biology Education},
volume = {23},
number = {1},
year = {2022},
issn = {1935-7877},
doi = {https://doi.org/10.1128/jmbe.00333-21},
url = {https://www.sciencedirect.com/science/article/pii/S1935787722000995},
author = {Joseph M. Battistelli and Rima B. Franklin},
keywords = {quantitative literacy, quantitative biology, problem solving, word problems, math skills, formula question, Canvas, spreadsheets, algebra, formula questions},
abstract = {Biology is an increasingly quantitative science. Thus, it is important that undergraduate biology curricula include frequent opportunities for students to practice their quantitative skills.
ABSTRACT
Biology is an increasingly quantitative science. Thus, it is important that undergraduate biology curricula include frequent opportunities for students to practice their quantitative skills. This can create a substantial grading burden for faculty teaching online and/or large enrollment courses, but the “formula question” feature present in many learning management systems (LMS) offers a solution. Using this feature, faculty set up a basic scaffold for an algebraic word problem, and the LMS can then automatically generate and grade many different versions of the question. In this paper, we describe the use of “formula questions” in an undergraduate microbiology course and specifically focus on how the strategic use of algebraic word problems at multiple points throughout the semester can help build quantitative literacy. Key to the success of this approach is that faculty provide a review of foundational mathematical skills early in the semester, even in upper-level classes. This should include reacquainting students with formatting conventions (e.g., rounding and scientific notation), familiarizing them with any idiosyncrasies of the technology platforms, and demonstrating how to solve math problems using spreadsheets. This initial effort increases student success when more complex problems are introduced later in the semester. Though the tips summarized in this paper focus on undergraduate microbiology teaching laboratories using Canvas, the approach can easily be modified to help students develop their critical thinking and quantitative reasoning skills at other levels and in other disciplines.}
}
@article{MOAVENI2018452,
title = {Modified Hankel Interaction Index Array for Input-Output Pairing with Improved Characteristics},
journal = {IFAC-PapersOnLine},
volume = {51},
number = {18},
pages = {452-457},
year = {2018},
note = {10th IFAC Symposium on Advanced Control of Chemical Processes ADCHEM 2018},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2018.09.342},
url = {https://www.sciencedirect.com/science/article/pii/S2405896318320251},
author = {Bijan Moaveni and Wolfgang Birk},
keywords = {Control configuration selection, Interaction measure, Hankel Interaction Index Array, System Gramians},
abstract = {In this study, a modified version of Hankel Interaction Index Array (HIIA) for control configuration selection is presented which can overcome some of its shortcomings, like e.g. scaling dependency, or not relating to closed loop system properties. Inspired by the relative gain array approach, the HIIA is reformulated in the relative gain thinking by considering the effect of closing loops. The ratio of the Hankel norm of the subsystems in closed and open loop are used to state a modified version of HIIA, which has improved characteristics compared to the original HIIA. Properties of the modified HIIA are discussed and benchmarked with established methods on three example cases.}
}
@incollection{HEGARTY2010265,
title = {Chapter 7 - Components of Spatial Intelligence},
series = {Psychology of Learning and Motivation},
publisher = {Academic Press},
volume = {52},
pages = {265-297},
year = {2010},
booktitle = {The Psychology of Learning and Motivation},
issn = {0079-7421},
doi = {https://doi.org/10.1016/S0079-7421(10)52007-3},
url = {https://www.sciencedirect.com/science/article/pii/S0079742110520073},
author = {Mary Hegarty},
abstract = {This chapter identifies two basic components of spatial intelligence, based on analyses of performance on tests of spatial ability and on complex spatial thinking tasks in domains such as mechanics, chemistry, medicine, and meteorology. The first component is flexible strategy choice between mental imagery (or mental simulation more generally) and more analytic forms of thinking. Research reviewed here suggests that mental simulation is an important strategy in spatial thinking, but that it is augmented by more analytic strategies such as task decomposition and rule-based reasoning. The second is meta-representational competence [diSessa, A. A. (2004). Metarepresentation: Native competence and targets for instruction. Cognition and Instruction, 22, 293–331], which encompasses ability to choose the optimal external representation for a task and to use novel external representations productively. Research on this aspect of spatial intelligence reveals large individual differences in ability to adaptively choose and use external visual–spatial representations for a task. This research suggests that we should not just think of interactive external visualizations as ways of augmenting spatial intelligence, but also consider the types of intelligence that are required for their use.}
}
@article{FEMIMOL2025104019,
title = {A comprehensive review of blockchain with artificial intelligence integration for enhancing food safety and quality control},
journal = {Innovative Food Science & Emerging Technologies},
volume = {102},
pages = {104019},
year = {2025},
issn = {1466-8564},
doi = {https://doi.org/10.1016/j.ifset.2025.104019},
url = {https://www.sciencedirect.com/science/article/pii/S1466856425001031},
author = {R. Femimol and L. Nalini Joseph},
keywords = {Blockchain technology, Artificial intelligence, Food safety, Quality control, Supply chain traceability, Internet of things},
abstract = {Food safety and quality control are some of the biggest hurdles in today's global food supply chain, with complex networks, contamination risks, and limited transparency undermining efficiency. This review introduces a novel hybrid framework that uniquely integrates blockchain technology, Artificial Intelligence (AI), and the Internet of Things (IoT) to address these persistent issues. An analysis of 525 studies conducted between 2019 and has led to the selection of 70 papers that focus on the review of AI applications utilizing blockchain technology. The proposed model does not take the conventional route in that it solves critical issues regarding scalability, computational requirements, and integration complexities with IoT-based data collection and energy-efficient algorithms applied in supply chain logistic processes and operations. This method enhances the real-time processing of supply chain logistics while respecting privacy and conforming to standard practices. The model promotes unprecedented transparency, trust, and operational efficiency in food safety by integrating AI, blockchain, and IoT, thus generating new and scalable solutions to protect consumer interests and safety in logistics. This combination improves the trust of consumers also, enhances the safety of food and logistics, improving novel safety solutions for food.}
}
@article{CHONG2016257,
title = {A generalized cognitive hierarchy model of games},
journal = {Games and Economic Behavior},
volume = {99},
pages = {257-274},
year = {2016},
issn = {0899-8256},
doi = {https://doi.org/10.1016/j.geb.2016.08.007},
url = {https://www.sciencedirect.com/science/article/pii/S0899825616300847},
author = {Juin-Kuan Chong and Teck-Hua Ho and Colin Camerer},
keywords = {Cognitive hierarchy, Level- model, Level- model, Generalized cognitive hierarchy, Non-equilibrium structural models, Behavioral game theory},
abstract = {Subjects in simple games frequently exhibit non-equilibrium behaviors. Cognitive hierarchy (CH) and level k (LK) are two prevailing structural models that capture such behaviors well. This paper proposes a generalized CH (GCH) model that nests a variant of the LK model, called LM. GCH differs from CH in two ways. First, each lower level's actual frequency is exponentially weighted with α to form level-k's belief on relative proportions; α captures stereotype bias. CH assumes no stereotype bias (α=1) and LM assumes extreme bias (α=∞). Second, GCH replaces random choice with minimum aversion for level 0. Level 0s are more likely to choose strategies that never yield the minimum payoff for any of the opponent's strategies. GCH captures behaviors better than CH and LK in fifty-five n×m games from four datasets. Robustness tests using three new games further validate GCHs descriptive strength over CH and LK.}
}
@article{KAZEMI2002203,
title = {Exploring test performance in mathematics: the questions children’s answers raise},
journal = {The Journal of Mathematical Behavior},
volume = {21},
number = {2},
pages = {203-224},
year = {2002},
issn = {0732-3123},
doi = {https://doi.org/10.1016/S0732-3123(02)00118-9},
url = {https://www.sciencedirect.com/science/article/pii/S0732312302001189},
author = {Elham Kazemi},
keywords = {Children’s thinking, Mathematical performance, Interpreting problems, Testing},
abstract = {This article investigates children’s mathematical performance on test items, specifically multiple-choice questions. Using interviews with 90 fourth-graders, it reveals why particular kinds of items are more or less difficult for students. By using multiple-choice questions and juxtaposing them with similar open-ended problems, the findings underscore the costs of not attending to children’s thinking in designing and interpreting problems. The data from this study suggest that when answering multiple-choice questions, students’ attention is drawn to the choices themselves. They do not necessarily think through the problem first and thus make their choices based on (often incorrect) generalizations they have made about problem-solving. Whether students answered a multiple-choice question or a similar open-ended problem first impacted both their performance and their reasoning. Moreover, children draw on their life experiences when the context of the problem is salient, thus ignoring important parameters of the stated problem. Implications for investigating children’s thinking, instruction, and test design are discussed.}
}
@incollection{MAMATHA2024259,
title = {Chapter Eleven - Bio-intelligent computing and optimization techniques for developing computerized solutions},
editor = {Anupam Biswas and Alberto Paolo Tonda and Ripon Patgiri and Krishn Kumar Mishra},
series = {Advances in Computers},
publisher = {Elsevier},
volume = {135},
pages = {259-288},
year = {2024},
booktitle = {Applications of Nature-Inspired Computing and Optimization Techniques},
issn = {0065-2458},
doi = {https://doi.org/10.1016/bs.adcom.2023.11.006},
url = {https://www.sciencedirect.com/science/article/pii/S006524582300089X},
author = {G.S. Mamatha and Haripriya V. Joshi and R. Amith},
keywords = {Bio-intelligent, Bio-inspired, Computing, Optimization technique, Bio-engineering},
abstract = {Bio-inspired computing is a field of study that Lois lee knits together subfields related to the connectionism, social behavior and emergence. It is often closely related to the field of artificial intelligence as many of its pursuits can be linked to machine learning. It relies heavily on fields of biology, computer science and mathematics. Briefly it is the use of computers to model the living phenomena and simultaneously the study of life to improve the usage of computer. Biologically inspired computation is a major subset of natural computation areas of research. Some areas of study encompassed under the canon of biologically inspired computing and their biological counterparts are, genetic algorithms, evolution, biodegradability prediction, biodegradation, cellular automata, life emergent system ants, termites, bees, wasps, neural networks, artificial immune systems rendering patterning and animal skins, bird feathers, mollusk shells and bacterial colonies. Linder Mayer systems, plant structures, communication networks and protocol, epidemiology and the spared of disease, intra membrane molecular processes in living cells, excitable media forest fires the wave heart conditions axons and sensor networks sensory organs. Optimization techniques takes more bottom-up decentralized approach and often involves the methods of specifying a set of simple rules, a set of simple organisms which adhere to those rules and method of iteratively applying those rules for example, training virtual insect to investigate to an unknown terrain for finding food includes six simple rules which can be adopted. After several generations of rules application, it is usually the case where some forms of complex behavior get built upon complexity until the end results is something markedly complex and quite often completely counterintuitive from what the original rules would be expected to produce. For this reason, most technology-oriented solutions like neural network models, algorithms and other techniques came in to existence for accurate measurements and analysis that can be used to refine statistical inference and extrapolation as system complexity increases. The rules of nature inspired computing are the principle simple rules yet after being used for over millions of years have produced remarkably complex optimization techniques. All these techniques for developing software applications along with optimization techniques are discussed in the chapter.}
}
@incollection{DORFMAN1998395,
title = {Chapter 8 Problem solving, inhibition, and frontal lobe function},
editor = {Naftali Raz},
series = {Advances in Psychology},
publisher = {North-Holland},
volume = {125},
pages = {395-448},
year = {1998},
booktitle = {The Other Side of the Error Term},
issn = {0166-4115},
doi = {https://doi.org/10.1016/S0166-4115(98)80010-1},
url = {https://www.sciencedirect.com/science/article/pii/S0166411598800101},
author = {Jennifer Dorfman},
abstract = {Traditionally, cognitive models of problem solving have not incorporated inhibitory mechanisms, conceiving of human thinking as similar to the computations carried out by a serial computer (e.g., Newell & Simon, 1972). This chapter seeks to demonstrate the importance of inhibition in problem solving by examining subject populations with selective impairments of cognitive inhibition associated with frontal lobe pathology. Studies of three groups with putative frontal lobe dysfunction are reviewed: patients with focal lesions of the prefrontal cortex; schizophrenics; and the normal elderly. It is argued that the basic deficits observed in these groups reflect breakdowns in a supervisory attentional system that modulates problem-solving activity and that is subserved by the frontal cortex (Shallice, 1982). It is concluded that it is time to abandon the computer metaphor of human problem solving and adopt a brain metaphor.}
}
@incollection{OXMAN2001269,
title = {Chapter 12 - The Mind in Design: A Conceptual Framework for Cognition in Design Education},
editor = {Charles M. Eastman and W. Michael McCracken and Wendy C. Newstetter},
booktitle = {Design Knowing and Learning: Cognition in Design Education},
publisher = {Elsevier Science},
address = {Oxford},
pages = {269-295},
year = {2001},
isbn = {978-0-08-043868-9},
doi = {https://doi.org/10.1016/B978-008043868-9/50012-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780080438689500127},
author = {Rivka Oxman},
abstract = {Publisher Summary
This chapter considers the role of cognitive content of design and design thinking as a basis for developing an educational approach. Various design researchers discussed cognitive approaches in design, and the role of knowledge and representations as a cognitive design-thinking tool. Most of these studies are related directly to design and design thinking rather than to the learning task in design learning and design education. Irrespective of the specific design domain, traditional educational models in design education are based upon the replication of professional-task performance. The measure of learning is generally equated with the evaluation of the product of designing rather than on what might be considered a learning increment. The cognitive properties of design learning have never been the subject of design education. As a consequence, there presently exists a lack of educational theories of learning that function as an underpinning of design education. It is now possible to demonstrate that the derivation of design knowledge through constructive processes, in itself, provides a medium for design learning. This chapter suggests that special design learning environments must be developed to enhance and supplement formal education and foster personal development in design learning.}
}
@article{DEAN2020482,
title = {Deep into that darkness peering: A computational analysis of the role of depression in Edgar Allan Poe's life and death},
journal = {Journal of Affective Disorders},
volume = {266},
pages = {482-491},
year = {2020},
issn = {0165-0327},
doi = {https://doi.org/10.1016/j.jad.2020.01.098},
url = {https://www.sciencedirect.com/science/article/pii/S0165032719322554},
author = {Hannah J. Dean and Ryan L. Boyd},
keywords = {Edgar Allan Poe, LIWC, Depression, Suicide, Digital humanities},
abstract = {Background
To help shed light on the peculiar circumstances surrounding the death of the famed macabre and mystery writer, poet, editor, and literary critic, we explored the potential role of depression in the life and death of Edgar Allan Poe via his written language.
Method
Using computerized language analysis, we analyzed works from Poe's corpora of personal letters (N = 309), poems (N = 49), and short stories (N = 63), and investigated whether a pattern of linguistic cues consistent with depression and suicidal cognition were discernible throughout the writer's life, particularly in his final years. Building on past work, language scores were collapsed into a composite depression metric for each text. Data from each work type was subsequently compiled and graphed into a single plot by year, with scores exceeding the 95th percentile (p < 0.05) considered statistically significant and treated as potential depressive episodes.
Results
Significant, consistent patterns of depression were not found and do not support suicide as a cause of death. However, linguistic evidence was found suggesting the presence of several potential depressive episodes over the course of Poe's life – these episodes were the most pronounced during years of Poe's greatest success, as well as those following the death of his late wife.
Limitations
Given the sampling method, it is not possible to establish direct causality; results should be considered informed but tentative.
Conclusion
This investigation demonstrates the utility of language analysis for capturing disruptive/maladaptive emotional responses to life events.}
}