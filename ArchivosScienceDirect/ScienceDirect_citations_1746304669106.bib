@article{PFOTENHAUER201638,
title = {Architecting complex international science, technology and innovation partnerships (CISTIPs): A study of four global MIT collaborations},
journal = {Technological Forecasting and Social Change},
volume = {104},
pages = {38-56},
year = {2016},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2015.12.006},
url = {https://www.sciencedirect.com/science/article/pii/S0040162515004102},
author = {Sebastian M. Pfotenhauer and Danielle Wood and Dan Roos and Dava Newman},
keywords = {Innovation policy, Research collaboration, Regional development, University partnerships, System architecture, Policy design, MIT, International partnerships},
abstract = {Complex international partnerships have emerged as a policy instrument of choice for many governments to build domestic capacity in science, technology and innovation with the help of foreign partners. At present, these flagship initiatives tend to be primarily practitioner-driven with limited systematic understanding of available design options and trade-offs. Here, we present an analysis of four such partnerships from the university sector between the Massachusetts Institute of Technology (MIT) and governments in the UK, Portugal, Abu Dhabi, and Singapore. Using a system architecture approach in conjunctions with in-depth case studies and elements of interpretive policy analysis, we map how in each country distinct capacity-building goals, activities, and political and institutional contexts translate into different partnership architectures: a bilateral hub-&-spokes architecture (UK), a consortium architecture (Portugal), an institution-building architecture (Abu Dhabi), and a functional expansion architecture (Singapore). Despite these differences in emergent macro-architectures, we show that each partnership draws on an identical, limited set of ‘forms’ that can by organized around four architectural views (education, research, innovation & entrepreneurship, institution-building) and four levels of interaction between partners (people, programs/projects, objects, organization/process). Based on our analysis, we derive a design matrix that can help guide the development future partnerships through a systematic understanding of available design choices. Our research underscores the utility and flexibility of complex international partnerships as systemic policy instruments. It suggests a greater role for global research universities in capacity-building and international development, and emphasizes the potential of targeted cross-border funding. Our research also demonstrates the analytic power of system architecture for policy analysis and design. We argue that architectural thinking provides a useful stepping stone for STS-type interpretive policy analysis into national innovation initiatives in different political cultures, as well as more custom-tailored approaches to program evaluation.}
}
@article{NOROOZI201279,
title = {Argumentation-Based Computer Supported Collaborative Learning (ABCSCL): A synthesis of 15 years of research},
journal = {Educational Research Review},
volume = {7},
number = {2},
pages = {79-106},
year = {2012},
issn = {1747-938X},
doi = {https://doi.org/10.1016/j.edurev.2011.11.006},
url = {https://www.sciencedirect.com/science/article/pii/S1747938X11000522},
author = {Omid Noroozi and Armin Weinberger and Harm J.A. Biemans and Martin Mulder and Mohammad Chizari},
keywords = {Argumentation, Argumentative knowledge construction, Collaborative argumentation, Computer-Supported Collaborative Learning, Argumentation-Based Computer Supported Collaborative Learning},
abstract = {Learning to argue is an essential objective in education; and online environments have been found to support the sharing, constructing, and representing of arguments in multiple formats for what has been termed Argumentation-Based Computer Supported Collaborative Learning (ABCSCL). The purpose of this review is to give an overview of research in the field of ABCSCL and to synthesize the findings. For this review, 108 publications (89 empirical studies and 19 conceptual papers) on ABCSCL research dating from 1995 through 2011 were studied to highlight the foci of the past 15 years. Building on Biggs’ (2003) model, the ABCSCL publications were systematically categorized with respect to student prerequisites, learning environment, processes, and outcomes. Based on the quantitative and qualitative findings, this paper concludes that ABCSCL environments should be designed in a systematic way that takes the variety of specific conditions for learning into account. It also offers suggestions for educational practice and future research.}
}
@incollection{TAIEF2025582,
title = {1.44 - The Application of Machine Learning for Green Hydrogen Production},
editor = {Professor Abdul Ghani Olabi},
booktitle = {Comprehensive Green Materials (First Edition)},
publisher = {Elsevier},
edition = {First Edition},
address = {Oxford},
pages = {582-593},
year = {2025},
isbn = {978-0-443-15739-4},
doi = {https://doi.org/10.1016/B978-0-443-15738-7.00030-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780443157387000301},
author = {Wafa Taief and Amani Al-Othman and Muhammad Tawalbeh},
keywords = {Green hydrogen, Hydrogen production, Machine learning, Optimization algorithms., Water electrolysis},
abstract = {It is no secret that the amount of pollution that fossil fuels cause in the environment when burned. When searching for alternative solutions, it is noted that hydrogen is an attractive option because of its many advantages that are included in this work. Hydrogen can be produced by different methods. In this work, there is a comprehensive review of the hydrogen production methods divided according to the sources, either renewable or nonrenewable, and the type of energy used based on both single and combined forms. In this work, there is a general view of the machine learning algorithms and models and the categories that they belong to, as well as their application in green hydrogen production. This work is very useful for the readers to organize their ideas about hydrogen production methods and machine learning and its applications to produce green hydrogen, so after reading this work, they can know their specific interests and search for details about it.}
}
@article{DELIMA2023100590,
title = {Managing the plot structure of character-based interactive narratives in games},
journal = {Entertainment Computing},
volume = {47},
pages = {100590},
year = {2023},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2023.100590},
url = {https://www.sciencedirect.com/science/article/pii/S1875952123000459},
author = {Edirlei Soares {de Lima} and Bruno Feijó and Antonio L. Furtado},
keywords = {Interactive storytelling, Narrative generation, Drama management, Plot structure, Automated planning},
abstract = {The use of narrative generation methods in games is a complex challenge that involves multiple problems of plot-based processes integrated with character-based methods. Examples of these problems are the high computational complexity of many story generation algorithms, the difficulties associated with the generation of interactive narratives that are compelling and emotionally impactful, the complex interactions among characters, and the need for tools and methods to support story writers in the process of creating and managing the narrative structure of interactive stories. In this work, we present and evaluate a new approach to generate and manage the plot structure of character-based interactive narratives in games, which combines multi-agent planning with a drama management strategy based on narrative structures. The proposed method is supported by an authoring tool that allows authors to create and test interactive narratives using graphical interfaces and intuitive diagrams. The results of our study suggest the effectiveness of our approach in generating interactive narratives for highly interactive game environments. In addition, a user study of the proposed authoring tool indicates that it can successfully support the development of character-based interactive narratives without requiring programming knowledge.}
}
@article{LATIF2023726,
title = {Design and Development a Virtual Planetarium Learning Media Using Augmented Reality},
journal = {Procedia Computer Science},
volume = {227},
pages = {726-733},
year = {2023},
note = {8th International Conference on Computer Science and Computational Intelligence (ICCSCI 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.10.577},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923017441},
author = {Jazlyn Jan Keyla Latif and Augustinus Adrian Triputra and Michael Awarsa Kesuma and Fairuz Iqbal Maulana},
keywords = {Augmented Reality, Planetarium, Virtual, Virtual Planetarium, Application},
abstract = {The solar system has always been a mystery to many. If not for the advanced technologies, most humans would not have the opportunity to gain knowledge about the planets. Although Earth is a part of the solar system, the solar system is simply too dangerous and expensive for humans to explore casually. Humans do not interact with the Sun or planets actively. This is especially concerning for children who often require visual aids in studying. An Augmented Reality (AR) based application can solve that problem. Through Virtual Planetarium, children may interact with the Sun or the planets and gain information. This will help aid children's guardians in studying the solar system. The application is made by Systems Development Life Cycle (SDLC) method. Through the making of this application, it is expected that children will have a better understanding of the solar system.}
}
@article{BURGESS2011427,
title = {On system rollback and totalized fields: An algebraic approach to system change},
journal = {The Journal of Logic and Algebraic Programming},
volume = {80},
number = {8},
pages = {427-443},
year = {2011},
issn = {1567-8326},
doi = {https://doi.org/10.1016/j.jlap.2011.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S1567832611000488},
author = {Mark Burgess and Alva Couch},
abstract = {In system operations the term rollback is often used to imply that arbitrary changes can be reversed i.e. ‘rolled back’ from an erroneous state to a previously known acceptable state. We show that this assumption is flawed and discuss error-correction schemes based on absolute rather than relative change. Insight may be gained by relating change management to the theory of computation. To this end, we reformulate previously-defined ‘convergent change operators’ of Burgess into the language of groups and rings. We show that, in this form, the problem of rollback from a convergent operation becomes equivalent to that of ‘division by zero’ in computation. Hence, we discuss how recent work by Bergstra and Tucker on zero-totalized fields helps to clear up long-standing confusion about the options for ‘rollback’ in change management.}
}
@article{FERNANDES202191,
title = {Pruning of generative adversarial neural networks for medical imaging diagnostics with evolution strategy},
journal = {Information Sciences},
volume = {558},
pages = {91-102},
year = {2021},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2020.12.086},
url = {https://www.sciencedirect.com/science/article/pii/S0020025521000189},
author = {Francisco Erivaldo Fernandes and Gary G. Yen},
keywords = {Deep Neural Networks, Convolutional Neural Networks, Generative Adversarial Networks, Medical Imaging Diagnostics, Evolution Strategy, Pruning},
abstract = {Deep Convolutional Neural Networks (DCNNs) have the potential to revolutionize the field of Medical Imaging Diagnostics due to their capabilities of learning by using only raw data. However, DCNNs can only learn when trained using thousands of data points, which is not always available when dealing with medical data. Moreover, due to patient privacy concerns and the small prevalence of certain diseases in the population, medical data often presents unbalanced classes and fewer data points than other data types. Researchers often rely on Generative Adversarial Networks (GANs) to synthesize more data from a given distribution to solve this problem. Nevertheless, GANs are computationally intensive models requiring the use of powerful hardware to run. In the present work, an algorithm for pruning GANs based on Evolution Strategy (ES) and Multi-Criteria Decision Making (MCDM) is proposed in which a model with the best trade-off between computational complexity and synthesis performance can be found without the use of any trade-off parameter. In the proposed algorithm, the model with the best trade-off is defined geometrically as the candidate solution with the minimum Manhattan distance (MMD) in a two-dimensional objective space established by the number of Floating-Point Operations (FLOPs) and the Wasserstein distance of all candidate solutions, also known as the knee solution. The results show that the pruned GAN model achieves similar performance compared with the original model with up to 70% fewer Floating-Point Operations.}
}
@article{KAPETANIOU2025106115,
title = {Beyond impulse control – toward a comprehensive neural account of future-oriented decision making},
journal = {Neuroscience & Biobehavioral Reviews},
volume = {172},
pages = {106115},
year = {2025},
issn = {0149-7634},
doi = {https://doi.org/10.1016/j.neubiorev.2025.106115},
url = {https://www.sciencedirect.com/science/article/pii/S0149763425001150},
author = {Georgia E. Kapetaniou and Alexander Soutschek},
keywords = {Self-control, Delay discounting, Dorsolateral prefrontal cortex, Construal theory, Prospection, Metacognition},
abstract = {The dominant focus of current neural models of future-oriented decision making is on the interplay between the brain’s reward system and a frontoparietal network thought to implement impulse control. Here, we propose a re-interpretation of the contribution of frontoparietal activation to future-oriented behavior and argue that future-oriented decisions are influenced by a variety of psychological mechanisms implemented by dissociable brain mechanisms. We review the literature on the neural mechanisms underlying the influence of prospection, retrospection, framing, metacognition, and automatization on future-oriented decisions. We propose that the prefrontal cortex contributes to future-oriented decisions not by exerting impulse control but by constructing and updating the value of abstract future rewards. These prefrontal value representations interact with regions involved in reward processing (neural reward system), prospection (hippocampus, temporal cortex), metacognition (frontopolar cortex), and habitual behavior (dorsal striatum). The proposed account of the brain mechanisms underlying future-oriented decisions has several implications for both basic and clinical research: First, by reconciling the idea of frontoparietal control processes with construal accounts of intertemporal choice, we offer an alternative interpretation of the canonical prefrontal activation during future-oriented decisions. Second, we highlight the need for obtaining a better understanding of the neural mechanisms underlying future-oriented decisions beyond impulse control and of their contribution to myopic decisions in clinical disorders. Such a widened focus may, third, stimulate the development of novel neural interventions for the treatment of pathological impulsive decision making.}
}
@article{TURNER2025100843,
title = {Meet the author: Tychele N. Turner, PhD},
journal = {Cell Genomics},
volume = {5},
number = {4},
pages = {100843},
year = {2025},
issn = {2666-979X},
doi = {https://doi.org/10.1016/j.xgen.2025.100843},
url = {https://www.sciencedirect.com/science/article/pii/S2666979X25000990},
author = {Tychele N. Turner},
abstract = {Dr. Tychele Turner is an assistant professor of genetics at the Washington University in St. Louis. This issue of Cell Genomics presents research from her lab in “Proteome-wide assessment of differential missense variant clustering in neurodevelopmental disorders and cancer” by Ng et al. This paper used their newly developed program, 3D-CLUMP, which can perform proteome-wide significant case-control analysis and clustering of protein-coding variants related to disease. Using 3D-CLUMP, they examine how patients with mostly cancer or neurodevelopmental disorders (NDDs) are observed to have the same gene, even if the specific mutations causing them are not shared.}
}
@article{HIEBERT1992439,
title = {Chapter 3 Reflection and communication: Cognitive considerations in school mathematics reform},
journal = {International Journal of Educational Research},
volume = {17},
number = {5},
pages = {439-456},
year = {1992},
issn = {0883-0355},
doi = {https://doi.org/10.1016/S0883-0355(05)80004-7},
url = {https://www.sciencedirect.com/science/article/pii/S0883035505800047},
author = {James Hiebert},
abstract = {The mathematics education reform efforts in the United States are shaped partially by our understanding of how students learn mathematics. Two traditions in psychology influence our current thinking most forcefully — cognitive psychology with its emphasis on individual mental operations and social cognition with its emphasis on context and group interaction. Reflection and communication, as cognitive processes and as representatives of these respective traditions, are used to establish the cognitive-based rationale for the reform and to analyze the nature of recommended changes. Issues addressed include the interdependence of reflection and communication and the way in which these processes can be used to analyze aspects of the school mathematics program, such as the way textbooks ordinarily treat written symbols. Although the theoretical arguments for reflection and communication are being increasingly well-articulated, the empirical data that address the claims are comparatively sparse. Future research efforts should aim to test theoretical claims for reflection and communication and to increase our understanding of the relationships between these cognitive processes and learning mathematics.}
}
@article{PAI201872,
title = {Assessing mobile health applications with twitter analytics},
journal = {International Journal of Medical Informatics},
volume = {113},
pages = {72-84},
year = {2018},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2018.02.016},
url = {https://www.sciencedirect.com/science/article/pii/S1386505618301199},
author = {Rajesh R. Pai and Sreejith Alathur},
keywords = {Mobile health, Sentiment analysis, Twitter analytics, Causal loop diagram, Technology adoption model},
abstract = {Introduction
Advancement in the field of information technology and rise in the use of Internet has changed the lives of people by enabling various services online. In recent times, healthcare sector which faces its service delivery challenges started promoting and using mobile health applications with the intention of cutting down the cost making it accessible and affordable to the people.
Objectives
The objective of the study is to perform sentiment analysis using the Twitter data which measures the perception and use of various mobile health applications among the citizens.
Methods
The methodology followed in this research is qualitative with the data extracted from a social networking site “Twitter” through a tool RStudio. This tool with the help of Twitter Application Programming Interface requested one thousand tweets each for four different phrases of mobile health applications (apps) such as “fitness app”, “diabetes app”, “meditation app”, and “cancer app”. Depending on the tweets, sentiment analysis was carried out, and its polarity and emotions were measured.
Results
Except for cancer app there exists a positive polarity towards the fitness, diabetes, and meditation apps among the users. Following a system thinking approach for our results, this paper also explains the causal relationships between the accessibility and acceptability of mobile health applications which helps the healthcare facility and the application developers in understanding and analyzing the dynamics involved the adopting a new system or modifying an existing one.}
}
@article{SCHIFFMAN20041079,
title = {Mainstream economics, heterodoxy and academic exclusion: a review essay},
journal = {European Journal of Political Economy},
volume = {20},
number = {4},
pages = {1079-1095},
year = {2004},
issn = {0176-2680},
doi = {https://doi.org/10.1016/j.ejpoleco.2004.06.003},
url = {https://www.sciencedirect.com/science/article/pii/S0176268004000643},
author = {Daniel A. Schiffman},
keywords = {Academic exclusion, Pluralism, Economics education, Historical specificity, Heterodoxy},
abstract = {Does the mainstream of economic thinking and analysis tend systematically to exclude ideas and approaches that could enrich the field, and, as a consequence, have important questions and issues been shunted aside for nonobjective reasons? Two recent volumes by heterodox economists that address these questions are Geoffrey Hodgson's How Economics Forgot History: The Problem of Historical Specificity in Social Science, and Steve Keen's Debunking Economics: The Naked Emperor of the Social Sciences. I evaluate their claims of academic exclusion and assess the current state of (selective) pluralism within mainstream economics.}
}
@article{WU2023101906,
title = {Human–machine hybrid intelligence for the generation of car frontal forms},
journal = {Advanced Engineering Informatics},
volume = {55},
pages = {101906},
year = {2023},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2023.101906},
url = {https://www.sciencedirect.com/science/article/pii/S1474034623000344},
author = {Yu Wu and Lisha Ma and Xiaofang Yuan and Qingnan Li},
keywords = {Car frontal form, Creative generation, Human–machine hybrid intelligence, Human–machine shared knowledge base, generative adversarial network (GAN)},
abstract = {With the acceleration of the upgrading of the automobile consumption market, artificial intelligence has become an increasingly effective means of enhancing the creative design of automobile appearance modeling. However, when artificial intelligence processes specific design tasks, creativity is primarily based on data drive, resulting in machine-generated design schemes that do not match human-specific psychological intentions. Due to the absence of design knowledge in the process of machine design, there is a data gap between human cognitive thought and machine information processing. This paper aims to structure the human's complex cognitive knowledge of car frontal form, establish the consistency between human and machine cognitive structures, and reduce communication barriers in the process of human–machine hybrid creative design. To achieve this objective, a human–machine hybrid intelligence methodology – a combination of human cognitive mental model, human–machine shared knowledge base, and Generative Adversarial Networks (GAN) – was developed to generate a large number of car frontal forms that are consistent with the design intent. First, we constructeda mental model of human cognition based on three dimensions: design intent, drawing behavior, and functional structure. Second, we created a shared human–machine knowledge base with design Knowledge. This knowledge base contains 12,560 images of car frontal form designs with corresponding morphological semantic labels and 3,140 sketches of car frontal forms drawn by hand. Human–machine shared knowledge base datawasutilized in a machine learning training network. In addition, a conditional cross-domain generative adversarial network was developed to investigate the implicit relationship between sketch characteristics, morphological semantics, and image visual effects. Using the suggested method, a large number of images with the specified morphological semantic category and resembling the hand-drawn sketch of a car frontal form can be generated rapidly. In terms of the quality of car frontal form generation, our research is superior to the baseline model according to qualitative and quantitative assessments. In comparison to the designer's output, the human–machine hybrid intelligent generation also demonstrates excellent creative performance.}
}
@article{BAUERNEGRINI2022105785,
title = {Usability evaluation of circRNA identification tools: Development of a heuristic-based framework and analysis},
journal = {Computers in Biology and Medicine},
volume = {147},
pages = {105785},
year = {2022},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2022.105785},
url = {https://www.sciencedirect.com/science/article/pii/S0010482522005522},
author = {Guilherme Bauer-Negrini and Guilherme {Cordenonsi da Fonseca} and Carmem Gottfried and Juliana Herbert},
keywords = {Usability, Bioinformatics, circRNA, Heuristic evaluation, Command-line interface},
abstract = {Background and objective
Circular RNAs (circRNAs) are endogenous molecules of non-coding RNA that form a covalently closed loop at the 3′ and 5′ ends. Recently, the role of these molecules in the regulation of gene expression and their involvement in several human pathologies has gained notoriety. The identification of circRNAs is highly dependent on computational methods for analyzing RNA sequencing data. However, bioinformatics software is known to be problematic in terms of usability. Evidence points out that tools for identifying circRNAs can have such problems, negatively impacting researchers in this field. Here we present a heuristic-based framework for evaluating the usability of command-line circRNA identification software.
Methods
We used heuristics evaluation to comprehensively identify the usability issues in a sample of circRNA identification tools.
Results
We identified 46 usability issues presented individually in four tools. Most of the issues had cosmetic or minor severity. These are unlikely to challenge experienced users but may cause inconvenience for novice users. We also identified severe issues with the potential to harm users regardless of their experience. The areas most affected were the documentation and the installability of the tools.
Conclusions
With the proposed framework, we formally describe, for the first time, the usability problems that can affect users in this area of circRNA research. We hope that our framework can help researchers evaluate their software's usability during development.}
}
@article{SPROULE2002412,
title = {Fuzzy pharmacology: theory and applications},
journal = {Trends in Pharmacological Sciences},
volume = {23},
number = {9},
pages = {412-417},
year = {2002},
issn = {0165-6147},
doi = {https://doi.org/10.1016/S0165-6147(02)02055-2},
url = {https://www.sciencedirect.com/science/article/pii/S0165614702020552},
author = {Beth A. Sproule and Claudio A. Naranjo and I.Burhan Türksen},
keywords = {fuzzy logic, pharmacodynamics, fuzzy sets, modelling, predictions, pharmacokinetics},
abstract = {Fuzzy pharmacology is a term coined to represent the application of fuzzy logic and fuzzy set theory to pharmacological problems. Fuzzy logic is the science of reasoning, thinking and inference that recognizes and uses the real world phenomenon that everything is a matter of degree. It is an extension of binary logic that is able to deal with complex systems because it does not require crisp definitions and distinctions for the system components. In pharmacology, fuzzy modeling has been used for the mechanical control of drug delivery in surgical settings, and work has begun evaluating its use in other pharmacokinetic and pharmacodynamic applications. Fuzzy pharmacology is an emerging field that, based on these initial explorations, warrants further investigation.}
}
@article{TILLS2025111783,
title = {Bioimaging and the future of whole-organismal developmental physiology},
journal = {Comparative Biochemistry and Physiology Part A: Molecular & Integrative Physiology},
volume = {300},
pages = {111783},
year = {2025},
issn = {1095-6433},
doi = {https://doi.org/10.1016/j.cbpa.2024.111783},
url = {https://www.sciencedirect.com/science/article/pii/S1095643324002101},
author = {Oliver Tills and Ziad Ibbini and John I. Spicer},
keywords = {Imaging, Phenomics, Deep learning, Embryonic development, Developmental physiology, Comparative developmental physiology, Computer vision},
abstract = {While omics has transformed the study of biology, concomitant advances made at the level of the whole organism, i.e. the phenome, have arguably not kept pace with lower levels of biological organisation. In this personal commentary we evaluate the importance of imaging as a means of measuring whole organismal developmental physiology. Image acquisition, while an important process itself, has become secondary to image analysis as a bottleneck to the use of imaging in research. Here, we explore the significant potential for increasingly sophisticated approaches to image analysis, including deep learning, to advance our understanding of how developing animals grow and function. Furthermore, unlike many species-specific methodologies, tools and technologies, we explore how computer vision has the potential to be transferable between species, life stages, experiments and even taxa in which embryonic development can be imaged. We identify what we consider are six of the key challenges and opportunities in the application of computer vision to developmental physiology carried out in our lab, and more generally. We reflect on the tangibility of transferrable computer vision models capable of measuring the integrative physiology of a broad range of developing organisms, and thereby driving the adoption of phenomics for developmental physiology. We are at an exciting time of witnessing the move from computer vision as a replacement for manual observation, or manual image analysis, to it enabling a fundamentally more powerful approach to exploring and understanding the complex biology of developing organisms, the quantification of which has long posed a challenge to researchers.}
}
@article{VEKONY2025111703,
title = {Mind wandering enhances statistical learning},
journal = {iScience},
volume = {28},
number = {2},
pages = {111703},
year = {2025},
issn = {2589-0042},
doi = {https://doi.org/10.1016/j.isci.2024.111703},
url = {https://www.sciencedirect.com/science/article/pii/S2589004224029304},
author = {Teodóra Vékony and Bence C. Farkas and Bianka Brezóczki and Matthias Mittner and Gábor Csifcsák and Péter Simor and Dezső Németh},
keywords = {Psychology},
abstract = {Summary
The human brain spends 30–50% of its waking hours engaged in mind-wandering (MW), a common phenomenon in which individuals either spontaneously or deliberately shift their attention away from external tasks to task-unrelated internal thoughts. Despite the significant amount of time dedicated to MW, its underlying reasons remain unexplained. Our pre-registered study investigates the potential adaptive aspects of MW, particularly its role in predictive processes measured by statistical learning. We simultaneously assessed visuomotor task performance as well as the capability to extract probabilistic information from the environment while assessing task focus (on-task vs. MW). We found that MW was associated with enhanced extraction of hidden, but predictable patterns. This finding suggests that MW may have functional relevance in human cognition by shaping behavior and predictive processes. Overall, our results highlight the importance of considering the adaptive aspects of MW, and its potential to enhance certain fundamental cognitive abilities.}
}
@article{ROLAND2002183,
title = {Dynamic depolarization fields in the cerebral cortex},
journal = {Trends in Neurosciences},
volume = {25},
number = {4},
pages = {183-190},
year = {2002},
issn = {0166-2236},
doi = {https://doi.org/10.1016/S0166-2236(00)02125-1},
url = {https://www.sciencedirect.com/science/article/pii/S0166223600021251},
author = {Per E. Roland},
keywords = {general computational elements, voltage sensitive dyes, cortical dynamics, layer II-III neurons, memory, cognition},
abstract = {Recent physiological evidence shows that in response to stimuli and preceding motor activity, large fields of the upper layers of the cerebral cortex depolarize. It is argued that this finding is a general one and that these dynamic depolarization fields represent the computational elements of the cerebral cortex. Each depolarization field engages many more neurons than do columns and hyper-columns. These fields can be explained by cooperative neuronal computing in layers I–III of the cortex. In these layers, the computing modes might be general for all parts of the cerebral cortex and be sufficiently flexible to handle all sorts of cortical computations, including perception, memory storage, memory retrieval, thought and the production of behavior.}
}
@article{ARIFOVIC20071971,
title = {Call market book information and efficiency},
journal = {Journal of Economic Dynamics and Control},
volume = {31},
number = {6},
pages = {1971-2000},
year = {2007},
note = {Tenth Workshop on Economic Heterogeneous Interacting Agents},
issn = {0165-1889},
doi = {https://doi.org/10.1016/j.jedc.2007.01.006},
url = {https://www.sciencedirect.com/science/article/pii/S0165188907000073},
author = {Jasmina Arifovic and John Ledyard},
keywords = {Computer testbeds, Call markets, Learning, Experiments with human subjects, Closed book, Market design},
abstract = {What are the consequences of making bids and offers in the book available to traders in a call market? This is a problem in market design. We employ a computational mechanism design methodology to attack this problem and find that allocative efficiencies are higher in a closed book design. We validate our computational approach by running a series of tests with human subjects in exactly the same environments.}
}
@article{ELZANFALY201579,
title = {[I3] Imitation, Iteration and Improvisation: Embodied interaction in making and learning},
journal = {Design Studies},
volume = {41},
pages = {79-109},
year = {2015},
note = {Special Issue: Computational Making},
issn = {0142-694X},
doi = {https://doi.org/10.1016/j.destud.2015.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X1500071X},
author = {Dina El-Zanfaly},
keywords = {computational making, design education, design technology, interaction design, reflective practice},
abstract = {I introduce in this paper a new learning and making process that fosters a new ability to make things through the body's direct, iterative engagement with materials, tools, machines and objects. Tested in a variety of educational settings, this method, which I call ‘I3’ for its three-layer operation of ‘Imitation, Iteration and Improvisation’, allows learners to develop their sensory experiences to improvise and create on their own. I introduce case studies in order to test I3. I challenge the separation of design and construction often reinforced by the use of digital fabrication. I show that learning to make and learning from making emerge together through a situated and embodied interaction among the learner, the materials, the tools and the object in-the-making.}
}
@article{FIORELLI2025150,
title = {Digital models and 3D biomechanics analysis in orthodontics. Part 1: Vector calculations},
journal = {Seminars in Orthodontics},
volume = {31},
number = {1},
pages = {150-157},
year = {2025},
issn = {1073-8746},
doi = {https://doi.org/10.1053/j.sodo.2024.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S1073874624001348},
author = {Giorgio Fiorelli},
keywords = {Biomechanics, Vectors, 3D Force System},
abstract = {Biomechanics is essential for optimizing orthodontic appliances and controlling dental movement. Charles J. Burstone pioneered a three-dimensional (3D) approach in orthodontics, advocating for a shift beyond appliance-focused methods. Initially, biomechanics studies were constrained to two-dimensional (2D) analysis due to the complexities of 3D evaluation. Despite progress in computational tools and digital modeling, orthodontic biomechanics has largely maintained a 2D orientation. This paper advances orthodontic biomechanics into 3D, re-evaluating concepts previously limited to 2D frameworks. A dedicated software, DDP-Ortho (Ortolab, Poland), is introduced to enable orthodontists to analyze and resolve biomechanical challenges in 3D, facilitating appliance designs with precise 3D force systems. The representation and calculation of force vectors and moments in 3D are detailed, emphasizing the inherent complexity absent computational support. Key processes such as vector subtraction and addition, fundamental for assessing and refining orthodontic force systems, are explained. Additionally, the vector split (couple replacement) method, previously described in 2D, is extended to 3D, addressing the unique constraints and challenges of this approach. These tools promise to refine the accuracy and effectiveness of orthodontic treatments, setting the stage to examine the interactions between 3D force systems and dental movement, which will be addressed in a subsequent paper, to broaden the potential of contemporary orthodontic therapy.}
}
@article{PALHARESDEMELO200121,
title = {Recommendation for fertilizer application for soils via qualitative reasoning},
journal = {Agricultural Systems},
volume = {67},
number = {1},
pages = {21-30},
year = {2001},
issn = {0308-521X},
doi = {https://doi.org/10.1016/S0308-521X(00)00044-5},
url = {https://www.sciencedirect.com/science/article/pii/S0308521X00000445},
author = {L.A.M. {Palhares de Melo} and D.J. Bertioli and E.V.M. Cajueiro and R.C. Bastos},
keywords = {Fertilizer application, Qualitative reasoning, Approximate reasoning},
abstract = {In Brazil, liming and fertilization are essential practices in agriculture due to the soils being acidic and poor in nutrients. Distinct decision tables that serve as support for recommendation of fertilizer application are used, but one of their features is that they are based on a strictly quantitative analysis of input variables. This sometimes causes dificulties in their use when calculating the output (recommended fertilizer application). This work presents a model for the use and interpretation of decision tables for fertilizer application. It is based on a qualitative characterization for the rules and input variables used. The results have shown that this approach gives feasible results which more accurately reflect human thinking about the decision table.}
}
@article{ZHENG2025,
title = {Machine Memory Intelligence: Inspired by Human Memory Mechanisms},
journal = {Engineering},
year = {2025},
issn = {2095-8099},
doi = {https://doi.org/10.1016/j.eng.2025.01.012},
url = {https://www.sciencedirect.com/science/article/pii/S2095809925000293},
author = {Qinghua Zheng and Huan Liu and Xiaoqing Zhang and Caixia Yan and Xiangyong Cao and Tieliang Gong and Yong-Jin Liu and Bin Shi and Zhen Peng and Xiaocen Fan and Ying Cai and Jun Liu},
keywords = {Machine memory intelligence, Neural mechanism, Associative representation, Continual learning, Collaborative reasoning},
abstract = {Large models, exemplified by ChatGPT, have reached the pinnacle of contemporary artificial intelligence (AI). However, they are plagued by three inherent drawbacks: excessive training data and computing power consumption, susceptibility to catastrophic forgetting, and a deficiency in logical reasoning capabilities within black-box models. To address these challenges, we draw insights from human memory mechanisms to introduce “machine memory,” which we define as a storage structure formed by encoding external information into a machine-representable and computable format. Centered on machine memory, we propose the brand-new machine memory intelligence (M2I) framework, which encompasses representation, learning, and reasoning modules and loops. We explore the key issues and recent advances in the four core aspects of M2I, including neural mechanisms, associative representation, continual learning, and collaborative reasoning within machine memory. M2I aims to liberate machine intelligence from the confines of data-centric neural networks and fundamentally break through the limitations of existing large models, driving a qualitative leap from weak to strong AI.}
}
@article{FORTINI2023107058,
title = {An experimental and numerical study of the solid particle erosion damage in an industrial cement large-sized fan},
journal = {Engineering Failure Analysis},
volume = {146},
pages = {107058},
year = {2023},
issn = {1350-6307},
doi = {https://doi.org/10.1016/j.engfailanal.2023.107058},
url = {https://www.sciencedirect.com/science/article/pii/S1350630723000122},
author = {Annalisa Fortini and Alessio Suman and Nicola Zanini},
keywords = {Wear damage, Hardfacing, Centrifugal fan, Computational fluid dynamics, Solid particle erosion, Metallographic analysis},
abstract = {The present paper addresses the wear failure analysis of a large-sized centrifugal fan operating in a cement clinker grinding plant. Within cement production, the calcination at middle and high-temperature values (from 120 °C to 400 °C depending on the process parameters) of the raw material requires such a process fan, which also ensures the draft and feed of the flue gases and combustion air needed for the operation of the main equipment of the cement factory. To detect and analyze the impact conditions within the heavy-duty fan, Computational Fluid Dynamics (CFD) analyses were performed. The analysis of the numerical results shows that the relevant fan surfaces are affected by different impact velocities and angles, generating non-uniform erosion patterns similar to the on-field detections. Besides, the obtained comprehensive description of the flow and contaminants behaviors through the entire flow path enables setting up the subsequent experimental investigation. The erosive wear behavior of a Fe-Cr-C hardfacing cast iron and wear-resistant steel was tested through a test rig constructed for the purpose of being in accordance with the ASTM G76 standard. The test bench was adapted to manage the raw meal powder used in the cement factory to reproduce the actual operating conditions. The results show a greater capability of Fe-Cr-C hardfacing cast iron to face the erosion phenomenon in terms of lower values of material loss over the exposure time. These findings, coupled with the metallographic analysis to detect the erosion mechanisms (ductile and/or brittle), help a better prediction of the fan operating life. The investigation showed the reliability of the numerical/experimental coupled approach in assessing the actual erosion magnitude and the influence of the impact angle on the erosion phenomena. This coupled approach gains a further understanding of the proper design of manufacturing and maintenance activities, covering several project steps from material selections to the scheduled and overhaul operations. A reliable operating-life prediction allows manufacturers and operators to obtain production and economic goals.}
}
@article{RAN2024102578,
title = {Spatiotemporal characteristics and influencing factors of airport service quality in China},
journal = {Journal of Air Transport Management},
volume = {117},
pages = {102578},
year = {2024},
issn = {0969-6997},
doi = {https://doi.org/10.1016/j.jairtraman.2024.102578},
url = {https://www.sciencedirect.com/science/article/pii/S0969699724000437},
author = {Xinyue Ran and Lingling Li and Ruiling Han},
keywords = {Airport, Aviation complaint, Service quality, Influencing factors, Spatiotemporal differentiation characteristic},
abstract = {Airport service quality (ASQ) is essential for determining the quality of ground civil aviation services. In this study, ASQ was assessed using the monthly airport aviation complaint data from 2015 to 2019 of 196 airports in mainland China (except for airports in Hong Kong, Macao, and Taiwan, which are not included in the statistics). First, we constructed a seasonal index of aviation complaints to evaluate and compare the overall temporal characteristics of ASQ in China. Second, the spatial distribution pattern of ASQ in China was determined using the aviation complaint concentration index and hot spot analysis model. Finally, the major influencing factors and categories of ASQ in China were analyzed considering spatiotemporal dimensions using the correspondence analysis method. The results revealed that there were clear seasonal differences among ASQ in China, with a high–low–low–high distribution during all four seasons. The regional agglomeration trend of airport aviation complaints was obvious, and the spatial difference in ASQ was large. Northern and western China performed better than southern China. Spatiotemporal and influencing factors of ASQ were the most strongly correlated factors in each quarter and region. The predominant source of aviation complaints across all types of airports is related to fundamental services, with check-in services identified as the most impactful category affecting ASQ. This study, based on 60 months of statistical data, offers a comprehensive evaluation of ASQ throughout the entire airport network in mainland China, from the perspective of aviation complaints. Additionally, a systematic ASQ evaluation method and research system encompassing time, space, and elements were established. This framework not only stimulates the improvement and enhancement of ASQ but also provides a theoretical foundation for differentially enhancing ASQ in regional airports. Overall, our results contribute to breaking through the qualitative research thinking system in ASQ research from a theoretical perspective, paving the way for exploring broader research in enhancing the quality of ground civil aviation services.}
}
@article{FARAHI2021100326,
title = {A simulation–optimization approach for measuring emergency department resilience in times of crisis},
journal = {Operations Research for Health Care},
volume = {31},
pages = {100326},
year = {2021},
issn = {2211-6923},
doi = {https://doi.org/10.1016/j.orhc.2021.100326},
url = {https://www.sciencedirect.com/science/article/pii/S2211692321000424},
author = {Sorour Farahi and Khodakaram Salimifard},
keywords = {Crisis, Healthcare responsiveness, Resilience, Simulation–optimization},
abstract = {Crisis occurrence in the healthcare context is, for different reasons, a phenomenon that happens abundantly. The priority of the healthcare system during a crisis is to provide quality care and superior services to the injured people. However, given the usually extreme severity of the crisis that results in a significant number of injured people, proper and timely responsiveness of healthcare systems is a challenging issue This study proposes a novel framework using a hybrid simulation–optimization approach to measure the healthcare responsiveness in crisis to address this real-world problem. This paper closely connects operations research techniques to critical systems thinking notions to evaluate the behavior of a system in the face of crisis. Since all arriving casualties to the hospital are first taken to the emergency department (ED), the ED in a case study is used to illustrate the performance of the presented approach. We designed seven crisis scenarios and one scenario of the ED system in a normal situation and modeled them using discrete-event simulation (DES). Patients’ interarrival times act as the driver of workload experienced in ED during crisis scenarios of varying severity. For crisis simulation scenarios that are unable to cope with the severity of the crisis, we developed an optimization model in an optimization tool to determine the optimal configuration of resources. The optimal configuration can improve healthcare resilience. The results show that an interarrival time of 13.8 min is the maximum threshold, below which feasible solutions could not be found, and the ED system is likely to collapse.}
}
@article{FURIA2007164,
title = {Automated compositional proofs for real-time systems},
journal = {Theoretical Computer Science},
volume = {376},
number = {3},
pages = {164-184},
year = {2007},
note = {Fundamental Aspects of Software Engineering},
issn = {0304-3975},
doi = {https://doi.org/10.1016/j.tcs.2007.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S0304397507000643},
author = {Carlo A. Furia and Matteo Rossi and Dino Mandrioli and Angelo Morzenti},
keywords = {Formal verification, Modular systems, Real-time, Compositionality, Rely/guarantee},
abstract = {We present a framework for formally proving that the composition of the behaviors of the different parts of a complex, real-time system ensures a desired global specification of the overall system. The framework is based on a simple compositional rely/guarantee circular inference rule, plus a methodology concerning the integration of the different parts into a whole system. The reference specification language is the TRIO metric linear temporal logic. The novelty of our approach with respect to existing compositional frameworks–most of which do not deal explicitly with real-time requirements–consists mainly in its generality and abstraction from any assumptions about the underlying computational model and from any semantic characterizations of the temporal logic language used in the specification. Moreover, the framework deals equally well with continuous and discrete time. It is supported by a tool, implemented on top of the proof-checker PVS, to perform deduction-based verification through theorem-proving of modular real-time axiom systems. As an example of application, we show the verification of a real-time version of the old-fashioned but still relevant “benchmark” of the dining philosophers problem.}
}
@article{YOUSIF20241342,
title = {Safety 4.0: Harnessing computer vision for advanced industrial protection},
journal = {Manufacturing Letters},
volume = {41},
pages = {1342-1356},
year = {2024},
note = {52nd SME North American Manufacturing Research Conference (NAMRC 52)},
issn = {2213-8463},
doi = {https://doi.org/10.1016/j.mfglet.2024.09.161},
url = {https://www.sciencedirect.com/science/article/pii/S2213846324002451},
author = {Ibrahim Yousif and Jad Samaha and JuHyeong Ryu and Ramy Harik},
keywords = {Smart Manufacturing, Safety 4.0, Computer Vision, Industrial Protection, Musculoskeletal disorders (MSD), Effective Functional Training},
abstract = {In the pursuit of enhanced productivity, reduced costs, and minimized lead times, manufacturers are transitioning from traditional systems to autonomous systems. This shift, driven by the emergence of smart manufacturing and technological advancements such as robotics, collaborative robots (Cobots), automation, and digitalization, necessitates a parallel evolution in safety protocols—termed Safety 4.0—to mitigate the risks associated with such dynamic environments. The integration of smart technologies within manufacturing significantly transforms traditional workflows and intensifies the need for comprehensive safety training and guidelines. Innovations like smart personal protective equipment (PPE) and wearable sensors are pivotal in this transition, yet they often prove financially burdensome for manufacturers due to high costs and the scale of workforce deployment. Moreover, the effective use of these technologies requires continuous monitoring and data analysis, further straining resources. To address these challenges, this paper proposes the adoption of computer vision technology to enhance safety measures within manufacturing facilities, focusing on human and PPE detection. It details a holistic methodology encompassing data collection, preprocessing, training, and execution. The discussion extends to the implementation framework of this technology, emphasizing its role in enabling autonomous decision-making—a crucial step beyond mere detection. Furthermore, the paper explores the utilization of the accumulated data to develop immersive training modules employing Mixed Reality, thereby reinforcing safety protocols and fostering an environment of continuous learning and adaptation. This approach not only contributes to safeguarding personnel but also aligns with the financial and reputational interests of forward-thinking manufacturers.}
}
@article{KOPONEN202557,
title = {Sales managers' perceptions of interpersonal communication competence in leading AI-integrated sales teams},
journal = {Industrial Marketing Management},
volume = {124},
pages = {57-72},
year = {2025},
issn = {0019-8501},
doi = {https://doi.org/10.1016/j.indmarman.2024.11.012},
url = {https://www.sciencedirect.com/science/article/pii/S0019850124001846},
author = {Jonna Koponen and Saara Julkunen and Anne Laajalahti and Marianna Turunen and Brian Spitzberg},
keywords = {Artificial intelligence (AI), Interpersonal communication competence, Management},
abstract = {Adoption of artificial intelligence (AI) is no longer the issue for most professional organizations—the question is how to integrate it into the functions and organizational processes. Considering the current integration of AI in work processes, the requirements for sales managers' interpersonal communication competence (ICC) are likely to be modified. However, research on sales management competencies is surprisingly scarce. This longitudinal case study investigates sales managers' perceptions of their ICC needs in leading AI-integrated sales teams in the financial sector. During the years 2019–2024, 35 expert interviews with sales managers were collected from one of Scandinavia's largest financial groups. The findings indicate that AI system integration brought benefits, concerns and communication challenges to sales managers' job content. The main components related to sales managers' ICC in leading AI-integrated sales teams encompass both traditional competencies (motivation, knowledge, communication skills, and adaptability) but also include contextual AI factors and a concern for ethical reflectivity. A component model of managerial interpersonal communication competence in AI-integrated teams (MICCAIT) is produced and its implications are examined. Given the greater reliance on technology, sales managers may increasingly need to place greater emphasis on their empathy and people-oriented skills for the human employees remaining in the workplace.}
}
@article{FOSTER2021101214,
title = {Translating the grid: How a translational approach shaped the development of grid computing},
journal = {Journal of Computational Science},
volume = {52},
pages = {101214},
year = {2021},
note = {Case Studies in Translational Computer Science},
issn = {1877-7503},
doi = {https://doi.org/10.1016/j.jocs.2020.101214},
url = {https://www.sciencedirect.com/science/article/pii/S187775032030510X},
author = {Ian Foster and Carl Kesselman},
keywords = {Translational computer science, Grid computing},
abstract = {A growing gap between progress in biological knowledge and improved health outcomes inspired the new discipline of translational medicine, in which the application of new knowledge is an explicit part of a research plan. Abramson and Parashar argue that a similar gap between complex computational technologies and ever-more-challenging applications demands an analogous discipline of translational computer science, in which the deliberate movement of research results into large-scale practice becomes a central research focus rather than an afterthought. We revisit from this perspective the development and application of grid computing from the mid-1990s onwards, and find that a translational framing is useful for understanding the technology’s development and impact. We discuss how the development of grid computing infrastructure, and the Globus Toolkit, in particular, benefited from a translational approach. We identify lessons learned that can be applied to other translational computer science initiatives.}
}
@article{DANILOV2019108891,
title = {On the geometric origin of spurious waves in finite-volume discretizations of shallow water equations on triangular meshes},
journal = {Journal of Computational Physics},
volume = {398},
pages = {108891},
year = {2019},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2019.108891},
url = {https://www.sciencedirect.com/science/article/pii/S0021999119305893},
author = {S. Danilov and A. Kutsenko},
keywords = {Triangular meshes, Finite volume discretization, Computational dispersion branches},
abstract = {Computational wave branches are common to linearized shallow water equations discretized on triangular meshes. It is demonstrated that for standard finite-volume discretizations these branches can be traced back to the structure of the unit cell of triangular lattice, which includes two triangles with a common edge. Only subsets of similarly oriented triangles or edges possess the translational symmetry of unit cell. As a consequence, discrete degrees of freedom placed on triangles or edges are geometrically different, creating an internal structure inside unit cells. It implies a possibility of oscillations inside unit cells seen as computational branches in the framework of linearized shallow water equations, or as grid-scale noise generally. Adding dissipative operators based on smallest stencils to discretized equations is needed to control these oscillations in solutions. A review of several finite-volume discretization is presented with focus on computational branches and dissipative operators.}
}
@article{SUZUKI2008511,
title = {Research and development of fusion grid infrastructure based on atomic energy grid infrastructure (AEGIS)},
journal = {Fusion Engineering and Design},
volume = {83},
number = {2},
pages = {511-515},
year = {2008},
note = {Proceedings of the 6th IAEA Technical Meeting on Control, Data Acquisition, and Remote Participation for Fusion Research},
issn = {0920-3796},
doi = {https://doi.org/10.1016/j.fusengdes.2007.09.017},
url = {https://www.sciencedirect.com/science/article/pii/S092037960700498X},
author = {Y. Suzuki and K. Nakajima and N. Kushida and C. Kino and T. Aoyagi and N. Nakajima and K. Iba and N. Hayashi and T. Ozeki and T. Totsuka and H. Nakanishi and Y. Nagayama},
keywords = {Grid, AEGIS, JT-60, LHD, LABCOM},
abstract = {In collaboration with the Naka Fusion Institute of Japan Atomic Energy Agency (NFI/JAEA) and the National Institute for Fusion Science of National Institute of Natural Science (NIFS/NINS), Center for Computational Science and E-systems of Japan Atomic Energy Agency (CCSE/JAEA) aims at establishing an integrated framework for experiments and analyses in nuclear fusion research based on the atomic energy grid infrastructure (AEGIS). AEGIS has been being developed by CCSE/JAEA aiming at providing the infrastructure that enables atomic energy researchers in remote locations to carry out R&D efficiently and collaboratively through the Internet. Toward establishing the integrated framework, we have been applying AEGIS to pre-existing three systems: experiment system, remote data acquisition system, and integrated analysis system. For the experiment system, the secure remote experiment system with JT-60 has been successfully accomplished. For the remote data acquisition system, it will be possible to equivalently operate experimental data obtained from LHD data acquisition and management system (LABCOM system) and JT-60 Data System. The integrated analysis system has been extended to the system executable in heterogeneous computers among institutes.}
}
@article{FAVERO2023112755,
title = {Analysis of subjective thermal comfort data: A statistical point of view},
journal = {Energy and Buildings},
volume = {281},
pages = {112755},
year = {2023},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2022.112755},
url = {https://www.sciencedirect.com/science/article/pii/S0378778822009264},
author = {Matteo Favero and Antonio Luparelli and Salvatore Carlucci},
keywords = {Subjective thermal comfort data, Rating scales, Level of measurement, Ordinal regression, Bayesian analysis, Statistical thinking},
abstract = {Thermal comfort research aims to determine the relationship between the thermal environment and the human sense of warmth. This is usually achieved by measuring the subjective human thermal response to different thermal environments. However, it is common practice to use simple linear regression to analyse data collected using ordinal scales. This practice may lead to severe errors in inference. This study first set the methodological foundations to analyse subjective thermal comfort data from a statistical perspective. Subsequently, we show the practical consequences of fallacious assumptions by utilising a Bayesian approach and show, through an illustrative example, that a linear regression model applied to ordinal data suggests results different from those obtained using ordinal regression. Specifically, linear regression found no difference in means and effect size between genders, while the ordinal regression model led to the opposite conclusion. In addition, the linear regression model distorts the estimated regression coefficient for air temperature compared to the ordinal model. Finally, the ordinal model shows that the distance between adjacent response categories of the ASHRAE 7-point thermal sensation scale is not equidistant. Given the abovementioned issues, we advocate utilising ordinal models instead of metric models to analyse ordinal data.}
}
@article{SINGH2024,
title = {Unlocking microbial reservoirs for antimicrobial peptides and beyond},
journal = {Trends in Plant Science},
year = {2024},
issn = {1360-1385},
doi = {https://doi.org/10.1016/j.tplants.2024.11.013},
url = {https://www.sciencedirect.com/science/article/pii/S1360138524003145},
author = {Akanksha Singh and Shivam Chauhan and Prabodh Kumar Trivedi},
keywords = {antimicrobial peptides, global microbiome, machine learning, peptide based biologicals},
abstract = {Recently, Santos-Júnior et al. utilized a machine learning approach to identify nearly a million novel antimicrobial peptides (AMPs) from the global microbiome. Here we explore the untapped potential of plant- and soil-associated microbiomes as a source of novel peptides, highlighting their promising applications in advancing agricultural innovation and sustainability.}
}
@article{VANDECRUYS2021107,
title = {Mental distress through the prism of predictive processing theory},
journal = {Current Opinion in Psychology},
volume = {41},
pages = {107-112},
year = {2021},
note = {Psychopathology},
issn = {2352-250X},
doi = {https://doi.org/10.1016/j.copsyc.2021.07.006},
url = {https://www.sciencedirect.com/science/article/pii/S2352250X21001056},
author = {Sander {Van de Cruys} and Pieter {Van Dessel}},
keywords = {Predictive processing, Mental distress, Psychopathology, Emotion, Depression, Anxiety, Active inference, Addiction, Learning, Psychotherapy, Computational psychiatry},
abstract = {Summary
We review the predictive processing theory’s take on goals and affect, to shed new light on mental distress and how it develops into psychopathology such as in affective and motivational disorders. This analysis recovers many of the classical factors known to be important in those disorders, like uncertainty and control, but integrates them in a mechanistic model of adaptive and maladaptive cognition and behavior. We derive implications for treatment that have so far remained underexposed in existing predictive processing accounts of mental disorder, specifically with regard to the model-dependent construction of value, the importance of model validation (evidence), and the introduction and learning of new, adaptive beliefs that relieve suffering.}
}
@incollection{MACHINMASTROMATTEO2025569,
title = {Information Literacy and the Information Science Curriculum},
editor = {David Baker and Lucy Ellis},
booktitle = {Encyclopedia of Libraries, Librarianship, and Information Science (First Edition)},
publisher = {Academic Press},
edition = {First Edition},
address = {Oxford},
pages = {569-578},
year = {2025},
isbn = {978-0-323-95690-1},
doi = {https://doi.org/10.1016/B978-0-323-95689-5.00191-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780323956895001917},
author = {Juan D. Machin-Mastromatteo and César Saavedra-Alamillas and Alejandro Villegas-Muro},
keywords = {Advocacy, Critical competences, Curricular implementation, Curriculum, Curriculum methodologies, Digital literacy, Information literacy, Information literacy programs, Library and information science, Media literacy, Professional education and training, Social implications, Teaching and learning methodologies, Workplace implications},
abstract = {This entry provides a background to information literacy education in the library and information science (LIS) curriculum by presenting a summary of the elements and characteristics that information literacy courses should include. These are divided into six categories: (1) curricular implementation and general challenges; (2) topics the curriculum must include; (3) inclusion of education-related topics; (4) integration of other literacies; (5) methodologies for the information literacy curriculum; and (6) workplace and social implications. Then, it includes a brief and non-exhaustive review of 41 LIS programs, including courses on information literacy and related subjects. Finally, we offer some brief considerations for the future perspectives of this topic.}
}
@article{OLUBAMBI2025,
title = {Conceptualising a dynamic BIM-based waste management system in enabling net-zero cities},
journal = {Proceedings of the Institution of Civil Engineers - Waste and Resource Management},
year = {2025},
issn = {1747-6534},
doi = {https://doi.org/10.1680/jwarm.23.00043},
url = {https://www.sciencedirect.com/science/article/pii/S1747653425000026},
author = {Ademilade Olubambi and Clinton Aigbavboa and Bolanle Ikotun},
keywords = {building information modelling (BIM), life cycle assessment, life cycle analysis, LCA, sustainable cities and communities, sustainable development, waste disposal system},
abstract = {This study investigates the possibility of applying building information modelling as a tool for eliminating waste throughout the building life cycle toward achieving net-zero waste in the construction industry. To accomplish this goal, literature was reviewed to identify aspects that necessitate using mechanism in optimising a sustainable waste management system. A building information modelling-based conceptual framework with a high capacity for implementing net-zero waste management action throughout the building development stages was developed. The results indicate that the tool can generate an effective programme for ordering materials, assembling, and supplying all building components during the design phase. Any alterations to the building information model during the design phase are updated automatically. During the procurement phase, three-dimensional geometry can be used for project sequencing, take-offs, and integrating energy analyses. Virtual construction modelling, which is extremely cost-effective, can be used during the construction phase. Furthermore, the tools can be utilised as a waste estimation tool before any demolition or remodelling, as well as to help determine a reasonable price for waste disposal rates, throughout the construction phase. In conclusion, this study demonstrates how waste is minimised and consequently prevented in building construction by integrating dynamic building information modelling tools while enabling the possibility of a net-zero cities.}
}
@article{KELLEY2011228,
title = {Theoretical explorations of cognitive robotics using developmental psychology},
journal = {New Ideas in Psychology},
volume = {29},
number = {3},
pages = {228-234},
year = {2011},
note = {Special Issue: Cognitive Robotics and Reevaluation of Piaget Concept of Egocentrism},
issn = {0732-118X},
doi = {https://doi.org/10.1016/j.newideapsych.2009.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S0732118X0900035X},
author = {Troy D. Kelley and Daniel N. Cassenti},
keywords = {Development, Robotics, Cognition, Cognitive modeling},
abstract = {How can cognitive robotics inform developmental psychology researchers and what can developmental psychology tell us about creating robots? More importantly, how can cognitive robotics and developmental psychology nourish each other to become a symbiotic relationship for future research? We address the theoretical underpinnings of developmental change using a cognitive architecture implemented on a robotic system and how our theories of knowledge representation relate to critical periods of infant development. Next, we will show how descriptive theories of cognitive development, specifically Zelazo's Levels of Consciousness (LOC; Zelazo, 2000, Zelazo, 2004, Zelazo and Jacques, 1996), can be mapped onto a computational cognitive architecture (ACT-R; Anderson & Lebiere, 1998). Following our discussion of Zelazo's theory, we will apply the ACT-R architecture specifically to the problem of object permanence. Finally, we will address how cognitive robotics can serve as a computational proving ground of developmental psychology for future research.}
}
@article{BECK2024545,
title = {Understanding the cell: Future views of structural biology},
journal = {Cell},
volume = {187},
number = {3},
pages = {545-562},
year = {2024},
issn = {0092-8674},
doi = {https://doi.org/10.1016/j.cell.2023.12.017},
url = {https://www.sciencedirect.com/science/article/pii/S0092867423013491},
author = {Martin Beck and Roberto Covino and Inga Hänelt and Michaela Müller-McNicoll},
keywords = {structural biology, digital twin, computational modeling, cellular self-organization},
abstract = {Summary
Determining the structure and mechanisms of all individual functional modules of cells at high molecular detail has often been seen as equal to understanding how cells work. Recent technical advances have led to a flush of high-resolution structures of various macromolecular machines, but despite this wealth of detailed information, our understanding of cellular function remains incomplete. Here, we discuss present-day limitations of structural biology and highlight novel technologies that may enable us to analyze molecular functions directly inside cells. We predict that the progression toward structural cell biology will involve a shift toward conceptualizing a 4D virtual reality of cells using digital twins. These will capture cellular segments in a highly enriched molecular detail, include dynamic changes, and facilitate simulations of molecular processes, leading to novel and experimentally testable predictions. Transferring biological questions into algorithms that learn from the existing wealth of data and explore novel solutions may ultimately unveil how cells work.}
}
@article{2025335,
title = {In This Issue},
journal = {Biological Psychiatry: Cognitive Neuroscience and Neuroimaging},
volume = {10},
number = {4},
pages = {335-336},
year = {2025},
note = {Cognitive Neuroscience of Mindfulness},
issn = {2451-9022},
doi = {https://doi.org/10.1016/j.bpsc.2025.02.012},
url = {https://www.sciencedirect.com/science/article/pii/S2451902225000692}
}
@article{CAMELODAZA2024101760,
title = {Parameter estimation in single-phase transformers via the generalized normal distribution optimizer while considering voltage and current measurements},
journal = {Results in Engineering},
volume = {21},
pages = {101760},
year = {2024},
issn = {2590-1230},
doi = {https://doi.org/10.1016/j.rineng.2024.101760},
url = {https://www.sciencedirect.com/science/article/pii/S2590123024000136},
author = {Juan David Camelo-Daza and Diego Noel Betancourt-Alonso and Oscar Danilo Montoya and Ernesto Gómez-Vargas},
keywords = {Nonlinear optimization, Metaheuristic optimization algorithms, Generalized normal distribution optimizer, Parameter estimation, Single-phase transformers, Mean square error minimization, Voltage and current measurements},
abstract = {This research addresses, from a perspective of metaheuristic optimization, the problem regarding parametric estimation in single-phase transformers while considering voltage and current measures at the terminals of the transformer and weighing linear loads. Transformer parametric estimation is modeled as a nonlinear problem in order to minimize the mean square error between the calculated voltage and current variables and the measurements taken. The nonlinearities are associated with Kirchhoff's first and second laws applied to the equivalent electrical circuit of the single-phase transformer. The nonlinear optimization problem is solved by applying a metaheuristic optimization algorithm known as the generalized normal distribution optimizer (GNDO), which uses evolution rules that allow exploring and exploiting the solution space via the classical probability function based on normal distributions. Numerical results in three test transformers of 20, 45, and 112.5 kVA demonstrate the effectiveness and robustness of the proposed GNDO approach when compared to other optimizers reported in the literature, such as the crow search algorithm, the coyote optimization algorithm, and the exact solution of the nonlinear optimization model using the fmincon solver of the MATLAB software. All numerical simulations confirm the potential of the GNDO approach to deal with complex optimization problems in engineering and science with promising results and low computational effort.}
}
@article{SANZ2021103070,
title = {The entropic tongue: Disorganization of natural language under LSD},
journal = {Consciousness and Cognition},
volume = {87},
pages = {103070},
year = {2021},
issn = {1053-8100},
doi = {https://doi.org/10.1016/j.concog.2020.103070},
url = {https://www.sciencedirect.com/science/article/pii/S1053810020305377},
author = {Camila Sanz and Carla Pallavicini and Facundo Carrillo and Federico Zamberlan and Mariano Sigman and Natalia Mota and Mauro Copelli and Sidarta Ribeiro and David Nutt and Robin Carhart-Harris and Enzo Tagliazucchi},
keywords = {LSD, Psychedelics, Natural language, Entropy, Psychosis},
abstract = {Serotonergic psychedelics have been suggested to mirror certain aspects of psychosis, and, more generally, elicit a state of consciousness underpinned by increased entropy of on-going neural activity. We investigated the hypothesis that language produced under the effects of lysergic acid diethylamide (LSD) should exhibit increased entropy and reduced semantic coherence. Computational analysis of interviews conducted at two different time points after 75 μg of intravenous LSD verified this prediction. Non-semantic analysis of speech organization revealed increased verbosity and a reduced lexicon, changes that are more similar to those observed during manic psychoses than in schizophrenia, which was confirmed by direct comparison with reference samples. Importantly, features related to language organization allowed machine learning classifiers to identify speech under LSD with accuracy comparable to that obtained by examining semantic content. These results constitute a quantitative and objective characterization of disorganized natural speech as a landmark feature of the psychedelic state.}
}
@article{ESCAMILLA2021102697,
title = {Interaction designers’ perceptions of using motion-based full-body features},
journal = {International Journal of Human-Computer Studies},
volume = {155},
pages = {102697},
year = {2021},
issn = {1071-5819},
doi = {https://doi.org/10.1016/j.ijhcs.2021.102697},
url = {https://www.sciencedirect.com/science/article/pii/S1071581921001154},
author = {Antonio Escamilla and Javier Melenchón and Carlos Monzo and Jose Antonio Morán},
keywords = {Motion-based feature extraction, Full-body interaction, Interaction designers' perception, Designer-interpretable feature},
abstract = {Movement-based full-body interactions are increasingly being used in the design of interactive spaces, computer-mediated environments, and virtual user experiences due to the development and availability of diverse sensing technologies. In this context, the role of interaction designers is to find systematic and predictable relationships between bodily actions and the corresponding responses from technology. Sensor-based interaction design relies on sensor data analysis and higher-level feature extraction to improve detection capabilities. However, understanding human movement to inform the design of motion-based interactions is not straightforward if the detection capabilities of interaction technologies are unknown. We aim at understanding the problems and opportunities that practitioners—regardless of their technical background—perceive in using different motion-based full-body features. To achieve this, we conducted four separate focus groups with experienced practitioners, with and without technical backgrounds. We used a framework for the analysis of focus group data in information systems research to identify content areas and draw conclusions. Our findings suggest that most interaction designers, regardless of their technical background, consider motion-based feature extraction to be challenging and time-consuming. However, participants acknowledge they might use designer-interpretable features as a potential tool to foster user behavior exploration. Understanding how practitioners link sensor-based interaction design with feature extraction technology is relevant to design computational tools and reduce the technical effort required from designers to characterize the user’s movement.}
}
@article{WU2024103772,
title = {Fuser: An enhanced multimodal fusion framework with congruent reinforced perceptron for hateful memes detection},
journal = {Information Processing & Management},
volume = {61},
number = {4},
pages = {103772},
year = {2024},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2024.103772},
url = {https://www.sciencedirect.com/science/article/pii/S0306457324001328},
author = {Fan Wu and Bin Gao and Xiaoou Pan and Linlin Li and Yujiao Ma and Shutian Liu and Zhengjun Liu},
keywords = {Hateful memes detection, Multimodal fusion, Congruent reinforced perceptron, Main semantic, Auxiliary context},
abstract = {As a multimodal form of hate speech on social media, hateful memes are more aggressive and cryptic threats to the real life of humans. Automatic detection of hateful memes is crucial, but the images and texts in most memes are only weakly consistent or even irrelevant. Although existing works have achieved the initial goal of detecting hateful memes with pre-trained models, they are limited to monolithic inference methods while ignoring the semantic differences between multimodal representations. To strengthen the comprehension and reasoning of the hidden meaning behind the memes by combining real-world knowledge, we propose an enhanced multimodal fusion framework with congruent reinforced perceptron for hateful memes detection. Inspired by the human cognitive mechanism, we first divide the extracted multisource representations into main semantics and auxiliary contexts based on their strength and relevance, and then precode them into lightly correlated embeddings with unified spatial dimensions via a novel prefix uniform layer, respectively. To jointly learn the intrinsic correlation between primary and secondary semantics, a congruent reinforced perceptron with brain-like perceptual integration is designed to seamlessly fuse multimodal representations in a shared latent space while maintaining the feature integrity in the sub-fusion space, thereby implicitly reasoning about the subtle metaphors behind the memes. Extensive experiments on four benchmark datasets fully demonstrate the effectiveness and superiority of our architecture compared with previous state-of-the-art methods.}
}
@article{NEINHUIS2017394,
title = {Innovations from the “ivory tower”: Wilhelm Barthlott and the paradigm shift in surface science},
journal = {Beilstein Journal of Nanotechnology},
volume = {8},
pages = {394-402},
year = {2017},
issn = {2190-4286},
doi = {https://doi.org/10.3762/bjnano.8.41},
url = {https://www.sciencedirect.com/science/article/pii/S2190428617000363},
author = {Christoph Neinhuis},
keywords = {Wilhelm Barthlott, 70th birthday, self-cleaning surfaces, lotus-effect},
abstract = {This article is mainly about borders that have tremendous influence on our daily life, although many of them exist and act mostly unrecognized. In this article the first objective will be to address more generally the relation between university and society or industry, borders within universities, borders in thinking and the huge amount of misunderstandings and losses resulting from these obvious or hidden borders. In the second part and in more detail, the article will highlight the impact of the research conducted by Wilhelm Barthlott throughout his scientific career during which not only one border was removed, shifted or became more penetrable. Among the various fields of interest not mentioned here (e.g., systematics of Cactaceae, diversity and evolution of epiphytes, the unique natural history of isolated rocky outcrops called inselbergs, or the global distribution of biodiversity), plant surfaces and especially the tremendous diversity of minute structures on leaves, fruits, seeds and other parts of plants represent a common thread through 40 years of scientific career of Wilhelm Barthlott. Based on research that was regarded already old-fashioned in the 1970s and 1980s, systematic botany, results and knowledge were accumulated that, some 20 years later, initiated a fundamental turnover in how surfaces were recognized not only in biology, but even more evident in materials science.}
}
@article{RAMESH2021375,
title = {Activation energy process in hybrid CNTs and induced magnetic slip flow with heat source/sink},
journal = {Chinese Journal of Physics},
volume = {73},
pages = {375-390},
year = {2021},
issn = {0577-9073},
doi = {https://doi.org/10.1016/j.cjph.2021.07.016},
url = {https://www.sciencedirect.com/science/article/pii/S0577907321001696},
author = {G.K. Ramesh and J.K. Madhukesh},
keywords = {Carbon nanotubes, Slip flow, Induce magnetic flux, Activation energy, Chemical reaction},
abstract = {Effect of induced magnetic field is critical as a result of much controlled and focused on liquid flow is wanted in numerous modern and clinical procedures for example electromagnetic casting, drug delivery and cooling of nuclear reactors. Hence this investigation explains the behaviour of hybrid carbon nanotubes (CNTs) flow through slipped surface with induced magnetic field. Accumulation of SWCNTs (single wall) and MWCNTs (multi wall) nanomaterial with water base liquid is considered. Thermal performance is analyzed with regular heat source/sink effect. Chemical reaction and activation energy impacts are incorporated in mass equation. Solution of the similarity equations are obtained by adopting RKF45 method. Influence of flow variables are illustrated through graphs and computational values of drag force, Nusselt number and Sherwood number are presented in tables. It is noted that activation energy enhance the concentration field whereas opposite behaviour for reaction rate. Also induce magnetic field boosted with the larger values of magnetic Prandtl number. Furthermore it is observed that hybrid CNTs nanomaterial having higher rate of heating/cooling compare to singular CNTs nanomaterial.}
}
@article{VANRINSVELD201717,
title = {Mental arithmetic in the bilingual brain: Language matters},
journal = {Neuropsychologia},
volume = {101},
pages = {17-29},
year = {2017},
issn = {0028-3932},
doi = {https://doi.org/10.1016/j.neuropsychologia.2017.05.009},
url = {https://www.sciencedirect.com/science/article/pii/S0028393217301756},
author = {Amandine {Van Rinsveld} and Laurence Dricot and Mathieu Guillaume and Bruno Rossion and Christine Schiltz},
keywords = {Mathematics, Neuroimaging, Bilingualism, Numerical cognition, Arithmetics},
abstract = {How do bilinguals solve arithmetic problems in each of their languages? We investigated this question by exploring the neural substrates of mental arithmetic in bilinguals. Critically, our population was composed of a homogeneous group of adults who were fluent in both of their instruction languages (i.e., German as first instruction language and French as second instruction language). Twenty bilinguals were scanned with fMRI (3T) while performing mental arithmetic. Both simple and complex problems were presented to disentangle memory retrieval occuring in very simple problems from arithmetic computation occuring in more complex problems. In simple additions, the left temporal regions were more activated in German than in French, whereas no brain regions showed additional activity in the reverse constrast. Complex additions revealed the reverse pattern, since the activations of regions for French surpassed the same computations in German and the extra regions were located predominantly in occipital regions. Our results thus highlight that highly proficient bilinguals rely on differential activation patterns to solve simple and complex additions in each of their languages, suggesting different solving procedures. The present study confirms the critical role of language in arithmetic problem solving and provides novel insights into how highly proficient bilinguals solve arithmetic problems.}
}
@incollection{BISHT2022277,
title = {Chapter Twelve - Perceiving the level of depression from web text},
editor = {Shikha Jain and Kavita Pandey and Princi Jain and Kah Phooi Seng},
booktitle = {Artificial Intelligence, Machine Learning, and Mental Health in Pandemics},
publisher = {Academic Press},
pages = {277-298},
year = {2022},
isbn = {978-0-323-91196-2},
doi = {https://doi.org/10.1016/B978-0-323-91196-2.00008-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780323911962000089},
author = {Sankalp Singh Bisht and Herumb Shandilya and Vaibhav Gupta and Shriyansh Agrawal and Shikha Jain},
keywords = {Depression, Loneliness, Mental disorder, Solitude, Suicide},
abstract = {Depression is one of the deadliest diseases found in today's world, and unfortunately, it is also one of the most ignored problems. Depression is a fact that is very hard to accept for any individual and is always a multistep process. The initial stage of Depression is Loneliness, and thus the information about these emotions can be leveraged and can help in the early detection of Depression, which in turn leads to suicidal thoughts. Tweet data analysis is one of the most popular ways to determine the presence of depression and suicidal thoughts, through the concepts of Machine Learning. Twitter proves to be a very rich source of data, as their user base is potentially large enough, but is also increasing in a fast manner. For the scope of this paper, we predicted from a user's specific tweet, which is categorized for loneliness. These tweets are analyzed to check the level of depression as moderate or severe when people start thinking of suicide. The simulation is carried out using four different models for one level of classification and eight models are used at the second level of classification. It is observed that Gated Recurrent Unit with BERT outperformed all the models and showed the accuracy of 99% and 97%. However, for class-1 recall with XLNet gave the best result with class-1 recall being 0.99. This application can help the individual in early detection of depression without any human intervention and seek medical help. Moreover, it also provides an insight about the feelings of the individual to the medical practitioners, which, in turn, can help them provide better decision-making.}
}
@article{2024100670,
title = {Erratum regarding missing declaration of competing interest statements in previously published articles},
journal = {International Journal of Child-Computer Interaction},
volume = {41},
pages = {100670},
year = {2024},
issn = {2212-8689},
doi = {https://doi.org/10.1016/j.ijcci.2024.100670},
url = {https://www.sciencedirect.com/science/article/pii/S2212868924000382}
}
@article{MARSHALL2023131,
title = {The role of quantum mechanics in cognition-based evolution},
journal = {Progress in Biophysics and Molecular Biology},
volume = {180-181},
pages = {131-139},
year = {2023},
issn = {0079-6107},
doi = {https://doi.org/10.1016/j.pbiomolbio.2023.04.007},
url = {https://www.sciencedirect.com/science/article/pii/S007961072300041X},
author = {Perry Marshall},
abstract = {In 2021 I noted that in all information-based systems we understand, Cognition creates Code, which controls Chemical reactions. Known agents write software which controls hardware, and not the other way around. I proposed the same is true in all of biology. Though the textbook description of cause and effect in biology proposes the reverse, that Chemical reactions produce Code from which Cognition emerges, there are no examples in the literature demonstrating either step. A mathematical proof for the first step, cognition generating code, is based on Turing's halting problem. The second step, code controlling chemical reactions, is the role of the genetic code. Thus a central question in biology: What is the nature and source of cognition? In this paper I propose a relationship between biology and Quantum Mechanics (QM), hypothesizing that the same principle that enables an observer to collapse a wave function also grants biology its agency: the organism's ability to act on the world instead of merely being a passive recipient. Just as all living cells are cognitive (Shapiro 2021, 2007; McClintock 1984; Lyon 2015; Levin 2019; Pascal and Pross, 2022), I propose humans are quantum observers because we are made of cells and all cells are observers. This supports the century-old view that in QM, the observer does not merely record the event but plays a fundamental role in its outcome.The classical world is driven by laws, which are deductive; the quantum world is driven by choices, which are inductive. When the two are combined, they form the master feedback loop of perception and action for all biology. In this paper I apply basic definitions of induction, deduction and computation to known properties of QM to show that the organism altering itself (and its environment) is a whole shaping its parts. It is not merely parts comprising a whole. I propose that an observer collapsing the wave function is the physical mechanism for producing negentropy. The way forward in solving the information problem in biology is understanding the relationship between cognition and QM.}
}
@article{UCAN2022104878,
title = {Advice hierarchies among finite automata},
journal = {Information and Computation},
volume = {288},
pages = {104878},
year = {2022},
note = {Special Issue: Selected Papers of the 14th International Conference on Language and Automata Theory and Applications, LATA 2020},
issn = {0890-5401},
doi = {https://doi.org/10.1016/j.ic.2022.104878},
url = {https://www.sciencedirect.com/science/article/pii/S0890540122000207},
author = {Ahmet Bilal Uçan and A.C. Cem Say},
keywords = {Formal languages, Automata theory, Advised computation},
abstract = {We examine the effects of supplying increasing amounts of trusted advice to a finite automaton. Previous work has shown that allowing such automata with a single advice tape to make a single pass over their input renders them unable to recognize the palindromes language, whereas both two-way machines reading advice from a single tape and one-way machines with multiple advice tapes can recognize all languages with exponentially bounded amounts of advice. We study several architectural variants and demonstrate the existence of language hierarchies based on increased advice length, runtime (measured in terms of the number of allowed left-to-right passes on the input), and number of advice tapes. We also prove some lower bounds for recognizing certain concrete languages.}
}
@article{MEMARIAN2023100022,
title = {ChatGPT in education: Methods, potentials, and limitations},
journal = {Computers in Human Behavior: Artificial Humans},
volume = {1},
number = {2},
pages = {100022},
year = {2023},
issn = {2949-8821},
doi = {https://doi.org/10.1016/j.chbah.2023.100022},
url = {https://www.sciencedirect.com/science/article/pii/S2949882123000221},
author = {Bahar Memarian and Tenzin Doleck},
keywords = {ChatGPT, Large language models, Education, Artificial intelligence, Machine learning, Data science, Pedagogy},
abstract = {ChatGPT has been under the scrutiny of public opinion including in education. Yet, less work has been done to analyze studies conducted on ChatGPT in educational contexts. This review paper examines where ChatGPT is employed in educational literature and areas of potential, challenges, and future work. A total of 63 publications were included in this review using the general framework of open and axial coding. We coded and summarized the methods, and reported potentials, limitations, and future work of each study. Thematic analysis of reviewed studies revealed that most extant studies in the education literature explore ChatGPT through a commentary and non-empirical lens. The potentials of ChatGPT include but are not limited to the development of personalized and complex learning, specific teaching and learning activities, assessments, asynchronous communication, feedback, accuracy in research, personas, and task delegation and cognitive offload. Several areas of challenge that ChatGPT is or will be facing in education are also shared. Examples include but are not limited to plagiarism deception, misuse or lack of learning, accountability, and privacy. There are both concerns and optimism about the use of ChatGPT in education, yet the most pressing need is to ensure student learning and academic integrity are not sacrificed. Our review provides a summary of studies conducted on ChatGPT in education literature. We further provide a comprehensive and unique discussion on future considerations for ChatGPT in education.}
}
@article{ALBERT2008401,
title = {A formal framework for modelling the developmental course of competence and performance in the distance, speed, and time domain},
journal = {Developmental Review},
volume = {28},
number = {3},
pages = {401-420},
year = {2008},
issn = {0273-2297},
doi = {https://doi.org/10.1016/j.dr.2008.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S0273229708000257},
author = {Dietrich Albert and Michael D. Kickmeier-Rust and Fumiko Matsuda},
keywords = {Distance–speed–time system, Cognitive development, Overgeneralization, Competence-based Knowledge Space Theory},
abstract = {The developmental course in the distance–speed–time domain is still a matter of debate. Traditional stage models are contested by theories of continuous development and adaptive thinking. In the present work, we introduce a formal framework for modelling the developmental course in this domain, grounding on Competence-based Knowledge Space Theory. This framework, as a more general case, widely includes assumptions and facets of previous models and covers empirical findings collected based on different experimental paradigms. By a distinction of latent competences and observable performance, model validation is not bound to a certain experimental paradigm and no one-to-one correspondence between competences and tasks is required. Therefore, the framework has the potential to bridge the gap between stage models and models of continuous development. The approach also precisely defines misconceptions, for example overgeneralization, and empirically investigates their occurrence. In the present work, we established a prototypical model for the development of understanding the distance–speed–time system. We extended this model with definitions based on different perspectives of overgeneralization. The assumptions of the model and its extensions were examined on the basis of the results of two empirical investigations using six judgment task types. The results yielded a reasonably good fit of model and data. No evidence was found for the occurrence of overgeneralization in this domain. The theoretical model and empirical results are discussed with respect to their relationship to other developmental models and theories.}
}
@article{RAMAN2002135,
title = {Coordinating informal and formal aspects of mathematics: student behavior and textbook messages},
journal = {The Journal of Mathematical Behavior},
volume = {21},
number = {2},
pages = {135-150},
year = {2002},
issn = {0732-3123},
doi = {https://doi.org/10.1016/S0732-3123(02)00119-0},
url = {https://www.sciencedirect.com/science/article/pii/S0732312302001190},
author = {Manya Raman},
keywords = {Informal mathematics, Formal mathematics, Precalculus textbooks, Calculus textbooks},
abstract = {In this paper I illustrate difficulties students have coordinating informal and formal aspects of mathematics. I also discuss two ways in which precalculus and calculus textbooks treat mathematics that may make this coordination difficult: emphasizing the informal at the expense of the formal and emphasizing the formal at the expense of the informal. By looking at student difficulties in light of textbook treatments, we see evidence that student difficulties are not merely developmental. Students are not given many opportunities to make the kinds of connections which, while difficult, are an essential component of mathematical thinking.}
}
@article{DELIMANETO2018225,
title = {A semiotic-inspired machine for personalized multi-criteria intelligent decision support},
journal = {Data & Knowledge Engineering},
volume = {117},
pages = {225-238},
year = {2018},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2018.07.012},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X17300757},
author = {Fernando Buarque {de Lima Neto} and Denis Mayr {Lima Martins} and Gottfried Vossen},
keywords = {Multi-criteria decision support, Computational intelligence, Computational semiotics, Intelligent semiotic machine},
abstract = {The need for appropriate decisions to tackle complex problems increases every day. Selecting destinations for vacation, comparing and optimizing resources to create valuable products, or purchasing a suitable car are just a few examples of puzzling situations in which there is no standard form to find an appropriate solution. Such scenarios become arduous when the number of possibilities, restrictions, and factors affecting the decision rise, thereby turning decision makers into almost mere spectators. In such circumstances, decision support systems (DSS) can play an important role in guiding people and organizations towards more accurate decision making. However, conventional DSS lack the necessary adaptability to account for dynamic changes and are frequently inadequate to tackle the subjectivity inherent in decision-maker's preferences and intention. We argue that these shortcomings can be addressed by a suitable combination of Semiotic Theory and Computational Intelligence algorithms, which together can make up a new generation of DSS. In this article, a formal description of an Intelligent Semiotic Machine is provided and tried out in practical decision contexts. The results obtained show that our approach can provide well-suited decisions based on user preferences, achieving appropriateness while fanning out subjective options without losing decision context, objectivity, or accuracy.}
}
@article{NARASIMHAN197879,
title = {Modelling behaviour: the need for a computational approach},
journal = {Journal of Social and Biological Structures},
volume = {1},
number = {1},
pages = {79-94},
year = {1978},
issn = {0140-1750},
doi = {https://doi.org/10.1016/0140-1750(78)90020-9},
url = {https://www.sciencedirect.com/science/article/pii/0140175078900209},
author = {R. Narasimhan},
abstract = {The principal objective of this paper is to argue the thesis that a science concerned with the study of behaviour requires the computational approach in a serious way for its theoretical advancement. It is pointed out that modelling behaviour requires the articulation of explanations at three levels. The methodology of computational simulations is indispensable to articulating explanations at the first level which underlie explanations at the other two levels. The paper contrasts the currently fashionable approaches in artificial intelligence studies to the kind of constraints viable behavioural models must satisfy. Teachability and open-endedness are two of the essential characteristics of organisms that any satisfactory model must have. It is argued that analogy-based computational techniques and paradigmatic learning/teaching techniques are two modelling aspects that require imaginative study.}
}
@incollection{MADIAJAGAN2019245,
title = {Chapter 15 - Parallel Machine Learning and Deep Learning Approaches for Bioinformatics},
editor = {Arun Kumar Sangaiah},
booktitle = {Deep Learning and Parallel Computing Environment for Bioengineering Systems},
publisher = {Academic Press},
pages = {245-255},
year = {2019},
isbn = {978-0-12-816718-2},
doi = {https://doi.org/10.1016/B978-0-12-816718-2.00022-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128167182000221},
author = {M. Madiajagan and S. Sridhar Raj},
keywords = {Machine learning, Deep Learning, Parallel processing, Bioinformatics, Parallel deep neural networks},
abstract = {Deep learning uses multiple layers of artificial neurons for classification and pattern recognition. The biggest drawbacks of deep learning algorithms have been the high computation cost, inter-processor communication bottlenecks and parameters training time. Hence, incorporating parallel computing into deep learning decreases the computation time of complex deep learning algorithms. This chapter presents how parallelization is applied over many processors which are loosely coupled. Up to 4096 processes are scaled linearly with higher accuracy and zero loss percentage. This capacity of huge scaling helps in training billions of training examples in just a few hours. Various applications of Hessian-free parallelization mechanism on bioinformatics applications are in gene therapy, drug development, antibiotic resistance research, waste cleanup, climate change studies, bioweapon creation, improving nutritional quality and veterinary science.}
}
@article{LIM2024127512,
title = {Progressive expansion: Cost-efficient medical image analysis model with reversed once-for-all network training paradigm},
journal = {Neurocomputing},
volume = {581},
pages = {127512},
year = {2024},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.127512},
url = {https://www.sciencedirect.com/science/article/pii/S0925231224002832},
author = {Shin Wei Lim and Chee Seng Chan and Erma Rahayu {Mohd Faizal} and Kok Howg Ewe},
keywords = {Medical image analysis, Machine learning, Model optimization, Cost-effective model},
abstract = {Low computational cost artificial intelligence (AI) models are vital in promoting the accessibility of real-time medical services in underdeveloped areas. The recent Once-For-All (OFA) network (without retraining) can directly produce a set of sub-network designs with Progressive Shrinking (PS) algorithm; however, the training resource and time inefficiency downfalls are apparent in this method. In this paper, we propose a new OFA training algorithm, namely the Progressive Expansion (ProX) to train the medical image analysis model. It is a reversed paradigm to PS, where technically we train the OFA network from the minimum configuration and gradually expand the training to support larger configurations. Empirical results showed that the proposed paradigm could reduce training time up to 68%; while still being able to produce sub-networks that have either similar or better accuracy compared to those trained with OFA-PS on ROCT (classification), BRATS and Hippocampus (3D-segmentation) public medical datasets. The code implementation for this paper is accessible at: https://github.com/shin-wl/ProX-OFA.}
}
@article{LEVINSON2002155,
title = {Returning the tables: language affects spatial reasoning},
journal = {Cognition},
volume = {84},
number = {2},
pages = {155-188},
year = {2002},
issn = {0010-0277},
doi = {https://doi.org/10.1016/S0010-0277(02)00045-8},
url = {https://www.sciencedirect.com/science/article/pii/S0010027702000458},
author = {Stephen C Levinson and Sotaro Kita and Daniel B.M Haun and Björn H Rasch},
keywords = {Language, Spatial reasoning, Linguistic relativity},
abstract = {Li and Gleitman (Turning the tables: language and spatial reasoning. Cognition, in press) seek to undermine a large-scale cross-cultural comparison of spatial language and cognition which claims to have demonstrated that language and conceptual coding in the spatial domain covary (see, for example, Space in language and cognition: explorations in linguistic diversity. Cambridge: Cambridge University Press, in press; Language 74 (1998) 557): the most plausible interpretation is that different languages induce distinct conceptual codings. Arguing against this, Li and Gleitman attempt to show that in an American student population they can obtain any of the relevant conceptual codings just by varying spatial cues, holding language constant. They then argue that our findings are better interpreted in terms of ecologically-induced distinct cognitive styles reflected in language. Linguistic coding, they argue, has no causal effects on non-linguistic thinking – it simply reflects antecedently existing conceptual distinctions. We here show that Li and Gleitman did not make a crucial distinction between frames of spatial reference relevant to our line of research. We report a series of experiments designed to show that they have, as a consequence, misinterpreted the results of their own experiments, which are in fact in line with our hypothesis. Their attempts to reinterpret the large cross-cultural study, and to enlist support from animal and infant studies, fail for the same reasons. We further try to discern exactly what theory drives their presumption that language can have no cognitive efficacy, and conclude that their position is undermined by a wide range of considerations.}
}
@incollection{SHEN20231,
title = {Interdisciplinary science learning},
editor = {Robert J Tierney and Fazal Rizvi and Kadriye Ercikan},
booktitle = {International Encyclopedia of Education (Fourth Edition)},
publisher = {Elsevier},
edition = {Fourth Edition},
address = {Oxford},
pages = {1-9},
year = {2023},
isbn = {978-0-12-818629-9},
doi = {https://doi.org/10.1016/B978-0-12-818630-5.13030-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128186305130301},
author = {Ji Shen and Changzhao Wang},
keywords = {Cross-disciplinary learning, Integrated learning, Interdisciplinary science learning, Interdisciplinary understanding, Interdisciplinary practices, Knowledge integration, Multidisciplinary learning, STEM education, STEAM education},
abstract = {This article presents a conceptual review of studies and programs related to interdisciplinary science learning in different boundary-crossing scenarios including within sciences, across STEM, and with non-STEM fields. Specific examples are also included to illuminate the four core interdisciplinary practices, namely, translation, transfer, integration, and transformation, that cut across these interdisciplinary learning contexts. The article also discusses challenges for interdisciplinary science learning and strategies proposed to address these challenges. More empirical studies are called to test the effectiveness of these strategies to facilitate and assess interdisciplinary science learning in different domains and contexts.}
}
@article{VARAS2023101289,
title = {Teachers’ strategies and challenges in teaching 21st century skills: Little common understanding},
journal = {Thinking Skills and Creativity},
volume = {48},
pages = {101289},
year = {2023},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2023.101289},
url = {https://www.sciencedirect.com/science/article/pii/S1871187123000597},
author = {Diego Varas and Macarena Santana and Miguel Nussbaum and Susana Claro and Patricia Imbarack},
keywords = {21st century skills, In-service teacher perceptions, Teacher education, Teaching practice, 6 Cs, 4 Cs},
abstract = {Faced with a world of accelerating change and rapidly-evolving technology, education systems must provide students with the skills they need to succeed in the 21st century. However, many countries have failed to incorporate the teaching of these skills within their schools. Our study therefore looks to portray teachers' understanding, strategies and obstacles in teaching these skills across Latin American classrooms. To do so, we analyzed the responses to an online survey from 1391 active teachers across 20 countries in the region. This revealed varying understandings of 21st century skills, with little common understanding. Most teachers failed to mention the skills included in the most popular framework (the 4 Cs); those who did reported using the same strategies, regardless of the skill being taught. These strategies included project-based learning, oracy activities, literacy strategies, and teamwork. We conclude that there is little or no common understanding around these skills, nor the best strategies for developing them. Our study helps understand the potential causes preventing the teaching of these skills in the classroom, a problem that extends beyond Latin America.}
}
@article{20241231,
title = {In This Issue},
journal = {Biological Psychiatry: Cognitive Neuroscience and Neuroimaging},
volume = {9},
number = {12},
pages = {1231},
year = {2024},
issn = {2451-9022},
doi = {https://doi.org/10.1016/j.bpsc.2024.10.008},
url = {https://www.sciencedirect.com/science/article/pii/S2451902224003070}
}
@article{RAMIREZPEDRAZA2020293,
title = {A bio-inspired model of behavior considering decision-making and planning, spatial attention and basic motor commands processes},
journal = {Cognitive Systems Research},
volume = {59},
pages = {293-303},
year = {2020},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2019.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S138904171930508X},
author = {Raymundo Ramirez-Pedraza and Natividad Vargas and Carlos Sandoval and Juan Luis {del Valle-Padilla} and Félix Ramos},
keywords = {Brain model, Decision-making, Planning, Spatial attention, Motor system, Goal-driven},
abstract = {Cognitive architectures (CA) are an IA approach to implement computer systems with human-like behavior. Fundamental exhibited human capabilities include planning and decision-making. In that regard, numerous AI systems successfully exhibit human-like behavior but are limited to either achieving specific objectives or are restrained to too heavily constrained environments, which makes them unsuitable in the presence of unforeseen situations where autonomy is required. To try to alleviate the problem, we present a bio-inspired computational model to solve the autonomous navigation problem of a computational entity in a controlled context. This proposal is the result of the interaction between planning and decision-making, spatial attention and the motor cognitive functions. The proposed model is based on neuroscientific evidence concerning the involved cognitive functions and is part of a more general cognitive architecture. In the case study developed to validate our idea, we can see that the processes previously identified play an important role to accomplish spatial navigation. In the case study presented, an agent achieves the navigation over an unexplored maze from an initial to a final position successfully. The reunited results motivate us to continue improving our model considering attentional information to influence the agent’s motor behavior.}
}
@article{MARCHAND1995179,
title = {Policy analysis as a tool for habitat restoration: A case study of a Danube river floodplain, Hungary},
journal = {Water Science and Technology},
volume = {31},
number = {8},
pages = {179-186},
year = {1995},
note = {Integrated Water Resources Management},
issn = {0273-1223},
doi = {https://doi.org/10.1016/0273-1223(95)00399-8},
url = {https://www.sciencedirect.com/science/article/pii/0273122395003998},
author = {M. Marchand* and E.C.L. Marteijn** and P. Bakonyi***},
keywords = {Floodplain rehabilitation, Danube, policy analysis, water quality modelling},
abstract = {This paper will elaborate a policy analysis approach especially designed for habitat restoration. It will be illustrated by a case study example of a floodplain area along the Danube river, Hungary. The case study used hydrodynamic and water quality models and expertise from a range of disciplines. This made it possible to unravel the complex relations between the environment and human interventions. Crucial was the participation of local experts in the design and screening of measures, as well as the feedback from local interest groups at several occasions during the project. This resulted in the formulation of rehabilitation ideas, most of which have hitherto not been discussed. The combination of creative thinking with practical possibilities and limitations has been worked out in a cyclic process from which three different alternatives emerged. These have been analyzed for their feasibility with regard to the goals to be achieved, their costs and their impacts on other interests.}
}
@incollection{CLEEREMANS20012584,
title = {Conscious and Unconscious Processes in Cognition},
editor = {Neil J. Smelser and Paul B. Baltes},
booktitle = {International Encyclopedia of the Social & Behavioral Sciences},
publisher = {Pergamon},
address = {Oxford},
pages = {2584-2589},
year = {2001},
isbn = {978-0-08-043076-8},
doi = {https://doi.org/10.1016/B0-08-043076-7/03560-9},
url = {https://www.sciencedirect.com/science/article/pii/B0080430767035609},
author = {A. Cleeremans},
abstract = {Characterizing the relationships between conscious and unconscious processes is one of the most important and long-standing goals of cognitive psychology. Renewed interest in the nature of consciousness—long considered not to be scientifically explorable—as well as the increasingly widespread availability of functional brain-imaging techniques, now offer the possibility of detailed exploration of the neural, behavioral, and computational correlates of conscious and unconscious cognition. This article reviews some of the relevant experimental work, highlights the methodological challenges involved in establishing the extent to which cognition can occur unconsciously, and situates ongoing debates in the theoretical context provided by current thinking about consciousness.}
}
@article{OSCARIDO2023539,
title = {The impact of competitive FPS video games on human's decision-making skills},
journal = {Procedia Computer Science},
volume = {216},
pages = {539-546},
year = {2023},
note = {7th International Conference on Computer Science and Computational Intelligence 2022},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.12.167},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922022451},
author = {Juan Oscarido and Zulfikar Airlangga Siswanto and Devin Akwila Maleke and Alexander Agung Santoso Gunawan},
keywords = {decision making, comparison strategy, video games, cognitive skill, influence-of-games, game-based learning},
abstract = {The problem we face today is that many people think that playing games only has a negative impact on a person's brain and behavior. but the fact is that playing games has a positive impact in many ways. The aim of this document is to prove whether video games can really influence human behavior on their decision-making skills. We will test 22 respondents directly who are teenagers and adults around 17 - 25 years old, and we will score them after they have finished playing games with the genre that we decided. The results proved that competitive First-person shooter (FPS) games increase human ability to make decisions quickly and correctly. Many of our participants agree that after playing competitive FPS games, they feel a positive impact on their cognitive skills. Our participants said that they can quickly compare the impact of the decisions they make and choose exactly which is the best course of action.}
}
@article{GORCUN2025110793,
title = {Strategic tour operator selection in the tourism sector using a quantum picture fuzzy rough set-based multi-criteria decision-making approach},
journal = {Engineering Applications of Artificial Intelligence},
volume = {153},
pages = {110793},
year = {2025},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2025.110793},
url = {https://www.sciencedirect.com/science/article/pii/S0952197625007936},
author = {Ömer Faruk Görçün and Dragan Pamucar and Hasan Dinçer and Serhat Yüksel and Ismail İyigün and Vladimir Simic},
keywords = {Strategic selection, Tour operators, Tourism industry, Quantum picture fuzzy rough sets, Decision-making trial and evaluation laboratory},
abstract = {Tour operator selection is critical for ensuring high-quality services, customer satisfaction, and sustainable tourism development. However, traditional decision-making methods often fail to address the complexities and uncertainties involved in this process. This study introduces a robust decision-making framework that integrates quantum picture fuzzy rough sets (QPFR) with advanced Multi-Criteria Decision-Making (MCDM) techniques to enhance the evaluation and selection of tour operators. The methodology incorporates QPFR, the Decision-Making Trial and Evaluation Laboratory (DEMATEL), and the Technique for Order Preference by Similarity to Ideal Solution (TOPSIS) to assess and rank seven prominent tour operators in the Turkish tourism sector. The evaluation is based on 16 comprehensive criteria: quality, safety, environmental impact, authenticity, and economic contribution. Expert inputs and artificial intelligence techniques were utilized to ensure the model's reliability and accuracy. The findings reveal that the proposed model effectively minimizes uncertainties, provides consistent rankings, and highlights the critical importance of specific criteria in decision-making. Sensitivity analysis confirms the robustness of the results, demonstrating the model's applicability to dynamic and complex decision-making contexts. This study offers theoretical contributions and practical insights for decision-makers, emphasizing the value of integrating advanced computational methods to support sustainable tourism development.}
}
@article{AYDIN2013173,
title = {A swarm intelligence based sample average approximation algorithm for the capacitated reliable facility location problem},
journal = {International Journal of Production Economics},
volume = {145},
number = {1},
pages = {173-183},
year = {2013},
issn = {0925-5273},
doi = {https://doi.org/10.1016/j.ijpe.2012.10.019},
url = {https://www.sciencedirect.com/science/article/pii/S0925527312004604},
author = {Nezir Aydin and Alper Murat},
keywords = {Reliable, Facility location, Stochastic programming, Sample average approximation, Swarm intelligence},
abstract = {We present a novel hybrid method, swarm intelligence based sample average approximation (SIBSAA), for solving the capacitated reliable facility location problem (CRFLP). The CRFLP extends the well-known capacitated fixed-cost facility problem by accounting for the unreliability of facilities. The standard SAA procedure, while effectively used in many applications, can lead to poor solution quality if the selected sample sizes are not sufficiently large. With larger sample sizes, however, the SAA method is not practical due to the significant computational effort required. The proposed SIBSAA method addresses this limitation by using smaller samples and repetitively applying the SAA method while injecting social learning in the solution process inspired by the swarm intelligence of particle swarm optimization. We report on experimental study results showing that the SIBSAA improves the computational efficiency significantly while attaining same or better solution quality than the SAA method.}
}
@article{HAN2014106,
title = {Toward an understanding of the impact of production pressure on safety performance in construction operations},
journal = {Accident Analysis & Prevention},
volume = {68},
pages = {106-116},
year = {2014},
note = {Systems thinking in workplace safety and health},
issn = {0001-4575},
doi = {https://doi.org/10.1016/j.aap.2013.10.007},
url = {https://www.sciencedirect.com/science/article/pii/S0001457513004041},
author = {SangUk Han and Farzaneh Saba and SangHyun Lee and Yasser Mohamed and Feniosky Peña-Mora},
keywords = {Safety, Systems thinking, Accident prevention, Simulation, Causal loop analysis},
abstract = {It is not unusual to observe that actual schedule and quality performances are different from planned performances (e.g., schedule delay and rework) during a construction project. Such differences often result in production pressure (e.g., being pressed to work faster). Previous studies demonstrated that such production pressure negatively affects safety performance. However, the process by which production pressure influences safety performance, and to what extent, has not been fully investigated. As a result, the impact of production pressure has not been incorporated much into safety management in practice. In an effort to address this issue, this paper examines how production pressure relates to safety performance over time by identifying their feedback processes. A conceptual causal loop diagram is created to identify the relationship between schedule and quality performances (e.g., schedule delays and rework) and the components related to a safety program (e.g., workers’ perceptions of safety, safety training, safety supervision, and crew size). A case study is then experimentally undertaken to investigate this relationship with accident occurrence with the use of data collected from a construction site; the case study is used to build a System Dynamics (SD) model. The SD model, then, is validated through inequality statistics analysis. Sensitivity analysis and statistical screening techniques further permit an evaluation of the impact of the managerial components on accident occurrence. The results of the case study indicate that schedule delays and rework are the critical factors affecting accident occurrence for the monitored project.}
}
@article{JACOB2020102142,
title = {Neural correlates of rumination in major depressive disorder: A brain network analysis},
journal = {NeuroImage: Clinical},
volume = {25},
pages = {102142},
year = {2020},
issn = {2213-1582},
doi = {https://doi.org/10.1016/j.nicl.2019.102142},
url = {https://www.sciencedirect.com/science/article/pii/S2213158219304887},
author = {Yael Jacob and Laurel S Morris and Kuang-Han Huang and Molly Schneider and Sarah Rutter and Gaurav Verma and James W Murrough and Priti Balchandani},
keywords = {Default mode network, Depression, Entropy, Graph Theory, High-field MRI, Precuneus},
abstract = {Patients with major depressive disorder (MDD) exhibit higher levels of rumination, i.e., repetitive thinking patterns and exaggerated focus on negative states. Rumination is known to be associated with the cortical midline structures / default mode network (DMN) region activity, although the brain network topological organization underlying rumination remains unclear. Implementing a graph theoretical analysis based on ultra-high field 7-Tesla functional MRI data, we tested whether whole brain network connectivity hierarchies during resting state are associated with rumination in a dimensional manner across 20 patients with MDD and 20 healthy controls. Applying this data-driven approach we found a significant correlation between rumination tendency and connectivity strength degree of the right precuneus, a key node of the DMN. In order to interrogate this region further, we then applied the Dependency Network Analysis (DEPNA), a recently developed method used to quantify the connectivity influence of network nodes. This revealed that rumination was associated with lower connectivity influence of the left medial orbito-frontal cortex (MOFC) cortex on the right precuneus. Lastly, we used an information theory entropy measure that quantifies the cohesion of a network's correlation matrix. We show that subjects with higher rumination scores exhibit higher entropy levels within the DMN i.e. decreased overall connectivity within the DMN. These results emphasize the general DMN involvement during self-reflective processing related to maladaptive rumination in MDD. This work specifically highlights the impact of the MOFC on the precuneus, which might serve as a target for clinical neuromodulation treatment.}
}
@article{SINGH2023103044,
title = {A survey of mobility-aware Multi-access Edge Computing: Challenges, use cases and future directions},
journal = {Ad Hoc Networks},
volume = {140},
pages = {103044},
year = {2023},
issn = {1570-8705},
doi = {https://doi.org/10.1016/j.adhoc.2022.103044},
url = {https://www.sciencedirect.com/science/article/pii/S1570870522002165},
author = {Ramesh Singh and Radhika Sukapuram and Suchetana Chakraborty},
keywords = {Mobility, Multi-access Edge Computing, Task offloading, Service migration, Content caching, Resource allocation},
abstract = {Many mobile and pervasive applications avail cloud services to reduce overheads in on-device computation. The performance of these services depends on the available bandwidth of the underlying network, the physical proximity of the cloud server and the end devices, the volume of data, the computational capacity of the server, and, importantly, the mobility of the devices hosting the applications. Edge computing promises to provide better performance by bringing services (e.g., a video streaming service) from the cloud to servers near the user. It also enables partial or full offloading of the computation (tasks) and storage functionalities from the User Equipment (UE) to the edge of the network. This saves power and benefits from relatively more powerful devices at the edge. Multi-access Edge Computing (MEC), which supports wireless and wired access technologies, has gained significant research interest. When UEs move, services must continue to operate, tasks may need to be offloaded again, and states related to tasks and services may need to be migrated. In this paper, we focus on four functional components (task/service offloading, resource allocation, content/task caching, and service/task migration) of MEC. We survey the challenges to these and their solutions in the context of UE mobility. Mobility creates challenges during offloading resource-intensive tasks as the user may move while the task is being offloaded. Some of the other challenges are how to jointly allocate computing and communication resources, minimize service down time during migration, and share the backhaul network if the same MEC host must continue to be used. Some key research areas include intelligent task offloading and service migration algorithms, exploiting group mobility to improve task migration time, studying the interplay of MEC parameters such as capabilities of the target MEC host, etc. In addition, predicting the mobile trajectory through intelligent methods and implementations with datasets from real-world scenarios are required. We compare this paper on 11 parameters (service migration, task offloading, resource allocation, content caching, mobility, use cases, architecture, computing paradigm, mobility model, system model, virtualization/Software Defined Networks) with 31 other survey papers from 2018 to April 2022 in MEC and related domains. We discuss the Edge Computing paradigm, the system architecture and model descriptions, and use cases. We briefly explain the relevant challenges and future directions in emerging domains, such as the Internet of drones and Digital twins. We also discuss future research directions in task/service migration, offloading, resource management, distributed computing, reliability, and Quality of Service, all related to mobility in MEC.}
}
@article{LAWLER1996241,
title = {Thinkable models},
journal = {The Journal of Mathematical Behavior},
volume = {15},
number = {3},
pages = {241-259},
year = {1996},
issn = {0732-3123},
doi = {https://doi.org/10.1016/S0732-3123(96)90004-8},
url = {https://www.sciencedirect.com/science/article/pii/S0732312396900048},
author = {Robert W. Lawler},
abstract = {A primary objective of technical education should be the development of thinkable models in the minds of students. Thinkable models are representations of things and processes simple enough that people can use them in thought experiments. The organization of cognitive structures for technical domains can be imagined to be a network of appropriately connected thinkable models. Artificial intelligence (AI), as the science of representations, has focused in the main on languagelike representations. If we can enrich our vision of representations to include a greater variety of ways of thinking that are useful to people, we may hope to broaden access to scientific ideas. To pursue these notions in some detail, a taxonomy of models is developed and the issue of how representations relate to human modes of perception and action is raised. The notions are explored first through contrasting of several approaches to the Pythagorean Theorem.}
}
@incollection{GORI2018122,
title = {Chapter 3 - Linear Threshold Machines},
editor = {Marco Gori},
booktitle = {Machine Learning},
publisher = {Morgan Kaufmann},
pages = {122-184},
year = {2018},
isbn = {978-0-08-100659-7},
doi = {https://doi.org/10.1016/B978-0-08-100659-7.00003-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780081006597000038},
author = {Marco Gori},
keywords = {Linear machines, Least mean square, Linear-threshold machines, Normal equations, Ridge regression, Linear separability, Predicate order, Gradient descent, Perceptron algorithm, Terminal attractors},
abstract = {One of the simplest ways of modeling the interactions of intelligent agents with the environment is to expose them to a collection of supervised pairs (example, target). This chapter is about the learning mechanisms that arise from the assumption of dealing with linear and linear-threshold machines. In most cases, the covered topics nicely intercept different disciplines, and are of remarkable importance to better grasp many approaches to machine learning. The chapter covers classic topics, like normal equations and ridge regression, as well as representational issues in pattern recognition that are connected with the notion of predicate order. Linear-threshold machines are described along with related computational geometry issues, and the view that arises from Bayesian decision. Classic gradient learning algorithms, including the stochastic version, are described in the continuum setting, as well as the Rosenblatt perceptron algorithm. Finally, some complexity issues are covered in both the discrete and continuous setting of computation.}
}
@article{J2023105690,
title = {Deep learning based multi-labelled soil classification and empirical estimation toward sustainable agriculture},
journal = {Engineering Applications of Artificial Intelligence},
volume = {119},
pages = {105690},
year = {2023},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2022.105690},
url = {https://www.sciencedirect.com/science/article/pii/S0952197622006807},
author = {Padmapriya J. and Sasilatha T.},
keywords = {Soil classification, Q-HOG, SVM, Deep Neural Network, VGG16},
abstract = {Agriculture is the underlying occupation of the vast people in India and it is a major economic contribution. Soil is prime for the vital nutrient supply to the crops and its yield. Determination of the type of soil which comprises of the clay, sand and silt particles in the respective proportion is indeed significant for the suitable crop selection and to identify the weeds growth. The most commonly utilized soil determination methods were International Pipette method and Pressure-plate apparatus method. In this research work, multiclass soil classification using machine learning and deep learning models for the appropriate determination of the soil type as Multi-Stacking ensemble model and a novel feature selection algorithm Q-HOG is proposed; since the Artificial Intelligence has led to furtherance in the smart agriculture. Besides, the images are collected from the exploration site vriddhachalam along with the soil datasets will increase the classification accuracy. The deep learning models Recurrent Neural Network(RNN), Long Short Term Memory(LSTM), Gated Recurrent Unit(GRU) and VGG16 are considered and the comprehensive evaluation of these different deep learning architectures and also the machine learning algorithms such as Naïve-bayes, KNN, SVM are carried out and the obtained results are tabulated. Multi-stacking ensemble model for multi-classification is proposed with the Machine learning and deep learning algorithms and evaluated the performance with increased computation time. Among these models the proposed model outperformed in soil classification in-terms of accuracy as 98.96 percent, achieved precision as 96.14 percent, recall as 99.65 percent and the achieved F1-Score is 97.87 percent.}
}
@article{SPIVEY2025149477,
title = {A linking hypothesis for eyetracking and mousetracking in the visual world paradigm},
journal = {Brain Research},
volume = {1851},
pages = {149477},
year = {2025},
issn = {0006-8993},
doi = {https://doi.org/10.1016/j.brainres.2025.149477},
url = {https://www.sciencedirect.com/science/article/pii/S0006899325000356},
author = {Michael J. Spivey},
keywords = {Psycholinguistics, Eyetracking, Mousetracking, Spoken word recognition, Action-perception cycle, Perception–action cycle, Dynamical systems, Embodied cognition},
abstract = {For a linking hypothesis in the visual world paradigm to clearly accommodate existing findings and make unambiguous predictions, it needs to be computationally implemented in a fashion that transparently draws the causal connection between the activations of internal representations and the measured output of saccades and reaching movements. Quantitatively implemented linking hypotheses provide an opportunity to not only demonstrate an existence proof of that causal connection but also to test the fidelity of the measuring methods themselves. When a system of interest is measured one way (e.g., ballistic dichotomous outputs) or another way (e.g., smooth graded outputs), the apparent results can differ substantially. What is needed is one linking hypothesis that can produce both types of outputs. The localist attractor network simulation of spoken word recognition demonstrated here recreates eye and mouse movements that capture key findings in the visual world paradigm, and especially relies on one particularly powerful theoretical construct: feedback from the action-perception cycle. Visual feedback from the eye position enhancing the cognitive prominence of the fixated object allows the simulation to fit a wider range of findings, and points to predictions for new experiments. When that feedback is absent, the linking hypothesis simulation no longer fits human data as well. Future experiments, and improvements of this network simulation, are discussed.}
}
@article{PATTEE20025,
title = {The origins of Michael Conrad's research programs (1964–1979)},
journal = {Biosystems},
volume = {64},
number = {1},
pages = {5-11},
year = {2002},
issn = {0303-2647},
doi = {https://doi.org/10.1016/S0303-2647(01)00169-1},
url = {https://www.sciencedirect.com/science/article/pii/S0303264701001691},
author = {H.H Pattee},
keywords = {Artificial ecosystems, Adaptability, Evolvability, Non-programmable computation, Brain-computer disanalogy, Enzymatic neurons},
abstract = {This paper summarizes Michael Conrad's academic and professional career from the time he began his Ph.D. studies in 1964 to his appointment at Wayne State University in 1979. It describes the origins of several of his major research interests and presents a personal evaluation of how this early work continues to be of fundamental importance.}
}
@article{ZHANG2024308,
title = {Empathetic Language in LLMs under Prompt Engineering: A Comparative Study in the Legal Field},
journal = {Procedia Computer Science},
volume = {244},
pages = {308-317},
year = {2024},
note = {6th International Conference on AI in Computational Linguistics},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.10.204},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924030059},
author = {Yifan Zhang and Christopher Radishian and Sabine Brunswicker and Dan Whitenack and Daniel W. Linna},
keywords = {LLM, Human-AI Interaction, Empathetic Response},
abstract = {The demand for empathetic conversations increases with conversational AIs’ rise and exponentially spreading applications. In areas like law and healthcare, where professional and empathetic conversations are essential, conversational AIs must strive to retain the correctness of information and logic while improving on empathetic language use. When addressing such an issue, we focus on linguistic empathy, relating only to syntactic and rhetoric choices in language while disregarding the emotional aspect of influence. By performing this study, we are interested in finding whether current open-sourced Large Language Models (LLMs) can match human experts in the legal field by using empathetic language while not compromising facts and logic in responses. We compare responses from three open-sourced LLMs under four prompting strategies with the expert responses. In the comparison, we use metrics from three aspects: text and semantic similarity, factual consistency, and ten rules of linguistic empathy from previous research literature. After statistical tests, the comparison results show that language models can use empathetic language without compromising the default knowledge base of LLMs when properly prompt-engineered. To accomplish this, additional domain knowledge is still needed to match factually. The data supporting this study is publicly available at huggingface.co/datasets/RCODI/empathy-prompt and code is available at github.com/RCODI-ConversationalAI/Empathy-Prompt.}
}
@article{HUANG2011183,
title = {On the intrinsic inevitability of cancer: From foetal to fatal attraction},
journal = {Seminars in Cancer Biology},
volume = {21},
number = {3},
pages = {183-199},
year = {2011},
note = {Why Systems Biology and Cancer?},
issn = {1044-579X},
doi = {https://doi.org/10.1016/j.semcancer.2011.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S1044579X11000320},
author = {Sui Huang},
keywords = {Tumorigenesis, Tumour progression, Epigenetic landscape, Gene regulatory network, Attractor, State space, Somatic mutation, Oncogene},
abstract = {The cracks in the paradigm of oncogenic mutations and somatic evolution as driving force of tumorigenesis, lucidly exposed by the dynamic heterogeneity of “cancer stem cells” or the diffuse results of cancer genome sequencing projects, indicate the need for a more encompassing theory of cancer that reaches beyond the current proximate explanations based on individual genetic pathways. One such integrative concept, derived from first principles of the dynamics of gene regulatory networks, is that cancerous cell states are attractor states, just like normal cell types are. Here we extend the concept of cancer attractors to illuminate a more profound property of cancer initiation: its inherent inevitability in the light of metazoan evolution. Using Waddington's Epigenetic Landscape as a conceptual aid, for which we present a mathematical and evolutionary foundation, we propose that cancer is intrinsically linked to ontogenesis and phylogenesis. This explanatory rather than enumerating review uses a formal argumentation structure that is atypical in modern experimental biology but may hopefully offer a new coherent perspective to reconcile many conflicts between new findings and the old thinking in the categories of linear oncogenic pathways.}
}
@incollection{ROSCHELLE20071,
title = {Designing Networked Handheld Devices to Enhance School Learning},
editor = {Marvin V. Zelkowitz},
series = {Advances in Computers},
publisher = {Elsevier},
volume = {70},
pages = {1-60},
year = {2007},
issn = {0065-2458},
doi = {https://doi.org/10.1016/S0065-2458(06)70001-8},
url = {https://www.sciencedirect.com/science/article/pii/S0065245806700018},
author = {Jeremy Roschelle and Charles Patton and Deborah Tatar},
abstract = {Handheld devices, especially networked handheld devices, are growing in importance in education, largely because their affordability and accessibility create an opportunity for educators to transition from occasional, supplemental use of computers, to frequent and integral use of portable computational technology. Why and how might these new devices enhance school learning? We begin by discussing a simple but important factor: networked handhelds can allow a 1:1 student:device ratio for the first time, enabling ready-at-hand access to technology throughout the school day and throughout the learner's personal life. We argue that designers need to understand the capabilities of the new generation of handheld computers and wireless networks that are most relevant for learning. We follow this with a discussion of Learning Science theories that connect those capabilities to enhanced learning. The capabilities and features feed into design practices. We describe a set of example applications that are arising from the capabilities, theories and design practices previously described. Finally, we close with a discussion of the challenge of scale.}
}
@article{VANLAAR2025106259,
title = {Towards desirable futures for the circular adaptive reuse of buildings: A participatory approach},
journal = {Sustainable Cities and Society},
volume = {122},
pages = {106259},
year = {2025},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2025.106259},
url = {https://www.sciencedirect.com/science/article/pii/S2210670725001362},
author = {Brian {van Laar} and Angela Greco and Hilde Remøy and Vincent Gruis and Mohammad B. Hamida},
keywords = {Adaptive reuse, Scenario development, Cross-impact balance analysis, Participatory scenario workshops, Normative narrative scenarios, Circularity},
abstract = {Adaptive reuse of buildings offers a sustainable strategy for reducing global CO2 emissions by repurposing existing structures, conserving resources, reducing the need to extract new materials, and minimizing waste. However, the decision-making process in adaptive reuse projects is often complex, involving conflicting criteria and diverse stakeholders. Current approaches tend to polarize alternatives, focusing either on broad functional use or specific design options, which can limit decision effectiveness and quality. This study addresses these challenges by developing a participatory mixed-methods approach that integrates Cross-Impact Balance (CIB) analysis with creative scenario-building techniques, including generative AI and participatory workshops. This approach balances the extremes of current decision-making processes, offering a more comprehensive overview of desirable futures for decision-makers. The methodology was applied to create 15 “big picture” circular adaptive reuse scenarios, each incorporating circular building adaptability (CBA) strategies, and enriched with AI generated narratives and visualizations. These scenarios provide stakeholders with a nuanced understanding of potential future pathways, enhancing decision-making processes. This mixed-method approach demonstrates the potential of participatory CIB scenario development in advancing circularity, offering a valuable tool for navigating the complexities of adaptive reuse decision-making.}
}
@incollection{SAHU2022127,
title = {Artificial Intelligence and Machine Learning: New Age Tools for Augmenting Plastic Materials Designing, Processing, and Manufacturing},
editor = {M.S.J. Hashmi},
booktitle = {Encyclopedia of Materials: Plastics and Polymers},
publisher = {Elsevier},
address = {Oxford},
pages = {127-152},
year = {2022},
isbn = {978-0-12-823291-0},
doi = {https://doi.org/10.1016/B978-0-12-820352-1.00108-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128203521001085},
author = {Kisor Kumar Sahu and Shibu Meher and Abhilash M. Menon and M.K. Sridhar and Gangala V. {Harsha Vardhan} and Saurabh Pandey and Ashutosh Kumar and Shreeja Das},
keywords = {Artificial intelligence (AI), Artificial neural network (ANN), Autoencoders, Deep learning, Machine learning (ML), Principal component analysis (PCA)},
abstract = {Plastic and polymers are late entrants in the repository of engineering materials, compared to bronze and iron (early phases of human civilizations are named after them). However, the extent of usage of the former is growing at an exponential rate because of the near-infinite combinatorial possibilities. In fact, there are hardly few engineering disciplines that can potentially offer so large and endless unexploited possibilities. Plastic industries are widespread across the world due to their easy scalability, favorable economics and extremely diverse applications. Plastic manufacturing and recycling are especially important for the economy of a country and provide livelihood for a large population. It is imperative that the current processes in plastic be improved upon by the advantages offered by computational tools and digital technologies. At this stage, we desperately need new age tools that can properly guide the human enterprise of innovation in designing, perfection in processing while maintaining stringent quality requirements in manufacturing. Artificial intelligence (AI) and machine learning (ML) perfectly fits this bill for the new age tools. We are at a very nascent stage of this AI/ML revolution. This article samples some of the pioneering works from the very discreet space of AI/ML applications in the field of plastic and polymer designing, processing and manufacturing and attempts to tie them up in a cohesive narrative. For the sake of completeness, applications of AI/ML for limiting the adverse environmental impact and future outlook have also been covered.}
}
@article{HOLZINGER2025103032,
title = {Enhancing trust in automated 3D point cloud data interpretation through explainable counterfactuals},
journal = {Information Fusion},
volume = {119},
pages = {103032},
year = {2025},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2025.103032},
url = {https://www.sciencedirect.com/science/article/pii/S1566253525001058},
author = {Andreas Holzinger and Niko Lukač and Dzemail Rozajac and Emile Johnston and Veljka Kocic and Bernhard Hoerl and Christoph Gollob and Arne Nothdurft and Karl Stampfer and Stefan Schweng and Javier {Del Ser}},
keywords = {Explainable AI, Point cloud data, Counterfactual reasoning, Information fusion, Interpretability, Human-centered AI},
abstract = {This paper introduces a novel framework for augmenting explainability in the interpretation of point cloud data by fusing expert knowledge with counterfactual reasoning. Given the complexity and voluminous nature of point cloud datasets, derived predominantly from LiDAR and 3D scanning technologies, achieving interpretability remains a significant challenge, particularly in smart cities, smart agriculture, and smart forestry. This research posits that integrating expert knowledge with counterfactual explanations – speculative scenarios illustrating how altering input data points could lead to different outcomes – can significantly reduce the opacity of deep learning models processing point cloud data. The proposed optimization-driven framework utilizes expert-informed ad-hoc perturbation techniques to generate meaningful counterfactual scenarios when employing state-of-the-art deep learning architectures. The optimization process minimizes a multi-criteria objective comprising counterfactual metrics such as similarity, validity, and sparsity, which are specifically tailored for point cloud datasets. These metrics provide a quantitative lens for evaluating the interpretability of the counterfactuals. Furthermore, the proposed framework allows for the definition of explicit interpretable counterfactual perturbations at its core, thereby involving the audience of the model in the counterfactual generation pipeline and ultimately, improving their overall trust in the process. Results demonstrate a notable improvement in both the interpretability of the model’s decisions and the actionable insights delivered to end-users. Additionally, the study explores the role of counterfactual reasoning, coupled with expert input, in enhancing trustworthiness and enabling human-in-the-loop decision-making processes. By bridging the gap between complex data interpretations and user comprehension, this research advances the field of explainable AI, contributing to the development of transparent, accountable, and human-centered artificial intelligence systems.}
}
@article{MOHIT20221713,
title = {Approach of artificial intelligence for analysing properties of concrete},
journal = {Materials Today: Proceedings},
volume = {48},
pages = {1713-1717},
year = {2022},
note = {SCPINM-2021},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2021.10.028},
url = {https://www.sciencedirect.com/science/article/pii/S221478532106507X},
author = { Mohit and Balwinder Lallotra},
keywords = {Concrete, Artificial intelligence, Artificial neural network, Workability, Compressive strength},
abstract = {Technological progress is often measured by computation power. At the moment we are in the golden age where we are blessed with a perfect trio, machine learning algorithms, huge datasets across disciplines, and processing hardware. The constant desire to understand the human brain has led us to try mimicking it, thus forming the basis of neural networks creating a way for deep learning algorithms. Such algorithms have proven to work on non-linear data sets effectively, generating results that could find patterns just like our brains. In this paper, we explore a recently rising application for Neural Network frameworks; in particular, concrete in basic designing. We design and implement tests to analyze various properties of concrete of different concrete mixes. Customarily, the ability of concrete to perform is influenced by numerous non-straight factors, and testing its quality includes the destructive procedure of concrete samples.}
}
@incollection{SADEGHI2024457,
title = {Chapter Thirteen - Dynamic framework for large-scale modeling of membranes and peripheral proteins},
editor = {Markus Deserno and Tobias Baumgart},
series = {Methods in Enzymology},
publisher = {Academic Press},
volume = {701},
pages = {457-514},
year = {2024},
booktitle = {Biophysical Approaches for the Study of Membrane Structure—Part B: Theory and Simulations},
issn = {0076-6879},
doi = {https://doi.org/10.1016/bs.mie.2024.03.018},
url = {https://www.sciencedirect.com/science/article/pii/S0076687924001137},
author = {Mohsen Sadeghi and David Rosenberger},
keywords = {Membranes, Mesoscopic modeling, Particle-based modeling, Hydrodynamics, Membrane-protein interaction},
abstract = {In this chapter, we present a novel computational framework to study the dynamic behavior of extensive membrane systems, potentially in interaction with peripheral proteins, as an alternative to conventional simulation methods. The framework effectively describes the complex dynamics in protein-membrane systems in a mesoscopic particle-based setup. Furthermore, leveraging the hydrodynamic coupling between the membrane and its surrounding solvent, the coarse-grained model grounds its dynamics in macroscopic kinetic properties such as viscosity and diffusion coefficients, marrying the advantages of continuum- and particle-based approaches. We introduce the theoretical background and the parameter-space optimization method in a step-by-step fashion, present the hydrodynamic coupling method in detail, and demonstrate the application of the model at each stage through illuminating examples. We believe this modeling framework to hold great potential for simulating membrane and protein systems at biological spatiotemporal scales, and offer substantial flexibility for further development and parametrization.}
}
@article{JIANG2024100078,
title = {Human-AI interaction research agenda: A user-centered perspective},
journal = {Data and Information Management},
volume = {8},
number = {4},
pages = {100078},
year = {2024},
issn = {2543-9251},
doi = {https://doi.org/10.1016/j.dim.2024.100078},
url = {https://www.sciencedirect.com/science/article/pii/S2543925124000147},
author = {Tingting Jiang and Zhumo Sun and Shiting Fu and Yan Lv},
keywords = {Human-AI interaction, Human-AI collaboration, Human-AI competition, Human-AI conflict, Human-AI symbiosis},
abstract = {The rapid growth of artificial intelligence (AI) has given rise to the field of Human-AI Interaction (HAII). This study meticulously reviewed the research themes, theoretical foundations, and methodological frameworks of the HAII field, aiming to construct a comprehensive overview of this field and provide robust support for future investigations. HAII research themes include human-AI collaboration, competition, conflict, and symbiosis. Theories drawn from communication, psychology, and sociology support these studies, while the employed methods include both self-reporting and observational approaches commonly utilized in user studies. It is suggested that future research should broaden its focus to encompass diverse user groups, AI roles, and tasks. Moreover, it is necessary to develop multi-disciplinary theories and integrate multi-level research methods to support the sustained development of the field. This study not only furnishes indispensable theoretical and practical insights for forthcoming research endeavors but also catalyzes the realization of a future distinguished by seamless interaction between humans and AI.}
}
@article{CORDASCO201815,
title = {Distributed MASON: A scalable distributed multi-agent simulation environment},
journal = {Simulation Modelling Practice and Theory},
volume = {89},
pages = {15-34},
year = {2018},
issn = {1569-190X},
doi = {https://doi.org/10.1016/j.simpat.2018.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S1569190X18301230},
author = {Gennaro Cordasco and Vittorio Scarano and Carmine Spagnuolo},
keywords = {Agent-based simulation, Parallel computing, Distributed computing, Scalable computational science, Cloud computing},
abstract = {Computational Social Science (CSS) involves interdisciplinary fields and exploits computational methods, such as social network analysis as well as computer simulation with the goal of better understanding social phenomena. Agent-Based Models (ABMs) represent an effective research tool for CSS and consist of a class of models, which, aim to emulate or predict complex phenomena through a set of simple rules (i.e., independent actions, interactions and adaptation), performed by multiple agents. The efficiency and scalability of ABMs systems are typically obtained distributing the overall computation on several machines, which interact with each other in order to simulate a specific model. Unfortunately, the design of a distributed simulation model is particularly challenging, especially for domain experts who sporadically are computer scientists and are not used to developing parallel code. D-MASON framework is a distributed version of the MASON library for designing and executing ABMs in a distributed environment ensuring scalability and easiness. D-MASON enable the developer to exploit the computing power of distributed environment in a transparent manner; the developer has to do simple incremental modifications to existing MASON models, without re-designing them. This paper presents several novel features and architectural improvements introduced in the D-MASON framework: an improved space partitioning strategy, a distributed 3D field, a distributed network field, a decentralized communication layer, a novel memory consistency mechanism and the integration to cloud environments. Full documentation, additional tutorials, and other material can be found at https://github.com/isislab-unisa/dmason where the framework can be downloaded.}
}
@article{WANG2023458,
title = {The Hutong neighbourhood grammar: A procedural modelling approach to unravel the rationale of historical Beijing urban structure},
journal = {Frontiers of Architectural Research},
volume = {12},
number = {3},
pages = {458-476},
year = {2023},
issn = {2095-2635},
doi = {https://doi.org/10.1016/j.foar.2022.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S2095263523000031},
author = {Yuyang Wang and Andrew Crompton and Asterios Agkathidis},
keywords = {Urban morphology, Siheyuan, Hutong neighbourhood, Procedural modelling, Shape grammar},
abstract = {Hutong neighbourhoods, composed of Chinese courtyard dwellings (Siheyuan), are historically and socially significant urban spaces that embody the traditional Chinese way of life and philosophy. As part of the national heritage, there is an increasing research interest in Hutong neighbourhoods, many of which are facing oblivion. This study presents a formal grammar for Hutong neighbourhood generation. This research investigates traditional principles of urban planning of ancient Beijing, based on examples on the historical map Qianlong Jingcheng Quantu, to derive the lost design rules. These rules are used to build up a procedural modelling framework, which reveals the development of Beijing's urban structure from the Yuan (1271–1368) to the Qing (1644–1911) dynasty. Our findings present a grammar incorporated into the procedural modelling framework to parametrically generate Hutong neighbourhoods, which replicates the morphological characteristics of historic cases. It contributes to the understanding of the generation of Hutong neighbourhoods. In support of heritage sustainability, this grammar can be implemented in a computational environment by visual scripting that enables the generation of new instances of Hutong neighbourhoods, both real and virtual.}
}
@article{COLOM2017385,
title = {Collaborative building of behavioural models based on internet of things},
journal = {Computers & Electrical Engineering},
volume = {58},
pages = {385-396},
year = {2017},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2016.08.019},
url = {https://www.sciencedirect.com/science/article/pii/S0045790616302191},
author = {José Francisco Colom and Higinio Mora and David Gil and María Teresa Signes-Pont},
keywords = {Social internet of things, Big data, Embedded systems, Healthcare, Distributed system framework},
abstract = {This paper proposes a new framework that takes advantage of the computing capabilities provided by the Internet of Thing (IoT) paradigm in order to support collaborative applications. It looks at the requirements needed to run a wide range of computing tasks on a set of devices in the user environment with limited computing resources. This approach contributes to building the social dimension of the IoT by enabling the addition of computing resources accessible to the user without harming the other activities for which the IoT devices are intended. The framework mainly includes a model of the computing load, a scheduling mechanism and a handover procedure for transferring tasks between available devices. The experiments show the feasibility of the approach and compare different implementation alternatives.}
}
@article{COHEN2017208,
title = {Where Does EEG Come From and What Does It Mean?},
journal = {Trends in Neurosciences},
volume = {40},
number = {4},
pages = {208-218},
year = {2017},
issn = {0166-2236},
doi = {https://doi.org/10.1016/j.tins.2017.02.004},
url = {https://www.sciencedirect.com/science/article/pii/S0166223617300243},
author = {Michael X Cohen},
keywords = {EEG, neural microcircuit, oscillations, electrophysiology, computation},
abstract = {Electroencephalography (EEG) has been instrumental in making discoveries about cognition, brain function, and dysfunction. However, where do EEG signals come from and what do they mean? The purpose of this paper is to argue that we know shockingly little about the answer to this question, to highlight what we do know, how important the answers are, and how modern neuroscience technologies that allow us to measure and manipulate neural circuits with high spatiotemporal accuracy might finally bring us some answers. Neural oscillations are perhaps the best feature of EEG to use as anchors because oscillations are observed and are studied at multiple spatiotemporal scales of the brain, in multiple species, and are widely implicated in cognition and in neural computations.}
}
@incollection{AHAMED2017465,
title = {Chapter 29 - The Architecture of a Mind-Machine},
editor = {Syed V. Ahamed},
booktitle = {Evolution of Knowledge Science},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {465-479},
year = {2017},
isbn = {978-0-12-805478-9},
doi = {https://doi.org/10.1016/B978-0-12-805478-9.00029-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128054789000297},
author = {Syed V. Ahamed},
keywords = {Mind, Knowledge, Machines, Technology, Human Needs, Knowledge Windows, Perceptual Spaces},
abstract = {Chapter Summary
In this chapter, we take bold step and propose the unthinkable: The genesis of a Customizable Mind-Machine. Thought that stems from the mind is deeply seated in a biological framework of neurons. The biological origin lies in the marvel of evolution over the eons and refined ever so fast, faster than in the prior centuries. Three (a, b, and c), triadic objects are ceaselessly at work. At a personal level (a) mind, knowledge, and machines have been intertwined like inspiration, words, and language since the dawn of the human evolution and more recently, (b) technology, manufacturing, and economics have formed a hub of progress, (c) wealth, global marketing, and insatiable needs of humans and civilization. These triadic cycles of nine essential objects of human existence are spinning quicker and quicker every year. The Internet offers the mind no choice but to leap and soar over history and over the globe. Alternatively, human mind can sink deeper and deeper into ignorance and oblivion. More recently, the Artificial Intelligence at work in the Internet had challenged the natural intelligence at the cognizance level in the mind to find its way to breakthroughs and innovations. We integrate functions of the mind with the processing of knowledge in the hardware of machines by freely traversing the neural, mental, physical, psychological, social, knowledge, and computational spaces. The laws of neural biology and mind, laws of knowledge and social sciences, and finally the laws of physics and mechanics in each of the spaces are unique and executed by distinctive processors for each space. Much as mind rules over matter, the triad of mind, space, and time creates a human-space that rules over the Relativistic-space of matter, space, and time.}
}
@article{PANG2024121485,
title = {A concept lattice-based expert opinion aggregation method for multi-attribute group decision-making with linguistic information},
journal = {Expert Systems with Applications},
volume = {237},
pages = {121485},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.121485},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423019875},
author = {Kuo Pang and Luis Martínez and Nan Li and Jun Liu and Li Zou and Mingyu Lu},
keywords = {Concept lattice, Linguistic truth-valued lattice implication algebra, Linguistic information processing, Multi-attribute group decision-making},
abstract = {During the multi-attribute group decision-making (MAGDM) processing, the individuals often hold different opinions about the alternatives. It is necessary to aggregate the different individual opinions into a unified group opinion. In the real world, experts sometimes use linguistic expressions to evaluate attributes in uncertain environments. To address the problem of reducing the information loss of expert opinion aggregation in MAGDM, this paper proposes a MAGDM approach based on linguistic concept lattices in the context of uncertain linguistic expression. A linguistic concept lattice for multi-expert linguistic formal context is first constructed based on linguistic truth-valued lattice implication algebra, which can express both comparable and incomparable linguistic information in the decision-making process. Different expert opinions are aggregated via the extent of fuzzy linguistic concepts, which can reduce information loss in the aggregation process. Second, meet-irreducible elements in the linguistic concept lattice are introduced to reduce the computational complexity of obtaining all fuzzy linguistic concepts in the decision-making process. the distance between the intents of different fuzzy linguistic concepts is considered to enhance the rationality of linguistic decision results. In addition, the expert’s decision-making process for each alternative is visualized via linguistic concept lattices. Finally, the case study and comparative analysis illustrate the validity and rationality of the proposed approach in MAGDM with linguistic information.}
}
@article{TERZOPOULOU2024104133,
title = {Iterative voting with partial preferences},
journal = {Artificial Intelligence},
volume = {332},
pages = {104133},
year = {2024},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2024.104133},
url = {https://www.sciencedirect.com/science/article/pii/S0004370224000699},
author = {Zoi Terzopoulou and Panagiotis Terzopoulos and Ulle Endriss},
keywords = {Social choice theory, Iterative voting, Partial preferences},
abstract = {Voting platforms can offer participants the option to sequentially modify their preferences, whenever they have a reason to do so. But such iterative voting may never converge, meaning that a state where all agents are happy with their submitted preferences may never be reached. This problem has received increasing attention within the area of computational social choice. Yet, the relevant literature hinges on the rather stringent assumption that the agents are able to rank all alternatives they are presented with, i.e., that they hold preferences that are linear orders. We relax this assumption and investigate iterative voting under partial preferences. To that end, we define and study two families of rules that extend the well-known k-approval rules in the standard voting framework. Although we show that for none of these rules convergence is guaranteed in general, we also are able to identify natural conditions under which such guarantees can be given. Finally, we conduct simulation experiments to test the practical implications of our results.}
}
@article{WAHYUNINGSIH2024349,
title = {Comparison of Effectiveness of Logistic Regression, Naive Bayes, and Random Forest Algorithms in Predicting Student Arguments},
journal = {Procedia Computer Science},
volume = {234},
pages = {349-356},
year = {2024},
note = {Seventh Information Systems International Conference (ISICO 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.03.014},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924003715},
author = {Tri Wahyuningsih and Danny Manongga and Irwan Sembiring and Sutarto Wijono},
keywords = {Comparison Algorithm, Logistic Regression, Naive Bayes, Random Forest, Student Arguments},
abstract = {Currently, in the process of assessing and giving feedback on students' argumentative writing, educators have to spend a considerable amount of time reading and analyzing each essay individually. This can be a complicated and time-consuming process, especially if the number of students to be assessed is quite large. The problem of this research is to find the most effective algorithm in providing accurate and reliable predictions in the context of evaluation and feedback of students' argumentation. This study compares three algorithms (logistic regression, Naive Bayes, and Random Forest) to predict student argumentation using essays from grades 6-12. Logistic regression performed best with 94.34% accuracy, followed by random forest with 91.98% accuracy, and Naive Bayes with 88.93% accuracy. The study optimized preprocessing and selected algorithms for an automated guidance model. It is the first stage of a three-part study for developing automated guidance models. Data came from Kaggle, and the study aims to improve the accuracy of automated guidance models for student argumentation.}
}
@article{KALLEYA2024147,
title = {An innovative artificial intelligence-based visualization and rendering for e-commerce startup booth design},
journal = {Procedia Computer Science},
volume = {245},
pages = {147-154},
year = {2024},
note = {9th International Conference on Computer Science and Computational Intelligence 2024 (ICCSCI 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.10.238},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924030461},
author = {Calista Kalleya and Andi Pramono and Chelsea Putri Angelina and Wilbert Alvin Cuarista and Riefky Prabowo and Fairuz Iqbal Maulana},
keywords = {Interior Design, Artificial Intelligence, Prototype, Render, Start-up},
abstract = {Marketplace development is increasing, as is the number of people with active internet access. Unive is a startup that runs an e-commerce platform connecting Student Activity Units with consumers. Focuses on fostering student entrepreneurship through practical business experience. For its development, this startup needs an interior for business escalation. Designers need software integrated with artificial intelligence (AI) in interior design. This study explores the integration of AI-powered rendering tools in the design of e-commerce platforms, focusing on how these tools enhance the user experience and satisfaction. The research employs a mixed-method approach, combining qualitative and quantitative analyses. The author collected data through user surveys, expert interviews, and performance metrics of AI rendering tools. The author based the evaluation criteria on adherence to interior design principles and the quality of rendering results. Findings indicate that selecting furniture and supporting components that adhere to interior design principles results in optimal rendering outcomes. These outcomes are close to user expectations, significantly improving user engagement and satisfaction. Detailed analysis shows the effectiveness of AI rendering in various design scenarios. The discussion highlights the implications of integrating AI rendering tools in e-commerce platforms. It addresses the potential of these tools to revolutionize user interaction and provides insights into future developments and applications in the field. This study demonstrates the value of AI-powered rendering tools in enhancing e-commerce platform design. By adhering to interior design principles, these tools can achieve rendering results that meet user expectations, ultimately leading to improved user experiences.}
}
@article{CHEN2022105882,
title = {Neural connectome features of procrastination: Current progress and future direction},
journal = {Brain and Cognition},
volume = {161},
pages = {105882},
year = {2022},
issn = {0278-2626},
doi = {https://doi.org/10.1016/j.bandc.2022.105882},
url = {https://www.sciencedirect.com/science/article/pii/S0278262622000409},
author = {Zhiyi Chen and Tingyong Feng},
keywords = {Procrastination, Neural connectome, Self-control network, DLPFC},
abstract = {Procrastination refers to an irrationally delay for intended courses of action despite of anticipating a negative consequence due to this delay. Previous studies tried to reveal the neural substrates of procrastination in terms of connectome-based biomarkers. Based on this, we proposed a unified triple brain network model for procrastination and pinpointed out what challenges we are facing in understanding neural mechanism of procrastination. Specifically, based on neuroanatomical features, the unified triple brain network model proposed that connectome-based underpinning of procrastination could be ascribed to the abnormalities of self-control network (i.e., dorsolateral prefrontal cortex, DLPFC), emotion-regulation network (i.e., orbital frontal cortex, OFC), and episodic prospection network (i.e., para-hippocampus cortex, PHC). Moreover, based on the brain functional features, procrastination had been attributed to disruptive neural circuits on FPN (frontoparietal network)-SCN (subcortical network) and FPN-SAN (salience network), which led us to hypothesize the crucial roles of interplay between these networks on procrastination in unified triple brain network model. Despite of these findings, poor interpretability and computational model limited further understanding for procrastination from theoretical and neural perspectives. On balance, the current study provided an overview to show current progress on the connectome-based biomarkers for procrastination, and proposed the integrative neurocognitive model of procrastination.}
}
@article{PAULIUK2022130997,
title = {Co-design of digital transformation and sustainable development strategies - What socio-metabolic and industrial ecology research can contribute},
journal = {Journal of Cleaner Production},
volume = {343},
pages = {130997},
year = {2022},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2022.130997},
url = {https://www.sciencedirect.com/science/article/pii/S0959652622006321},
author = {Stefan Pauliuk and Maximilian Koslowski and Kavya Madhu and Simon Schulte and Sebastian Kilchert},
keywords = {Sustainable development, Digital transformation, Systems thinking, Research agenda, Technology scale-up, Development constraints},
abstract = {Sustainable development and digital transformation profoundly re-shape industrial societies but have been studied largely independently. In light of pressing global environmental and social challenges, both transformations need to be well aligned with each other to achieve multiple objectives such as listed under the UN Sustainable Development goals (SDGs). Quantitative research on interlinkages, energy and material implications, and co-dependencies between the different digital transformation (DT) and sustainable development (SD) strategies is emerging and has so far focused on estimating the overall potential and on life cycle assessment (LCA). To frame the problem systematically, we developed a hierarchy of system levels for studying society's material and energy use, including the four levels: product/process, process cluster, life cycle/material cycle, and economy-wide. We mapped major DT strategies and the SDGs to the hierarchy and found a wide gap in system coverage: While most DT strategies focus on the product, process and process cluster levels, the SDGs predominantly target the economy-wide level. Socio-metabolic and industrial ecology research is needed to inform decision makers on how the two transformations can be aligned to reach overarching societal goals, such as the SDGs, expanding on and moving beyond LCA. Future research needs to assess combinations of multiple DT and SD strategies. It needs to study how DT can help decouple human wellbeing from negative environmental and social impacts. Research needs to focus on the strategies’ deployment potential, infrastructure needs, impacts on material cycles, and potential to transform both service demand and industrial production.}
}
@article{NAPIER2014331,
title = {Insight into the numerical challenges of implementing 2-dimensional SOA models in atmospheric chemical transport models},
journal = {Atmospheric Environment},
volume = {96},
pages = {331-344},
year = {2014},
issn = {1352-2310},
doi = {https://doi.org/10.1016/j.atmosenv.2014.07.048},
url = {https://www.sciencedirect.com/science/article/pii/S1352231014005780},
author = {W.J. Napier and J.J. Ensberg and J.H. Seinfeld},
keywords = {Secondary organic aerosol, 2-Dimensional SOA model, Chemical transport model, Probability distribution, Computational efficiency},
abstract = {The new generation of secondary organic aerosol (SOA) models that represent gas- and particle-phase chemistry and thermodynamic partitioning using discrete two-dimensional grids (e.g. SOM, 2D-VBS) cannot be efficiently implemented into three-dimensional atmospheric chemical transport models (CTMs) due to the large number of bins (tracers) required. In this study, we introduce a novel mathematical framework, termed the Oxidation State/Volatility Moment Method, that is designed to address these computational burdens so as to allow the new generation of SOA models to be implemented into CTMs. This is accomplished by mapping the two-dimensional grids onto probability distributions that conserve carbon and oxygen mass. Assessment of the Moment Method strengths (speed, carbon and oxygen conservation) and weaknesses (numerical drift) provide valuable insight that can guide future development of SOA modules for atmospheric CTMs.}
}
@incollection{BUCHANAN2014183,
title = {Chapter Seven - Edge Replacement and Minimality as Models of Causal Inference in Children},
editor = {Janette B. Benson},
series = {Advances in Child Development and Behavior},
publisher = {JAI},
volume = {46},
pages = {183-213},
year = {2014},
issn = {0065-2407},
doi = {https://doi.org/10.1016/B978-0-12-800285-8.00007-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128002858000078},
author = {David W. Buchanan and David M. Sobel},
keywords = {Causal reasoning, Causal graphical models, Edge replacement, Cognitive Development, Computational Models},
abstract = {Recently, much research has focused on causal graphical models (CGMs) as a computational-level description of how children represent cause and effect. While this research program has shown promise, there are aspects of causal reasoning that CGMs have difficulty accommodating. We propose a new formalism that amends CGMs. This edge replacement grammar formalizes one existing and one novel theoretical commitment. The existing idea is that children are determinists, in the sense that they believe that apparent randomness comes from hidden complexity, rather than inherent nondeterminism in the world. The new idea is that children think of causation as a branching process: causal relations grow not directly from the cause, but from existing relations between the cause and other effects. We have shown elsewhere that these two commitments together, when formalized, can explain and quantitatively fit the otherwise puzzling effect of nonindependence observed in the adult causal reasoning literature. We then test the qualitative predictions of this new formalism on children in a series of three experiments.}
}
@article{VALERY201844,
title = {A collaborative CPU–GPU approach for principal component analysis on mobile heterogeneous platforms},
journal = {Journal of Parallel and Distributed Computing},
volume = {120},
pages = {44-61},
year = {2018},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2018.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S0743731518303411},
author = {Olivier Valery and Pangfeng Liu and Jan-Jan Wu},
keywords = {OpenCL, GPGPU, Mobile computing, Heterogeneous system, PCA, Energy efficient, Acceleration, Data analysis, Machine learning},
abstract = {The advent of the modern GPU architecture has enabled computers to use General Purpose GPU capabilities (GPGPU) to tackle large scale problem at a low computational cost. This technological innovation is also available on mobile devices, addressing one of the primary problems with recent devices: the power envelope. Unfortunately, recent mobile GPUs suffer from a lack of accuracy that can prevent them from running any large scale data analysis tasks, such as principal component analysis (Shlens, 0000) (PCA). The goal of our work is to address this limitation by combining the high precision available on a CPU with the power efficiency of a mobile GPU. In this paper, we exploit the shared memory architecture of mobile devices in order to enhance the CPU–GPU collaboration and speed up PCA computation without sacrificing precision. Experimental results suggest that such an approach drastically reduces the power consumption of the mobile device while accelerating the overall workload. More generally, we claim that this approach can be extended to accelerate other vectorized computations on mobile devices while still maintaining numerical accuracy.}
}
@article{DRAWEL2017632,
title = {Reasoning about Trust and Time in a System of Agents},
journal = {Procedia Computer Science},
volume = {109},
pages = {632-639},
year = {2017},
note = {8th International Conference on Ambient Systems, Networks and Technologies, ANT-2017 and the 7th International Conference on Sustainable Energy Information Technology, SEIT 2017, 16-19 May 2017, Madeira, Portugal},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2017.05.369},
url = {https://www.sciencedirect.com/science/article/pii/S1877050917310384},
author = {Nagat Drawel and Jamal Bentahar and Elhadi Shakshuki},
keywords = {Multi-Agent Systems (MASs), trust, temporal logic},
abstract = {Abstract:
The study of trust in Multi-Agent Systems (MASs) has been an area of interest for many researchers over the last years. This is due to the fact that trust is the basis for agent communication wherein entities have to operate in a dynamic and uncertain environment. Several approaches have been proposed to define logical semantics for trust in MASs. However, these approaches are limited to reason about trust based on the sole agents’ mental states. Therefore, this paper considers trust from a high-level abstraction based on the social correct behaviors of agents. Specifically, we propose a logical framework that allows us to reason about unconditional trust and time. In particular, we introduce a new logical language called Trust Computation Tree logic (TCTL) that extends the Computation Tree Logic (CTL) with a new modality to represent trust. We describe the semantics by extending the interpreted systems formalism and consider a set of reasoning rules along with proofs to support our logic. Finally, we evaluate our approach using a real-life case study in the e-business domain to explain our proposed logic in a practical application.}
}