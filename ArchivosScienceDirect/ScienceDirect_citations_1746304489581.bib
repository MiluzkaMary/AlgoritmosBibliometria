@incollection{ULLMAN1988548,
title = {Visual routines**This report describes research done at the Artificial Intelligence Laboratory of the Massachusetts Institute of Technology. Support for the Laboratory's artificial intelligence research is provided in part by the Advanced Research Projects Agency of the Department of Defense under Office of Naval Research contract N00014-80-C-0505 and in part by National Science Foundation Grant 79-23110MCS. Reprint requests should be sent to Shimon Ullman Department of Psychology and Artificial Intelligence Laboratory, M.I.T., Cambridge, MA 02139. U.S.A.},
editor = {Allan Collins and Edward E. Smith},
booktitle = {Readings in Cognitive Science},
publisher = {Morgan Kaufmann},
pages = {548-579},
year = {1988},
isbn = {978-1-4832-1446-7},
doi = {https://doi.org/10.1016/B978-1-4832-1446-7.50047-9},
url = {https://www.sciencedirect.com/science/article/pii/B9781483214467500479},
author = {SHIMON ULLMAN},
abstract = {This paper examines the processing of visual information beyond the creation of the early representations. A fundamental requirement at this level is the capacity to establish visually abstract shape properties and spatial relations. This capacity plays a major role in object recognition, visually guided manipulation, and more abstract visual thinking. For the human visual system, the perception of spatial properties and relations that are complex from a computational standpoint nevertheless often appears deceivingly immediate and effortless. The proficiency of the human system in analyzing spatial information far surpasses the capacities of current artificial systems. The study of the computations that underlie this competence may therefore lead to the development of new more efficient methods for the spatial analysis of visual information. The perception of abstract shape properties and spatial relations raises fundamental difficulties with major implications for the overall processing of visual information. It will be argued that the computation of spatial relations divides the analysis of visual information into two main stages. The first is the bottom-up creation of certain representations of the visible environment. The second stage involves the application of processes called ‘visual routines’ to the representations constructed in the first stage. These routines can establish properties and relations that cannot be represented explicitly in the initial representations. Visual routines are composed of sequences of elemental operations. Routines for different properties and relations share elemental operations. Using a fixed set of basic operations, the visual system can assemble different routines to extract an unbounded variety of shape properties and spatial relations.}
}
@article{MONROE2020293,
title = {Moral elevation: Indications of functional integration with welfare trade-off calibration and estimation mechanisms},
journal = {Evolution and Human Behavior},
volume = {41},
number = {4},
pages = {293-302},
year = {2020},
issn = {1090-5138},
doi = {https://doi.org/10.1016/j.evolhumbehav.2020.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S1090513820300581},
author = {Amy Monroe},
keywords = {Moral elevation, Welfare trade-off ratios, Competitive altruism, Emotion},
abstract = {Moral elevation is a positive social emotion, which is triggered by observing third parties behaving benevolently, and which in turn triggers a motivation to behave benevolently towards others in general. It has been suggested that this relatively obscure emotion may be the output of a naturally selected cognitive adaptation which functions to help us retain our position in the competition for access to beneficial social relationships. This suggestion is here interpreted within the framework of ‘recalibrational emotions’. This framework offers the computational vocabulary necessary to understand how mental adaptations governing affect and motivation perform their functions at the cognitive level. Parallels are drawn between the suggested function and known phenomenological attributes of moral elevation, and the recently explicated functional operation of other social emotions (such as anger, guilt, and gratitude). Specifically, these other social emotions are thought to share a common computational pathway; recalibration of our welfare trade-off ratios (WTRs). WTRs are the computational element which dictate our willingness to benefit others at some cost to ourselves. A series of studies was conducted to explore whether a reliable relationship exists between moral elevation and WTRs. The results suggest that elevation does have a positive recalibrational effect on our WTRs, and that it may also be functionally integrated with a mental mechanism designed by natural selection to estimate the WTRs of other social actors.}
}
@article{ESSEX2018554,
title = {Model falsifiability and climate slow modes},
journal = {Physica A: Statistical Mechanics and its Applications},
volume = {502},
pages = {554-562},
year = {2018},
issn = {0378-4371},
doi = {https://doi.org/10.1016/j.physa.2018.02.090},
url = {https://www.sciencedirect.com/science/article/pii/S0378437118301766},
author = {Christopher Essex and Anastasios A. Tsonis},
keywords = {Climate complexity, Computer errors, Computational over-stabilization, Dynamical and thermodynamical sensitivity, Slow climate modes},
abstract = {The most advanced climate models are actually modified meteorological models attempting to capture climate in meteorological terms. This seems a straightforward matter of raw computing power applied to large enough sources of current data. Some believe that models have succeeded in capturing climate in this manner. But have they? This paper outlines difficulties with this picture that derive from the finite representation of our computers, and the fundamental unavailability of future data instead. It suggests that alternative windows onto the multi-decadal timescales are necessary in order to overcome the issues raised for practical problems of prediction.}
}
@incollection{BLACKBURN19941,
title = {1 - Structures, Languages and Translations: the Structural Approach to Feature Logic},
editor = {C.J. Rupp and M.A. Rosner and R.L. Johnson},
booktitle = {Constraints, Language and Computation},
publisher = {Academic Press},
address = {San Diego},
pages = {1-27},
year = {1994},
series = {Cognitive Science},
isbn = {978-0-08-050296-0},
doi = {https://doi.org/10.1016/B978-0-08-050296-0.50008-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780080502960500085},
author = {Patrick Blackburn},
abstract = {Publisher Summary
This chapter reviews methodological issues involved in the structural approach used in feature logic. A direct consequence of the systematic approach to a variety of feature logics is that it clarifies the relationships between them. This is most explicit in the presentation of translations between various existing and putative feature logics which draws heavily on the correspondence theory that relates modal and classical languages. The chapter describes a general approach to the subject called the structural approach, and explains that thinking in structural terms is a useful way of thinking about unification formalisms and their interrelationships. At present, in contemporary unification-based linguistic frameworks, linguistic data is modelled by certain kinds of decorated labelled (directed) graphs. Perhaps the most prevalent way of thinking about unification-based grammar formalisms is that they are languages for expressing constraints on feature structures. Two basic ideas drive modal logic: one syntactic, and the other semantic. Modal languages are interpreted on Kripke models, which are set theoretic entities providing the following information. Propositional Dynamic Logic (PDL) is an extension of modal logic; PDL and some of its extensions are natural constraint languages for dealing with feature structures. Subsequently, Attribute Value Matrices (AVMs) are one of the most widely used methods of describing feature structures. A general setting for feature logic is the space of relational structures of model theory, together with the various languages for describing these structures, and the satisfiability preserving translations that exist among these languages. The basic ideas are very simple: feature structures are certain sorts of relational structures, and while there is a vast range of languages for talking about these structures, these languages are interrelated by satisfiability preserving translations.}
}
@article{MAO2022109671,
title = {A decision support engine for infill drilling attractiveness evaluation using rule-based cognitive computing under expert uncertainties},
journal = {Journal of Petroleum Science and Engineering},
volume = {208},
pages = {109671},
year = {2022},
issn = {0920-4105},
doi = {https://doi.org/10.1016/j.petrol.2021.109671},
url = {https://www.sciencedirect.com/science/article/pii/S0920410521013000},
author = {Qiangqiang Mao and Xiaohua Ma and Yuhe Wang},
keywords = {Cognitive computing, Fuzzy inference, Infill well placement, Drilling attractiveness evaluation, Expert uncertainties quantification},
abstract = {Optimally drilling new wells in a developed reservoir is an essential strategy to potentially tap remaining oil for a complete life circle of oilfield development. Further, the determination of optimal infill drilling targets is a challenging issue which involves the integration of data, experts' knowledge and human decisions. The decision process can be essentially regarded as a systematic evaluation of drilling attractiveness. To automate drilling attractiveness evaluation, we develop a decision support engine using rule-based cognitive computing to rank and recommend drilling candidates. Such drilling candidates are chosen by the quantification of regional drilling attractiveness. Then we use two cases with different settings to show its general applicability and human-like reasoning abilities. The reasoning process considers expertise and human-involved uncertainties. The expertise is characterized by certain representation of fuzzy rules sets. Our results highlight the potential of our recommendation engine in pinpointing the most productive drilling location. And our method avoids the expensive reservoir simulation runs. Moreover, fuzzy drilling attractiveness evaluation can serve as an alternative initialization method of model-based infill well optimization, which avoids local optimum problem and greatly saves iteration time. Our approach extends human's reasoning capability and accelerates human's decision-making process with very low computational cost.}
}
@article{VAIS2013718,
title = {Laplacians on flat line bundles over 3-manifolds},
journal = {Computers & Graphics},
volume = {37},
number = {6},
pages = {718-729},
year = {2013},
note = {Shape Modeling International (SMI) Conference 2013},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2013.05.013},
url = {https://www.sciencedirect.com/science/article/pii/S0097849313000873},
author = {Alexander Vais and Daniel Brandes and Hannes Thielhelm and Franz-Erich Wolter},
keywords = {Spectral geometry, Vector bundles, Computational topology, Laplace operator, Knots, Seifert surfaces, FEM},
abstract = {The well-known Laplace–Beltrami operator, established as a basic tool in shape processing, builds on a long history of mathematical investigations that have induced several numerical models for computational purposes. However, the Laplace–Beltrami operator is only one special case of many possible generalizations that have been researched theoretically. Thereby it is natural to supplement some of those extensions with concrete computational frameworks. In this work we study a particularly interesting class of extended Laplacians acting on sections of flat line bundles over compact Riemannian manifolds. Numerical computations for these operators have recently been accomplished on two-dimensional surfaces. Using the notions of line bundles and differential forms, we follow up on that work giving a more general theoretical and computational account of the underlying ideas and their relationships. Building on this we describe how the modified Laplacians and the corresponding computations can be extended to three-dimensional Riemannian manifolds, yielding a method that is able to deal robustly with volumetric objects of intricate shape and topology. We investigate and visualize the two-dimensional zero sets of the first eigenfunctions of the modified Laplacians, yielding an approach for constructing characteristic well-behaving, particularly robust homology generators invariant under isometric deformation. The latter include nicely embedded Seifert surfaces and their non-orientable counterparts for knot complements.}
}
@article{WEN201811,
title = {Fast ranking nodes importance in complex networks based on LS-SVM method},
journal = {Physica A: Statistical Mechanics and its Applications},
volume = {506},
pages = {11-23},
year = {2018},
issn = {0378-4371},
doi = {https://doi.org/10.1016/j.physa.2018.03.076},
url = {https://www.sciencedirect.com/science/article/pii/S0378437118303947},
author = {Xiangxi Wen and Congliang Tu and Minggong Wu and Xurui Jiang},
keywords = {Complex network, Node importance, AHP, LS-SVM},
abstract = {Achieving high accuracy and comprehensiveness in node importance evaluation of complex networks is time-consuming. To solve this problem, a method based on Least Square Support Vector Machine (LS-SVM) was proposed. Firstly, four complicated importance indicators which reflect the node importance globally and comprehensively were selected. Then analytic hierarchy process (AHP) method was applied to obtain the node’s importance evaluation. On this basis, three simple indicators with low computational complexity were proposed, and LS-SVM was adopted to find the mapping rules between simple indicators and AHP evaluation. The experiments on artificial network and actual network show the validity of proposed method: the evaluation based on complicated indicators is consistent with reality and reflects node importance accurately; simple indicators evaluation by LS-SVM saved a lot of computational time and improved the evaluating efficiency. Our method can provide guidance on influential node identification in large scale complex networks.}
}
@article{VELAVELUPILLAI201436,
title = {Constructive and computable Hahn–Banach theorems for the (second) fundamental theorem of welfare economics},
journal = {Journal of Mathematical Economics},
volume = {54},
pages = {36-39},
year = {2014},
issn = {0304-4068},
doi = {https://doi.org/10.1016/j.jmateco.2014.08.004},
url = {https://www.sciencedirect.com/science/article/pii/S0304406814001062},
author = {K. {Vela Velupillai}},
keywords = {Fundamental theorems of welfare economics, Hahn–Banach theorem, Constructive analysis, Computable analysis},
abstract = {The Hahn–Banach Theorem plays a crucial role in the second fundamental theorem of welfare economics. To date, all mathematical economics and advanced general equilibrium textbooks concentrate on using non-constructive or incomputable versions of this celebrated theorem. In this paper we argue for the introduction of constructive or computable Hahn–Banach theorems in mathematical economics and advanced general equilibrium theory. The suggested modification would make applied and policy-oriented economics intrinsically computational.}
}
@article{VARDOULI2015137,
title = {Making use: Attitudes to human-artifact engagements},
journal = {Design Studies},
volume = {41},
pages = {137-161},
year = {2015},
note = {Special Issue: Computational Making},
issn = {0142-694X},
doi = {https://doi.org/10.1016/j.destud.2015.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X15000563},
author = {Theodora Vardouli},
keywords = {design theory, philosophy of design, user behavior, function theory, computational models},
abstract = {‘Function’ and ‘use’ are keywords that design researchers customarily employ when referring to human-artifact engagements. However, there is little consensus about how the concepts of function and use relate to each other, to the intentions of ‘designers’ and ‘users’, or to their actions and encompassing contexts. In this paper, I synthesize literature from design research, material culture studies, design anthropology, and function theory in order to critically compare different attitudes to human-artifact engagements, implicit in characterizations of function and use. I identify design-centric, communicative, and use-centric attitudes, and discuss their assumptions and implications for design theory. I conclude by outlining principles for theoretically and computationally approaching use as an embodied and temporally contingent process – as a form of ‘making’.}
}
@article{KASONGO2023113,
title = {A deep learning technique for intrusion detection system using a Recurrent Neural Networks based framework},
journal = {Computer Communications},
volume = {199},
pages = {113-125},
year = {2023},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2022.12.010},
url = {https://www.sciencedirect.com/science/article/pii/S0140366422004601},
author = {Sydney Mambwe Kasongo},
keywords = {Machine learning, Feature selection, Intrusion detection, Feature extraction},
abstract = {In recent years, the spike in the amount of information transmitted through communication infrastructures has increased due to the advances in technologies such as cloud computing, vehicular networks systems, the Internet of Things (IoT), etc. As a result, attackers have multiplied their efforts for the purpose of rendering network systems vulnerable. Therefore, it is of utmost importance to improve the security of those network systems. In this study, an IDS framework using Machine Learning (ML) techniques is implemented. This framework uses different types of Recurrent Neural Networks (RNNs), namely, Long-Short Term Memory (LSTM), Gated Recurrent Unit (GRU) and Simple RNN. To assess the performance of the proposed IDS framework, the NSL-KDD and the UNSW-NB15 benchmark datasets are considered. Moreover, existing IDSs suffer from low test accuracy scores in detecting new attacks as the feature dimension grows. In this study, an XGBoost-based feature selection algorithm was implemented to reduce the feature space of each dataset. Following that process, 17 and 22 relevant attributes were picked from the UNSW-NB15 and NSL-KDD, respectively. The accuracy obtained through the test subsets was used as the main performance metric in conjunction with the F1-Score, the validation accuracy, and the training time (in seconds). The results showed that for the binary classification tasks using the NSL-KDD, the XGBoost-LSTM achieved the best performance with a test accuracy (TAC) of 88.13%, a validation accuracy (VAC) of 99.49% and a training time of 225.46 s. For the UNSW-NB15, the XGBoost-Simple-RNN was the most efficient model with a TAC of 87.07%. For the multiclass classification scheme, the XGBoost-LSTM achieved a TAC of 86.93% over the NSL-KDD and the XGBoost-GRU obtained a TAC of 78.40% over the UNSW-NB15 dataset. These results demonstrated that our proposed IDS framework performed optimally in comparison to existing methods.}
}
@article{BAHK2013298,
title = {Analytical investigation of tooth profile modification effects on planetary gear dynamics},
journal = {Mechanism and Machine Theory},
volume = {70},
pages = {298-319},
year = {2013},
issn = {0094-114X},
doi = {https://doi.org/10.1016/j.mechmachtheory.2013.07.018},
url = {https://www.sciencedirect.com/science/article/pii/S0094114X13001584},
author = {Cheon-Jae Bahk and Robert G. Parker},
keywords = {Tooth profile modification, Planetary gear, Vibration, Nonlinear, Perturbation method},
abstract = {This study investigates the impact of tooth profile modification on spur planetary gear vibration. An analytical model is proposed to capture the excitation from tooth profile modifications at the sun–planet and ring–planet meshes. The accuracy of the proposed model for dynamic analysis is correlated against a benchmark finite element analysis. Perturbation analysis yields a closed-form approximation of the vibration response with tooth profile modifications. The perturbation solution is used to investigate the effects of tooth profile modification. The tooth profile modification parameters that minimize response are readily obtained. Static transmission error and dynamic response are minimized at different amounts of profile modification, which contradicts common practical thinking regarding the correlation between static transmission error and dynamic response. Contrary to expectations, the optimal sun–planet and ring–planet tooth profile modifications that minimize response when applied individually may increase dynamic response when applied simultaneously. System parameters such as mesh stiffness and mesh phase significantly affect the influence of tooth profile modification.}
}
@article{QIAO2024120105,
title = {Towards retraining-free RNA modification prediction with incremental learning},
journal = {Information Sciences},
volume = {660},
pages = {120105},
year = {2024},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2024.120105},
url = {https://www.sciencedirect.com/science/article/pii/S0020025524000185},
author = {Jianbo Qiao and Junru Jin and Haoqing Yu and Leyi Wei},
keywords = {RNA modification, Deep learning, Incremental learning},
abstract = {RNA modifications are important for deciphering the function of cells and their regulatory mechanisms. In recent years, researchers have developed many deep learning methods to identify specific modifications. However, these methods require model retraining for each new RNA modification and cannot progressively identify the newly identified RNA modifications. To address this challenge, we propose an innovative incremental learning framework that incorporates multiple incremental learning methods. Our experimental results confirm the efficacy of incremental learning strategies in addressing the RNA modification challenge. By uniquely targeting 10 RNA modification types in a class incremental setting, our framework exhibits superior performance. Notably, it can be extended to new category methylation predictions without the need for retraining with previous data, improving computational efficiency. Through the accumulation of knowledge, the model is able to evolve and continuously learn the differences across methylation, mitigating the problem of catastrophic forgetting during deep learning model training. Overall, our framework provides various alternatives to enhance the prediction of novel RNA modifications and illuminates the potential of incremental learning in tacking numerous genome data.}
}
@article{NG2023116585,
title = {Development of a system model to predict flows and performance of regional waste management planning: A case study of England},
journal = {Journal of Environmental Management},
volume = {325},
pages = {116585},
year = {2023},
issn = {0301-4797},
doi = {https://doi.org/10.1016/j.jenvman.2022.116585},
url = {https://www.sciencedirect.com/science/article/pii/S0301479722021582},
author = {Kok Siew Ng and Aidong Yang},
keywords = {Circular economy, Recycling, Stock-and-flow, Sustainable waste management, Resource recovery, Systems thinking},
abstract = {Significant loss of valuable resources and increasing burdens on landfills are often associated with a lack of proper planning in waste management and resource recovery strategy. A sustainable waste management model is thus urgently needed to improve resource efficiency and divert more waste from landfills. This paper proposes a comprehensive system model using stock-and-flow diagram to examine the current waste management performance and project the future waste generation, treatment and disposal scenarios, using England as a case study. The model comprises three integrated modules to represent household waste generation and collection; waste treatment and disposal; and energy recovery. A detailed mass and energy balance has been established and waste management performance has been evaluated using six upstream and downstream indicators. The base case scenario that assumes constant waste composition shows that waste to landfills can be reduced to less than 10% of the total amount, by 2035. However, it entails greater diversion of waste to energy-from-waste facilities, which is not sustainable and would incur higher capital investment and gate fees. Alternative case scenarios that promote recycling instead of energy recovery result in lower capital investment and gate fees. Complete elimination of the food and organic fraction from the residual waste stream will help meet the 65% recycling target by 2035. In light of the need for achieving a more circular economy in England, enhancing material recovery through reuse and recycling, reducing reliance on energy-from-waste and deploying more advanced waste valorisation technologies should be considered in future policy and planning for waste management.}
}
@article{ZHAO2025100817,
title = {User entertainment experience analysis of artificial intelligence entertainment robots based on convolutional neural networks in park plant landscape design},
journal = {Entertainment Computing},
volume = {52},
pages = {100817},
year = {2025},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2024.100817},
url = {https://www.sciencedirect.com/science/article/pii/S187595212400185X},
author = {Jingjing Zhao and Juan Yin and Yaqi Shi and Liang Qiao and Guihua Ma},
keywords = {Convolutional neural network, AI entertainment robots, Park plants, Landscape design, User experience},
abstract = {Currently, the application of artificial intelligence entertainment robots in park plant landscape design has attracted increasing attention. This study aims to design an artificial intelligence entertainment robot that can provide a high-quality user experience. Through virtual reality and robotics technology, designers can be provided with visual and entertaining design solutions, and more interactive experiences can be provided for design clients. Convolutional neural networks can effectively extract features from images, and utilizing spectral feature extraction technology to further improve the accuracy of image recognition. Subsequently, this study designed a robot control system and calibrated the hand eye system. The robot control system can coordinate the various functions of the robot and ensure its smooth operation in the park plant landscape design. The calibration of the hand eye system is to ensure that the robot can accurately perceive the environment and locate its own position. Through real-time control strategies, robots can respond and adjust in a timely manner based on current environmental changes and user needs. By comparing with the actual position on the ground, the accuracy of robot positioning is obtained, and the system is further optimized and improved.}
}
@incollection{NG202451,
title = {Chapter 4 - System modeling and mapping},
editor = {Kok Siew Ng and Elias Martinez Hernandez and Aki Yamaguchi},
booktitle = {A New Systems Thinking Approach to Sustainable Resource Management},
publisher = {Elsevier},
pages = {51-140},
year = {2024},
isbn = {978-0-323-99869-7},
doi = {https://doi.org/10.1016/B978-0-323-99869-7.00003-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780323998697000036},
author = {Kok Siew Ng and Elias Martinez Hernandez and Aki Yamaguchi},
keywords = {Causal loop diagram, GIS, MFA, Resource availability, Sensitivity and uncertainty analyses, System dynamics},
abstract = {This chapter presents a series of representative techniques for system modeling and mapping, including resource availability analysis, material flow analysis, system dynamics, and sensitivity and uncertainty analyses. These are among the well-established computational modeling methods adopted widely in engineering, environmental, and social science disciplines. They are particularly useful in the context of resource and waste management, providing clearer visualization of the system and enabling reliable prediction of system behavior. Resource availability assessment provides a bird's eye view of the interdependencies among resources and the overall feasibility for a system to operate with the available resources. Material flow analysis facilitates a robust mapping of resource consumption, production, and losses, offering insights for identifying opportunities to improve resource efficiency and minimize waste. System dynamics enables us to understand the complex behavior of a system through exploring the interaction of different factors, serving as a forecasting tool for future scenarios. Sensitivity analysis determines the system's responsiveness to different input values, identifying the most influential inputs in the system's response. Uncertainty analysis quantifies variations and uncertainties in potential system responses due to variations in inputs.}
}
@article{SANGAIAH2020347,
title = {Cognitive IoT system with intelligence techniques in sustainable computing environment},
journal = {Computer Communications},
volume = {154},
pages = {347-360},
year = {2020},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2020.02.049},
url = {https://www.sciencedirect.com/science/article/pii/S0140366419314616},
author = {Arun Kumar Sangaiah and Jerline Sheebha Anni Dhanaraj and Prabu Mohandas and Aniello Castiglione},
keywords = {Computational intelligence, Cognition, Multi-sensor, Data fusion, IoT},
abstract = {Forest border crossing animals creates major societal related issues, in addition to endangering their own lives. This is the objective focused in this paper targeting the species “The Elephant”, incorporating with technical methodologies namely, multi-sensor data fusion, cognition theories and computational intelligence techniques. Multi-sensor data fusion provides three level detection of target, along with its related outputs, which improves performance metrics. Cognition theory resulted in obtaining other interesting features about the target. Computational intelligence techniques integrate and conclude the presence of the target in the pseudo-boundary. The technical combination enhances the novelty of the research work, resulting in achieving remarkable accuracy and minimized false alert. An IoT kit was designed and deployed in the real time wild environment in Hosur forest region for collecting the data of Elephant. Further, the notification is sent to the registered mobile of the forest authority, as an early warning for chasing the pachyderm back to the forest.}
}
@article{ZADEH20082751,
title = {Is there a need for fuzzy logic?},
journal = {Information Sciences},
volume = {178},
number = {13},
pages = {2751-2779},
year = {2008},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2008.02.012},
url = {https://www.sciencedirect.com/science/article/pii/S0020025508000716},
author = {Lotfi A. Zadeh},
keywords = {Fuzzy logic, Fuzzy sets, Approximate reasoning, Computing with words, Computing with perceptions, Generalized theory of uncertainty},
abstract = {“Is there a need for fuzzy logic?” is an issue which is associated with a long history of spirited discussions and debate. There are many misconceptions about fuzzy logic. Fuzzy logic is not fuzzy. Basically, fuzzy logic is a precise logic of imprecision and approximate reasoning. More specifically, fuzzy logic may be viewed as an attempt at formalization/mechanization of two remarkable human capabilities. First, the capability to converse, reason and make rational decisions in an environment of imprecision, uncertainty, incompleteness of information, conflicting information, partiality of truth and partiality of possibility – in short, in an environment of imperfect information. And second, the capability to perform a wide variety of physical and mental tasks without any measurements and any computations [L.A. Zadeh, From computing with numbers to computing with words – from manipulation of measurements to manipulation of perceptions, IEEE Transactions on Circuits and Systems 45 (1999) 105–119; L.A. Zadeh, A new direction in AI – toward a computational theory of perceptions, AI Magazine 22 (1) (2001) 73–84]. In fact, one of the principal contributions of fuzzy logic – a contribution which is widely unrecognized – is its high power of precisiation. Fuzzy logic is much more than a logical system. It has many facets. The principal facets are: logical, fuzzy-set-theoretic, epistemic and relational. Most of the practical applications of fuzzy logic are associated with its relational facet. In this paper, fuzzy logic is viewed in a nonstandard perspective. In this perspective, the cornerstones of fuzzy logic – and its principal distinguishing features – are: graduation, granulation, precisiation and the concept of a generalized constraint. A concept which has a position of centrality in the nontraditional view of fuzzy logic is that of precisiation. Informally, precisiation is an operation which transforms an object, p, into an object, p∗, which in some specified sense is defined more precisely than p. The object of precisiation and the result of precisiation are referred to as precisiend and precisiand, respectively. In fuzzy logic, a differentiation is made between two meanings of precision – precision of value, v-precision, and precision of meaning, m-precision. Furthermore, in the case of m-precisiation a differentiation is made between mh-precisiation, which is human-oriented (nonmathematical), and mm-precisiation, which is machine-oriented (mathematical). A dictionary definition is a form of mh-precisiation, with the definiens and definiendum playing the roles of precisiend and precisiand, respectively. Cointension is a qualitative measure of the proximity of meanings of the precisiend and precisiand. A precisiand is cointensive if its meaning is close to the meaning of the precisiend. A concept which plays a key role in the nontraditional view of fuzzy logic is that of a generalized constraint. If X is a variable then a generalized constraint on X, GC(X), is expressed as X isr R, where R is the constraining relation and r is an indexical variable which defines the modality of the constraint, that is, its semantics. The primary constraints are: possibilistic, (r=blank), probabilistic (r=p) and veristic (r=v). The standard constraints are: bivalent possibilistic, probabilistic and bivalent veristic. In large measure, science is based on standard constraints. Generalized constraints may be combined, qualified, projected, propagated and counterpropagated. The set of all generalized constraints, together with the rules which govern generation of generalized constraints, is referred to as the generalized constraint language, GCL. The standard constraint language, SCL, is a subset of GCL. In fuzzy logic, propositions, predicates and other semantic entities are precisiated through translation into GCL. Equivalently, a semantic entity, p, may be precisiated by representing its meaning as a generalized constraint. By construction, fuzzy logic has a much higher level of generality than bivalent logic. It is the generality of fuzzy logic that underlies much of what fuzzy logic has to offer. Among the important contributions of fuzzy logic are the following: 1.FL-generalization. Any bivalent-logic-based theory, T, may be FL-generalized, and hence upgraded, through addition to T of concepts and techniques drawn from fuzzy logic. Examples: fuzzy control, fuzzy linear programming, fuzzy probability theory and fuzzy topology.2.Linguistic variables and fuzzy if–then rules. The formalism of linguistic variables and fuzzy if–then rules is, in effect, a powerful modeling language which is widely used in applications of fuzzy logic. Basically, the formalism serves as a means of summarization and information compression through the use of granulation.3.Cointensive precisiation. Fuzzy logic has a high power of cointensive precisiation. This power is needed for a formulation of cointensive definitions of scientific concepts and cointensive formalization of human-centric fields such as economics, linguistics, law, conflict resolution, psychology and medicine.4.NL-Computation (computing with words). Fuzzy logic serves as a basis for NL-Computation, that is, computation with information described in natural language. NL-Computation is of direct relevance to mechanization of natural language understanding and computation with imprecise probabilities. More generally, NL-Computation is needed for dealing with second-order uncertainty, that is, uncertainty about uncertainty, or uncertainty2 for short. In summary, progression from bivalent logic to fuzzy logic is a significant positive step in the evolution of science. In large measure, the real-world is a fuzzy world. To deal with fuzzy reality what is needed is fuzzy logic. In coming years, fuzzy logic is likely to grow in visibility, importance and acceptance.}
}
@article{MCLOUGHLIN2022173,
title = {Midfrontal Theta Activity in Psychiatric Illness: An Index of Cognitive Vulnerabilities Across Disorders},
journal = {Biological Psychiatry},
volume = {91},
number = {2},
pages = {173-182},
year = {2022},
note = {Biomarkers of Psychosis},
issn = {0006-3223},
doi = {https://doi.org/10.1016/j.biopsych.2021.08.020},
url = {https://www.sciencedirect.com/science/article/pii/S0006322321015651},
author = {Gráinne McLoughlin and Máté Gyurkovics and Jason Palmer and Scott Makeig},
keywords = {Biomarker, Cognitive control, EEG, ERP, Oscillations, Theta},
abstract = {There is an urgent need to identify the mechanisms that contribute to atypical thinking and behavior associated with psychiatric illness. Behavioral and brain measures of cognitive control are associated with a variety of psychiatric disorders and conditions as well as daily life functioning. Recognition of the importance of cognitive control in human behavior has led to intensive research into behavioral and neurobiological correlates. Oscillations in the theta band (4–8 Hz) over medial frontal recording sites are becoming increasingly established as a direct neural index of certain aspects of cognitive control. In this review, we point toward evidence that theta acts to coordinate multiple neural processes in disparate brain regions during task processing to optimize behavior. Theta-related signals in human electroencephalography include the N2, the error-related negativity, and measures of theta power in the (time-)frequency domain. We investigate how these theta signals are affected in a wide range of psychiatric conditions with known deficiencies in cognitive control: anxiety, obsessive-compulsive disorder, attention-deficit/hyperactivity disorder, and substance abuse. Theta-related control signals and their temporal consistency were found to differ in most patient groups compared with healthy control subjects, suggesting fundamental deficits in reactive and proactive control. Notably, however, clinical studies directly investigating the role of theta in the coordination of goal-directed processes across different brain regions are uncommon and are encouraged in future research. A finer-grained analysis of flexible, subsecond-scale functional networks in psychiatric disorders could contribute to a dimensional understanding of psychopathology.}
}
@article{ZHOU2019104484,
title = {Long-term forecasts for energy commodities price: What the experts think},
journal = {Energy Economics},
volume = {84},
pages = {104484},
year = {2019},
issn = {0140-9883},
doi = {https://doi.org/10.1016/j.eneco.2019.104484},
url = {https://www.sciencedirect.com/science/article/pii/S0140988319302658},
author = {Fan Zhou and Lionel Page and Robert K. Perrons and Zuduo Zheng and Simon Washington},
keywords = {Crude oil prices, Natural gas prices, Expert elicitation, Bayesian Truth Serum, Surprisingly popular},
abstract = {The ability to forecast energy prices in the long-term is important for a wide range of reasons, from the formulation of countries' energy and transportation policies to the defensive strategies of nations to investment decisions within the private sector. Despite the importance of these predictions, however, forecasters and market pundits face a difficult challenge when trying to forecast over the long-term. While statistical models can credibly rely on assumptions about the relationship between variables in the short-term, they are frequently less reliable in the long-term as political and technological transformations profoundly change how the economy works over time. Towards improving long-term predictions for energy commodities, this paper uses the elicitation and aggregation of experts' beliefs to put forward forecasts for crude oil and natural gas prices by incentivizing experts to tell the truth and minimising their own biases through the application of the Bayesian Truth Serum. With this approach, we generated both short-term and long-term forecasts, and used the short-term forecast to validate the quality of the experts' predictions.}
}
@article{BORATYNSKA20165529,
title = {FsQCA in corporate bankruptcy research. An innovative approach in food industry},
journal = {Journal of Business Research},
volume = {69},
number = {11},
pages = {5529-5533},
year = {2016},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2016.04.166},
url = {https://www.sciencedirect.com/science/article/pii/S0148296316303733},
author = {Katarzyna Boratyńska},
keywords = {Complexity theory, fsQCA, Corporate bankruptcy, Food industry},
abstract = {This study focuses on fsQCA in corporate bankruptcy research. This research aims at revealing how an fsQCA approach can overcome the knowledge gap of current conceptual and methodological attempts to expose corporate bankruptcy's architecture of causalities. The article discusses the economic literature concerning using fsQCA in corporate bankruptcy studies through complexity theory and a critical perspective. The study concentrates on implementing fsQCA and asymmetric thinking to corporate bankruptcy cases in food industry. The research examines the main reasons for corporate bankruptcy, namely: lack of financial liquidity, too high level of liabilities, losses, weak management, and too late recovery actions. The study attempts to build theory from food industry cases.}
}
@article{PAZZANI1991401,
title = {A computational theory of learning causal relationships},
journal = {Cognitive Science},
volume = {15},
number = {3},
pages = {401-424},
year = {1991},
issn = {0364-0213},
doi = {https://doi.org/10.1016/0364-0213(91)80003-N},
url = {https://www.sciencedirect.com/science/article/pii/036402139180003N},
author = {Michael Pazzani},
abstract = {I present a cognitive model of the human ability to acquire causal relationships. I report on experimental evidence demonstrating that human learners acquire accurate causal relationships more rapidly when training examples are consistent with a general theory of causality. This article describes a learning process that uses a general theory of causality as background knowledge. The learning process, which I call theory-driven learning (TDL), hypothesizes causal relationships consistent both with observed data and the general theory of causality. TDL accounts for data on both the rate at which human learners acquire causal relationships, and the types of causal relationships they acquire. Experiments with TDL demonstrate the advantage of TDL for acquiring causal relationships over similarity-based approaches to learning: Fewer examples are required to learn an accurate relationship.}
}
@incollection{KEENAN2015394,
title = {Psychology of Inferences},
editor = {James D. Wright},
booktitle = {International Encyclopedia of the Social & Behavioral Sciences (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {394-399},
year = {2015},
isbn = {978-0-08-097087-5},
doi = {https://doi.org/10.1016/B978-0-08-097086-8.57011-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780080970868570111},
author = {Janice M. Keenan},
keywords = {Backward inferences, Coherence, Cortical networks, Explicit inferences, Forward inferences, Implicit inferences, Individual differences, Inferences, Inferencing, computational processes of, Inferencing, methodological issues, Inferencing, neural basis of, Knowledge, Language, Right hemisphere language},
abstract = {The goal of comprehension is to understand what the speaker intended the text to mean. Inferences are driven by a desire to make the interpretation both more coherent and more elaborate than the text itself. The goal of research on inferencing is to specify how the computational processes involved in making inferences vary with the comprehender's knowledge, the conditions that promote inferencing, the various types of inferences and methodological problems involved in assessing them, and, most recently, the neural bases of inferencing.}
}
@article{RAVI2023151,
title = {Evolving trends in student assessment in chemical engineering education},
journal = {Education for Chemical Engineers},
volume = {45},
pages = {151-160},
year = {2023},
issn = {1749-7728},
doi = {https://doi.org/10.1016/j.ece.2023.09.003},
url = {https://www.sciencedirect.com/science/article/pii/S1749772823000453},
author = {Manoj Ravi},
keywords = {Assessment, Authentic assessment, Digitalisation},
abstract = {Alongside innovation in teaching practice, student assessment in chemical engineering has seen significant changes in the recent past. This article undertakes a systematic review of the recent advances that have been reported in assessment practice in chemical engineering education. The main trends that emerge are: a shift towards authentic assessment methods, an increase in emphasis on peer-assessment and other approaches for group-based assignments, and a greater use of digital tools for the delivery of authentic assessments and improvement of marking and feedback practice. The analysis also examines the diversity of assessment methods used across the different chemical engineering subjects and how these map against assessment frameworks reported in the wider pedagogical literature. The emerging strand of research on synoptic and interdisciplinary assessment is used to develop an assessment framework for producing chemical engineering graduates who are also socially responsible and competent global citizens.}
}
@article{CHENG20115100,
title = {Equilibrium Conditions In Service Supply Chain},
journal = {Procedia Engineering},
volume = {15},
pages = {5100-5104},
year = {2011},
note = {CEIS 2011},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2011.08.946},
url = {https://www.sciencedirect.com/science/article/pii/S1877705811024477},
author = {Fei Cheng and Shanlin Yang and Xijun Ma},
keywords = {service supply chain, service volume, equilibrium},
abstract = {Service supply chain features human players as service vendor, service integrator, customer and service resource. It tends to be digitally connected, such as consulting, e-business and integrated enterprises. Our study uses a formal model and simulations to develop the effect of a service supply chain on equilibrium computation. Two insights arise on how a network can obtain equilibrium computation: forming the network structure of service supply chain; exploring entities behavior and equilibrium conditions. These results highlight the importance for service supply chain of adapting its network structure to equilibrium and application.}
}
@article{LI2021151508,
title = {Reverse vaccinology approach for the identifications of potential vaccine candidates against Salmonella},
journal = {International Journal of Medical Microbiology},
volume = {311},
number = {5},
pages = {151508},
year = {2021},
issn = {1438-4221},
doi = {https://doi.org/10.1016/j.ijmm.2021.151508},
url = {https://www.sciencedirect.com/science/article/pii/S1438422121000370},
author = {Jie Li and Jingxuan Qiu and Zhiqiang Huang and Tao Liu and Jing Pan and Qi Zhang and Qing Liu},
keywords = {, Reverse vaccinology, Computational model, Vaccine target, Immunoprotective},
abstract = {Salmonella is a leading cause of foodborne pathogen which causes intestinal and systemic diseases across the world. Vaccination is the most effective protection against Salmonella, but the identification and design of an effective broad-spectrum vaccine is still a great challenge, because of the multi-serotypes of Salmonella. Reverse vaccinology is a new tool to discovery and design vaccine antigens combining human immunology, structural biology and computational biology with microbial genomics. In this study, reverse vaccinology, an in-silico approach was established to screen appropriate immunogen targets by calculating the immunogenicity score of 583 non-redundant outer membrane and secreted proteins of Salmonella. Herein among 100 proteins identified with top-ranked scores, 15 representative antigens were selected randomly. Applying the sequence conservation test, four proteins (FliK, BcsZ, FhuA and FepA) remained as potential vaccine candidates for in vivo evaluation of immunogenicity and immunoprotection. All four candidates were capable to trigger the immune response and stimulate the production of antiserum in mice. Furthermore, top-ranked proteins including FliK and BcsZ provided wide antigenic coverage among the multi-serotype of Salmonella. The S. Typhimurium LT2 challenge model used in mice immunized with FliK and BcsZ showed a high relative percentage survival (RPS) of 52.74 % and 64.71 % respectively. In conclusion, this study constructed an in-silico pipeline able to successfully pre-screen the vaccine targets characterized by high immunogenicity and protective immunity. We show that reverse vaccinology allowed screening of appropriate broad-spectrum vaccines for Salmonella.}
}
@article{LIU2023115384,
title = {Trajectory planning for unmanned surface vehicles in multi-ship encounter situations},
journal = {Ocean Engineering},
volume = {285},
pages = {115384},
year = {2023},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2023.115384},
url = {https://www.sciencedirect.com/science/article/pii/S0029801823017687},
author = {Jianjian Liu and Huizi Chen and Shaorong Xie and Yan Peng and Dan Zhang and Huayan Pu},
keywords = {Tordsdrajectory planning, Collision avoidance, Velocity obstacle, Multiship encounters, COLREGS},
abstract = {Unmanned surface vehicles (USVs) can encounter traffic ships while navigating toward the target location. For the USVs, collision avoidance (CA) trajectories need to be planned according to the international regulations for preventing collisions at sea (COLREGS). A novel trajectory planning approach is proposed for the collision-free trajectories planning of USVs in the case of multiship encounters. Unlike the existing trajectory planning approaches, the proposed approach uses the holistic thinking to simplify the analysis of encounter situations. Ships approaching from all sides of the USV are treated as one or two equivalent obstacles based on consistent offset velocity direction (COVD) method. Furthermore, planned velocity is designed using the proposed CA strategy and kinematic constraints. This strategy is compliant with COLREGS and includes an emergency CA module to further ensure a safe distance between the USV and traffic ships. The performance of the proposed trajectory planning approach is verified through physical simulations using an existing simulator. The simulation results show that the proposed trajectory planning approach can implement multiple USVs to simultaneously avoid collisions and reach their respective target positions. Moreover, the approach remains effective when other USVs do not follow the COLREGS protocols.}
}
@article{DAVIES2023100692,
title = {Idea generation and knowledge creation through maker practices in an artifact-mediated collaborative invention project},
journal = {Learning, Culture and Social Interaction},
volume = {39},
pages = {100692},
year = {2023},
issn = {2210-6561},
doi = {https://doi.org/10.1016/j.lcsi.2023.100692},
url = {https://www.sciencedirect.com/science/article/pii/S2210656123000089},
author = {Sini Davies and Pirita Seitamaa-Hakkarainen and Kai Hakkarainen},
keywords = {Constructionism, Design practices, Engineering practices, Epistemic object, Epistemic practices, Knowledge creation, Learning by making, Material mediation},
abstract = {This investigation involved carrying out interventions that engaged teams of lower-secondary (13–14-year-old) Finnish students in using traditional and digital fabrication technologies to make materially embodied collaborative inventions. By relying on video data and ethnographic observations of the student teams' collaborative invention processes, the investigation focused on investigating 1) how the teams generated and developed their design ideas in their materially anchored making process and 2) what kinds of maker practices they relied on during open-ended invention projects. The study focused on a microanalytic study of three teams of students, and we utilized and developed visual data analysis methods. Our findings reveal the complex nature of the student teams' materially contextualized ideation and the knowledge creation activities that took place within their projects. The findings suggest that open-ended, materially mediated co-invention projects offer ample opportunities for creative cultural participation and practice-based knowledge creation in schools.}
}
@article{NAKAMURA2021198,
title = {Explanation of emotion regulation mechanism of mindfulness using a brain function model},
journal = {Neural Networks},
volume = {138},
pages = {198-214},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.01.029},
url = {https://www.sciencedirect.com/science/article/pii/S089360802100037X},
author = {Haruka Nakamura and Yoshimasa Tawatsuji and Siyuan Fang and Tatsunori Matsui},
keywords = {Emotion regulation in mindfulness, Mechanism, Brain function model, Top-down, Bottom-up},
abstract = {The emotion regulation mechanism of mindfulness plays an important role in the stress reduction effect. Many researchers in the fields of cognitive psychology and cognitive neuroscience have attempted to elucidate this mechanism by documenting the cognitive processes that occur and the neural activities that characterize each process. However, previous findings have not revealed the mechanism of information propagation in the brain that achieves emotion regulation during mindfulness. In this study, we constructed a functional brain model based on its anatomical network structure and a computational model representing the propagation of information between brain regions. We then examined the effects of mindfulness meditation on information propagation in the brain using simulations of changes in the activity of each region. These simulations of changes represent the degree of processing resource allocation to the neural activity via changes in the weights of each region’s output. As a result of the simulations, we reveal how the neural activity characteristic of emotion regulation in mindfulness, which has been reported in previous studies, is realized in the brain. Mindfulness meditation increases the weight of the output from each region of the thalamus and sensory cortex, which processes sensory stimuli from the external world. This sensory information activates the insula and anterior cingulate cortex (ACC). The orbitofrontal cortex and dorsolateral prefrontal cortex inhibit amygdala activity (i.e., top-down emotion regulation). However, when mindfulness meditation dominates bottom-up processing via sensory stimuli from the external world, amygdala activity increases through the insula and ACC activation.}
}
@article{BERGER2024537,
title = {Enmeshed with the digital: satellite navigation and the phenomenology of drivers’ spaces},
journal = {Mobilities},
volume = {19},
number = {3},
pages = {537-555},
year = {2024},
issn = {1745-0101},
doi = {https://doi.org/10.1080/17450101.2023.2285304},
url = {https://www.sciencedirect.com/science/article/pii/S1745010123001431},
author = {Viktor Berger},
keywords = {Satellite navigation, GPS, driving, automobilities, Merleau-Ponty, hybrid spaces, mesh, mediatization},
abstract = {This paper aims to develop a theoretical interpretation of how satellite navigation transforms drivers’ experience of automotive spaces. The use of satellite navigation has, so far, been predominantly studied from a cognitivist perspective based on the computer model of cognition and the theory of spatial disengagement. Experimental studies have concluded that over-reliance on digital navigation tools diminishes spatial orientation and spatial memory. According to the dominant interpretation, satellite navigation causes disengagement from space. After addressing these approaches, the paper introduces an embodied perspective of satellite navigation. This is accomplished by applying the phenomenology of perception of Maurice Merleau-Ponty, whose notions, such as perception, body schema, motor habit, and virtual body, illuminate otherwise undertheorized dimensions of drivers’ spaces. By using digital tools for wayfinding, drivers’ body schema, virtual body, and perception of space are modified, thereby enabling an engagement with convoluted ‘mesh spaces.’ This new term is integral to the interpretation of drivers’ spaces, as well as being distinct from that of ‘hybrid space,’ although both aim to conceptualize spaces, including physical objects and their visual representations. Conclusions will be drawn against the broader context of the mediatization of everyday life.}
}
@article{RECANATINI20208653,
title = {Drug Research Meets Network Science: Where Are We?},
journal = {Journal of Medicinal Chemistry},
volume = {63},
number = {16},
pages = {8653-8666},
year = {2020},
issn = {1520-4804},
doi = {https://doi.org/10.1021/acs.jmedchem.9b01989},
url = {https://www.sciencedirect.com/science/article/pii/S1520480420001702},
author = {Maurizio Recanatini and Chiara Cabrelle},
abstract = {Network theory provides one of the most potent analysis tools for the study of complex systems. In this paper, we illustrate the network-based perspective in drug research and how it is coherent with the new paradigm of drug discovery. We first present data sources from which networks are built, then show some examples of how the networks can be used to investigate drug-related systems. A section is devoted to network-based inference applications, i.e., prediction methods based on interactomes, that can be used to identify putative drug–target interactions without resorting to 3D modeling. Finally, we present some aspects of Boolean networks dynamics, anticipating that it might become a very potent modeling framework to develop in silico screening protocols able to simulate phenotypic screening experiments. We conclude that network applications integrated with machine learning and 3D modeling methods will become an indispensable tool for computational drug discovery in the next years.
}
}
@article{LI2018122,
title = {Uncertainty, politics, and technology: Expert perceptions on energy transitions in the United Kingdom},
journal = {Energy Research & Social Science},
volume = {37},
pages = {122-132},
year = {2018},
issn = {2214-6296},
doi = {https://doi.org/10.1016/j.erss.2017.10.003},
url = {https://www.sciencedirect.com/science/article/pii/S2214629617303304},
author = {Francis G.N. Li and Steve Pye},
keywords = {Climate policy, Energy policy, Uncertainty analysis, Decision-making},
abstract = {Energy policy is beset by deep uncertainties, owing to the scale of future transitions, the long-term timescales for action, and numerous stakeholders. This paper provides insights from semi-structured interviews with 31 UK experts from government, industry, academia, and civil society. Participants were asked for their views on the major uncertainties surrounding the ability of the UK to meet its 2050 climate targets. The research reveals a range of views on the most critical uncertainties, how they can be mitigated, and how the research community can develop approaches to better support strategic decision-making. The study finds that the socio-political dimensions of uncertainty are discussed by experts almost as frequently as technological ones, but that there exist divergent perspectives on the role of government in the transition and whether or not there is a requirement for increased societal engagement. Finally, the study finds that decision-makers require a new approach to uncertainty assessment that overcomes analytical limits to existing practice, is more flexible and adaptable, and which better integrates qualitative narratives with quantitative analysis. Policy design must escape from ‘caged’ thinking concerning what can or cannot be included in models, and therefore what types of uncertainties can or cannot be explored.}
}
@article{WILKINS2007635,
title = {Inexpensive fusion methods for enhancing feature detection},
journal = {Signal Processing: Image Communication},
volume = {22},
number = {7},
pages = {635-650},
year = {2007},
note = {"Special Issue on Content-Based Multimedia Indexing and Retrieval"},
issn = {0923-5965},
doi = {https://doi.org/10.1016/j.image.2007.05.012},
url = {https://www.sciencedirect.com/science/article/pii/S0923596507000732},
author = {Peter Wilkins and Tomasz Adamek and Noel E. O’Connor and Alan F. Smeaton},
keywords = {Feature detection, Data fusion, TRECVID},
abstract = {Recent successful approaches to high-level feature detection in image and video data have treated the problem as a pattern classification task. These typically leverage the techniques learned from statistical machine learning, coupled with ensemble architectures that create multiple feature detection models. Once created, co-occurrence between learned features can be captured to further boost performance. At multiple stages throughout these frameworks, various pieces of evidence can be fused together in order to boost performance. These approaches whilst very successful are computationally expensive, and depending on the task, require the use of significant computational resources. In this paper we propose two fusion methods that aim to combine the output of an initial basic statistical machine learning approach with a lower-quality information source, in order to gain diversity in the classified results whilst requiring only modest computing resources. Our approaches, validated experimentally on TRECVid data, are designed to be complementary to existing frameworks and can be regarded as possible replacements for the more computationally expensive combination strategies used elsewhere.}
}
@article{SURYADI2023730,
title = {”Read on”: comprehending challenging texts at university through gamification App},
journal = {Procedia Computer Science},
volume = {216},
pages = {730-738},
year = {2023},
note = {7th International Conference on Computer Science and Computational Intelligence 2022},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.12.190},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922022682},
author = {Phillip Suryadi and Irfan Rifai and Hady Pranoto},
keywords = {gamification, reading, application, students, texts},
abstract = {Despite the common misperception of playing games as wasteful activity, studies found that some of its components may contribute to users’ knowledge generation, soft skill improvement, and foreign language learning. This article reports the development of an application for reading and the initial impacts of the gamification-based application on students’ reading comprehension in English. The application was aimed to support generation Z's university students who are well exposed to gadgets with the ability in comprehending challenging texts. In addition to the sociocultural theory of learning and second language acquisition theories, we considered factors like university students as users, texts’ complexity offered at the university level, and gamification features in designing the application. The study resulted in a prototype of a gaming activity called” ReadOn”. Surveys, interviews, and experiments were carried out on a small group of participants during, and after designing processes. The Survey data was used as a foundation to design the app while the interview and the experiments provided data to explore the usability of the newly built prototype. The data of students’ experience in using the prototype was used as feedback for future development of the platform.}
}
@article{GANNON2025R152,
title = {Motion integration: A case of misdirection},
journal = {Current Biology},
volume = {35},
number = {4},
pages = {R152-R154},
year = {2025},
issn = {0960-9822},
doi = {https://doi.org/10.1016/j.cub.2025.01.016},
url = {https://www.sciencedirect.com/science/article/pii/S0960982225000168},
author = {Sara M. Gannon and Lindsey L. Glickfeld},
abstract = {Summary
Integrating complex motion signals from the environment is essential for behavior. A recent study in the mouse has revealed that both encoding in the superior colliculus and the optokinetic reflex follow a novel motion integration rule.}
}
@article{BELLA2023123268,
title = {Vibrationally resolved deep–red circularly polarised luminescence spectra of C70 derivative through Gaussian curvature analysis of ground and excited states},
journal = {Journal of Molecular Liquids},
volume = {391},
pages = {123268},
year = {2023},
issn = {0167-7322},
doi = {https://doi.org/10.1016/j.molliq.2023.123268},
url = {https://www.sciencedirect.com/science/article/pii/S0167732223020743},
author = {Giovanni Bella and Giuseppe Bruno and Antonio Santoro},
keywords = {Fullerene, Chirality, Curvature, Vibronic, Circularly polarized luminescence},
abstract = {Over the last years, the interaction of fullerene with circularly polarized light has attracted growing attention for potential electronic and optical applications. However, in literature there is only one example of fullerene derivative capable of emitting circularly polarized light, showing an active circularly polarized luminescence (CPL) signal in the deep-red visible region. This unique luminophore offered us the possibility to study the connection between the topological features of C70 spheroid and its chiral emission properties. In light of these considerations, we proposed a theoretical protocol that combines three different step: (1) The Ball Pivoting Algorithm for C70 surface reconstruction. (2) The discrete gaussian curvature analysis in the ground (S0) and excited states (S1). (3) The computation of the vibrationally-resolved CPL spectrum. The first step allowed us to extract useful information that linked the topological shape of C70 to the sp2 carbon network chemistry. The DFT benchmark in the second step guided us in grasping the best functional for the C70 curvature simulation in the ground state, spotlighting how B97D3 excellently succeed for this task. The curvature investigation in the first excited state showed that (for all the twenty exchange–correlation functional tested) the C70 fragment is more curved in S1 than in S0. The final step collected the topological information from the previous sections to provide a detailed overview of the theoretical factors (such as the quantum formalism, the potential energy surface description and the transition dipole moment approximation) impacting on the C70 vibrationally resolved CPL spectrum. We found that the adiabatic hessian model coupled with the Franck-Condon Herzberg-Teller approximation computed at PW6B95D3/6-311G(d,p) level provides excellent results in emulating the band-shape and position of the experimental CPL spectrum.}
}
@article{MASSO2025100112,
title = {Research ethics committees as knowledge gatekeepers: The impact of emerging technologies on social science research},
journal = {Journal of Responsible Technology},
volume = {21},
pages = {100112},
year = {2025},
issn = {2666-6596},
doi = {https://doi.org/10.1016/j.jrt.2025.100112},
url = {https://www.sciencedirect.com/science/article/pii/S2666659625000083},
author = {Anu Masso and Jevgenia Gerassimenko and Tayfun Kasapoglu and Mai Beilmann},
keywords = {Research ethics, Ethics committees, Social sciences, Research methods, Data, Algorithms, Artificial intelligence},
abstract = {This article investigates the evolution of research ethics within the social sciences, emphasising the shift from procedural norms borrowed from medical and natural sciences to social scientific discipline-specific and method-based principles. This transformation acknowledges the unique challenges and opportunities in social science research, particularly in the context of emerging data technologies such as digital data, algorithms, and artificial intelligence. Our empirical analysis, based on a survey conducted among international social scientists (N = 214), highlights the precariousness researchers face regarding these technological shifts. Traditional methods remain prevalent, despite the recognition of new digital methodologies that necessitate new ethical principles. We discuss the role of ethics committees as influential gatekeepers, examining power dynamics and access to knowledge within the research landscape. The findings underscore the need for tailored ethical guidelines that accommodate diverse methodological approaches, advocate for interdisciplinary dialogue, and address inequalities in knowledge production. This article contributes to the broader understanding of evolving research ethics in an increasingly data-driven world.}
}
@incollection{PIGGOTT2022176,
title = {8.10 - Optimization of Marine Renewable Energy Systems},
editor = {Trevor M. Letcher},
booktitle = {Comprehensive Renewable Energy (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {176-220},
year = {2022},
isbn = {978-0-12-819734-9},
doi = {https://doi.org/10.1016/B978-0-12-819727-1.00179-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128197271001795},
author = {Matthew D. Piggott and Stephan C. Kramer and Simon W. Funke and David M. Culley and Athanasios Angeloudis},
keywords = {Tidal stream, Tidal range, Optimization, Modelling},
abstract = {Optimizing marine renewable energy systems to maximize performance is key to their success. However, a range of physical, environmental, engineering, economic as well as computational challenges means that this is not straightforward. This article considers this topic, focusing on those systems whose performance is coupled to the hydrodynamics providing the resource; tidal power represents a clear example of this. In such cases system design must be optimal in relation to the resource׳s magnitude as well as its spatial and temporal variation, which are all dependent on the system׳s configuration and operation and so cannot be assumed to be known at the design stage. Designing based on the ambient resource could lead to under-performance. Coupling between the design and the resource has implications for the complexity of the optimization problem and potential hydrodynamical and environmental impacts. This coupling distinguishes many marine energy systems from other renewables which do not impact in any significant manner on the resource. The optimal design of marine energy systems thus represents a challenging and somewhat unique problem. However, feedback also opens up a number of possibilities where the resource can be ‘controlled’, to maximize the cumulative power obtained from multiple devices or plants, or to achieve some other complementary goal. Design optimization is thus critical, with many issues to consider. Due to the complexity of the problem a computational based solution is a necessity in all but the simplest scenarios. However, the coupled feedback requires that an iterative solution approach be used, which combined while the vast range of spatial and temporal scales means that methodological compromises need to be made. These compromises need to be understood, with the correct computational tool used at the appropriate point in the design process. This article reviews these challenges as well as the progress that has been made in addressing them.}
}
@article{TERZIYAN20242540,
title = {Can ChatGPT Challenge the Scientific Impact of Published Research, Particularly in the Context of Industry 4.0 and Smart Manufacturing?},
journal = {Procedia Computer Science},
volume = {232},
pages = {2540-2550},
year = {2024},
note = {5th International Conference on Industry 4.0 and Smart Manufacturing (ISM 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.02.072},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924002497},
author = {Vagan Terziyan and Olena Kaikova and Mariia Golovianko and Oleksandra Vitko},
keywords = {Artificial Intelligence, ChatGPT, Industry 4.0, Smart Manufacturing, academic impact},
abstract = {The released ChatGPT as a powerful language model is capable of assisting with a wide range of tasks, including answering questions, summarizing, paraphrasing, proofreading, classifying, and integrating texts. In this study, we tested ChatGPT capability to assist researchers in evaluating the academic articles’ contribution. We suggest a dialogue schema in which ChatGPT is asked to answer research questions from the target article and then to compare its own answers with the answers from the article. Finally, ChatGPT is asked to integrate both solutions coherently. We experimented with Proceedings of ISM-2022 Conference on Industry 4.0 and Smart Manufacturing, utilizing explicit research questions. The chat context enabled assessing studied articles’ contributions to Industry 4.0, uncovering advancements beyond the state-of-the-art. However, ChatGPT demonstrates limitations in content understanding and contribution evaluation. We conclude that while it collaborates with humans on academic tasks, human guidance remains essential, while ChatGPT's assistance efficiently complements traditional academic processes.}
}
@article{KAR2023102661,
title = {Guest Editorial: Big data-driven theory building: Philosophies, guiding principles, and common traps},
journal = {International Journal of Information Management},
volume = {71},
pages = {102661},
year = {2023},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2023.102661},
url = {https://www.sciencedirect.com/science/article/pii/S0268401223000427},
author = {Arpan Kumar Kar and Spyros Angelopoulos and H. Raghav Rao},
keywords = {Big data, Information systems, Artificial intelligence, Machine learning, Theory building, Computational social science},
abstract = {While data availability and access used to be a major challenge for information systems research, the growth and ease of access to large datasets and data analysis tools has increased interest to use such resources for publishing. Such publications, however, seem to offer weak theoretical contributions. While big data-driven studies increasingly gain popularity, they rarely introspect why a phenomenon is better explained by a theory and limit the analysis to data descriptive by mining and visualizing large volumes of big data. We address this pressing need and provide directions to move towards theory building with Big Data. We differentiate based on inductive and deductive approaches and provide guidelines how may undertake steps for theory building. In doing so, we further provide directions surrounding common pitfalls that should be avoided in this journey of Big-Data driven theory building.}
}
@article{ARTHURS2013443,
title = {Efficient simulation of cardiac electrical propagation using high-order finite elements II: Adaptive p-version},
journal = {Journal of Computational Physics},
volume = {253},
pages = {443-470},
year = {2013},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2013.07.011},
url = {https://www.sciencedirect.com/science/article/pii/S0021999113004841},
author = {Christopher J. Arthurs and Martin J. Bishop and David Kay},
keywords = {Adaptive finite element method, -version, Monodomain simulation, Computational cardiology, Numerical efficiency},
abstract = {We present a computationally efficient method of simulating cardiac electrical propagation using an adaptive high-order finite element method to automatically concentrate computational effort where it is most needed in space on each time-step. We drive the adaptivity using a residual-based error indicator, and demonstrate using norms of the error that the indicator allows us to control it successfully. Our results using two-dimensional domains of varying complexity demonstrate that significant improvements in efficiency are possible over the standard linear FEM in our single-thread studies, and our preliminary three-dimensional results suggest that improvements are also possible in 3D. We do not work in parallel or investigate the challenges for adaptivity such as dynamic load-balancing which are associated with parallelisation. However, based upon recent work demonstrating that in some circumstances and with moderate processor counts parallel h-adaptive methods are efficient, and upon the claim that p-adaptivity will outperform h-adaptivity, we argue that p-adaptivity should be investigated for efficiency in parallel for simulation on moderate numbers of processors.}
}
@article{GROOTHUIJSEN2024100290,
title = {AI chatbots in programming education: Students’ use in a scientific computing course and consequences for learning},
journal = {Computers and Education: Artificial Intelligence},
volume = {7},
pages = {100290},
year = {2024},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2024.100290},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X24000936},
author = {Suzanne Groothuijsen and Antoine {van den Beemt} and Joris C. Remmers and Ludo W. {van Meeuwen}},
keywords = {AI chatbots, ChatGPT, Programming education, Pair programming, Student learning, Engineering education},
abstract = {Teaching and learning in higher education require adaptation following students' inevitable use of AI chatbots. This study contributes to the empirical literature on students' use of AI chatbots and how they influence learning. The aim of this study is to identify how to adapt programming education in higher engineering education. A mixed-methods case study was conducted of a scientific computing course in a Mechanical Engineering Master's program at a Eindhoven University of Technology in the Netherlands. Data consisted of 29 student questionnaires, a semi-structured group interview with three students, a semi-structured interview with the teacher, and 29 students' grades. Results show that students used ChatGPT for error checking and debugging of code, increasing conceptual understanding, generating, and optimizing solution code, explaining code, and solving mathematical problems. While students reported advantages of using ChatGPT, the teacher expressed concerns over declining code quality and student learning. Furthermore, both students and teacher perceived a negative influence from ChatGPT usage on pair programming, and consequently on student collaboration. The findings suggest that learning objectives should be formulated in more detail, to highlight essential programming skills, and be expanded to include the use of AI tools. Complex programming assignments remain appropriate in programming education, but pair programming as a didactic approach should be reconsidered in light of the growing use of AI Chatbots.}
}
@article{ISLAM2025,
title = {Memory-enhancing effects of daidzin, possibly through dopaminergic and AChEergic dependent pathways},
journal = {The Journal of Nutrition},
year = {2025},
issn = {0022-3166},
doi = {https://doi.org/10.1016/j.tjnut.2025.04.024},
url = {https://www.sciencedirect.com/science/article/pii/S002231662500269X},
author = {Muhammad Torequl Islam and Abdullah {Al Shamsh Prottay} and Md. Shimul Bhuia and Md. Showkot Akbor and Raihan Chowdhury and Siddique Akber Ansari and Irfan Aamer Ansari and Md. Amirul Islam and Catarina Martins Tahim and Henrique Douglas {Melo Coutinho}},
keywords = {Daidzin, memory-enhancing capacity, dopamine receptors, AChEergic interaction, molecular docking},
abstract = {The soy isoflavone daidzin (DZN) possesses cognitive-enhancing effects in animals. However, the mechanism for this effect is yet to be discovered. For this, we investigate its memory-enhancing capacity using the mouse models of marble burying, dust removal, an open-field study, and in silico studies. Adult male Swiss albino mice were randomly divided into different groups consisting of Control (vehicle: 10 mL/kg), DZN 5, 10, and 20 mg/kg, dopamine (agonist: 22 mg/kg), galantamine (inhibitor: 3 mg/kg), and a combination of DZN-10 with standards. DZN dose-dependently and significantly (p <0.05) increased marble burying and removed dust while decreasing the total distance in OFT. DZN-10 enhanced dopamine’s effect significantly (p < 0.05). In silico findings suggest that DZN has strong binding capacities of –10.3, –7.5, –9.8, and –9.2 kcal/mol to the AChE, D1, D3, and D5 receptors, respectively. Taken together, DZN may exert its memory-enhancing ability by interacting with AChE and dopamine receptors.}
}
@article{TEMIZER2011114,
title = {Thermomechanical contact homogenization with random rough surfaces and microscopic contact resistance},
journal = {Tribology International},
volume = {44},
number = {2},
pages = {114-124},
year = {2011},
issn = {0301-679X},
doi = {https://doi.org/10.1016/j.triboint.2010.09.011},
url = {https://www.sciencedirect.com/science/article/pii/S0301679X10002318},
author = {İ. Temizer},
keywords = {Contact mechanics, Homogenization, Thermal contact resistance, Randomness},
abstract = {We extend an earlier computational thermomechanical contact homogenization framework [Temizer İ, Wriggers P. International Journal for Numerical Methods in Engineering 2010; 83:27–58] to random rough surfaces generated through the random-field model based on the concepts of ensemble averaging and sample enlargement towards the effective limit. Additionally, the homogenization theory is revisited in order to incorporate thermal dissipation at the microscopic contact interface within a thermodynamically consistent approach that preserves dissipation across the scales. Large-scale three-dimensional computations were performed to demonstrate the effectiveness and feasibility of the computational framework for an accurate characterization of the macroscopic thermomechanical response of rough surfaces in contact.}
}
@article{ANDERSON1998159,
title = {Mental retardation general intelligence and modularity},
journal = {Learning and Individual Differences},
volume = {10},
number = {3},
pages = {159-178},
year = {1998},
issn = {1041-6080},
doi = {https://doi.org/10.1016/S1041-6080(99)80128-9},
url = {https://www.sciencedirect.com/science/article/pii/S1041608099801289},
author = {Mike Anderson},
abstract = {This article presents a case for distinguishing between mental retardation as a general deficit of thinking and mental retardation that might result from the global effects of a specific deficit in a cognitive module. Using Anderson's (1992a) theory of the minimal cognitive architecture of intelligence and developmental, I show how this distinction can explain the pattern of intellectual strengths and weaknesses in Savant syndrome, Williams syndrome, Down syndrome, and autism. In addition, I discuss the developmental versus difference view and the distinction between organic and cultural familial mental retardation in the light of this theory. I conclude that not only is there no inherent incompatibility between the constructs of general intelligence and modularity of mind but that both are essential to understanding the different patterns of abilities and developmental profiles found in individuals with low IQ.}
}
@article{PEZZULO2014647,
title = {Internally generated sequences in learning and executing goal-directed behavior},
journal = {Trends in Cognitive Sciences},
volume = {18},
number = {12},
pages = {647-657},
year = {2014},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2014.06.011},
url = {https://www.sciencedirect.com/science/article/pii/S1364661314001570},
author = {Giovanni Pezzulo and Matthijs A.A. {van der Meer} and Carien S. Lansink and Cyriel M.A. Pennartz},
keywords = {forward sweep, generative models, hippocampus, decision making, reinforcement learning, spatial navigation, replay, inference, prospection, theta rhythm, ventral striatum},
abstract = {A network of brain structures including hippocampus (HC), prefrontal cortex, and striatum controls goal-directed behavior and decision making. However, the neural mechanisms underlying these functions are unknown. Here, we review the role of ‘internally generated sequences’: structured, multi-neuron firing patterns in the network that are not confined to signaling the current state or location of an agent, but are generated on the basis of internal brain dynamics. Neurophysiological studies suggest that such sequences fulfill functions in memory consolidation, augmentation of representations, internal simulation, and recombination of acquired information. Using computational modeling, we propose that internally generated sequences may be productively considered a component of goal-directed decision systems, implementing a sampling-based inference engine that optimizes goal acquisition at multiple timescales of on-line choice, action control, and learning.}
}
@article{OSPINAAGUDELO2021121224,
title = {Application domain extension of incremental capacity-based battery SoH indicators},
journal = {Energy},
volume = {234},
pages = {121224},
year = {2021},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2021.121224},
url = {https://www.sciencedirect.com/science/article/pii/S0360544221014729},
author = {Brian {Ospina Agudelo} and Walter Zamboni and Eric Monmasson},
keywords = {Battery, State of health, Battery ageing, Capacity degradation, Incremental capacity, Randomised usage pattern},
abstract = {The Incremental Capacity (IC) analysis is used to characterise the capacity and the battery state of health, aged by cycling patterns with randomly selected pulsed current levels and duration. The batteries are periodically characterised at 1C current, which is a high value with respect to the typical IC tests in pseudo-equilibrium condition. The high-current IC curves generation from raw voltage/current data includes two filtering stages, one for the input voltage and one for the incremental capacity curve smoothing, which are optimised for the application on the basis of the data characteristics. The correlations between the IC main peak features and the battery full capacity for 28 Lithium–Cobalt oxide batteries with 18650 packaging were evaluated, finding that the main peak area is a general feature to evaluate the state of health under high current tests and random usage pattern, and, therefore, it can be used as a battery health indicator in practical applications. The effects of the computational parameters on the relationship between the peak area and the battery capacity are also investigated. The results are confirmed by a further analysis performed over an additional set of cells with different technology, aged with a fixed cycling pattern. Additionally, the performance of the peak area as a health indicator was compared with an ohmic resistance-based estimation approach.}
}
@article{SAAVEDRA20091324,
title = {Experimental transition state for the Corey–Bakshi–Shibata reduction},
journal = {Tetrahedron Letters},
volume = {50},
number = {12},
pages = {1324-1327},
year = {2009},
issn = {0040-4039},
doi = {https://doi.org/10.1016/j.tetlet.2009.01.033},
url = {https://www.sciencedirect.com/science/article/pii/S0040403909000793},
author = {Jaime Saavedra and Sean E. Stafford and Matthew P. Meyer},
abstract = {Asymmetric reductions of prochiral ketones are important transformations in the syntheses of natural products, pharmaceuticals, and fine chemicals. The Corey–Bakshi–Shibata reduction is unique among hydride transfer reductions in its tremendous substrate range and catalytic nature. Here, a coordinated computational and experimental approach is taken toward understanding the origins of the high selectivity and broad substrate range, which are hallmarks of this reduction.}
}
@article{MULLER2021103546,
title = {Kandinsky Patterns},
journal = {Artificial Intelligence},
volume = {300},
pages = {103546},
year = {2021},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2021.103546},
url = {https://www.sciencedirect.com/science/article/pii/S0004370221000977},
author = {Heimo Müller and Andreas Holzinger},
keywords = {Explainable AI, Explainability, Synthetic test data, Ground truth},
abstract = {Kandinsky Figures and Kandinsky Patterns are mathematically describable, simple, self-contained hence controllable synthetic test data sets for the development, validation and training of visual tasks and explainability in artificial intelligence (AI). Whilst Kandinsky Patterns have these computationally manageable properties, they are at the same time easily distinguishable by human observers. Consequently, controlled patterns can be described by both humans and computers. We define a Kandinsky Pattern as a set of Kandinsky Figures, where for each figure an “infallible authority” defines that the figure belongs to the Kandinsky Pattern. With this simple principle we build training and validation data sets for testing explainability, interpretability and context learning. In this paper we describe the basic idea and some underlying principles of Kandinsky Patterns. We provide a Github repository and invite the international AI research community to a challenge to experiment with our Kandinsky Patterns. The goal is to help expand and advance the field of AI, and in particular to contribute to the increasingly important field of explainable AI.}
}
@article{ERIOLI2011729,
title = {Interwoven landscape},
journal = {Procedia Engineering},
volume = {21},
pages = {729-736},
year = {2011},
note = {2011 International Conference on Green Buildings and Sustainable Cities},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2011.11.2071},
url = {https://www.sciencedirect.com/science/article/pii/S1877705811049058},
author = {Alessio Erioli and Mirco Bianchini and Piero Bruschi and Andrea Baschieri},
keywords = {architecture, ecology, infrastructure, highway, photocatalysis, dazzle, new materials ;},
abstract = {Human specie has always engineered the environment to set the conditions for its own settlement, producing in its evolutionary development superorganisms (cities) and the necessary networks of connections among them. Instead of rejecting cars as an extraneous object to a picturesque nature, this project starts from a perspective in which cities and technology are the metabolic extension of human specie and therefore a necessary part of its own nature; the vessels (vehicles) for human transportation, or better, the vehicle-host symbiotic system thus becomes a necessary part of human ecology, and so the network of connections upon which they live, operate and interact with: infrastructures. The project of an environmental enhancer for the Nogara mare highway in Veneto (Italy) provides the unique chance to bring together ecological thinking, host interaction and active materials. Its location (an open country planar area among cultivated fields) enucleates as critical variables the impact of pollutants and the phenomenon of dazzling. With respect to such criticalities, the project uses digital generative and parametric strategies to generate a performative structure in which densification and rarefaction of elements is a local morphological response to dazzle. The structure itself acts as a scaffold for a photo catalytic PET based material that, mimicking the behavior of coccoluti (marine microorganisms) is able to reduce CO2 (and potentially other pollutants) to salts and nitrates that are then naturally deployed to the neighboring cultivated fields as fertilizers. The material has been tested for photo catalytic integration and is currently under development. Present building and production techniques privilege the industrial assembly of inert materials, with a one-way flow of energy and process from raw material to finished product. Instead of this mono-directional energy consumption the project promotes the continuous exchange of information (as code and matter-energy) at all levels and from the digital to the material domains: use of dazzle information, morphogenetic rules and structural behavior to generate the scaffold, a photo catalytic material that responds to pollutants and produces fertilizers, making the structure symbiotic with their hosts and the environment.}
}
@article{YIN2024110392,
title = {Embrace sustainable AI: Dynamic data subset selection for image classification},
journal = {Pattern Recognition},
volume = {151},
pages = {110392},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110392},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324001432},
author = {Zimo Yin and Jian Pu and Ru Wan and Xiangyang Xue},
keywords = {Data selection, Dynamic subset selection, Weighted sampling, Class distribution, Training efficiency},
abstract = {Data selection is commonly used to reduce costs and energy usage by training on a subset of available data. However, determining the appropriate subset size requires extensive dataset knowledge and experimentation, limiting transferability. Varying the validation set also produces unstable results and wastes computational resources. In this paper, we propose a data selection method for dynamically determining subset ratios based on model performance using only a training set. The data search space is narrowed through weighted sampling, leveraging statistical selection patterns. Parallel analysis of class distributions identifies the most representative samples with high selection potential. Extensive experiments validate our approach and demonstrate improved training efficiency. Our method speeds up various subset ratios by up to 2.2x on CIFAR-10, 1.9x on CIFAR-100, 2.0x on TinyImageNet, and 2.3x on ImageNet with negligible accuracy drops.}
}
@article{WU2024111235,
title = {Intelligent strategic bidding in competitive electricity markets using multi-agent simulation and deep reinforcement learning},
journal = {Applied Soft Computing},
volume = {152},
pages = {111235},
year = {2024},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2024.111235},
url = {https://www.sciencedirect.com/science/article/pii/S1568494624000097},
author = {Jiahui Wu and Jidong Wang and Xiangyu Kong},
keywords = {Intelligent bidding strategy, Competitive electricity markets, Multi-agent simulation(MAS), Deep reinforcement learning(DRL), Async n-step QL, Improved Async n-step QL},
abstract = {Aiming at the lack of comprehension of agents in Multi-Agent Simulation (MAS) based on classic Reinforcement Learning algorithms of competitive electricity markets, an intelligent strategic bidding method using Deep Reinforcement Learning (DRL) and MAS is proposed in this paper, which not only can provide more intelligent strategies for market participants to maximize their profits, but can enhance the performance of simulation models dealing with high-dimensional continuous data in electricity markets. Firstly, a theoretical framework of intelligent strategic bidding in competitive electricity markets based on MAS and DRL is proposed, and the process of intelligent bidding in electricity markets based on MAS and DRL is described. Then, three MAS models of intelligent strategic bidding are built based on three classic DRL algorithms, including Deep Q-Network (DQN), Double Deep Q-Network (DDQN), and Asynchronous n-step Q-learning (Async n-step QL), and three algorithms’ convergence speed, computational efficiency, and response sensitivity are compared and analyzed. Finally, a novel Improved Async n-step QL (IAsync n-step QL) algorithm is proposed, the MAS model based on the IAsync n-step QL algorithm for intelligent strategic bidding is established. Simulation results show that the model using the novel DRL algorithm is more profitable and responsive than the classic DRL algorithms.}
}
@article{STEANE2003469,
title = {A quantum computer only needs one universe},
journal = {Studies in History and Philosophy of Science Part B: Studies in History and Philosophy of Modern Physics},
volume = {34},
number = {3},
pages = {469-478},
year = {2003},
note = {Quantum Information and Computation},
issn = {1355-2198},
doi = {https://doi.org/10.1016/S1355-2198(03)00038-8},
url = {https://www.sciencedirect.com/science/article/pii/S1355219803000388},
author = {A.M Steane},
keywords = {Quantum computation, Classical computation, Parallel universes, Entanglement},
abstract = {The nature of quantum computation is discussed. It is argued that, in terms of the amount of information manipulated in a given time, quantum and classical computation are equally efficient. Quantum superposition does not permit quantum computers to “perform many computations simultaneously” except in a highly qualified and to some extent misleading sense. Quantum computation is therefore not well described by interpretations of quantum mechanics which invoke the concept of vast numbers of parallel universes. Rather, entanglement makes available types of computation processes which, while not exponentially larger than classical ones, are unavailable to classical systems. The essence of quantum computation is that it uses entanglement to generate and manipulate a physical representation of the correlations between logical entities, without the need to completely represent the logical entities themselves.}
}
@incollection{AKAN20253,
title = {Chapter 0 - From the ground up!},
editor = {Aydin Akan and Luis F. Chaparro},
booktitle = {Signals and Systems Using MATLAB ® (Fourth Edition)},
publisher = {Academic Press},
edition = {Fourth Edition},
pages = {3-62},
year = {2025},
isbn = {978-0-443-15709-7},
doi = {https://doi.org/10.1016/B978-0-44-315709-7.00009-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780443157097000094},
author = {Aydin Akan and Luis F. Chaparro},
keywords = {Signals and systems, mathematical models, digital signal processing applications, concrete mathematics, complex variables, system dynamics, MATLAB},
abstract = {This chapter provides an overview of the material in the book, briefly illustrates the applications and highlights the mathematical background needed to understand the analysis of signals and systems. A signal is a function of time like a voice signal, or of space like an image, or of time and space like a video. A system then is a mathematical model of a device, just like the ordinary differential equations representing circuits. We illustrate the importance of the theory of signals and systems by means of practical applications, hint to how to implement them, and connect concepts in Calculus with more concrete mathematics from a computational point of view—using computers. A review of complex variables and their connection with the dynamics of systems is given. We end the chapter with a soft introduction to MATLAB®, a widely used high-level computational tool for analysis and design.}
}
@article{PALANIYAPPAN2020109911,
title = {Cortical thickness and formal thought disorder in schizophrenia: An ultra high-field network-based morphometry study},
journal = {Progress in Neuro-Psychopharmacology and Biological Psychiatry},
volume = {101},
pages = {109911},
year = {2020},
issn = {0278-5846},
doi = {https://doi.org/10.1016/j.pnpbp.2020.109911},
url = {https://www.sciencedirect.com/science/article/pii/S0278584619310309},
author = {Lena Palaniyappan and Ali Al-Radaideh and Penny A. Gowland and Peter F. Liddle},
keywords = {Disorganisation, Thought disorder, Salience network, Cognitive control, Language network, Coherence},
abstract = {Background
Persistent formal thought disorder (FTD) is a core feature of schizophrenia. Recent cognitive and neuroimaging studies indicate a distinct mechanistic pathway underlying the persistent positive FTD (pFTD or disorganized thinking), though its structural determinants are still elusive. Using network-based cortical thickness estimates from ultra-high field 7-Tesla Magnetic Resonance Imaging (7T MRI), we investigated the structural correlates of pFTD.
Methods
We obtained speech samples and 7T MRI anatomical scans from medicated clinically stable patients with schizophrenia (n = 19) and healthy controls (n = 20). Network-based morphometry was used to estimate the mean cortical thickness of 17 functional networks covering the entire cortical surface from each subject. We also quantified the vertexwise variability of thickness within each network to quantify the spatial coherence of the 17 networks, estimated patients vs. controls differences, and related the thickness of the affected networks to the severity of pFTD.
Results
Patients had reduced thickness of the frontoparietal and default mode networks, and reduced spatial coherence affecting the salience and the frontoparietal control network. A higher burden of positive FTD related to reduced frontoparietal thickness and reduced spatial coherence of the salience network. The presence of positive FTD, but not its severity, related to the reduced thickness of the language network comprising of the superior temporal cortex.
Conclusions
These results suggest that cortical thickness of both cognitive control and language networks underlie the positive FTD in schizophrenia. The structural integrity of cognitive control networks is a critical determinant of the expressed severity of persistent FTD in schizophrenia.}
}
@article{YAHIAOUI20243958,
title = {Two parallel expansions for improving supersonic axisymmetric nozzle performance},
journal = {Advances in Space Research},
volume = {74},
number = {8},
pages = {3958-3982},
year = {2024},
issn = {0273-1177},
doi = {https://doi.org/10.1016/j.asr.2024.06.066},
url = {https://www.sciencedirect.com/science/article/pii/S0273117724006562},
author = {Toufik Yahiaoui},
keywords = {MLN, BPN, DEN, HT, MOC, Error computation},
abstract = {The aim of this work is to develop a numerical computation program allowing designing new contours of a supersonic axisymmetric nozzle having two expansions at the throat, named by DEN (Dual Expansion Nozzle). This new nozzle gives a uniform and parallel flow at the exit section, to improve considerably the performances compared to the conventional Minimum Length Nozzle (MLN), and the Best Performances Nozzle (BPN). The present nozzle has a two unknowns external and central body curved walls. Each of them is started by an initial expansion angle to give a uniform and horizontal flow at the exit section. Two others transition regions are calculated in parallel with the contours points to give the desired exit Mach number. The walls are determined point by point by the High Temperature Method of Characteristics (HT MOC) model. The resolution of the four compatibility and characteristics equations is done numerically by the finite difference predictor corrector algorithm. The validation of the results is controlled by the convergence of the calculated critical sections ratio to that given by the theory. The design depends on four parameters, where MLN and BPN become special cases of DEN. A comparison is made with MLN, since it is currently used in the aerospace propulsion and with BPN aiming to improve their performances. The comparison is made for the same critical mass flow rate. The results demonstrate a remarkable reduction up of 45 %, and 52 % in the mass of DEN when the exit Mach number ME = 3.00 and the stagnation temperature T0 = 2000 K. The application is made for air and for future aerospace missiles in order to improve their trajectory parameters. The chosen example demonstrates an improvement of 13 % and 16 % on the missile range compared, respectively to MLN, and BPN.}
}
@article{LI2025113564,
title = {Exploring formal defeasible reasoning of large language models: A Chain-of-Thought approach},
journal = {Knowledge-Based Systems},
pages = {113564},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.113564},
url = {https://www.sciencedirect.com/science/article/pii/S0950705125006100},
author = {Zhaoqun Li and Chen Chen and Mengze Li and Beishui Liao},
keywords = {Defeasible reasoning, Large language model, Defeasible logic programming},
abstract = {Defeasible reasoning, critical for commonsense reasoning and uncertainty handling, has garnered significant attention in AI community. This interest is particularly pronounced in the development and evaluation of Large Language Models (LLMs), which often involve reconciling inconsistent and incomplete knowledge. However, it remains uncertain whether LLMs possess generalizable defeasible reasoning abilities. Besides, the lack of a formal defeasible reasoning benchmark and appropriate evaluations limits further exploration in this domain. In this study, we aim to investigate the capacity of LLMs for defeasible reasoning, particularly within the framework of formal defeasible logic. Specifically, we select the popular defeasible logic framework, DeLP, as the basis for evaluating the LLMs’ defeasible logical reasoning capabilities. We initially create a synthetic dataset comprising logical programs that encompass a variety of DeLP programs with differing depths of reasoning. To address the challenges encountered during inference, we introduce a Chain-of-Thought (CoT) framework that prompts LLMs to conduct formal deduction and engage in multi-step defeasible reasoning, thereby enhancing problem-solving performance. Employing this argumentative solving approach, we observe that LLMs struggle to manage defeasible information effectively. This observation raises questions about whether contemporary LLMs possess reasoning abilities comparable to human intelligence, challenging the reliable deployment and advancement of AI systems in real-world scenarios.}
}
@article{MAMAT201311,
title = {MAR: Maximum Attribute Relative of soft set for clustering attribute selection},
journal = {Knowledge-Based Systems},
volume = {52},
pages = {11-20},
year = {2013},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2013.05.009},
url = {https://www.sciencedirect.com/science/article/pii/S0950705113001706},
author = {Rabiei Mamat and Tutut Herawan and Mustafa Mat Deris},
keywords = {Data mining, Soft set theory, Clustering attributes, Attribute relative, Complexity},
abstract = {Clustering, which is a set of categorical data into a homogenous class, is a fundamental operation in data mining. One of the techniques of data clustering was performed by introducing a clustering attribute. A number of algorithms have been proposed to address the problem of clustering attribute selection. However, the performance of these algorithms is still an issue due to high computational complexity. This paper proposes a new algorithm called Maximum Attribute Relative (MAR) for clustering attribute selection. It is based on a soft set theory by introducing the concept of the attribute relative in information systems. Based on the experiment on fourteen UCI datasets and a supplier dataset, the proposed algorithm achieved a lower computational time than the three rough set-based algorithms, i.e. TR, MMR, and MDA up to 62%, 64%, and 40% respectively and compared to a soft set-based algorithm, i.e. NSS up to 33%. Furthermore, MAR has a good scalability, i.e. the executing time of the algorithm tends to increase linearly as the number of instances and attributes are increased respectively.}
}
@incollection{MARCHAND201831,
title = {Chapter 2 - Analogical Mapping in Numerical Development},
editor = {Daniel B. Berch and David C. Geary and Kathleen {Mann Koepke}},
booktitle = {Language and Culture in Mathematical Cognition},
publisher = {Academic Press},
pages = {31-47},
year = {2018},
series = {Mathematical Cognition and Learning},
isbn = {978-0-12-812574-8},
doi = {https://doi.org/10.1016/B978-0-12-812574-8.00002-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012812574800002X},
author = {Elisabeth Marchand and David Barner},
keywords = {Analogy, Number acquisition, Successor function, Numerical estimation, Structure mapping},
abstract = {This chapter outlines the contribution of analogical thinking in numerical cognition and specifically to number-word learning and numerical estimation. We begin with an overview of number-word learning, followed by a description of analogical mapping as defined by Gentner, 1983, Gentner, 2010, and discuss how children might acquire the meaning of counting based on analogical mapping. Next, we review the claim that very similar processes of analogical mapping may support numerical estimation, based on findings from studies of dot-array and number-line estimation. These studies suggest that children's knowledge of how the count list is structured and in particular the ordering and distance between numbers affects their ability to make accurate estimates. Finally, we discuss extensions of this idea to other cases where analogy has been proposed as a source of representational change. We conclude that analogical mappings enrich how humans transcend core numerical abilities to represent abstract content.}
}
@article{LIN2024122254,
title = {Reinforcement learning and bandits for speech and language processing: Tutorial, review and outlook},
journal = {Expert Systems with Applications},
volume = {238},
pages = {122254},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.122254},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423027562},
author = {Baihan Lin},
keywords = {Reinforcement learning, Bandits, Speech processing, Natural language processing, Speech recognition, Large language models, Survey, Perspective},
abstract = {In recent years, reinforcement learning and bandits have transformed a wide range of real-world applications including healthcare, finance, recommendation systems, robotics, and last but not least, the speech and natural language processing. While most speech and language applications of reinforcement learning algorithms are centered around improving the training of deep neural networks with its flexible optimization properties, there are still many grounds to explore to utilize the benefits of reinforcement learning, such as its reward-driven adaptability, state representations, temporal structures and generalizability. In this survey, we present an overview of recent advancements of reinforcement learning and bandits including those in the large language models, and discuss how they can be effectively employed to solve speech and natural language processing problems with models that are adaptive, interactive and scalable.}
}
@article{YU2022158,
title = {Bioinspired interactive neuromorphic devices},
journal = {Materials Today},
volume = {60},
pages = {158-182},
year = {2022},
issn = {1369-7021},
doi = {https://doi.org/10.1016/j.mattod.2022.09.012},
url = {https://www.sciencedirect.com/science/article/pii/S1369702122002413},
author = {Jinran Yu and Yifei Wang and Shanshan Qin and Guoyun Gao and Chong Xu and Zhong {Lin Wang} and Qijun Sun},
keywords = {Neuromorphic devices, Synaptic transistors, Interactive, Neuromorphic computing, Bioinspired},
abstract = {The performance of conventional computer based on von Neumann architecture is limited due to the physical separation of memory and processor. By synergistically integrating various sensors with synaptic devices, recently emerging interactive neuromorphic devices can directly sense/store/process various stimuli information from external environments and implement functions of perception, learning, memory, and computation. In this review, we present the basic model of bioinspired interactive neuromorphic devices and discuss the performance metrics. Next, we summarize the recent progress and development of bioinspired interactive neuromorphic devices, which are classified into neuromorphic tactile systems, visual systems, auditory systems, and multisensory system. They are discussed in detail from the aspects of materials, device architectures, operating mechanisms, synaptic plasticity, and potential applications. Additionally, the bioinspired interactive neuromorphic devices that can fuse multiple/mixed sensing signals are proposed to address more realistic and sophisticated problems. Finally, we discuss the pros and cons regarding to the computing neurons and integrating sensory neurons and deliver the perspectives on interactive neuromorphic devices at the material, device, network, and system levels. It is believed the neuromorphic devices can provide promising solutions to next generation of interactive sensation/memory/computation toward the development of multimodal, low-power, and large-scale intelligent systems endowed with neuromorphic features.}
}
@article{KWIATKOWSKA201335,
title = {Fuzzy logic and semiotic methods in modeling of medical concepts},
journal = {Fuzzy Sets and Systems},
volume = {214},
pages = {35-50},
year = {2013},
note = {Soft Computing in the Humanities and Social Sciences},
issn = {0165-0114},
doi = {https://doi.org/10.1016/j.fss.2012.03.011},
url = {https://www.sciencedirect.com/science/article/pii/S0165011412001376},
author = {Mila Kwiatkowska and Krzysztof Kielan},
keywords = {Fuzzy system models, Medicine, Cognitive sciences, Decision support systems, Depression},
abstract = {The field of medicine is a quickly growing area of application for computer-based systems. However, the use of computerized methods in this knowledge-intensive and expert-based discipline brings multiple challenges. The major problem is the modeling, representing, and interpreting of diverse medical concepts. For example, some symptoms and their etiologies are described in terms of molecular biology and genetics, physiological processes are defined using models from chemistry and physics; yet mental disorders are defined in more subjective terms of feelings, behaviours, habits, and life events. Thus, the representation of medical concepts must be sufficiently expressive to model concepts which are inherently complex, context-dependent, evolving, and often imprecise. Furthermore, the representation must be formal or, at least, sufficiently rigorous in order to be processed by computers and at the same time, the representation must be human-readable in order to be validated by humans. In this paper, we describe the modeling process of medical concepts as a mapping from the real-world medical concepts into their computational models, and further into their physical implementation. First, we define the notion of a concept as a fundamental unit of knowledge and specify the fundamental principles of the computational representation of a concept. Second, we describe the characteristics of medical concepts, specifically their historical and cultural changeability, their social and cultural ambiguity, and their varied levels of precision. Third, we present a meta-modeling framework for computational representation of medical concepts. Our framework is based on fuzzy logic and semiotic methods which allow us to explicitly model two important characteristics of medical concepts: imprecision and context-dependency. We present the framework using an example of a mental disorder, specifically, the concept of clinical depression. To exemplify the changeable and evolutionary character of medical concepts, we discuss the development of the diagnostic criteria for depression. Finally, we use the example of the assessment of depression to describe the computational representation for polythetic and multi-dimensional concepts and for categorical and non-categorical concepts. We demonstrate how the proposed modeling framework utilizes (1) a fuzzy-logic approach to represent the non-categorical (continuous) nature of the symptoms and (2) a semiotic approach to represent the polythetic (contextual interpretation) and dimensional nature of the symptoms.}
}
@article{MARTIN20102089,
title = {Integrating learning theories and application-based modules in teaching linear algebra},
journal = {Linear Algebra and its Applications},
volume = {432},
number = {8},
pages = {2089-2099},
year = {2010},
note = {Special issue devoted to the 15th ILAS Conference at Cancun, Mexico, June 16-20, 2008},
issn = {0024-3795},
doi = {https://doi.org/10.1016/j.laa.2009.08.030},
url = {https://www.sciencedirect.com/science/article/pii/S0024379509004704},
author = {William Martin and Sergio Loch and Laurel Cooley and Scott Dexter and Draga Vidakovic},
keywords = {Linear algebra, Learning theory, Curriculum, Pedagogy, Constructivist theories, APOS – Action-Process-Object-Schema, Theoretical framework, Encapsulated process, Thematicized schema, Triad – intra, Inter, Trans, Genetic decomposition, Vector addition, Matrix, Matrix multiplication, Matrix representation, Basis, Column space, Row space, Null space, Eigenspace, Transformation},
abstract = {The research team of The Linear Algebra Project developed and implemented a curriculum and a pedagogy for parallel courses in (a) linear algebra and (b) learning theory as applied to the study of mathematics with an emphasis on linear algebra. The purpose of the ongoing research, partially funded by the National Science Foundation, is to investigate how the parallel study of learning theories and advanced mathematics influences the development of thinking of individuals in both domains. The researchers found that the particular synergy afforded by the parallel study of math and learning theory promoted, in some students, a rich understanding of both domains and that had a mutually reinforcing effect. Furthermore, there is evidence that the deeper insights will contribute to more effective instruction by those who become high school math teachers and, consequently, better learning by their students. The courses developed were appropriate for mathematics majors, pre-service secondary mathematics teachers, and practicing mathematics teachers. The learning seminar focused most heavily on constructivist theories, although it also examined socio-cultural and historical perspectives. A particular theory, Action–Process–Object–Schema (APOS) [10], was emphasized and examined through the lens of studying linear algebra. APOS has been used in a variety of studies focusing on student understanding of undergraduate mathematics. The linear algebra courses include the standard set of undergraduate topics. This paper reports the results of the learning theory seminar and its effects on students who were simultaneously enrolled in linear algebra and students who had previously completed linear algebra and outlines how prior research has influenced the future direction of the project.}
}
@article{UPADHYAY2024109796,
title = {Advancements in Alzheimer's disease classification using deep learning frameworks for multimodal neuroimaging: A comprehensive review},
journal = {Computers and Electrical Engineering},
volume = {120},
pages = {109796},
year = {2024},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2024.109796},
url = {https://www.sciencedirect.com/science/article/pii/S0045790624007237},
author = {Prashant Upadhyay and Pradeep Tomar and Satya Prakash Yadav},
keywords = {Multimodal, Neuroimaging, Alzheimer's disease, Classification, Images, Feature extraction},
abstract = {Over the past years, Alzheimer's disease has emerged as a serious concern for people's health. Researchers are facing challenges in effectively categorizing and diagnosing the different stages of Alzheimer's disease (AD). Current promising studies have shown that multimodal Neuroimaging has the potential to offer vital information about the structural and functional alterations associated with Alzheimer's. Using advanced computational techniques, Machine Learning calculations have been demonstrated to be highly precise in deciphering patterns and connections within the multimodal Neuroimaging data, eventually aiding in the arrangement of Alzheimer's illness stages. This research aimed to survey the adequacy of Machine Learning techniques in correctly categorizing stages of Alzheimer's disease by working on multiple neuroimaging modalities. In this review, a detailed analysis was carried out on the classification algorithms included. The study specifically examines publications published between 2016 and 2024. From the review, it was found that deep learning frameworks are more robust in Alzheimer's disease classification.}
}
@article{YUAN202317,
title = {MFGAD: Multi-fuzzy granules anomaly detection},
journal = {Information Fusion},
volume = {95},
pages = {17-25},
year = {2023},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2023.02.007},
url = {https://www.sciencedirect.com/science/article/pii/S1566253523000490},
author = {Zhong Yuan and Hongmei Chen and Chuan Luo and Dezhong Peng},
keywords = {Granular computing, Fuzzy rough set theory, Unsupervised anomaly detection, Multi-granularity, Hybrid data},
abstract = {Unsupervised anomaly detection is an important research direction in the process of unsupervised knowledge acquisition. It has been successfully applied in many fields, such as online fraud identification, loan approval, and medical diagnosis. Multi-granularity thinking is an effective information fusion method for solving problems in a multi-granular environment, which allows people to understand and analyze problems from multiple perspectives. However, there are few studies on building anomaly detection models using the idea of multi-fuzzy granules. To this end, this paper constructs a multi-fuzzy granules anomaly detection method by using a fuzzy rough computing model. In this method, a hybrid metric is first used to calculate the fuzzy relations. Then, two ranking sequences are constructed based on the significance of attributes. Furthermore, forward and reverse multi-fuzzy granules are constructed to define anomaly scores based on the ranking sequences. Finally, a multi-fuzzy granules-based anomaly detection algorithm is designed to detect anomalies. The experimental results compared with existing algorithms show the effectiveness of the proposed algorithm.}
}
@article{LIN2023103217,
title = {A novel personality detection method based on high-dimensional psycholinguistic features and improved distributed Gray Wolf Optimizer for feature selection},
journal = {Information Processing & Management},
volume = {60},
number = {2},
pages = {103217},
year = {2023},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2022.103217},
url = {https://www.sciencedirect.com/science/article/pii/S0306457322003181},
author = {Hao Lin and Chundong Wang and Qingbo Hao},
keywords = {Personality detection, Feature selection, Symmetric uncertainty, Grey Wolf Optimizer, Spark},
abstract = {Existing personality detection methods based on user-generated text have two major limitations. First, they rely too much on pre-trained language models to ignore the sentiment information in psycholinguistic features. Secondly, they have no consensus on the psycholinguistic feature selection, resulting in the insufficient analysis of sentiment information. To tackle these issues, we propose a novel personality detection method based on high-dimensional psycholinguistic features and improved distributed Gray Wolf Optimizer (GWO) for feature selection (IDGWOFS). Specifically, we introduced the Gaussian Chaos Map-based initialization and neighbor search strategy into the original GWO to improve the performance of feature selection. To eliminate the bias generated when using mutual information to select features, we adopt symmetric uncertainty (SU) instead of mutual information as the evaluation for correlation and redundancy to construct the fitness function, which can balance the correlation between features–labels and the redundancy between features–features. Finally, we improve the common Spark-based parallelization design of GWO by parallelizing only the fitness computation steps to improve the efficiency of IDGWOFS. The experiments indicate that our proposed method obtains average accuracy improvements of 3.81% and 2.19%, and average F1 improvements of 5.17% and 5.8% on Essays and Kaggle MBTI dataset, respectively. Furthermore, IDGWOFS has good convergence and scalability.}
}
@incollection{CHOUBEY2025319,
title = {Chapter 25 - Future directions on systems biology},
editor = {Babak Sokouti},
booktitle = {Systems Biology and In-Depth Applications for Unlocking Diseases},
publisher = {Academic Press},
pages = {319-328},
year = {2025},
isbn = {978-0-443-22326-6},
doi = {https://doi.org/10.1016/B978-0-443-22326-6.00025-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780443223266000250},
author = {Jyotsna Choubey and Jyoti Kant Choudhari and Biju Prava Sahariah},
keywords = {Agriculture, Biomedical research, Bioremediation, Bioresources, Drug discovery, Genetics, Healthcare and medicine, Proteomics},
abstract = {Biologists integrate engineering principles to design, construct, and transform biological systems for specific intents. Biological engineering involves creating new biological components, technologies, and systems, as well as redesigning existing ones, to execute specific functions and solve specific biological problems. To accomplish this objective, engineers utilize their expertise in the creation, assembly, and manipulation of biological systems, such as DNA and cells. The shift in the research paradigm toward systems biology can be largely attributed to significant advancements in protein and DNA sequencing technology. This approach is characterized by its emphasis on creating predictive models that can be applied to all levels of structural hierarchies found in biological systems. In addition, there is a strong focus on integrating data from various scales. The ultimate goal of systems biology is to create bio-based technologies that can be applied in a wide range of fields, including pharmaceuticals, health science, environmental remediation, energy production, and biotechnology. The impact of systems biology is increasingly being observed in various aspects of our lives, and this influence is anticipated to gain momentum in the future. This chapter centers on the historical background and extent of systems biology, its implementation in diverse domains, and its prospective future in various branches of biology.}
}
@article{LAWSON2013284,
title = {Sensory connection, interest/attention and gamma synchrony in autism or autism, brain connections and preoccupation},
journal = {Medical Hypotheses},
volume = {80},
number = {3},
pages = {284-288},
year = {2013},
issn = {0306-9877},
doi = {https://doi.org/10.1016/j.mehy.2012.12.005},
url = {https://www.sciencedirect.com/science/article/pii/S0306987712005415},
author = {Wendy Lawson},
abstract = {Does motivational interest increase gamma synchrony across neuronal networking to enable computation of related sensory inputs that might lead to greater social understanding in autism spectrum conditions (ASC)? Meaning, is it possible/likely that in autism because individuals process one aspect of sensory input at any one time (therefore missing the wider picture in general) when they are motivated/interested or attending to particular stimuli their attention window is widened due to increased gamma synchrony and they might be enabled to connect in ways that do not occur when they are not motivated? This is my current research question. If gamma synchrony is helping with the binding of information from collective sensory inputs, in ASC, when and only if the individual is motivated, then this has huge potential for how learning might be encouraged for individuals with an ASC.}
}
@article{ALDULAIMY2024101272,
title = {The computing continuum: From IoT to the cloud},
journal = {Internet of Things},
volume = {27},
pages = {101272},
year = {2024},
issn = {2542-6605},
doi = {https://doi.org/10.1016/j.iot.2024.101272},
url = {https://www.sciencedirect.com/science/article/pii/S2542660524002130},
author = {Auday Al-Dulaimy and Matthijs Jansen and Bjarne Johansson and Animesh Trivedi and Alexandru Iosup and Mohammad Ashjaei and Antonino Galletta and Dragi Kimovski and Radu Prodan and Konstantinos Tserpes and George Kousiouris and Chris Giannakos and Ivona Brandic and Nawfal Ali and André B. Bondi and Alessandro V. Papadopoulos},
keywords = {Computing continuum, Cloud computing, Fog computing, Edge computing, Mobile cloud computing, Multi-access edge computing, SDN, NFV, IoT, Use case, Reference architecture},
abstract = {In the era of the IoT revolution, applications are becoming ever more sophisticated and accompanied by diverse functional and non-functional requirements, including those related to computing resources and performance levels. Such requirements make the development and implementation of these applications complex and challenging. Computing models, such as cloud computing, can provide applications with on-demand computation and storage resources to meet their needs. Although cloud computing is a great enabler for IoT and endpoint devices, its limitations make it unsuitable to fulfill all design goals of novel applications and use cases. Instead of only relying on cloud computing, leveraging and integrating resources at different layers (like IoT, edge, and cloud) is necessary to form and utilize a computing continuum. The layers’ integration in the computing continuum offers a wide range of innovative services, but it introduces new challenges (e.g., monitoring performance and ensuring security) that need to be investigated. A better grasp and more profound understanding of the computing continuum can guide researchers and developers in tackling and overcoming such challenges. Thus, this paper provides a comprehensive and unified view of the computing continuum. The paper discusses computing models in general with a focus on cloud computing, the computing models that emerged beyond the cloud, and the communication technologies that enable computing in the continuum. In addition, two novel reference architectures are presented in this work: one for edge–cloud computing models and the other for edge–cloud communication technologies. We demonstrate real use cases from different application domains (like industry and science) to validate the proposed reference architectures, and we show how these use cases map onto the reference architectures. Finally, the paper highlights key points that express the authors’ vision about efficiently enabling and utilizing the computing continuum in the future.}
}
@article{PHILLIPS200930,
title = {Fiber tractography reveals disruption of temporal lobe white matter tracts in schizophrenia},
journal = {Schizophrenia Research},
volume = {107},
number = {1},
pages = {30-38},
year = {2009},
issn = {0920-9964},
doi = {https://doi.org/10.1016/j.schres.2008.10.019},
url = {https://www.sciencedirect.com/science/article/pii/S0920996408004854},
author = {Owen R. Phillips and Keith H. Nuechterlein and Kristi A. Clark and Liberty S. Hamilton and Robert F. Asarnow and Nathan S. Hageman and Arthur W. Toga and Katherine L. Narr},
keywords = {Diffusion tensor imaging, White matter, Uncinate fasciculus, Inferior longitudinal fasciculus, Arcuate fasciculus, Fractional anisotropy},
abstract = {Diffusion tensor imaging (DTI) studies have demonstrated abnormal anisotropic diffusion in schizophrenia. However, examining data with low spatial resolution and/or a low number of gradient directions and limitations associated with analysis approaches sensitive to registration confounds may have contributed to mixed findings concerning the regional specificity and direction of results. This study examined three major white matter tracts connecting lateral and medial temporal lobe regions with neocortical association regions widely implicated in systems-level functional and structural disturbances in schizophrenia. Using DTIstudio, a previously validated regions of interest tractography method was applied to 30 direction diffusion weighted imaging data collected from demographically similar schizophrenia (n=23) and healthy control subjects (n=22). The diffusion tensor was computed at each voxel after intra-subject registration of diffusion-weighted images. Three-dimensional tract reconstruction was performed using the Fiber Assignment by Continuous Tracking (FACT) algorithm. Tractography results showed reduced fractional anisotropy (FA) of the arcuate fasciculi (AF) and inferior longitudinal fasciculi (ILF) in patients compared to controls. FA changes within the right ILF were negatively correlated with measures of thinking disorder. Reduced volume of the left AF was also observed in patients. These results, which avoid registration issues associated with voxel-based analyses of DTI data, support that fiber pathways connecting lateral and medial temporal lobe regions with neocortical regions are compromised in schizophrenia. Disruptions of connectivity within these pathways may potentially contribute to the disturbances of memory, language, and social cognitive processing that characterize the disorder.}
}
@article{BENDER2024156,
title = {Dimension results for extremal-generic polynomial systems over complete toric varieties},
journal = {Journal of Algebra},
volume = {646},
pages = {156-182},
year = {2024},
issn = {0021-8693},
doi = {https://doi.org/10.1016/j.jalgebra.2024.01.029},
url = {https://www.sciencedirect.com/science/article/pii/S0021869324000553},
author = {Matías Bender and Pierre-Jean Spaenlehauer},
keywords = {Sparse polynomial systems, Toric varieties},
abstract = {We study polynomial systems with prescribed monomial supports in the Cox ring of a toric variety built from a complete polyhedral fan. We present combinatorial formulas for the dimension of their associated subvarieties under genericity assumptions on the coefficients of the polynomials. Using these formulas, we identify at which degrees generic systems in polytopal algebras form regular sequences. Our motivation comes from sparse elimination theory, where knowing the expected dimension of these subvarieties leads to specialized algorithms and to large speed-ups for solving sparse polynomial systems. As a special case, we classify the degrees at which regular sequences defined by weighted homogeneous polynomials can be found, answering an open question in the Gröbner bases literature. We also show that deciding whether a sparse system is generically a regular sequence in a polytopal algebra is hard from the point of view of theoretical computational complexity.}
}
@article{LIU2024115,
title = {Water Quality System Informatics: An Emerging Inter-Discipline of Environmental Engineering},
journal = {Engineering},
volume = {43},
pages = {115-124},
year = {2024},
issn = {2095-8099},
doi = {https://doi.org/10.1016/j.eng.2024.03.018},
url = {https://www.sciencedirect.com/science/article/pii/S2095809924002601},
author = {Hong Liu and Zhaoming Chen and Zhiwei Wang and Ming Xu and Yutao Wang and Jinju Geng and Fengjun Yin},
keywords = {Water quality system, Water quality system informatics, Environmental engineering, Emerging interdisciplinary, Research pattern},
abstract = {Water quality system informatics (WQSI) is an emerging field that employs cybernetics to collect and digitize data associated with water quality. It involves monitoring the physical, chemical, and biological processes that affect water quality and the ecological impacts and interconnections within water quality systems. WQSI integrates theories and methods from water quality engineering, information engineering, and system control theory, enabling the intelligent management and control of water quality. This integration revolutionizes the understanding and management of water quality systems with greater precision and higher resolution. WQSI is a new stage of development in environmental engineering that is driven by the digital age. This work explores the fundamental concepts, research topics, and methods of WQSI and its features and potential to promote disciplinary development. The innovation and development of WQSI are crucial for driving the digital and intelligent transformation of national industry patterns in China, positioning China at the forefront of environmental engineering and ecological environment research on a global scale.}
}
@article{ZHANG2024603,
title = {An intelligent management and decision model of operational research for edge computing aided planning},
journal = {Alexandria Engineering Journal},
volume = {109},
pages = {603-609},
year = {2024},
issn = {1110-0168},
doi = {https://doi.org/10.1016/j.aej.2024.09.072},
url = {https://www.sciencedirect.com/science/article/pii/S1110016824010962},
author = {Yuanshou Zhang},
keywords = {Intelligent Decision-making Model, Edge Algorithm, Task Offloading, Mobile Edge Computing Server},
abstract = {With the development of “Internet plus” and the deep integration of information and network based technology and network technology, Industry 4.0 has become a hot topic. Industrial production is faced with a large number of tasks, complex and changeable demands, and difficult to predict. In order to solve a series of objective problems such as task delay in actual production, this paper proposed a method based on edge computing (EC) to reduce the delay to meet the real-time requirements of industrial production. However, due to the limitation of its computing power and storage capacity, it is difficult to adapt to large-scale data decision-making. In terms of the lag rate of the algorithm in the experiment of the intelligent decision model, when the number of tasks of EC algorithm was 15 and 6, the lag rate was the highest and the lowest, and its values were 16 % and 2 % respectively. Therefore, it can be seen that EC algorithm can play a good role in the intelligent management and decision-making model of operational research.}
}
@article{CASAJUS202488,
title = {Random partitions, potential, value, and externalities},
journal = {Games and Economic Behavior},
volume = {147},
pages = {88-106},
year = {2024},
issn = {0899-8256},
doi = {https://doi.org/10.1016/j.geb.2024.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S089982562400085X},
author = {André Casajus and Yukihiko Funaki and Frank Huettner},
keywords = {Shapley value, Partition function form, Random partition, Restriction operator, Ewens distribution, Chinese restaurant process, Potential, Externalities, Null player, Expected accumulated worth},
abstract = {The Shapley value equals a player's contribution to the potential of a game. The potential is a most natural one-number summary of a game, which can be computed as the expected accumulated worth of a random partition of the players. This computation integrates the coalition formation of all players and readily extends to games with externalities. We investigate those potential functions for games with externalities that can be computed this way. It turns out that the potential that corresponds to the MPW solution introduced by Macho-Stadler et al. (2007, J. Econ. Theory 135, 339–356) is unique in the following sense. It is obtained as the expected accumulated worth of a random partition, it generalizes the potential for games without externalities, and it induces a solution that satisfies the null player property even in the presence of externalities.}
}
@article{PERRIN20227006,
title = {Malonic Anhydrides, Challenges from a Simple Structure},
journal = {The Journal of Organic Chemistry},
volume = {87},
number = {11},
pages = {7006-7012},
year = {2022},
issn = {0022-3263},
doi = {https://doi.org/10.1021/acs.joc.2c00453},
url = {https://www.sciencedirect.com/science/article/pii/S0022326322022642},
author = {Charles L. Perrin},
abstract = {ABSTRACT
After many years of unsuccessful attempts, monomeric malonic anhydrides were prepared by ozonolysis of ketene dimers, a procedure validated by model studies. The structure proof relied most heavily on IR absorption at 1820 cm–1 and a Raman band at 1947 cm–1. Malonic anhydrides are unstable, decomposing below room temperature to a ketene plus carbon dioxide. Surprisingly, according to kinetic studies, the dimethyl derivative is slightly less unstable than the parent, and the monomethyl is the fastest to decompose, with an enthalpy of activation of only 12.6 kcal/mol. Computations rationalize this behavior in terms of a concerted [2s + 2a] cycloreversion that requires a more highly organized transition state, as also manifested by a negative entropy of activation.}
}
@article{KAZIEVA20242933,
title = {Unpacking Complex Concepts to Enhance Use of Dynamic Simulations},
journal = {Procedia Computer Science},
volume = {246},
pages = {2933-2942},
year = {2024},
note = {28th International Conference on Knowledge Based and Intelligent information and Engineering Systems (KES 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.09.377},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924024098},
author = {Victoria Kazieva},
keywords = {simulation modeling, decision-making, complex systems, dynamic simulations, agent-based modeling, system dynamics},
abstract = {The paper suggests a research framework that can aid simulation model designers and users in understanding and modeling complex concepts. The aim is to enhance the role of simulation in supporting decision-making with agent-based modeling and system dynamics by investigating literature that outlines challenges in the field. This study advocates for involving decision-makers in unpacking the overarching concept into manageable components that are associates with the top concept. The research framework generates insights into the interpretation of complex concepts and guides model designers in formulating concrete variables for simulation modeling through successive iterations of the unpacking practices.}
}
@article{PETRELLA2024139,
title = {The AI Future of Emergency Medicine},
journal = {Annals of Emergency Medicine},
volume = {84},
number = {2},
pages = {139-153},
year = {2024},
issn = {0196-0644},
doi = {https://doi.org/10.1016/j.annemergmed.2024.01.031},
url = {https://www.sciencedirect.com/science/article/pii/S019606442400043X},
author = {Robert J. Petrella},
abstract = {In the coming years, artificial intelligence (AI) and machine learning will likely give rise to profound changes in the field of emergency medicine, and medicine more broadly. This article discusses these anticipated changes in terms of 3 overlapping yet distinct stages of AI development. It reviews some fundamental concepts in AI and explores their relation to clinical practice, with a focus on emergency medicine. In addition, it describes some of the applications of AI in disease diagnosis, prognosis, and treatment, as well as some of the practical issues that they raise, the barriers to their implementation, and some of the legal and regulatory challenges they create.}
}
@article{LIMASILVA2024109436,
title = {Dynamical homotopy transient-based technique to improve the convergence of ill-posed power flow problem},
journal = {International Journal of Electrical Power & Energy Systems},
volume = {155},
pages = {109436},
year = {2024},
issn = {0142-0615},
doi = {https://doi.org/10.1016/j.ijepes.2023.109436},
url = {https://www.sciencedirect.com/science/article/pii/S0142061523004933},
author = {Alisson Lima-Silva and Francisco Damasceno Freitas},
keywords = {Euler method, Homotopy, Newton–Raphson method, Numerical integration, Power flow problem, Ill-posed problem},
abstract = {This paper proposes a hybrid technique to solve the ill-posed Power Flow Problem (PFP), considering a homotopy approach. The primary proposal is to solve large-scale problems where the traditional Newton–Raphson (NR) fails to converge, as in the case of ill-posed systems. The method explores a dynamical homotopy transient-based technique to improve the convergence of the ill-conditioned problem instead of using the classical static method. Depending on the integration selected scheme and the integration step, the result furnished by the dynamical homotopy method has low accuracy. Then, the NR method is employed to refine the low-accuracy result and accurately determine the ill-posed PFP solution. The proposed approach can be implemented efficiently using only one Jacobian matrix computation and LU factorization per point of the homotopy path. In the static homotopy problem, a PFP using previous results must be solved per path point. In this case, some LU factorizations are necessary for each path point. The technique’s performance was evaluated through experiments, including a 70,000-bus large-scale system. The approximate dynamical homotopy result used as an initial estimate provided appropriate convergence quality for the NR method to determine a high-precision solution to the PFP.}
}
@article{DARROCH2023105989,
title = {The rangeomorph Pectinifrons abyssalis: Hydrodynamic function at the dawn of animal life},
journal = {iScience},
volume = {26},
number = {2},
pages = {105989},
year = {2023},
issn = {2589-0042},
doi = {https://doi.org/10.1016/j.isci.2023.105989},
url = {https://www.sciencedirect.com/science/article/pii/S2589004223000664},
author = {Simon A.F. Darroch and Susana Gutarra and Hale Masaki and Andrei Olaru and Brandt M. Gibson and Frances S. Dunn and Emily G. Mitchell and Rachel A. Racicot and Gregory Burzynski and Imran A. Rahman},
keywords = {Zoology, Evolutionary biology, Paleobiology},
abstract = {Summary
Rangeomorphs are among the oldest putative eumetazoans known from the fossil record. Establishing how they fed is thus key to understanding the structure and function of the earliest animal ecosystems. Here, we use computational fluid dynamics to test hypothesized feeding modes for the fence-like rangeomorph Pectinifrons abyssalis, comparing this to the morphologically similar extant carnivorous sponge Chondrocladia lyra. Our results reveal complex patterns of flow around P. abyssalis unlike those previously reconstructed for any other Ediacaran taxon. Comparisons with C. lyra reveal substantial differences between the two organisms, suggesting they converged on a similar fence-like morphology for different functions. We argue that the flow patterns recovered for P. abyssalis do not support either a suspension feeding or osmotrophic feeding habit. Instead, our results indicate that rangeomorph fronds may represent organs adapted for gas exchange. If correct, this interpretation could require a dramatic reinterpretation of the oldest macroscopic animals.}
}
@article{MARGINEANU2014131,
title = {Systems biology, complexity, and the impact on antiepileptic drug discovery},
journal = {Epilepsy & Behavior},
volume = {38},
pages = {131-142},
year = {2014},
note = {SI: NEWroscience 2013},
issn = {1525-5050},
doi = {https://doi.org/10.1016/j.yebeh.2013.08.029},
url = {https://www.sciencedirect.com/science/article/pii/S1525505013004344},
author = {Doru Georg Margineanu},
keywords = {Systems biology, Systems/network pharmacology, Drug resistance in epilepsy, Antiepileptic drug, Polypharmacology, Multitarget drug, Phenotypic screening, Modeling, Drug discovery},
abstract = {The number of available anticonvulsant drugs increased in the period spanning over more than a century, amounting to the current panoply of nearly two dozen so-called antiepileptic drugs (AEDs). However, none of them actually prevents/reduces the post-brain insult development of epilepsy in man, and in no less than a third of patients with epilepsy, the seizures are not drug-controlled. Plausibly, the enduring limitation of AEDs' efficacy derives from the insufficient understanding of epileptic pathology. This review pinpoints the unbalanced reductionism of the analytic approaches that overlook the intrinsic complexity of epilepsy and of the drug resistance in epilepsy as the core conceptual flaw hampering the discovery of truly antiepileptogenic drugs. A rising awareness of the complexity of epileptic pathology is, however, brought about by the emergence of nonreductionist systems biology (SB) that considers the networks of interactions underlying the normal organismic functions and of SB-based systems (network) pharmacology that aims to restore pathological networks. By now, the systems pharmacology approaches of AED discovery are fairly meager, but their forthcoming development is both a necessity and a realistic prospect, explored in this review. This article is part of a Special Issue entitled “NEWroscience 2013”.}
}
@article{BAIDYA2024100680,
title = {Comprehensive survey on resource allocation for edge-computing-enabled metaverse},
journal = {Computer Science Review},
volume = {54},
pages = {100680},
year = {2024},
issn = {1574-0137},
doi = {https://doi.org/10.1016/j.cosrev.2024.100680},
url = {https://www.sciencedirect.com/science/article/pii/S1574013724000649},
author = {Tanmay Baidya and Sangman Moh},
keywords = {Augmented reality, Edge computing, Metaverse, Offloading, Resource allocation, Virtual reality},
abstract = {With the rapid evaluation of virtual and augmented reality, massive Internet of Things networks and upcoming 6 G communication give rise to an emerging concept termed the “metaverse,” which promises to revolutionize how we interact with the digital world by offering immersive experiences between reality and virtuality. Edge computing, another novel paradigm, propels the metaverse functionality by enhancing real-time interaction and reducing latency, providing a responsive and seamless virtual environment. However, realizing the full potential of the metaverse requires dynamic and efficient resource-allocation strategies to handle the immense demand for communicational, computational, and storage resources required by its diverse applications. This survey comprehensively explores resource-allocation strategies in the context of an edge-computing-enabled metaverse, investigating various challenges, existing techniques, and emerging trends in this rapidly expanding field. We first explore the underlying metaverse characteristics and pivotal role of edge computing, after which we investigate various types of resources and their key issues and challenges. We also provide a brief discussion on offloading and caching strategies, which are the most prominent research issues in this context. In this study, we compare and analyze 35 different resource-allocation strategies, benchmark 19 algorithms, and investigate their suitability across diverse metaverse scenarios, offering a broader scope than existing surveys. The survey aims to serve as a comprehensive guide for researchers and practitioners, helping them navigate the complexities of resource allocation in the metaverse and supporting the development of more efficient, scalable, and user-centric virtual environments.}
}
@article{GRAYSON2022108844,
title = {R Markdown as a dynamic interface for teaching: Modules from math and biology classrooms},
journal = {Mathematical Biosciences},
volume = {349},
pages = {108844},
year = {2022},
issn = {0025-5564},
doi = {https://doi.org/10.1016/j.mbs.2022.108844},
url = {https://www.sciencedirect.com/science/article/pii/S0025556422000499},
author = {Kristine L. Grayson and Angela K. Hilliker and Joanna R. Wares},
keywords = {R markdown, Data visualization, Pedagogy, Herd immunity, Teaching programming},
abstract = {Advancing technologies, including interactive tools, are changing classroom pedagogy across academia. Here, we discuss the R Markdown interface, which allows for the creation of partial or complete interactive classroom modules for courses using the R programming language. R Markdown files mix sections of R code with formatted text, including LaTeX, which are rendered together to form complete reports and documents. These features allow instructors to create classroom modules that guide students through concepts, while providing areas for coding and text response by students. Students can also learn to create their own reports for more independent assignments. After presenting the features and uses of R Markdown to enhance teaching and learning, we present examples of materials from two courses. In a Computational Modeling course for math students, we used R Markdown to guide students through exploring mathematical models to understand the principle of herd immunity. In a Data Visualization and Communication course for biology students, we used R Markdown for teaching the fundamentals of R programming and graphing, and for students to learn to create reproducible data investigations. Through these examples, we demonstrate the benefits of R Markdown as a dynamic teaching and learning tool.}
}
@article{POVALA2022114712,
title = {Variational Bayesian approximation of inverse problems using sparse precision matrices},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {393},
pages = {114712},
year = {2022},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2022.114712},
url = {https://www.sciencedirect.com/science/article/pii/S0045782522000822},
author = {Jan Povala and Ieva Kazlauskaite and Eky Febrianto and Fehmi Cirak and Mark Girolami},
keywords = {Inverse problems, Bayesian inference, Variational Bayes, Precision matrix, Uncertainty quantification},
abstract = {Inverse problems involving partial differential equations (PDEs) are widely used in science and engineering. Although such problems are generally ill-posed, different regularisation approaches have been developed to ameliorate this problem. Among them is the Bayesian formulation, where a prior probability measure is placed on the quantity of interest. The resulting posterior probability measure is usually analytically intractable. The Markov Chain Monte Carlo (MCMC) method has been the go-to method for sampling from those posterior measures. MCMC is computationally infeasible for large-scale problems that arise in engineering practice. Lately, Variational Bayes (VB) has been recognised as a more computationally tractable method for Bayesian inference, approximating a Bayesian posterior distribution with a simpler trial distribution by solving an optimisation problem. In this work, we argue, through an empirical assessment, that VB methods are a flexible and efficient alternative to MCMC for this class of problems. We propose a natural choice of a family of Gaussian trial distributions parametrised by precision matrices, thus taking advantage of the inherent sparsity of the inverse problem encoded in its finite element discretisation. We utilise stochastic optimisation to efficiently estimate the variational objective and assess not only the error in the solution mean but also the ability to quantify the uncertainty of the estimate. We test this on PDEs based on the Poisson equation in 1D and 2D. A Tensorflow implementation is made publicly available on GitHub.}
}
@article{KRISHNANNAIR202437,
title = {Empowering Tomorrow's Scientists: ‘Girls In Control’ Workshop Promotes STEM Education For Young Girls},
journal = {IFAC-PapersOnLine},
volume = {58},
number = {25},
pages = {37-42},
year = {2024},
note = {3rd Control Conference Africa CCA 2024},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2024.10.234},
url = {https://www.sciencedirect.com/science/article/pii/S2405896324020093},
author = {S. Krishnannair and A. Krishnannair},
keywords = {STEM education, Control Engineering, Girls in Control, Empowering Girls},
abstract = {Enhancement of female students’ access to and success in STEM-related subjects and courses has acquired an unprecedented level of traction in academia. Considering female students’ increased participation in STEM education as a moral imperative, the University of Zululand (South Africa) in partnership with SACAC, South Africa hosted two Girls in Control Workshops at its Science Centre during 2022 and 2023. This article reports on the workshop's purpose and the experiences of the participants. It also makes references to the workshop's implications on various aspects of girls’ participation in STEM education in general and control engineering in particular. The article thus places the need for instilling a high degree of receptiveness to STEM-related careers among girls from previously marginalized communities.}
}
@article{LI20231485,
title = {A Data Driven Security Correction Method for Power Systems with UPFC},
journal = {Energy Engineering},
volume = {120},
number = {6},
pages = {1485-1502},
year = {2023},
issn = {0199-8595},
doi = {https://doi.org/10.32604/ee.2023.022856},
url = {https://www.sciencedirect.com/science/article/pii/S0199859523000519},
author = {Qun Li and Ningyu Zhang and Jianhua Zhou and Xinyao Zhu and Peng Li},
keywords = {Manuscript, security correction, data-driven, deep neural network (DNN), unified power flow controller (UPFC), overload of transmission lines},
abstract = {The access of unified power flow controllers (UPFC) has changed the structure and operation mode of power grids all across the world, and it has brought severe challenges to the traditional real-time calculation of security correction based on traditional models. Considering the limitation of computational efficiency regarding complex, physical models, a data-driven power system security correction method with UPFC is, in this paper, proposed. Based on the complex mapping relationship between the operation state data and the security correction strategy, a two-stage deep neural network (DNN) learning framework is proposed, which divides the offline training task of security correction into two stages: in the first stage, the stacked auto-encoder (SAE) classification model is established, and the node correction state (0/1) output based on the fault information; in the second stage, the DNN learning model is established, and the correction amount of each action node is obtained based on the action nodes output in the previous stage. In this paper, the UPFC demonstration project of Nanjing West Ring Network is taken as a case study to validate the proposed method. The results show that the proposed method can fully meet the real-time security correction time requirements of power grids, and avoid the inherent defects of the traditional model method without an iterative solution and can also provide reasonable security correction strategies for N-1 and N-2 faults.}
}
@article{CRAIG2018300,
title = {Metaphors of knowing, doing and being: Capturing experience in teaching and teacher education},
journal = {Teaching and Teacher Education},
volume = {69},
pages = {300-311},
year = {2018},
issn = {0742-051X},
doi = {https://doi.org/10.1016/j.tate.2017.09.011},
url = {https://www.sciencedirect.com/science/article/pii/S0742051X17301841},
author = {Cheryl J. Craig},
keywords = {Metaphors, Teachers' experiences, Narrative inquiry, School reform},
abstract = {In this article, Bateson's idea of human beings thinking with metaphors and learning through stories is examined as it played out within accumulated educational research studies. Five storied metaphors illuminating knowing, doing and being are highlighted from five investigations involving different research teams. In the cross-case analysis, the importance of narrative exemplars emerges, along with the significance of metaphors serving as proxies for teachers' experiences. The plotlines of the metaphors, the morals of the metaphors and the truths of the metaphors are also discussed. In the end result, the value of metaphors in surfacing teachers' embedded, embodied knowledge of experience is affirmed as well as the deftness of the narrative inquiry research method in metaphorically capturing pre-service and inservice teachers' storied experiences.}
}
@incollection{KALNOOR2021575,
title = {Chapter 24 - The brain-machine interface, nanosensor technology, and artificial intelligence: Their convergence with a novel frontier},
editor = {Chaudhery Mustansar Hussain and Suresh Kumar Kailasa},
booktitle = {Handbook of Nanomaterials for Sensing Applications},
publisher = {Elsevier},
pages = {575-587},
year = {2021},
series = {Micro and Nano Technologies},
isbn = {978-0-12-820783-3},
doi = {https://doi.org/10.1016/B978-0-12-820783-3.00013-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128207833000130},
author = {Gauri Kalnoor},
keywords = {Neuroscience, Machine learning, Nanotechnology, Artificial intelligence (AI), Brain-computer interface, Brain-machine interface (BMI), Computational neuroscience},
abstract = {A confluence of technological capabilities is creating an opportunity for machine learning and artificial intelligence (AI) to enable “smart” nanoengineered brain-machine interfaces (BMI). This new generation of technologies will be able to communicate with the brain in ways that support contextual learning and adaptation to change functional requirements. This applies to both invasive technologies aimed at restoring neurological function, as in the case of neural prosthesis, as well as noninvasive technologies enabled by signals such as electroencephalograph (EEG). Advances in computation, hardware, and algorithms that learn and adapt in a contextually dependent way will be able to leverage the capabilities that nanoengineering offers the design and functionality of BMI. We explore the enabling capabilities that these devices may exhibit, why they matter, and the state of the technologies necessary to build them. We also discuss a number of open technical challenges and problems that will need to be solved to achieve this.}
}
@article{MATLI2024100286,
title = {Extending the theory of information poverty to deepfake technology},
journal = {International Journal of Information Management Data Insights},
volume = {4},
number = {2},
pages = {100286},
year = {2024},
issn = {2667-0968},
doi = {https://doi.org/10.1016/j.jjimei.2024.100286},
url = {https://www.sciencedirect.com/science/article/pii/S2667096824000752},
author = {Walter Matli},
keywords = {Deepfake technology, Information poverty theory, Artificial intelligence (AI), Synthetic media, Societal implications, Technological advancements},
abstract = {The advent of deepfake technology has introduced complex challenges to the information technology landscape, simultaneously presenting benefits and novel risks and ethical considerations. This paper delves into the evolution of deepfakes through the prism of information poverty theory, scrutinising how deepfakes may contribute to a growing information access/use inequality. The research focuses on the risks of misinformation and the ensuing expansion of digital divides, particularly when manipulative media could delude individuals lacking access to legitimate information sources. The study outlines the potential exacerbation of information asymmetries and examines the societal implications across various demographics. By integrating an analytical discussion on the risks associated with deepfakes, the study aligns the observed trends with the theoretical underpinnings of information poverty. As part of its contribution, the paper offers actionable policy-making recommendations and educational strategies to combat the proliferation of harmful deepfake content. The article aims to ensure a more equitable distribution of authentic information and foster media literacy. Through a multifaceted approach, this study endeavours to provide a foundational understanding for stakeholders to navigate the ethical minefield posed by deepfakes and to instil a framework for information equity in the digital era. The article provides critical insights into the discourse on deepfake technology and its relation to information poverty, underscoring the urgent need for equitable access to informed digital spaces. As deepfake technology evolves and more data emerges, a societal demand exists for comprehensive knowledge about deepfakes to promote discernment, decision-making and awareness. Policymakers are tasked with recognising the significance of widening access to sophisticated information technologies whilst addressing their negative repercussions. Their efforts will be particularly crucial for disseminating knowledge about deepfakes to those with limited or non-existent information and communication awareness and infrastructures. Learning from past successes and failures becomes pivotal in shaping effective strategies to address the challenges posed by deepfakes and fostering accessible, informed digital communities.}
}
@article{BOWER2024455,
title = {Model-Based Analysis of Pathway Recruitment During Subthalamic Deep Brain Stimulation},
journal = {Neuromodulation: Technology at the Neural Interface},
volume = {27},
number = {3},
pages = {455-463},
year = {2024},
issn = {1094-7159},
doi = {https://doi.org/10.1016/j.neurom.2023.02.084},
url = {https://www.sciencedirect.com/science/article/pii/S109471592300140X},
author = {Kelsey L. Bower and Angela M. Noecker and Anneke M. Frankemolle-Gilbert and Cameron C. McIntyre},
keywords = {Axons, electrode, Parkinson’s disease, subthalamic nucleus},
abstract = {Background
Subthalamic deep brain stimulation (DBS) is an established clinical therapy, but an anatomically clear definition of the underlying neural target(s) of the stimulation remains elusive. Patient-specific models of DBS are commonly used tools in the search for stimulation targets, and recent iterations of those models are focused on characterizing the brain connections that are activated by DBS.
Objective
The goal of this study was to quantify axonal pathway activation in the subthalamic region from DBS at different electrode locations and stimulation settings.
Materials and Methods
We used an anatomically and electrically detailed computational model of subthalamic DBS to generate recruitment curves for eight different axonal pathways of interest, at three generalized DBS electrode locations in the subthalamic nucleus (STN) (ie, central STN, dorsal STN, posterior STN). These simulations were performed with three levels of DBS electrode localization uncertainty (ie, 0.5 mm, 1.0 mm, 1.5 mm).
Results
The recruitment curves highlight the diversity of pathways that are theoretically activated with subthalamic DBS, in addition to the dependence of the stimulation location and parameter settings on the pathway activation estimates. The three generalized DBS locations exhibited distinct pathway recruitment curve profiles, suggesting that each stimulation location would have a different effect on network activity patterns. We also found that the use of anodic stimuli could help limit activation of the internal capsule relative to other pathways. However, incorporating realistic levels of DBS electrode localization uncertainty in the models substantially limits their predictive capabilities.
Conclusions
Subtle differences in stimulation location and/or parameter settings can impact the collection of pathways that are activated during subthalamic DBS.}
}
@article{ZHU2024111294,
title = {Grey wolf optimizer based deep learning mechanism for music composition with data analysis},
journal = {Applied Soft Computing},
volume = {153},
pages = {111294},
year = {2024},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2024.111294},
url = {https://www.sciencedirect.com/science/article/pii/S1568494624000681},
author = {Qian Zhu and Achyut Shankar and Carsten Maple},
keywords = {Music composition, LSTM, GWO, MIDI, Data analysis},
abstract = {Music composition using artificial intelligence has gained increasing research attention recently. However, existing methods often generate music that needs more coherence and authenticity. This paper proposes an evolutionary computation-based deep learning approach for music composition with data analysis. Specifically, we utilize long short-term memory (LSTM) networks for generating melodic sequences and adopt a grey wolf optimizer to optimize LSTM hyperparameters. The training data is first converted to musical instrument digital interface (MIDI) format for data analysis, and melody lines are extracted using a similarity matrix method. The MIDI data is then encoded for input into the LSTM networks. The generated music is evaluated using objective metrics like mean squared error and subjective methods, including surveys of music professionals. Comparisons made to benchmark algorithms like generative adversarial networks demonstrate the advantages of our approach in accurately capturing tone, rhythm, artistic conception, and other attributes of high-quality music. The proposed mechanism provides a practical framework for AI-based music generation while ensuring authenticity.}
}
@article{SZYMANSKI201284,
title = {Information retrieval with semantic memory model},
journal = {Cognitive Systems Research},
volume = {14},
number = {1},
pages = {84-100},
year = {2012},
note = {Cognitive Systems Research: Special Issue on Modeling and Application of Cognitive Systems},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2011.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S1389041711000179},
author = {Julian Szymański and Włodzisław Duch},
abstract = {Psycholinguistic theories of semantic memory form the basis of understanding of natural language concepts. These theories are used here as an inspiration for implementing a computational model of semantic memory in the form of semantic network. Combining this network with a vector-based object-relation-feature value representation of concepts that includes also weights for confidence and support, allows for recognition of concepts by referring to their features, enabling a semantic search algorithm. This algorithm has been used for word games, in particular the 20-question game in which the program tries to guess a concept that a human player thinks about. The game facilitates lexical knowledge validation and acquisition through the interaction with humans via supervised dialog templates. The elementary linguistic competencies of the proposed model have been evaluated assessing how well it can represent the meaning of linguistic concepts. To study properties of information retrieval based on this type of semantic representation in contexts derived from on-going dialogs experiments in limited domains have been performed. Several similarity measures have been used to compare the completeness of knowledge retrieved automatically and corrected through active dialogs to a “golden standard”. Comparison of semantic search with human performance has been made in a series of 20-question games. On average results achieved by human players were better than those obtained by semantic search, but not by a wide margin.}
}
@article{LARSON201129,
title = {Interdisciplinary research training in a school of nursing},
journal = {Nursing Outlook},
volume = {59},
number = {1},
pages = {29-36},
year = {2011},
issn = {0029-6554},
doi = {https://doi.org/10.1016/j.outlook.2010.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S0029655410004379},
author = {Elaine L. Larson and Bevin Cohen and Kristine Gebbie and Sarah Clock and Lisa Saiman},
abstract = {Although interdisciplinarity has become a favored model of scholarly inquiry, the assumption that interdisciplinary work is intuitive and can be performed without training is short-sighted. This article describes the implementation of an interdisciplinary research training program within a school of nursing. We describe the key elements of the program and the challenges we encountered. From 2007-2010, eleven trainees from 6 disciplines have been accepted into the program and 7 have completed the program; the trainees have published 12 manuscripts and presented at 10 regional or national meetings. The major challenge has been to sustain and “push the envelope” toward interdisciplinary thinking among the trainees and their mentors, and to assure that they do not revert to their “safer” disciplinary silos. This training program, funded by National Institute of Nursing Research (NINR), has become well-established within the school of nursing and across the entire University campus, and is recognized as a high quality research training program across disciplines, as exemplified by excellent applicants from a number of disciplines.}
}
@article{SHIPLEY201948,
title = {Collaboration, cyberinfrastructure, and cognitive science: The role of databases and dataguides in 21st century structural geology},
journal = {Journal of Structural Geology},
volume = {125},
pages = {48-54},
year = {2019},
note = {Back to the future},
issn = {0191-8141},
doi = {https://doi.org/10.1016/j.jsg.2018.05.007},
url = {https://www.sciencedirect.com/science/article/pii/S0191814117303164},
author = {Thomas F. Shipley and Basil Tikoff},
keywords = {Spatial cognition, Cyberinfrastructure, Expert training},
abstract = {Structural geologists support their mind with tools, and these tools are increasingly computer based. The advent of Intelligent Systems will allow creation of research teams that combine the strengths of the human mind and computer processing to produce new research results. The efficacy of these approaches will require a solid grounding in cognitive science. Critical to this approach are databases, which are potentially transformative solely in their ability to allow access to data, in a primary form. Emerging more recently, however, is the concept of a dataguide, in which computer-aided analysis informs ongoing decisions about where and what data to collect. The creation of human and computer teams can expand the types of questions that can be addressed in structural geology and tectonics research, but it will take a community-based effort to understand the value of data to experts and how computers might aid an expert in the field.}
}
@article{LIU2014330,
title = {Large scale two sample multinomial inferences and its applications in genome-wide association studies},
journal = {International Journal of Approximate Reasoning},
volume = {55},
number = {1, Part 3},
pages = {330-340},
year = {2014},
note = {Theory and applications of belief functions – Belief 2012},
issn = {0888-613X},
doi = {https://doi.org/10.1016/j.ijar.2013.04.010},
url = {https://www.sciencedirect.com/science/article/pii/S0888613X13000881},
author = {Chuanhai Liu and Jun Xie},
keywords = {Belief functions, Inference model},
abstract = {Statistical analysis of multinomial counts with a large number K of categories and a small number n of sample size is challenging to both frequentist and Bayesian methods and requires thinking about statistical inference at a very fundamental level. Following the framework of Dempster–Shafer theory of belief functions, a probabilistic inferential model is proposed for this “large K and small n” problem. The inferential model produces a probability triplet (p,q,r) for an assertion conditional on observed data. The probabilities p and q are for and against the truth of the assertion, whereas r=1−p−q is the remaining probability called the probability of “donʼt know”. The new inference method is applied in a genome-wide association study with very high dimensional count data, to identify association between genetic variants to the disease Rheumatoid Arthritis.}
}
@article{LU2017138,
title = {Quasi-generalized least squares regression estimation with spatial data},
journal = {Economics Letters},
volume = {156},
pages = {138-141},
year = {2017},
issn = {0165-1765},
doi = {https://doi.org/10.1016/j.econlet.2017.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S0165176517301441},
author = {Cuicui Lu and Jeffrey M. Wooldridge},
keywords = {Quasi-GLS, Spatial correlation, Covariance tapering, Spatial HAC estimator},
abstract = {We use a particular quasi-generalized least squares (QGLS) approach to study a linear regression model with spatially correlated error terms. The QGLS estimator is consistent, asymptotically normal, computationally easier than GLS, and it appears to not lose much efficiency. A variance–covariance estimator for QGLS, which is robust to heteroskedasticity, spatial correlation and general variance–covariance misspecification is provided.}
}
@article{LEGLEITER20131,
title = {Introduction to the special issue: The field tradition in geomorphology},
journal = {Geomorphology},
volume = {200},
pages = {1-8},
year = {2013},
note = {The Field Tradition in Geomorphology 43rd Annual Binghamton Geomorphology Symposium, held 21-23 September 2012 in Jackson, Wyoming USA},
issn = {0169-555X},
doi = {https://doi.org/10.1016/j.geomorph.2013.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S0169555X13003140},
author = {Carl J. Legleiter and Richard A. Marston},
keywords = {Binghamton Geomorphology Symposium, Preface, Field work, Jackson Hole, Wyoming},
abstract = {In recognition of the critical role of field observations in the ongoing development of our discipline, the 43rd annual Binghamton Geomorphology Symposium (BGS) celebrated The Field Tradition in Geomorphology. By organizing a conference devoted to this theme, we sought to honor the contributions of pioneering, field-based geomorphologists and to encourage our community to contemplate how field work might continue to provide unique insight into a new, more technologically-driven era. For example, given recent advances in remote sensing methods such as LiDAR, what kind of added value can field work provide? Similarly, how can field-based studies contribute to societally relevant, large-scale questions related to climate change and sustainable management of the Earth system? Motivated by such questions, the 2012 BGS was convened in Jackson Hole, WY, a new, Western location that enabled participation by Rocky Mountain and west coast research groups underrepresented at previous Binghamton symposia. Also, in keeping with the field tradition theme, the 2012 BGS emphasized field trips, including a rafting excursion down the Snake River and an overview of the tectonic and glacial history of Jackson Hole. The on-site portion of the symposium consisted of invited oral and poster presentations and contributed posters, including many by graduate students. Topics ranged from an historical overview of the development of geomorphic thinking to long-term sediment tracer studies to a commentary on the synergy between LiDAR and field mapping. This special issue of Geomorphology consists of papers by invited authors from the 2012 BGS, and this overview provides some context for these contributions. Looking forward, we hope that the 43rd annual BGS will stimulate further discussion of the role of field work as the discipline of geomorphology continues to evolve, carrying on the field tradition into the future.}
}
@article{MOSS2013611,
title = {Senior Academic Physicians and Retirement Considerations},
journal = {Progress in Cardiovascular Diseases},
volume = {55},
number = {6},
pages = {611-615},
year = {2013},
note = {Symposium on Psychosocial Factors in Cardiovascular Disease},
issn = {0033-0620},
doi = {https://doi.org/10.1016/j.pcad.2013.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S003306201300056X},
author = {Arthur J. Moss and Henry Greenberg and Edward M. Dwyer and Helmut Klein and Daniel Ryan and Charles Francis and Frank Marcus and Shirley Eberly and Jesaia Benhorin and Monty Bodenheimer and Mary Brown and Robert Case and John Gillespie and Robert Goldstein and Mark Haigney and Ronald Krone and Edgar Lichstein and Emanuela Locati and David Oakes and Poul Erik Bloch Thomsen and Wojciech Zareba},
keywords = {Academic physicians, Retirement issues, Retirement options},
abstract = {An increasing number of academic senior physicians are approaching their potential retirement in good health with accumulated clinical and research experience that can be a valuable asset to an academic institution. Considering the need to let the next generation ascend to leadership roles, when and how should a medical career be brought to a close? We explore the roles for academic medical faculty as they move into their senior years and approach various retirement options. The individual and institutional considerations require a frank dialogue among the interested parties to optimize the benefits while minimizing the risks for both. In the United States there is no fixed age for retirement as there is in Europe, but European physicians are initiating changes. What is certain is that careful planning, innovative thinking, and the incorporation of new patterns of medical practice are all part of this complex transition and timing of senior academic physicians into retirement.}
}
@article{JARRAHI2018577,
title = {Artificial intelligence and the future of work: Human-AI symbiosis in organizational decision making},
journal = {Business Horizons},
volume = {61},
number = {4},
pages = {577-586},
year = {2018},
issn = {0007-6813},
doi = {https://doi.org/10.1016/j.bushor.2018.03.007},
url = {https://www.sciencedirect.com/science/article/pii/S0007681318300387},
author = {Mohammad Hossein Jarrahi},
keywords = {Artificial intelligence, Organizational decision making, Human-machine symbiosis, Human augmentation, Analytical and intuitive decision making},
abstract = {Artificial intelligence (AI) has penetrated many organizational processes, resulting in a growing fear that smart machines will soon replace many humans in decision making. To provide a more proactive and pragmatic perspective, this article highlights the complementarity of humans and AI and examines how each can bring their own strength in organizational decision-making processes typically characterized by uncertainty, complexity, and equivocality. With a greater computational information processing capacity and an analytical approach, AI can extend humans’ cognition when addressing complexity, whereas humans can still offer a more holistic, intuitive approach in dealing with uncertainty and equivocality in organizational decision making. This premise mirrors the idea of intelligence augmentation, which states that AI systems should be designed with the intention of augmenting, not replacing, human contributions.}
}
@article{DECARVALHOBOTEGA2022109893,
title = {A data-driven Machine Learning approach to creativity and innovation techniques selection in solution development},
journal = {Knowledge-Based Systems},
volume = {257},
pages = {109893},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2022.109893},
url = {https://www.sciencedirect.com/science/article/pii/S0950705122009868},
author = {Luiz Fernando {de Carvalho Botega} and Jonny Carlos {da Silva}},
keywords = {Decision support system, Creativity, Artificial intelligence, Design},
abstract = {The creation and refinement of new ideas is a strategic competence for teams and organization to innovate and prosper. This paper addresses the challenge of finding adequate creativity and innovation techniques (CITs) for improving individual or team creativity through the use of Machine Learning (ML). The process of choosing which CIT to use is complex and demanding, especially when taking into consideration the existence of hundreds of techniques and the plurality of different design contexts. This empiric knowledge, usually retained in an expert’s repertoire, can be extracted and implemented in a computational system, making it more available and permanent. This research focused on developing a Decision Support System embedded in an online application with a two-stage ML inference process able to evaluate users’ design scenario through an online form, and infer the most appropriate CITs from the database that would fit their needs. This paper presents two iterative development cycles of the prototype, first focused on core knowledge acquisition, representation, ML implementation, and verification; while second focused on system expansion, addition of web interface, and initial validation. After essaying 12 algorithms, the two-stage model achieved uses a Gradient Boosted Regression Trees algorithm using user provided information about the context to infer the required CITs characteristics; followed by a Logistic Regression classification-ranking algorithm that uses outputs from first model to define which CITs to present to users. To the best of our efforts, no other system was found to use ML approaches to address the problem of CIT selection.}
}
@article{ROWBOTTOM2013161,
title = {Kuhn vs. Popper on criticism and dogmatism in science, part II: How to strike the balance},
journal = {Studies in History and Philosophy of Science Part A},
volume = {44},
number = {2},
pages = {161-168},
year = {2013},
issn = {0039-3681},
doi = {https://doi.org/10.1016/j.shpsa.2012.11.011},
url = {https://www.sciencedirect.com/science/article/pii/S0039368112001161},
author = {Darrell P. Rowbottom},
abstract = {This paper is a supplement to, and provides a proof of principle of, Kuhn vs. Popper on Criticism and Dogmatism in Science: A Resolution at the Group Level. It illustrates how calculations may be performed in order to determine how the balance between different functions in science—such as imaginative, critical, and dogmatic—should be struck, with respect to confirmation (or corroboration) functions and rules of scientific method.}
}
@article{SHUKLA2025128716,
title = {Deep belief network with fuzzy parameters and its membership function sensitivity analysis},
journal = {Neurocomputing},
volume = {614},
pages = {128716},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.128716},
url = {https://www.sciencedirect.com/science/article/pii/S0925231224014875},
author = {Amit K. Shukla and Pranab K. Muhuri},
keywords = {Deep learning, Deep belief networks, Restricted Boltzmann machine, Fuzzy sets, Type-1 fuzzy sets, Contrastive divergence},
abstract = {Over the last few years, deep belief networks (DBNs) have been extensively utilized for efficient and reliable performance in several complex systems. One critical factor contributing to the enhanced learning of the DBN layers is the handling of network parameters, such as weights and biases. The efficient training of these parameters significantly influences the overall enhanced performance of the DBN. However, the initialization of these parameters is often random, and the data samples are normally corrupted by unwanted noise. This causes the uncertainty to arise among weights and biases of the DBNs, which ultimately hinders the performance of the network. To address this challenge, we propose a novel DBN model with weights and biases represented using fuzzy sets. The approach systematically handles inherent uncertainties in parameters resulting in a more robust and reliable training process. We show the working of the proposed algorithm considering four widely used benchmark datasets such as: MNSIT, n-MNIST (MNIST with additive white Gaussian noise (AWGN) and MNIST with motion blur) and CIFAR-10. The experimental results show superiority of the proposed approach as compared to classical DBN in terms of robustness and enhanced performance. Moreover, it has the capability to produce equivalent results with a smaller number of nodes in the hidden layer; thus, reducing the computational complexity of the network architecture. Additionally, we also study the sensitivity analysis for stability and consistency by considering different membership functions to model the uncertain weights and biases. Further, we establish the statistical significance of the obtained results by conducting both one-way and Kruskal-Wallis analyses of variance tests.}
}