@article{GAESSER2020104325,
title = {Episodic mindreading: Mentalizing guided by scene construction of imagined and remembered events},
journal = {Cognition},
volume = {203},
pages = {104325},
year = {2020},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2020.104325},
url = {https://www.sciencedirect.com/science/article/pii/S001002772030144X},
author = {Brendan Gaesser},
keywords = {Episodic simulation, Memory, Scene construction, Mentalizing, Theory of mind, Perspective taking, Social cognition, Morality},
abstract = {Attributing mental states to other people fundamentally shapes how we bond, coordinate, and predict the actions of others. Perceiving a person's facial expressions and body language in the present contribute to our ability to understand what they are thinking and feeling. Yet, people do not exist in a vacuum and individuals often think about people who are not directly in front of them. People inhabit remembered and imagined episodes, where the surrounding location and objects can guide attributions of their mental states. In this article, I propose the episodic mindreading hypothesis, arguing that the episodic representation of past and future events in which a target person is embedded will affect whether and how the target's mind is read. The content and phenomenological quality of imagined and remembered episodes can alter what mental states are attributed to a target and the accessibility of those mental states. This hypothesis encourages researchers to think about mentalizing as neither dependent on nor completely exclusive from the episodic memory system. Instead, the episodic memory system can modulate and inform mindreading, and likely vice versa. The article reviews extant knowledge and highlights open questions for future research to explore with implications for healthy and impaired social cognition.}
}
@article{MURAMATSU2005201,
title = {Emotions as a mechanism for boundedly rational agents: The fast and frugal way},
journal = {Journal of Economic Psychology},
volume = {26},
number = {2},
pages = {201-221},
year = {2005},
issn = {0167-4870},
doi = {https://doi.org/10.1016/j.joep.2004.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S0167487004000285},
author = {Roberta Muramatsu and Yaniv Hanoch},
keywords = {Bounded rationality, Emotion, Evolution, Mechanism},
abstract = {Herbert Simon has warned us that an explanatory account of human rationality must identify the significance of emotions for choice behavior. Customarily emphasizing the cognitive dimensions of decision making, relatively few researchers have paid close attention to specifying the complex ways in which emotion may shape human thinking and decisions. Accordingly, this paper is an attempt to follow Simon's suggestion and specify how emotions can enter into the theory of bounded rationality. To accomplish our task, we capitalize on Rom Harré's work on causal powers, from which we propose a strategy to study the significance of emotion in decision-making processes. In an attempt to elaborate on an explanation of behavior by mechanism, we discuss a version of bounded rationality recently put forward by Gigerenzer, Todd, and the ABC Research Group [Simple Heuristics that Make us Smart, Oxford University Press, New York, 1999] and Gigerenzer and Selten [Bounded Rationality: The Adaptive Toolbox, MIT Press, Cambridge, MA, 2001, pp. 1–12], the so-called adaptive toolbox of fast and frugal heuristics. Coupled with insights from evolutionary psychology and neuroscience, this version of bounded rationality gives us a better grasp of the functional role of emotions within the human decision machinery.}
}
@article{FUJINO201760,
title = {Role of Spontaneous Brain Activity in Explicit and Implicit Aspects of Cognitive Flexibility under Socially Conflicting Situations: A Resting-state fMRI Study using Fractional Amplitude of Low-frequency Fluctuations},
journal = {Neuroscience},
volume = {367},
pages = {60-71},
year = {2017},
issn = {0306-4522},
doi = {https://doi.org/10.1016/j.neuroscience.2017.10.025},
url = {https://www.sciencedirect.com/science/article/pii/S0306452217307534},
author = {Junya Fujino and Shisei Tei and Kathryn F. Jankowski and Ryosaku Kawada and Toshiya Murai and Hidehiko Takahashi},
keywords = {decision-making, cerebellum, prefrontal cortex, rationalism, experientialism},
abstract = {We are constantly exposed to socially conflicting situations in everyday life, and cognitive flexibility is essential for adaptively coping with such difficulties. Flexible goal choice and pursuit are not exclusively conscious, and therefore cognitive flexibility involves both explicit and implicit forms of processing. However, it is unclear how individual differences in explicit and implicit aspects of flexibility are associated with neural activity in a resting state. Here, we measured intrinsic fractional amplitude of low-frequency fluctuations (fALFF) by resting-state functional magnetic resonance imaging (RS-fMRI) as an indicator of regional brain spontaneous activity, together with explicit and implicit aspects of cognitive flexibility using the Cognitive Flexibility Scale (CFS) and Implicit Association Test (IAT). Consistent with the dual processing theory, there was a strong association between explicit aspects of flexibility (CFS score) and “rationalism” thinking style and between implicit aspects (IAT effect) and “experientialism.” The level of explicit flexibility was also correlated with fALFF values in the left lateral prefrontal cortex, whereas the level of implicit flexibility was correlated with fALFF values in the right cerebellum. Furthermore, the fALFF values in both regions predicted individual preference for flexible decision-making strategy in a vignettes simulation task. These results add to our understanding of the neural mechanisms underlying flexible decision-making for solving social conflicts. More generally, our findings highlight the utility of RS-fMRI combined with both explicit and implicit psychometric measures for better understanding individual differences in social cognition.}
}
@article{FAVELA2019156,
title = {Editor’s introduction: Innovative dynamical approaches to cognitive systems},
journal = {Cognitive Systems Research},
volume = {58},
pages = {156-159},
year = {2019},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2019.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S1389041719303389},
author = {Luis H. Favela},
keywords = {Cognition, Cognitive systems, Dynamical systems theory},
abstract = {This Editor’s Introduction to the Cognitive Systems Research special issue, “Innovative Dynamical Approaches to Cognitive Systems,” has three aims: First, the background and motivation for the topic are stated. Second, overviews of the contributing papers are presented. Third, based on the papers, speculations on future directions in dynamical approaches to the investigation of cognitive systems are presented. Here, the focus is on concepts, data analysis methods, and computational modeling.}
}
@article{RICHARDS2013113,
title = {Bayesian belief modeling of climate change impacts for informing regional adaptation options},
journal = {Environmental Modelling & Software},
volume = {44},
pages = {113-121},
year = {2013},
note = {Thematic Issue on Innovative Approaches to Global Change Modelling},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2012.07.008},
url = {https://www.sciencedirect.com/science/article/pii/S136481521200206X},
author = {R. Richards and M. Sanó and A. Roiko and R.W. Carter and M. Bussey and J. Matthews and T.F. Smith},
keywords = {Bayesian Belief Networks, Climate change, Adaptation, Group-model building, Stakeholder beliefs},
abstract = {A sequential approach to combining two established modeling techniques (systems thinking and Bayesian Belief Networks; BBNs) was developed and applied to climate change adaptation research within the South East Queensland Climate Adaptation Research Initiative (SEQ-CARI). Six participatory workshops involving 66 stakeholders based within SEQ produced six system conceptualizations and 22 alpha-level BBNs. The outcomes of the initial systems modeling exercise successfully allowed the selection of critical determinants of key response variables for in depth analysis within more homogeneous, sector-based groups of participants. Using two cases, this article focuses on the processes and methodological issues relating to the use of the BBN modeling technique when the data are based on expert opinion. The study expected to find both generic and specific determinants of adaptive capacity based on the perceptions of the stakeholders involved. While generic determinants were found (e.g. funding and awareness levels), sensitivity analysis identified the importance of pragmatic, context-based determinants, which also had methodological implications. The article raises questions about the most appropriate scale at which the methodology applied can be used to identify useful generic determinants of adaptive capacity when, at the scale used, the most useful determinants were sector-specific. Comparisons between individual BBN conditional probabilities identified diverging and converging beliefs, and that the sensitivity of response variables to direct descendant nodes was not always perceived consistently. It was often the accompanying narrative that provided important contextual information that explained observed differences, highlighting the benefits of using critical narrative with modeling tools.}
}
@article{JAHANIYEKTA2024100078,
title = {The general intelligence of GPT–4, its knowledge diffusive and societal influences, and its governance},
journal = {Meta-Radiology},
volume = {2},
number = {2},
pages = {100078},
year = {2024},
issn = {2950-1628},
doi = {https://doi.org/10.1016/j.metrad.2024.100078},
url = {https://www.sciencedirect.com/science/article/pii/S2950162824000316},
author = {Mohammad Mahdi {Jahani Yekta}},
keywords = {GPT–4, Artificial general intelligence, Knowledge diffusion, Interpretability and explainability, Societal influences, Governance},
abstract = {Recent breakthroughs in artificial intelligence (AI) research include advancements in natural language processing (NLP) achieved by large language models (LLMs), and; in particular, generative pre–trained transformer (GPT) architectures. The latest GPT developed by OpenAI, GPT–4, has shown remarkable intelligence across various domains and tasks. It exhibits capabilities in abstraction, comprehension, vision, computer coding, mathematics, and more, suggesting it to be a significant step towards artificial general intelligence (AGI), a level of AI that possesses capabilities similar to human intelligence. This paper explores this AGI, its knowledge diffusive and societal influences, and its governance. In addition to coverage of the major associated topics studied in the literature, and making up for their loopholes, we scrutinize how GPT-4 can facilitate the diffusion of knowledge across different areas of science by promoting their interpretability and explainability (IE) to inexperts. Where applicable, the topics are also accompanied by their specific potential implications on medical imaging.}
}
@article{BOYCE2004565,
title = {Critical accounting education: teaching and learning outside the circle},
journal = {Critical Perspectives on Accounting},
volume = {15},
number = {4},
pages = {565-586},
year = {2004},
note = {A Critical Response to Managerialism in the Academy},
issn = {1045-2354},
doi = {https://doi.org/10.1016/S1045-2354(03)00047-9},
url = {https://www.sciencedirect.com/science/article/pii/S1045235403000479},
author = {Gordon Boyce},
keywords = {Critical accounting, Corporate university, Accounting research, Accounting education, Intellectuals},
abstract = {The development of the corporate university is an element in the suite of “economically rational” public policy changes promulgated in recent decades. Working from a position that the practice of accounting is centrally implicated in these changes, it is contended in this paper that accounting, and accounting education, can in fact play a part in challenging these positions. Extant accounting research is sufficiently well-developed such that we are aware of the conflicts and contradictions both within accounting and flowing from the practice of the discipline, yet the effect of this body of knowledge on the content of teaching and learning within the accounting classroom remains limited. By and large, accounting education continues to be constrained within narrowly defined, but mis-conceived, disciplinary boundaries, focusing on the techniques and “skills” of accounting practice. In outlining a case for broadening the accounting education curriculum, this paper adopts the heuristic of “tangential thinking” as a means of transcending narrowly constructed disciplinary boundaries. In doing so, it is suggested that accounting education reform needs to go well beyond the putative reform agenda of the organised professional accounting bodies. The exploration of tangents lead to areas of knowledge that initially seem to be outside of accounting, but which nevertheless have an integral connection to the realities of the practice of the discipline. The paper outlines a case for tangential thinking in teaching and learning activities in the accounting classroom, within extant accreditation and curricular arrangements. Teaching and learning “outside the circle” in this manner is suggested as a way to make accounting education relevant in its socio-historical context and, particularly, relevant to the lived experience of students.}
}
@article{SCHROEDER2015530,
title = {Situated phenomenology and biological systems: Eastern and Western synthesis},
journal = {Progress in Biophysics and Molecular Biology},
volume = {119},
number = {3},
pages = {530-537},
year = {2015},
note = {Integral Biomathics: Life Sciences, Mathematics, and Phenomenological Philosophy},
issn = {0079-6107},
doi = {https://doi.org/10.1016/j.pbiomolbio.2015.06.019},
url = {https://www.sciencedirect.com/science/article/pii/S0079610715000942},
author = {Marcin J. Schroeder and Jordi Vallverdú},
keywords = {Phenomenology, Experience, Situated cognition, Eastern, Western, Cybernetics, Robotics, System, Biology},
abstract = {Phenomenology was born with the mission to give foundations for science of experience and to open consciousness to scientific study. The influence of phenomenology initiated in the works of Husserl and continued in a wide range of works of others was immense, but mainly within the confines of philosophy and the humanities. The actual attempts to develop a scientific discipline of the study of consciousness and to carry out research on cognition and consciousness were always based on the methods of traditional science in which elimination of the subjective has been always a primary tenet. Thus, focus was mainly on neurological correlates of conscious phenomena. The present paper is an attempt to initiate an extension and revision of phenomenological methodology with the use of philosophical and scientific experience and knowledge accumulated in a century of inquiry and research in relevant disciplines. The question which disciplines are relevant is crucial and our answer is innovative. The range of disciplines involved here is from information science and studies of computation, up to cultural psychology and the studies of philosophical traditions of the East. Concepts related to information and computation studies provide a general conceptual framework free from the limitations of particular languages and of linguistic analysis. This conceptual framework is extending the original perspective of phenomenology to issues of modern technology and science. Cultural psychology gives us tools to root out what in phenomenology was considered universal for humanity, but was a result of European ethnocentrism. Most important here is the contrast between individualistic and collectivistic cultural determinants of consciousness. Finally, philosophical tradition of the East gives alternatives in seeking solutions for fundamental problems. This general outline of the research methodology is illustrated by an example of its use when phenomenology is studied within the conceptual framework of information.}
}
@incollection{YUCESOY2024,
title = {Systems Biology in Immunotoxicology},
booktitle = {Reference Module in Biomedical Sciences},
publisher = {Elsevier},
year = {2024},
isbn = {978-0-12-801238-3},
doi = {https://doi.org/10.1016/B978-0-323-95488-4.00046-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780323954884000462},
author = {Berran Yucesoy and Randle Gallucci},
keywords = {Biological networks, Biomarkers, Epigenetics, Genetics, Immune system, Immunotoxicology, Proteomics, Risk assessment, System biology, Transcriptomics},
abstract = {Systems biology is an emerging field that focuses on the interactions between the components of biological systems. In the past two decades, high-throughput, large-scale molecular biology approaches (omics technologies) and advances in computational approaches have significantly grown and provided valuable insight into disease mechanisms, underlying toxicities, and gene–environment interactions. Systems biology approaches have also been widely used in immunotoxicology and helped understand the structure and function of immune system at multiple levels. Integrative models of the human immune response allowed for the assessment of complex immune effects, multilevel networks of interactions, identification of biomarkers and extrapolation of early molecular/cellular events to long-term outcomes at the organism level. Such efforts also created a potential for more predictive and accurate risk-assessment strategies. This chapter focuses on systems biology approaches and computational tools used in immunotoxicology and discuss their application in risk assessment.}
}
@article{HAN20212821,
title = {Artificial protein assemblies with well-defined supramolecular protein nanostructures},
journal = {Biochemical Society Transactions},
volume = {49},
number = {6},
pages = {2821-2830},
year = {2021},
issn = {1470-8752},
doi = {https://doi.org/10.1042/BST20210808},
url = {https://www.sciencedirect.com/science/article/pii/S1470875221001033},
author = {Suyeong Han and Yongwon Jung},
keywords = {protein assembly, protein engineering, protein nanostructures},
abstract = {Nature uses a wide range of well-defined biomolecular assemblies in diverse cellular processes, where proteins are major building blocks for these supramolecular assemblies. Inspired by their natural counterparts, artificial protein-based assemblies have attracted strong interest as new bio-nanostructures, and strategies to construct ordered protein assemblies have been rapidly expanding. In this review, we provide an overview of very recent studies in the field of artificial protein assemblies, with the particular aim of introducing major assembly methods and unique features of these assemblies. Computational de novo designs were used to build various assemblies with artificial protein building blocks, which are unrelated to natural proteins. Small chemical ligands and metal ions have also been extensively used for strong and bio-orthogonal protein linking. Here, in addition to protein assemblies with well-defined sizes, protein oligomeric and array structures with rather undefined sizes (but with definite repeat protein assembly units) also will be discussed in the context of well-defined protein nanostructures. Lastly, we will introduce multiple examples showing how protein assemblies can be effectively used in various fields such as therapeutics and vaccine development. We believe that structures and functions of artificial protein assemblies will be continuously evolved, particularly according to specific application goals.}
}
@article{SCHWARZ2014283,
title = {On computing time-to-collision for automation scenarios},
journal = {Transportation Research Part F: Traffic Psychology and Behaviour},
volume = {27},
pages = {283-294},
year = {2014},
note = {Vehicle Automation and Driver Behaviour},
issn = {1369-8478},
doi = {https://doi.org/10.1016/j.trf.2014.06.015},
url = {https://www.sciencedirect.com/science/article/pii/S1369847814000898},
author = {Chris Schwarz},
keywords = {TTC, Automation, Computational methods, Algorithms, Computational geometry},
abstract = {Time to collision (TTC) has been a key vehicle safety metric for decades. With the increasing prevalence of advanced driver assistance systems and vehicle automation, TTC and many related metrics are being applied to the analysis of more complicated scenarios, as well as being integrated into automation algorithms. While the TTC metric was originally conceived to be inclusive of generic two-dimensional situations, its applications has been mostly limited to one-dimensional scenarios. This paper derives general equations and algorithms using two-dimensional information. Additionally, methods from computational geometry, a field that didn’t exist when TTC was first used, are employed for the general case of computing TTC between bounding boxes. Parametric equations for lines play a prominent role and offer an elegant way to express the geometry of the scenarios described in this paper. Throughout, the approach is not to derive specific algebraic conditions as in previous efforts. Rather, the focus in on developing general algorithms for computation. The techniques presented are not necessary for traditional car following scenarios; but offer options for more complex situations that trade off analytic solutions for computational flexibility.}
}
@article{CORREA2008140,
title = {Connected and culturally embedded beliefs: Chinese and US teachers talk about how their students best learn mathematics},
journal = {Teaching and Teacher Education},
volume = {24},
number = {1},
pages = {140-153},
year = {2008},
issn = {0742-051X},
doi = {https://doi.org/10.1016/j.tate.2006.11.004},
url = {https://www.sciencedirect.com/science/article/pii/S0742051X06001715},
author = {Christopher A. Correa and Michelle Perry and Linda M. Sims and Kevin F. Miller and Ge Fang},
keywords = {Teacher beliefs, Mathematics education, China, United States, Cross-cultural, Culture},
abstract = {This study compares US and Chinese elementary mathematics teachers' beliefs about how students learn mathematics. Interviews with teachers in each country revealed that Chinese and US teachers have distinct ways of thinking about how mathematics should be taught and how students learn. Many Chinese teachers talked about developing students’ interest in mathematics and relating the content of mathematics lessons to real-life situations. The US teachers talked about students' learning styles and using hands-on approaches to learning mathematics. Furthermore, these beliefs may be widespread and persistent within each country because the set of ideas among teachers appear to be internally consistent. Implications for teacher change and the study of teachers' beliefs are discussed.}
}
@incollection{WANG202427,
title = {2 - Neuromorphic computing},
editor = {Min Gu and Elena Goi and Yangyundou Wang and Zhengfen Wan and Yibo Dong and Yuchao Zhang and Haoyi Yu},
booktitle = {Neuromorphic Photonic Devices and Applications},
publisher = {Elsevier},
pages = {27-45},
year = {2024},
series = {Photonic Materials and Applications Series},
isbn = {978-0-323-98829-2},
doi = {https://doi.org/10.1016/B978-0-323-98829-2.00006-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780323988292000062},
author = {Wenju Wang and Haoran Zhou and Wei Li and Elena Goi},
keywords = {Artificial intelligence, machine learning, deep learning, machine learning models, neuromorphic computing},
abstract = {One of the aims of neuromorphic engineering is to implement elements of artificial intelligence (AI) algorithms, in particular artificial neural networks, with hardware that reflects the massively distributed nature of these bioinspired architectures. In this chapter, we introduce history, basic concepts, and applications of AI, in order to establish a working framework for the development of neuromorphic systems. We present the fundamentals of machine learning (ML) and deep learning (DL) and expound on the relationship among these algorithms based on neural networks, to generate a broader understanding of the methodical underpinning of current AI systems. Basic concepts and working principles, such as neurons, activation functions, feedforward networks, etc., are presented, and the performances of these algorithms in terms of functionality, computational complexity, and energy consumption are reviewed. Moreover, strengths and limits of different AI architectures are discussed, giving an overview of the development and applications of neuromorphic computing architectures.}
}
@article{DECHARMS2007473,
title = {Reading and controlling human brain activation using real-time functional magnetic resonance imaging},
journal = {Trends in Cognitive Sciences},
volume = {11},
number = {11},
pages = {473-481},
year = {2007},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2007.08.014},
url = {https://www.sciencedirect.com/science/article/pii/S1364661307002471},
author = {R. Christopher deCharms},
abstract = {Understanding how to control how the brain's functioning mediates mental experience and the brain's processing to alter cognition or disease are central projects of cognitive and neural science. The advent of real-time functional magnetic resonance imaging (rtfMRI) now makes it possible to observe the biology of one's own brain while thinking, feeling and acting. Recent evidence suggests that people can learn to control brain activation in localized regions, with corresponding changes in their mental operations, by observing information from their brain while inside an MRI scanner. For example, subjects can learn to deliberately control activation in brain regions involved in pain processing with corresponding changes in experienced pain. This may provide a novel, non-invasive means of observing and controlling brain function, potentially altering cognitive processes or disease.}
}
@article{CHEN20247,
title = {QoE oriented intelligent online learning evaluation technology in B5G scenario},
journal = {Digital Communications and Networks},
volume = {10},
number = {1},
pages = {7-15},
year = {2024},
issn = {2352-8648},
doi = {https://doi.org/10.1016/j.dcan.2022.05.018},
url = {https://www.sciencedirect.com/science/article/pii/S2352864822001110},
author = {Mingzi Chen and Xin Wei and Peizhong Xie and Zhe Zhang},
keywords = {B5G, Online learning, Quality of experience},
abstract = {Students' demand for online learning has exploded during the post-COVID-19 pandemic era. However, due to their poor learning experience, students' dropout rate and learning performance of online learning are not always satisfactory. The technical advantages of Beyond Fifth Generation (B5G) can guarantee a good multimedia Quality of Experience (QoE). As a special case of multimedia services, online learning takes into account both the usability of the service and the cognitive development of the users. Factors that affect the Quality of Online Learning Experience (OL-QoE) become more complicated. To get over this dilemma, we propose a systematic scheme by integrating big data, Machine Learning (ML) technologies, and educational psychology theory. Specifically, we first formulate a general definition of OL-QoE by data analysis and experimental verification. This formula considers both the subjective and objective factors (i.e., video watching ratio and test scores) that most affect OL-QoE. Then, we induce an extended layer to the classic Broad Learning System (BLS) to construct an Extended Broad Learning System (EBLS) for the students' OL-QoE prediction. Since the extended layer can increase the width of the BLS model and reduce the redundant nodes of BLS, the proposed EBLS can achieve a trade-off between the prediction accuracy and computation complexity. Finally, we provide a series of early intervention suggestions for different types of students according to their predicted OL-QoE values. Through timely interventions, their OL-QoE and learning performance can be improved. Experimental results verify the effectiveness of the proposed scheme.}
}
@article{ABONDIO202237,
title = {Single Cell Multiomic Approaches to Disentangle T Cell Heterogeneity},
journal = {Immunology Letters},
volume = {246},
pages = {37-51},
year = {2022},
issn = {0165-2478},
doi = {https://doi.org/10.1016/j.imlet.2022.04.008},
url = {https://www.sciencedirect.com/science/article/pii/S0165247822000669},
author = {Paolo Abondio and Carlo {De Intinis} and João Lídio {da Silva Gonçalves Vianez Júnior} and Luigia Pace},
keywords = {T cells, scRNA-seq, Analysis pipeline, Pseudotime, TCR},
abstract = {Single-cell multi-omics is a rapidly evolving field, thanks to a fast technological improvement and the growing accuracy of dedicated computational tools for data analysis. Its importance is highlighted by the possibility to distinguish apparently identical cells based on their pattern of gene expression. In this review, the mostly used methodological pipelines for single-cell analysis, as well as the advantages and potential limitations of several analytical steps, are presented and discussed, with specific sections focusing on crucial parts of this procedure, their bioinformatic tools, as well as their advantages and potential drawbacks. The current bioinformatic approaches for T-cell receptor (TCR) reconstruction are also introduced, as well as a comparison of single-cell sequencing technologies. Critical points that may introduce analytical biases and potential inaccuracies in data interpretation are also highlighted.}
}
@article{SAMSONOVICH2015235,
title = {Cognitive Processes in Preparation for Problem Solving},
journal = {Procedia Computer Science},
volume = {71},
pages = {235-247},
year = {2015},
note = {6th Annual International Conference on Biologically Inspired Cognitive Architectures, BICA 2015, 6-8 November Lyon, France},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.12.218},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915036790},
author = {Alexei V. Samsonovich and Anastasia Kitsantas and Ellen O’Brien and Kenneth A. {De Jong}},
keywords = {self-regulation, planning, metacognition, intelligent tutoring systems},
abstract = {The aim of this study was to examine the role of a software tool in diagnosing student's thinking during problem solving in mathematics with 41 college students. Students were asked to select relevant steps, facts and strategies represented on the screen and connect them by arrows, indicating their plan of solution. Only after the diagram was completed, students were allowed to solve the problem. The findings are: (i) forward chaining is significantly more predominant, and backward chaining is significantly less frequent, compared to other possibilities or arrow entering. This result is unexpected, because classical planning methods produce backward chaining in this task. (ii) Students scoring in the middle are more likely to enter convergent pairs of arrows compared to students who scored low or high. This finding enables diagnosing student problem solving. Both findings imply constraints on selection of cognitive architectures used for modeling student problem solving.}
}
@article{KENETT2019271,
title = {A Semantic Network Cartography of the Creative Mind},
journal = {Trends in Cognitive Sciences},
volume = {23},
number = {4},
pages = {271-274},
year = {2019},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2019.01.007},
url = {https://www.sciencedirect.com/science/article/pii/S1364661319300245},
author = {Yoed N. Kenett and Miriam Faust},
keywords = {creativity, semantic networks, network science},
abstract = {The role of semantic memory in creativity is theoretically assumed, but far from understood. In recent years, computational network science tools have been applied to investigate this role. These studies shed unique quantitative insights on the role of semantic memory structure in creativity, via measures of connectivity, distance, and structure.}
}
@article{JI2023119326,
title = {Fast Progressive Differentiable Architecture Search based on adaptive task granularity reorganization},
journal = {Information Sciences},
volume = {645},
pages = {119326},
year = {2023},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2023.119326},
url = {https://www.sciencedirect.com/science/article/pii/S0020025523009118},
author = {Junzhong Ji and Xingyu Wang},
keywords = {Neural Architecture Search, Differentiable Architecture Search, Shrink search space, Granular Computing, Cluster candidate},
abstract = {Shrinkage methods reduce the search space of a Differentiable Architecture Search (DARTS) by progressively discarding candidates, which accelerates the search speed. However, their shrinkage strategy suffers from the vulnerability of too fine task granularity. In other words, they drop only one of the least promising candidates per round of shrinkage, which is suboptimal in terms of performance and efficiency. In this study, we introduce the concept of Granular Computing (GrC) into the shrinkage method and present a Fast Progressive Differentiable Architecture Search (FP-DARTS) method. This method effectively reduces the computational complexity of each round of shrinkage, thereby improving the efficiency and performance of the algorithm. FP-DARTS can be divided into three stages: adaptive granularity division and selection, granular-channel performance evaluation, and progressive shrinkage. In the first stage, to reorganize the task granularity, we cluster the candidate operations into granular-channels and adaptively select the appropriate task granularity. We also propose a dynamic clustering strategy to avoid introducing additional computation. In the second stage, we train the architecture parameters to measure the potential of the granular-channels. In the third stage, to improve the stability of the shrinkage results, we introduce a channel annealing mechanism to smoothly discard unpromising granular-channels. We conducted systematic experiments on CIFAR-10 and ImageNet and achieved a test accuracy of 97.56% on CIFAR-10 with 0.04 GPU-days, and a test accuracy of 75.5% on ImageNet with 1.2 GPU-days. We also conducted experiments on the search space of NAS-Bench-201, and obtained test accuracies of 94.22, 73.07, and 46.23% for CIFAR-10, CIFAR-100 and ImageNet16-120, respectively. The above experimental results demonstrate that FP-DARTS achieves higher search speed and competitive performance compared to other state-of-the-art shrinkage methods and non-shrinkage methods.}
}
@article{ZHANG2024111498,
title = {Modular reverse design of acoustic metamaterial and sound barrier engineering applications: High ventilation and broadband sound insulation},
journal = {Thin-Walled Structures},
volume = {196},
pages = {111498},
year = {2024},
issn = {0263-8231},
doi = {https://doi.org/10.1016/j.tws.2023.111498},
url = {https://www.sciencedirect.com/science/article/pii/S0263823123009758},
author = {Xinhao Zhang and Qi Yu and Caiyou Zhao and Duojia Shi and Mingjing Geng and Junyuan Zheng and Tao Lu and Ping Wang},
keywords = {Novel acoustic metamaterials, Modular inverse design, High ventilation broadband acoustic insulation, PSO-DNN algorithm, Impedance tube test, OMCAM sound barrier},
abstract = {A multi-gradient cavity acoustic metamaterial (MCAM) structure and a modular reverse design method (MRDM) that can realize high ventilation and broadband acoustic isolation are proposed. The method controls the deep neural network model of acoustic metamaterials through a particle swarm algorithm, and the optimized multi-gradient cavity acoustic metamaterial structure (OMCAM) can be reverse-designed by inputting only the constraints and the objective function such as the amount of noise reduction. Compared with the finite element method, the computational efficiency can be improved by about 500 times to achieve an optimized design. The acoustic simulation results show that the average noise reduction of the structure is 23.5 dB in the range of 0∼4000 Hz, and a broadband sound attenuation with 38 dB noise reduction is formed in the target frequency band of 500Hz∼2000 Hz. The acoustic experimental results of the 3D-printed structure are in agreement with the simulation results. Compared with the two existing ventilated acoustic metamaterials, the average noise reduction of OMCAM under equal ventilation capacity is improved by 10.6 dB and 17.4 dB, respectively. The sound barrier based on the proposed OMCAM design is implemented on an elevated rail transit line, showing an improvement of 9.4 dB of average noise reduction compared with existing upright railroad sound barriers. The noise reduction mechanism of the OMCAM structure was finally revealed by the sound field distribution in different modes.}
}
@incollection{QIN2025775,
title = {Representation of others' beliefs},
editor = {Jordan Henry Grafman},
booktitle = {Encyclopedia of the Human Brain (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {775-792},
year = {2025},
isbn = {978-0-12-820481-8},
doi = {https://doi.org/10.1016/B978-0-12-820480-1.00159-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128204801001595},
author = {Jingmin Qin and Haiyan Wu},
keywords = {Theory of mind, Mentalizing, Perspective-taking, Metacognition, Social inference, Social cognition, Developmental changes, Neural mechanisms, Psychopathology, Computational models},
abstract = {This article delves into the diverse aspects in which individuals make inferences about the beliefs and values held by others. By reviewing the psychological factors underlying representing the beliefs of others, as well as the individual differences for both the individuals being represented and those undertaking the representation, this article sheds light on the intricate nature of representation of other's belief. Furthermore, it discusses the ways and considerations involved in updating these beliefs (such as observation, active interaction, and Bayesian inference) and offers suggestions for future research.}
}
@article{GUO2023100246,
title = {Function approximation reinforcement learning of energy management with the fuzzy REINFORCE for fuel cell hybrid electric vehicles},
journal = {Energy and AI},
volume = {13},
pages = {100246},
year = {2023},
issn = {2666-5468},
doi = {https://doi.org/10.1016/j.egyai.2023.100246},
url = {https://www.sciencedirect.com/science/article/pii/S2666546823000186},
author = {Liang Guo and Zhongliang Li and Rachid Outbib and Fei Gao},
keywords = {Energy management strategy, Fuel cell hybrid electric vehicle, Reinforcement learning, Fuzzy inference system, Fuzzy policy gradient, Hardware-in-loop},
abstract = {In the paper, a novel self-learning energy management strategy (EMS) is proposed for fuel cell hybrid electric vehicles (FCHEV) to achieve the hydrogen saving and maintain the battery operation. In the EMS, it is proposed to approximate the EMS policy function with fuzzy inference system (FIS) and learn the policy parameters through policy gradient reinforcement learning (PGRL). Thus, a so-called Fuzzy REINFORCE algorithm is first proposed and studied for EMS problem in the paper. Fuzzy REINFORCE is a model-free method that the EMS agent can learn itself through interactions with environment, which makes it independent of model accuracy, prior knowledge, and expert experience. Meanwhile, to stabilize the training process, a fuzzy baseline function is adopted to approximate the value function based on FIS without affecting the policy gradient direction. Moreover, the drawbacks of traditional reinforcement learning such as high computation burden, long convergence time, can also be overcome. The effectiveness of the proposed methods were verified by Hardware-in-Loop experiments. The adaptability of the proposed method to the changes of driving conditions and system states is also verified.}
}
@article{PALITTA2023112068,
title = {Stein-based preconditioners for weak-constraint 4D-var},
journal = {Journal of Computational Physics},
volume = {482},
pages = {112068},
year = {2023},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2023.112068},
url = {https://www.sciencedirect.com/science/article/pii/S0021999123001638},
author = {Davide Palitta and Jemima M. Tabeart},
keywords = {4D-var, Data assimilation, Preconditioning, Stein equations},
abstract = {Algorithms for data assimilation try to predict the most likely state of a dynamical system by combining information from observations and prior models. Variational approaches, such as the weak-constraint four-dimensional variational data assimilation formulation considered in this paper, can ultimately be interpreted as a minimization problem. One of the main challenges of such a formulation is the solution of large linear systems of equations which arise within the inner linear step of the adopted nonlinear solver. Depending on the selected approach, these linear algebraic problems amount to either a saddle point linear system or a symmetric positive definite (SPD) one. Both formulations can be solved by means of a Krylov method, like GMRES or CG, that needs to be preconditioned to ensure fast convergence in terms of the number of iterations. In this paper we illustrate novel, efficient preconditioning operators which involve the solution of certain Stein matrix equations. In addition to achieving better computational performance, the latter machinery allows us to derive tighter bounds for the eigenvalue distribution of the preconditioned linear system for certain problem settings. A panel of diverse numerical results displays the effectiveness of the proposed methodology compared to current state-of-the-art approaches.}
}
@article{QIN2025100117,
title = {LingoTrip: Spatiotemporal context prompt driven large language model for individual trip prediction},
journal = {Journal of Public Transportation},
volume = {27},
pages = {100117},
year = {2025},
issn = {1077-291X},
doi = {https://doi.org/10.1016/j.jpubtr.2025.100117},
url = {https://www.sciencedirect.com/science/article/pii/S1077291X25000025},
author = {Zhenlin Qin and Pengfei Zhang and Leizhen Wang and Zhenliang Ma},
keywords = {Large Language Models, Individual Mobility, Spatiotemporal Context Prompt, Personalied Infromation, Public Transport},
abstract = {Large language models (LLMs) showed superior performance in many language-related tasks. It is promising to model the individual mobility prediction problem as a language model and use pretrained LLMs to predict the individual next trip information (e.g., time and location) for personalized travel recommendations. Theoretically, it is expected to overcome the common limitations of data-driven prediction models in zero/few shot learning, generalization, and interpretability. The paper proposes a LingoTrip model for predicting individual next trip location by designing the spatiotemporal context prompts for LLMs. The designed prompting strategies enable LLMs to capture implicit land use information (trip purposes), spatiotemporal mobility patterns (choice preferences), and geographical dependencies of the stations used (choice variability). The lingoTrip is validated using Hong Kong Mass Transit Railway trip data by comparing it with the state-of-the-art data-driven mobility prediction models under different training data sizes. Sensitivity analyses are performed for model hyperparameters and their tuning methods to adapt for other datasets. The results show that LingoTrip outperforms data-driven models in terms of prediction accuracy, transferability (between individuals), zero/few shot learning (limited training sample size) and interpretability of predictions. The LingoTrip model can facilitate the effective provision of personalized information for system crowding and disruption contexts (i.e., proactively providing information to targeted individuals).}
}
@article{PUTTEGOWDA2025109910,
title = {Artificial intelligence and machine learning in mechanical engineering: Current trends and future prospects},
journal = {Engineering Applications of Artificial Intelligence},
volume = {142},
pages = {109910},
year = {2025},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2024.109910},
url = {https://www.sciencedirect.com/science/article/pii/S0952197624020694},
author = {Madhu Puttegowda and Sharath {Ballupete Nagaraju}},
keywords = {Artificial intelligence, Machine learning, Structural health monitoring, Predictive maintenance, Renewable energy systems},
abstract = {This review examines the transformative influence of artificial intelligence (AI) and machine learning (ML) on mechanical engineering, emphasizing application-specific advancements that have contributed to the field's progress. Key applications, including predictive maintenance, design optimization, structural health monitoring, quality control, and renewable energy optimization, illustrate how AI techniques, including reinforcement learning, deep learning, and neural networks, improve efficiency, reduce costs, and promote sustainable practices. This review also looks at the more general, overarching features of AI, such as its ability to adapt, to be interpreted, and to combine physics-based and data-driven models, which makes it easier to use these applications in a range of engineering settings. Despite significant advancements, challenges remain, including the model's robustness, computational demands, ethical considerations, and data quality. This paper endeavors to provide a comprehensive resource for researchers and practitioners in the mechanical engineering domain by synthesizing current advancements, identifying critical challenges, and predicting future trajectory.}
}
@article{DICARLO2007333,
title = {Untangling invariant object recognition},
journal = {Trends in Cognitive Sciences},
volume = {11},
number = {8},
pages = {333-341},
year = {2007},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2007.06.010},
url = {https://www.sciencedirect.com/science/article/pii/S1364661307001593},
author = {James J. DiCarlo and David D. Cox},
abstract = {Despite tremendous variation in the appearance of visual objects, primates can recognize a multitude of objects, each in a fraction of a second, with no apparent effort. However, the brain mechanisms that enable this fundamental ability are not understood. Drawing on ideas from neurophysiology and computation, we present a graphical perspective on the key computational challenges of object recognition, and argue that the format of neuronal population representation and a property that we term ‘object tangling’ are central. We use this perspective to show that the primate ventral visual processing stream achieves a particularly effective solution in which single-neuron invariance is not the goal. Finally, we speculate on the key neuronal mechanisms that could enable this solution, which, if understood, would have far-reaching implications for cognitive neuroscience.}
}
@article{GRIESBAUER2025106014,
title = {London taxi drivers exploit neighbourhood boundaries for hierarchical route planning},
journal = {Cognition},
volume = {256},
pages = {106014},
year = {2025},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2024.106014},
url = {https://www.sciencedirect.com/science/article/pii/S0010027724003007},
author = {Eva-Maria Griesbauer and Pablo {Fernandez Velasco} and Antoine Coutrot and Jan M. Wiener and Jeremy G. Morley and Daniel McNamee and Ed Manley and Hugo J. Spiers},
keywords = {Spatial representation, Real-world evidence, Hierarchical representations, Spatial cognition, Wayfinding, Executive function},
abstract = {Humans show an impressive ability to plan over complex situations and environments. A classic approach to explaining such planning has been tree-search algorithms which search through alternative state sequences for the most efficient path through states. However, this approach fails when the number of states is large due to the time to compute all possible sequences. Hierarchical route planning has been proposed as an alternative, offering a computationally efficient mechanism in which the representation of the environment is segregated into clusters. Current evidence for hierarchical planning comes from experimentally created environments which have clearly defined boundaries and far fewer states than the real-world. To test for real-world hierarchical planning we exploited the capacity of London licensed taxi drivers to use their memory to construct a street by street plan across London, UK (>26,000 streets). The time to recall each successive street name was treated as the response time, with a rapid average of 1.8 s between each street. In support of hierarchical planning we find that the clustered structure of London's regions impacts the response times, with minimal impact of the distance across the street network (as would be predicted by tree-search). We also find that changing direction during the plan (e.g. turning left or right) is associated with delayed response times. Thus, our results provide real-world evidence for how humans structure planning over a very large number of states, and give a measure of human expertise in planning.}
}
@article{XU2025100002,
title = {Intuitive interaction flow: A dual-loop human‒machine collaboration task allocation model and an experimental study},
journal = {Design and Artificial Intelligence},
pages = {100002},
year = {2025},
issn = {3050-7413},
doi = {https://doi.org/10.1016/j.daai.2025.100002},
url = {https://www.sciencedirect.com/science/article/pii/S3050741325000023},
author = {Jiang XU and Qiyang MIAO and Ziyuan HUANG and Yilin LU and Tianyang YU and Jingru PEI and Qichao ZHAO},
keywords = {Human‒machine collaboration (HMC), Intuitive interaction flow, Electroencephalogram (EEG), Embodied cognition},
abstract = {This study investigates the issue of task allocation in human‒machine collaboration (HMC). By integrating philosophical insights and cognitive science, it clearly defines two typical modes of human states in human‒machine interactions: skill-based intuitive states and knowledge-based intellectual states. Building on this, the concept of “intuitive interaction flow” is innovatively introduced by combining human intuition with machine humanoid intelligence, leading to the construction of a dual-loop HMC task allocation model. Through comparative experiments measuring electroencephalogram (EEG) and electromyogram (EMG) activities, distinct physiological patterns associated with these states are identified, providing a preliminary foundation for future adaptive HMC frameworks. This work offers a pathway for developing intelligent HMC systems that effectively integrate human intuition and machine intelligence.}
}
@article{JINADU20106753,
title = {Globalization & State Capacity in Africa},
journal = {Procedia - Social and Behavioral Sciences},
volume = {2},
number = {5},
pages = {6753-6763},
year = {2010},
note = {The Harmony of Civilization and Prosperity for All: Selected Papers of Beijing Forum(2004-2008)},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2010.05.021},
url = {https://www.sciencedirect.com/science/article/pii/S1877042810011559},
author = {L. Adele Jinadu},
abstract = {The paper examines the impact of globalization on state capacity in Africa. It problematizes globalization as a central determining factor in building the capable state in Africa. Globalization, although it requires typologizing and contextualizing or historicizing, is used to refer to a complex set of interconnected multi-linear, multifaceted and dialectical and still unfolding historical processes, propelled by the transnationalization of finance capital, in search of new markets, and the logic of capital accumulation, and typically characterized by structural differentiation and unequal functional integration between metropolitan and dependent or satellite nations, peoples and markets. State capacity is used neither narrowly nor exclusively as human and physical resource capacity-building or capacity-enhancement, nor limited to econometric or statistical computations of gross domestic product or national income data, though it includes and requires both. Its use assumes a democratic, open, participatory, and socially inclusive political system, as important conditions for expanding and consolidating state capacity on a sustainable basis in Africa. The paper situates the problem of globalization for state capacity in Africa in the wider Pan African context. Historically, globalization has divided and balkanized African countries, carving out political, economic and cultural spheres of influence, and weakening their ability to act collectively to defend their common interests. Collective action by African countries to confront the challenges and opportunities of globalization requires new governance structures to strengthen African regional economic communities, the African Union and the New Partnership for Africa's Development, along lines that will, by democratizing decision-making and public political processes within their member-states, enhance state capacity in various sectors. Attributing the problematic character of state capacity in Africa to the massive problem of the structural condition of the African state, the paper argues that this is notably and significantly due to the contradictions arising from globalization and the dependent character of the African state, reflecting the lingering or residual colonial inheritance of dependent political and socioeconomic and psycho-cultural structures, institutions and processes, which are at the heart of the problem of state capacity in Africa. They reflect the dialectics or antinomies, the age-old or historically deep contradictory push and pull of globalization and localization or indigenization in Africa. The paper suggests that, resolving these antinomies or contradictions, requires the following: (a) Transforming contemporary globalization, on the basis of mutuality, recognition and reciprocity, emphasizing new Afrocentric epistemological foundations for thinking about African and global development, global social justice, global income redistribution, economic and socio-cultural rights, global inclusion, and global democracy. (b) Emphasizing the use of “appropriate” technologies, to ‘fit’ the lifestyles and social organizations of local communities, growing from them, requiring less reliance on outside experts and using more local expertise. (c) Re-designing new Pan-African approaches to state capacity, to strengthen the collective capacity of African continental and regional institutions to respond to globalization, turning its negative implications for Africa into opportunities to reform globalization, and make it truly global. (d) Reconceptualizing democracy, on the basis of the positive role of culture in generating and institutionalizing new modes of self-reliant, and transparent democratic governance.}
}
@article{MOINGEON2021566,
title = {Applications de l’intelligence artificielle au développement de nouveaux médicaments},
journal = {Annales Pharmaceutiques Françaises},
volume = {79},
number = {5},
pages = {566-571},
year = {2021},
issn = {0003-4509},
doi = {https://doi.org/10.1016/j.pharma.2021.01.008},
url = {https://www.sciencedirect.com/science/article/pii/S0003450921000092},
author = {P. Moingeon},
keywords = {Biotechnologies, Développement médicamenteux, Intelligence artificielle, Machines intelligentes, Médecine computationnelle, Médecine de précision, Modèle de maladie, Artificial intelligence, Biotechnologies, Computational medicine, Disease model, Drug development, Intelligent machines, Precision medicine},
abstract = {Résumé
L’intelligence artificielle (IA) recouvre les technologies qui reproduisent, par la machine, quatre dimensions de l’intelligence humaine, à savoir la perception, l’analyse, l’action et l’apprentissage. Les progrès technologiques, combinés dans ces domaines, permettent de générer et d’analyser des données massives pour modéliser la réalité d’un phénomène. Ces modèles sont ensuite réactualisés par l’accumulation de nouvelles données afin d’aider à la prise de décision et prédire le futur. Appliquée à la problématique du développement d’un médicament, l’IA permet d’établir des modèles de maladies à partir de données de profilage moléculaire de patients. Par sa puissance de calcul, l’IA intègre ces données multimodales massives dans un modèle permettant : (i) de rendre compte de l’hétérogénéité des maladies ; et (ii) d’identifier des cibles thérapeutiques importantes dans la physiopathologie. D’autres analyses computationnelles sont utilisées pour identifier des molécules thérapeutiques interagissant avec ces cibles, optimiser ces molécules ou repositionner des molécules anciennes dans de nouvelles indications. La modélisation par l’IA aide également à identifier des biomarqueurs d’efficacité, définir des combinaisons de molécules thérapeutiques pertinentes, concevoir des études cliniques innovantes avec des groupes placebo virtuels… Cette convergence révolutionnaire entre les biotechnologies, les sciences du médicament et l’IA donne aujourd’hui naissance à une médecine computationnelle de précision applicable à toutes les maladies chroniques, qui offrira des traitements parfaitement ciblés prenant en compte les spécificités du patient quant à sa physiologie, sa maladie, sa relation à l’environnement.
Summary
Artificial intelligence (AI) encompasses technologies recapitulating four dimensions of human intelligence, i.e. sensing, thinking, acting and learning. The convergence of technological advances in those fields allows to integrate massive data and build probabilistic models of a problem. The latter can be continuously updated by incorporating new data to inform decision-making and predict the future. In support of drug discovery and development, AI allows to generate disease models using data obtained following extensive molecular profiling of patients. Given its superior computational power, AI can integrate those big multimodal data to generate models allowing: (i) to represent patient heterogeneity; and (ii) identify therapeutic targets with inferences of causality in the pathophysiology. Additional computational analyses can help identifying and optimizing drugs interacting with these targets, or even repurposing existing molecules for a new indication. AI-based modeling further supports the identification of biomarkers of efficacy, the selection of appropriate combination therapies and the design of innovative clinical studies with virtual placebo groups. The convergence of biotechnologies, drug sciences and AI is fostering the emergence of a computational precision medicine predicted to yield therapies or preventive measures precisely tailored to patient characteristics in terms of their physiology, disease features and environmental risk exposure.}
}
@article{PICCIONI2024114222,
title = {From layer to building: Multiscale modeling of thermo-optical properties in 3D-printed facades},
journal = {Energy and Buildings},
volume = {314},
pages = {114222},
year = {2024},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2024.114222},
url = {https://www.sciencedirect.com/science/article/pii/S0378778824003384},
author = {Valeria Piccioni and Lars O. Grobe and Illias Hischier and Arno Schlueter},
keywords = {3D-printed facades, Thermo-optical properties, Multi-scale modeling, Experimental validation},
abstract = {The challenge of building sector decarbonization has driven an integral rethinking of the way we design and build facades. Recently, large scale 3D-printing has emerged as an alternative manufacturing technique for novel facade components aiming at high operational efficiency and low environmental impact. Focusing on translucent polymer 3DPFs, this study tackles the challenges of modeling thermal and optical effects in geometrically complex components where interactions across multiple domains and scales occur. In particular, we introduce a novel method for modeling the irregular thermo-optical properties of 3DPFs, capable of capturing relevant effects often out of the scope of traditional modeling approaches. Our model accounts for geometry-dependent physical effects ranging from millimeter-scale fabrication details that impact optical behavior to centimeter-scale geometric features influencing heat and radiation transfer, extending up to the meter-scale implications for the building application. By employing computational techniques such as ray-tracing, computational fluid dynamics, and finite element analysis, we establish a model that offers detailed thermal and optical analysis to support performance-driven design iterations. Finally, demonstrating this approach in an office building context, we show that 3DPFs can match the performance of double glazing with dynamic shading, providing effective solar and thermal management over the year. This is achieved in a single, mono-material component with no active control, suggesting 3DPFs are a promising direction for low-environmental impact facade design.}
}
@article{LIENERT1996845,
title = {LSD response in Eysenckian trait types identified by polypredictive CFA},
journal = {Personality and Individual Differences},
volume = {21},
number = {6},
pages = {845-850},
year = {1996},
issn = {0191-8869},
doi = {https://doi.org/10.1016/S0191-8869(96)00143-2},
url = {https://www.sciencedirect.com/science/article/pii/S0191886996001432},
author = {Gustav A. Lienert and Petra Netter},
abstract = {The four personality type combinations derived from high and low extraversion (E+E−) and high and low neuroticism (N+N−) have been related to response patterns composed of three symptoms (affective disturbances, thinking disturbances, and blackouts) scored as present (+) or absent (−) after a single oral dose of the hallucinogenic drug LSD-25. Hypotheses for expected response patterns for each personality group were derived from a data set obtained by Kohnen and Lienert (1987). Significance of associations was tested by two strategies of polyprediction configural frequency analysis (CFA): multiple uniprediction and biprediction CFA. Both strategies yielded a significant hyperpresentation of all three symptoms present in E+N+ (hysterics), merely thinking disorders in dysthymics (E−N+), merely affective symptoms in E+N− (stable extraverts), and merely blackouts in N−E− (stable introverts). Authors tried to relate these symptoms to Kretschmer's temperament types and could afterwards show by a chessboard modification of prediction CFA, that by applying two combined hypotheses for two personality types each, the significance of the predicted associations could be increased.}
}
@article{NAVEIRO2019133,
title = {Adversarial classification: An adversarial risk analysis approach},
journal = {International Journal of Approximate Reasoning},
volume = {113},
pages = {133-148},
year = {2019},
issn = {0888-613X},
doi = {https://doi.org/10.1016/j.ijar.2019.07.003},
url = {https://www.sciencedirect.com/science/article/pii/S0888613X18304705},
author = {Roi Naveiro and Alberto Redondo and David {Ríos Insua} and Fabrizio Ruggeri},
keywords = {Classification, Bayesian methods, Adversarial machine learning, Influence diagrams, Robustness},
abstract = {Classification techniques are widely used in security settings in which data can be deliberately manipulated by an adversary trying to evade detection and achieve some benefit. However, traditional classification systems are not robust to such data modifications. Most attempts to enhance classification algorithms in adversarial environments have focused on game theoretical ideas under strong underlying common knowledge assumptions, which are not actually realistic in security domains. We provide an alternative framework to such problems based on adversarial risk analysis which we illustrate with examples. Computational, implementation and robustness issues are discussed.}
}
@article{LAI202333,
title = {Impact of social cognitive propensity on the processing of nontransparent sentential meaning},
journal = {Journal of Pragmatics},
volume = {205},
pages = {33-62},
year = {2023},
issn = {0378-2166},
doi = {https://doi.org/10.1016/j.pragma.2022.12.014},
url = {https://www.sciencedirect.com/science/article/pii/S0378216622003010},
author = {Yao-Ying Lai and Huei-ling Lai},
keywords = {Nontransparent meaning, Meaning contextualization, Combinatorial semantic processing, Individual differences, Social cognition, Autism-spectrum quotient},
abstract = {This study investigates the influence of individual social-cognitive propensity in the processing of nontransparent sentential meaning, exemplified by the morpho-syntactically unsupported iterative meaning in “The frog hopped for five minutes.” Results of our speeded questionnaire in Mandarin Chinese showed that social cognitive propensity of typically-developed individuals, indexed by autistic-like traits, significantly correlated with online response times (RTs) of naturalness rating, while the effect was absent in offline rating scores. Individuals with higher autistic-like traits (i.e., lower social skills) took longer to process sentences with nontransparent meaning for making judgments. We argue that the computation of these sentences involves meaning contextualization—construing a coherent conceptual representation by integrating multiple lexical representations and evaluating sentential-discourse context. Such context-dependent meaning processing requires sufficient context sensitivity, which varies across individuals in association with social cognitive propensity. The pattern is captured by the Dual-Process Approach to information processing and social cognition: individuals with higher autistic-like traits are prone to deliberative reasoning with lower contextual sensitivity. This cognitive bias leads to greater cost when the full comprehension demands meaning contextualization, and therefore longer RTs in evaluating appropriate interpretations. The findings show that individual variability in social cognitive propensity modulates the online computation of nontransparent sentential meaning.}
}
@article{SELVARAJ2022100471,
title = {Capture Based Trust Dependence framework for authorized node identification in mobile agent systems},
journal = {Measurement: Sensors},
volume = {24},
pages = {100471},
year = {2022},
issn = {2665-9174},
doi = {https://doi.org/10.1016/j.measen.2022.100471},
url = {https://www.sciencedirect.com/science/article/pii/S2665917422001052},
author = {Priyanka Selvaraj and Vijay Bhanu Srinivasan},
keywords = {Capture based, Dependence framework mobile agents, Network performance, Trust identification},
abstract = {A mobile agent is a self-learning machine entity that uses the system infrastructure to keep running in another remote zone, check and compile the results, interact with various locations and return to his home site after completing the relegated activities. Mobile Agent-based solutions for the testing community have grown in popularity and are now used in a variety of fields, including boardroom management, electronic commerce, renewable energy and power management. Addition to these applications, Broadband Interactive Sensors, network performance improvement, disseminated knowledge mining, multimedia, human monitoring, surveillance, affective computing, weather and environment, e-learning and semantic web administrations are only a few of the topics covered. In an extremely non trusty environment, focus should be taken to shield the portable operator from acquiring altered. Existing works on mobile agent frameworks with very surprising instruments does not offer complete security. In this paper a capture based trust dependence framework is proposed for identification of authorized or trust nodes inside mobile agents systems. Here we consider the mobile adhoc networks for computational analysis of network performance using network simulator. The framework provides efficient results in identification of authorized nodes.}
}
@article{MULET200632,
title = {Functional requirements for computer-based design support systems, derived from experimental studies},
journal = {Knowledge-Based Systems},
volume = {19},
number = {1},
pages = {32-42},
year = {2006},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2005.08.003},
url = {https://www.sciencedirect.com/science/article/pii/S0950705105000869},
author = {Elena Mulet and Rosario Vidal},
keywords = {Computer design support, Knowledge-based design systems, Design process, Computational synthesis methods, Experimental research in engineering design},
abstract = {In this paper, we examine the functions that a computational system for knowledge-based design support may undertake. We present a set of functions that bring together previous approaches and allow us to locate the work that has been developed to enhance these systems concerning those functions. We describe some new proposals, based on experimental research work, for improving some of these functions so that they can be taken into account in the development of design support systems to help the designer or group of designers reach a suitable solution in a more effective way.}
}
@article{SOYEL20131312,
title = {Towards an affect sensitive interactive companion},
journal = {Computers & Electrical Engineering},
volume = {39},
number = {4},
pages = {1312-1319},
year = {2013},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2013.03.010},
url = {https://www.sciencedirect.com/science/article/pii/S0045790613000712},
author = {Hamit Soyel and Peter W. McOwan},
abstract = {As robots are increasingly being viewed as social entities to be integrated in our daily lives, social perceptive abilities seem a necessary requirement for enabling more natural interaction with human users. In this paper, we present an interaction scenario where user play chess with an iCat robot and propose an affect recognition system that uses computational models to automatically extract visual features allowing the detection of the level of engagement with a social robot that acts as a game companion. Experimental results show that the multimodal integration of head direction information with facial expressions displayed by the user improves the recognition of the user’s affective states.}
}
@article{HUGHES2021338,
title = {High-content phenotypic and pathway profiling to advance drug discovery in diseases of unmet need},
journal = {Cell Chemical Biology},
volume = {28},
number = {3},
pages = {338-355},
year = {2021},
issn = {2451-9456},
doi = {https://doi.org/10.1016/j.chembiol.2021.02.015},
url = {https://www.sciencedirect.com/science/article/pii/S2451945621001008},
author = {Rebecca E. Hughes and Richard J.R. Elliott and John C. Dawson and Neil O. Carragher},
keywords = {high-content imaging, machine learning, structural similarity, network pharmacology, esophageal cancer, glioblastoma},
abstract = {Summary
Conventional thinking in modern drug discovery postulates that the design of highly selective molecules which act on a single disease-associated target will yield safer and more effective drugs. However, high clinical attrition rates and the lack of progress in developing new effective treatments for many important diseases of unmet therapeutic need challenge this hypothesis. This assumption also impinges upon the efficiency of target agnostic phenotypic drug discovery strategies, where early target deconvolution is seen as a critical step to progress phenotypic hits. In this review we provide an overview of how emerging phenotypic and pathway-profiling technologies integrate to deconvolute the mechanism-of-action of phenotypic hits. We propose that such in-depth mechanistic profiling may support more efficient phenotypic drug discovery strategies that are designed to more appropriately address complex heterogeneous diseases of unmet need.}
}
@article{KELLIHER201536,
title = {Design futures in action: Documenting experiential futures for participatory audiences},
journal = {Futures},
volume = {70},
pages = {36-47},
year = {2015},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2014.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S0016328714001980},
author = {Aisling Kelliher and Daragh Byrne},
keywords = {Documentation, Multimedia, Experiential futures, Summarization, Representation, Annotation, Exhibition, Social platform},
abstract = {The futures field demonstrates a willing openness in embracing methodologies, approaches, and influences from a diversity of disciplines and perspectives. This plurality of practice is evidenced in a growing body of work that increasingly embodies futures thinking in the design of everyday material and networked experiences. The intersection of design and futures produces artifacts, applications and interactions created to provoke dialog in an accessible manner. As part of the Futures special issue on the Emerge: Artists and Scientists Redesign the Future event, this article describes the documentation and public representation of the creative outcomes from nine Emerge design futures workshops. These workshops provided a rich opportunity to study how designers and futurists collaboratively engage, implement and communicate alternative futures. The goal of the documentation effort described is to capture the experience of creating experiential futures and extend the capacity for developing social foresight through a participatory exhibit and online social platform.}
}
@article{CAI2021106,
title = {The Neural Instantiation of an Abstract Cognitive Map for Economic Choice},
journal = {Neuroscience},
volume = {477},
pages = {106-114},
year = {2021},
issn = {0306-4522},
doi = {https://doi.org/10.1016/j.neuroscience.2021.09.011},
url = {https://www.sciencedirect.com/science/article/pii/S0306452221004784},
author = {Xinying Cai},
keywords = {cognitive map, economic choice, orbitofrontal cortex, subjective value},
abstract = {Since the discovery of cognitive maps in rodent hippocampus (HC), the cognitive map has evolved from originally referring to spatial representations encoding locations and objects in Euclidean spaces to a general low-dimensional organization of information along selected feature dimensions. A cognitive map includes hypothetical constructs that bridge between environmental stimuli and the final overt behavior. To neuroeconomists, utility and utility functions are such constructs with neurobiological basis that drive choice behavior. Emergence of distinct functional neuron groups in the primate orbitofrontal cortex (OFC) during simple economic choice indicates the formation of an abstract cognitive map for organizing information of goods for value computation. Experimental evidence suggests that organization of neuronal activity in such cognitive map reflects the abstraction of core task features. Thus, such map can be adapted to accommodate economic choices under various task contexts.}
}
@article{ANDERSON2011R123,
title = {Neuroscience: What We Cannot Model, We Do Not Understand},
journal = {Current Biology},
volume = {21},
number = {3},
pages = {R123-R125},
year = {2011},
issn = {0960-9822},
doi = {https://doi.org/10.1016/j.cub.2010.12.049},
url = {https://www.sciencedirect.com/science/article/pii/S0960982210017173},
author = {William S. Anderson and Gabriel Kreiman},
abstract = {Summary
To understand computations in neuronal circuits, a model of a small patch of cortex has been developed that can describe the firing regime in the visual system remarkably well.}
}
@article{VANDERVERT2003159,
title = {How working memory and cognitive modeling functions of the cerebellum contribute to discoveries in mathematics},
journal = {New Ideas in Psychology},
volume = {21},
number = {2},
pages = {159-175},
year = {2003},
issn = {0732-118X},
doi = {https://doi.org/10.1016/S0732-118X(03)00012-6},
url = {https://www.sciencedirect.com/science/article/pii/S0732118X03000126},
author = {Larry Vandervert},
keywords = {Einstein, Cerebellum, Intuition, Mathematics, Mental models, Working memory},
abstract = {A theory of how connections between working memory and cognitive functions of the cerebellum lead to mathematical discovery is presented. It is proposed that (a) patterns of repetitious working memory processing are learned in the cerebellum, and (b) when these cerebellar patterns are subsequently fed back to control processing in working memory, they are learned in visuospatial imagery and language as the concepts and axioms that underlie mathematical discovery. Paralleling Einstein's description of “thinking,” a working memory/cerebellar model of mathematical intuition is presented. It is concluded that the collaboration of the cerebellum and working memory constructs the only fundamental patterns (mathematics) of the joint framework that binds our cognitive consciousness with the socially verifiable operational specification of an external world.}
}
@incollection{BERNARD20221,
title = {Chapter One - Understanding cerebellar function through network perspectives: A review of resting-state connectivity of the cerebellum},
editor = {Kara D. Federmeier},
series = {Psychology of Learning and Motivation},
publisher = {Academic Press},
volume = {76},
pages = {1-49},
year = {2022},
issn = {0079-7421},
doi = {https://doi.org/10.1016/bs.plm.2022.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S0079742122000019},
author = {Jessica A. Bernard},
keywords = {Cerebellum, Resting-state connectivity, Cognition, Internal models},
abstract = {The human cerebellum, though relatively small in total volume, makes up for it in its neuronal density and immense computational power. As we seek to understand complex higher order human behavior, it is critical to consider how this structure may contribute to these domains. While historically conceptualized as a motor structure, likely in large part due to the overt motor deficits often experienced by those with cerebellar damage, it is now known to play a critical role in cognition. In the last decade in particular there has been a great deal of growth in the literature in this regard (though these ideas have been percolating since the 1980s). The development of resting-state functional connectivity magnetic resonance imaging (fcMRI) also resulted in a boom of literature seeking to clarify cerebellar interactions with the cortex. In this chapter, cerebellar anatomy and function are reviewed, with a particular focus on how fcMRI has impacted our understanding of the human cerebellum and what this has meant for our accounts of cerebellar processing (that is, the underlying computations). This work has broadened our appreciation of the cerebellum's networks linked to higher order processing, and resulted in thought provoking findings with respect to the functional organization of the cerebellum that may in the future impact our understanding of how the “little brain” helps the cortex produce nuanced and complicated human behavior.}
}
@article{BAUMANN20101561,
title = {Numerical solution of level dependent quasi-birth-and-death processes},
journal = {Procedia Computer Science},
volume = {1},
number = {1},
pages = {1561-1569},
year = {2010},
note = {ICCS 2010},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2010.04.175},
url = {https://www.sciencedirect.com/science/article/pii/S1877050910001766},
author = {Hendrik Baumann and Werner Sandmann},
keywords = {Continuous-time Markov chains, Block-tridiagonal generator matrices, Level dependent quasi-birth-and-death processes, Numerical solution, Matrix continued fractions},
abstract = {We consider the numerical computation of stationary distributions for level dependent quasi-birth-and-death processes. An algorithm based on matrix continued fractions is presented and compared to standard solution techniques. Its computational efficiency and numerical stability is demonstrated by numerical examples.}
}
@incollection{BLOCKLEY2013229,
title = {9 - Earthquake risk management of civil infrastructure: integrating soft and hard risks},
editor = {S. Tesfamariam and K. Goda},
booktitle = {Handbook of Seismic Risk Analysis and Management of Civil Infrastructure Systems},
publisher = {Woodhead Publishing},
pages = {229-254},
year = {2013},
series = {Woodhead Publishing Series in Civil and Structural Engineering},
isbn = {978-0-85709-268-7},
doi = {https://doi.org/10.1533/9780857098986.2.229},
url = {https://www.sciencedirect.com/science/article/pii/B9780857092687500098},
author = {D. Blockley},
keywords = {seismic risk, uncertainty, safety, systems thinking, integration, hard and soft systems},
abstract = {Abstract:
Risk is an inevitable part of all human activity. Similarly sized earthquakes can have very different impacts in different countries depending on the degree of engineering input into the design and construction of the facilities. In this chapter we will propose an approach based on systems thinking and new systems boundaries. We will identify and characterise three different sources of uncertainty: hard physical system parameter uncertainty, hard system model uncertainty and soft system human uncertainty. We will explore ways in which evidence from previously disparate sources can be managed in an integrated way.}
}
@article{SUWA1997385,
title = {What do architects and students perceive in their design sketches? A protocol analysis},
journal = {Design Studies},
volume = {18},
number = {4},
pages = {385-403},
year = {1997},
note = {Descriptive models of design},
issn = {0142-694X},
doi = {https://doi.org/10.1016/S0142-694X(97)00008-2},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X97000082},
author = {Masaki Suwa and Barbara Tversky},
keywords = {architectural design, design cognition, drawings, perception, protocol analysis},
abstract = {The present research aims at examining what information architects think of and read off from their own freehand sketches, and at revealing how they perceptually interact with and benefit from sketches. We explored this in a protocol analysis of retrospective reports; each participant worked on an architectural design task while drawing freehand sketches and later reported what she/he had been thinking of during the design task. This research lies within the scope of examinations of why freehand sketches as external representation are essential for crystallizing design ideas in early design processes.}
}
@article{REINOSOCARVALHO2020389,
title = {A sprinkle of emotions vs a pinch of crossmodality: Towards globally meaningful sonic seasoning strategies for enhanced multisensory tasting experiences},
journal = {Journal of Business Research},
volume = {117},
pages = {389-399},
year = {2020},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2020.04.055},
url = {https://www.sciencedirect.com/science/article/pii/S0148296320302800},
author = {Felipe Reinoso-Carvalho and Laura Gunn and German Molina and Takuji Narumi and Charles Spence and Yuji Suzuki and Enrique {ter Horst} and Johan Wagemans},
keywords = {Crossmodal, Emotions, Flavors, Multisensory, Music, Purchase intention},
abstract = {We report a study designed to determine the most efficient means of pursuing sonic seasoning in international marketing. For the first time, music chosen to trigger specific emotional responses was directly and cross-culturally compared with music chosen as crossmodally congruent with specific taste/flavors (the latter usually referred to as ‘sonic seasoning’). The effects triggered by ‘emotional’ music were more prominent than those triggered by ‘crossmodally-corresponding’ music. Specifically, chocolate was liked more, rated as sweeter, and the purchase intent was higher, when tasted while listening to music that conveyed positive, as compared to negative, emotion. By contrast, the same chocolate was mostly rated as tasting more bitter with the negative music, as compared to the positive music. Companies looking to use sonic seasoning in marketing strategies, should therefore principally aim at intelligently classifying music based on the likely emotions that they can trigger in their customers (at least when thinking globally).}
}
@incollection{VELINGKAR2025239,
title = {Chapter 16 - Smart computing in brain-computer interface and neuroscientific research: opportunities, methods, and challenges},
editor = {Bikesh Kumar Singh and G.R. Sinha},
booktitle = {Intelligent Computing Techniques in Biomedical Imaging},
publisher = {Academic Press},
pages = {239-249},
year = {2025},
isbn = {978-0-443-15999-2},
doi = {https://doi.org/10.1016/B978-0-443-15999-2.00005-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780443159992000050},
author = {Harish Velingkar and Roopa R. Kulkarni and Prashant P. Patavardhan},
keywords = {BCI, BMI, AI algorithms, machine learning, neuroscientific, brain waves, EEG},
abstract = {Brain-machine interfacing (BMI), also known as brain-computer interfacing (BCI), is an innovative field of technology with a primary objective to establish a connection and foster seamless interaction between the human brain and machines. The underlying concept aims to facilitate direct communication between the brain and external devices, enabling a direct interface between the two, allowing individuals to control various applications, such as prosthetics, robotics, or computer software, with their thoughts alone. BCI is a multidisciplinary field that involves expertise from different areas, such as neuroscience, computer science, engineering, and psychology. BMI is also being extensively used in early prediction of neurophysiological abnormalities and brain disorders. This article will lay out various benefits of using computational intelligence, specifically machine learning, in mental health disciplines and will also explain some of the popular AI algorithms from a neuroscientific research point of view.}
}
@incollection{ODENBAUGH2011421,
title = {Complex Ecological Systems},
editor = {Cliff Hooker},
booktitle = {Philosophy of Complex Systems},
publisher = {North-Holland},
address = {Amsterdam},
pages = {421-439},
year = {2011},
volume = {10},
series = {Handbook of the Philosophy of Science},
issn = {18789846},
doi = {https://doi.org/10.1016/B978-0-444-52076-0.50015-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780444520760500158},
author = {Jay Odenbaugh},
abstract = {Publisher Summary
This chapter considers some of the ways in which nonlinear dynamics is changing the science of ecology. Specifically, it considers a bit of history; namely, debates over the stability of populations and communities in population and community ecology. Further, it explores arguments over population regulation in population ecology and the debate over diversity-complexity-stability in community ecology. This serves to highlight how ecological systems have been evaluated with the tools and assumptions of linear dynamical systems. Second, it turns to some conceptual issues. That is, what different concepts of stability are at work in ecology. Additionally, it provides a taxonomy of these concepts and how they are related to one another. Unfortunately, many of these concepts are mostly applicable when thinking of linear systems. As an example of nonlinear dynamics in ecology, it considers the case of deterministic chaos. Using very simple discrete difference equations suited for insect populations, for example, one can see the characteristic of sensitivity to initial conditions. Finally, it discusses the impact of complex systems analysis on issues in and around ecology. Specifically, it examines the rise of “resilience thinking” and debates over ecological laws as examples of how nonlinear dynamics is challenging ecological theory and practice.}
}
@article{CHUDERSKI2014258,
title = {How well can storage capacity, executive control, and fluid reasoning explain insight problem solving},
journal = {Intelligence},
volume = {46},
pages = {258-270},
year = {2014},
issn = {0160-2896},
doi = {https://doi.org/10.1016/j.intell.2014.07.010},
url = {https://www.sciencedirect.com/science/article/pii/S0160289614001020},
author = {Adam Chuderski},
keywords = {Insight problem solving, Fluid reasoning, Working memory, Storage capacity, Executive control},
abstract = {Previous studies have found discrepant results on the relationship between insight problem solving and the processes underlying analytic thinking: storage capacity, executive control (two components of working memory; WM), as well as fluid reasoning. Some research showed that WM and/or reasoning are positively related to insight, supporting the “nothing-special” account, whereas other studies demonstrated null or negative relationships favoring the “special-process” view. This study examined a large sample with a battery of insight, reasoning, and WM tasks, to estimate the pattern of links between investigated constructs using structural equation modeling. WM and reasoning together explained about two thirds of the variance in insight. Both WM components similarly contributed to insight. WM's contribution was mediated by reasoning. These results support the nothing-special view. However, after WM variance was partialed out, the link between insight and reasoning substantially weakened, that makes room for the special-process view. Both accounts can be integrated in the view that insight is “nothing special with special add-ons” – the latter understood as the processes and strategies specific only to insight problem solving.}
}
@article{BEZERRAFEITOSA2024843,
title = {Multiple criteria evaluation of hydrogen production processes for use in automotive sector},
journal = {International Journal of Hydrogen Energy},
volume = {49},
pages = {843-861},
year = {2024},
issn = {0360-3199},
doi = {https://doi.org/10.1016/j.ijhydene.2023.09.232},
url = {https://www.sciencedirect.com/science/article/pii/S0360319923048784},
author = {Francisco Edvan {Bezerra Feitosa} and Antonella Lombardi Costa},
keywords = {Hydrogen, Gasification, Steam reforming, Electrolysis, Solar and nuclear thermochemical, MACBETH},
abstract = {This work evaluates large-scale hydrogen production processes - those capable of providing hydrogen for an eventual automotive hydrogen program in a community and along highways - with the aim to Sidentify which of the production processes of hydrogen is more attractive for an eventual automotive hydrogen program. To perform the research, the MACBETH (Measuring Attractiveness by the Category-Based Assessment Technique) method and the computational code M-MACBETH 3.2.0 are used taking into account multiple criteria. Thus, this work used the computational code M-MACBETH 3.2.0 to evaluate twelve hydrogen production processes and build rankings of attractiveness of hydrogen production processes, considering the following criteria: economic (invested capital and production cost), technical (the purity of the hydrogen produced) and environmental (the amount of energy used to produce 1 kg of hydrogen and the CO2 emissions to produce 1 kg of hydrogen). In the end, three rankings are produced considering three scenarios, which allow to conclude that: if decisions regarding hydrogen are made taking into account that economic and financial aspects have the same weight as sustainability aspects, the more attractive hydrogen plants are those that use the Solar Thermochemical Plants; if decisions are made taking into account that economic and financial aspects are more important than aspects of sustainability, the production process more attractive will be the one that uses the SR technology - Ethanol and Natural Gas Steam Reforming; and that Solar Thermochemical Plants becomes more attractive than all others alternatives when, in decision-making, sustainability environmental aspects prevail over economic and financial aspects.}
}
@article{GUO202377,
title = {Notes on the improvement of concept-cognitive learning accuracy},
journal = {International Journal of Approximate Reasoning},
volume = {156},
pages = {77-96},
year = {2023},
issn = {0888-613X},
doi = {https://doi.org/10.1016/j.ijar.2023.02.010},
url = {https://www.sciencedirect.com/science/article/pii/S0888613X23000294},
author = {Keyi Guo and Jinhai Li and Xiao Zhang},
keywords = {Granular computing, Rough set, Concept lattice, Concept-cognitive learning, Learning accuracy},
abstract = {The concept-cognitive learning (CCL) process is the specific implementation step of simulating the human brain to learn concepts, and the CCL model is its core carrier. Different CCL models constructed by different cognitive minds will produce different concept learning results. The existing CCL model based on sufficient and necessary granule approximations regards the human's approximation idea in the face of inconsistent information as a logical criterion, and aims at finding the closest concept pair of the clue as concept learning results with a certain learning accuracy. However, the existing CCL method based on sufficient and necessary granule approximations cannot guarantee that the clue must be between its lower approximation and upper approximation, which causes the fact that the learning accuracy may not effectively measure the consistency of concept learning results. What is more, although the computational process obeys logical cognitive condition, the concept learning results may not conform to the actual situation, such as the case of merely generating full concepts and empty concepts. For the first problem, we improve the learning accuracy of the existing CCL method, propose a new CCL method with learning accuracy under hybrid lattice structure, and develop CCL algorithms for the cases of objects and attributes as clues. Moreover, experiments show the effectiveness of the proposed CCL method with learning accuracy under hybrid lattice structure. For the second problem, we put forward a CCL method based on non-logical associative mechanism to handle the unreasonable situation where the concept learning results are full concepts and empty concepts. Finally, two associative CCL algorithms are explored, and experiments are conducted to show their effectiveness.}
}
@article{BONANI2025100709,
title = {A rapid and inclusive instrument for assessing children’s basic understanding of physical computing},
journal = {International Journal of Child-Computer Interaction},
volume = {43},
pages = {100709},
year = {2025},
issn = {2212-8689},
doi = {https://doi.org/10.1016/j.ijcci.2024.100709},
url = {https://www.sciencedirect.com/science/article/pii/S2212868924000783},
author = {Andrea Bonani and Rosella Gennari and Alessandra Melonio and Pierpaolo Vittorini},
keywords = {Physical computing, Assessment, Understanding, Rapid questionnaire, Inclusive questionnaire, School, Teacher, Children},
abstract = {While there are many initiatives that strive to empower children with physical computing, there seems to be no validated questionnaire for rapidly measuring different children’s understanding of the basics of physical computing. This paper presents the design of such an instrument—PCBUQ. It is rapid in that it consists of few items. It is inclusive because designed for different young children. It is for the basics of physical computing in that it considers physical input and output devices, basic patterns and programs that use them for interacting with the physical world. Data gathered from experts, primary and middle schools were used to validate PCBUQ. The first items assess children’s capability of classifying physical devices as input (e.g., buttons), and output devices (e.g., LED, speaker). The other items evaluate whether children can interpret problematic scenarios and infer how to resolve them with adequate input and output devices, patterns and programs. PCBUQ was found to have adequate reliability. The reported statistical analyses highlight the items that strongly and weakly correlate with the construct under analysis, their difficulty and discrimination. Results are discussed to guide future physical computing initiatives for children and their assessment.}
}
@article{TERAMOTO201893,
title = {Betti number ratios as quantitative indices for bone morphometry in three dimensions},
journal = {Computer Methods and Programs in Biomedicine},
volume = {162},
pages = {93-98},
year = {2018},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2018.05.012},
url = {https://www.sciencedirect.com/science/article/pii/S0169260718300695},
author = {Takashi Teramoto and Takeshi Kamiya and Taira Sakurai and Fuminori Kanaya},
keywords = {Computational homology, Bone morphometry, Image processing},
abstract = {Background and objective: Computational homology is an emerging mathematical tool for characterizing shapes of data. In this work, we present a methodology using computational homology for obtaining quantitative measurements of the connectivity in bone morphometry. We introduce the Betti number ratios as novel morphological descriptor for the classification of bone fine structures in three dimensions.
Methods
A total of 51 Japanese white rabbits were used to investigate the connectivity of bone trabeculae after the administration of alendronate in a tendon graft model in rabbits. They were divided into a control group C and an experimental group A. Knee joints specimens were harvested for examination of their bone trabecular structure by micro-CT. Applying the computational homology software to the reconstructed 3D image data, we extract the morphological feature of a steric bone structure as the Betti numbers set (β0, β1, β2). The zeroth Betti number β0 indicates the number of the connected components corresponding to isolated bone fragments. The first and second Betti numbers, β1 and β2, indicate the numbers of open pores and closed pores of bone trabeculae, corresponding to a 2D empty space enclosed by a 1D curve and a 3D empty space enclosed by a 2D surface, respectively.
Results
We define the Betti number ratios β1/β0 and β2/β0 to better distinguish the two groups A and C in the scatter plots. Testing the discriminant function line for 29 data points of A (22 data points of C), the 17 points (resp. 18 points) are correctly classified into group A (resp. C). The accuracy rate is 35/51. The classification results in terms of the Betti number ratios are consistent with the histomorphometric measurements observed by medical doctors. Conclusions: This study is the first application of computational homology to bone morphometry in three dimensions. We show the mathematical basis of the Betti numbers index which are useful in a statistical description of the topological features of sponge-like structures. The potential benefits associated with our method include both improved quantification and reproducibility for the stereology.}
}
@article{KEELEY2020194,
title = {Modeling statistical dependencies in multi-region spike train data},
journal = {Current Opinion in Neurobiology},
volume = {65},
pages = {194-202},
year = {2020},
note = {Whole-brain interactions between neural circuits},
issn = {0959-4388},
doi = {https://doi.org/10.1016/j.conb.2020.11.005},
url = {https://www.sciencedirect.com/science/article/pii/S0959438820301720},
author = {Stephen L Keeley and David M Zoltowski and Mikio C Aoi and Jonathan W Pillow},
abstract = {Neural computations underlying cognition and behavior rely on the coordination of neural activity across multiple brain areas. Understanding how brain areas interact to process information or generate behavior is thus a central question in neuroscience. Here we provide an overview of statistical approaches for characterizing statistical dependencies in multi-region spike train recordings. We focus on two classes of models in particular: regression-based models and shared latent variable models. Regression-based models describe interactions in terms of a directed transformation of information from one region to another. Shared latent variable models, on the other hand, seek to describe interactions in terms of sources that capture common fluctuations in spiking activity across regions. We discuss the advantages and limitations of each of these approaches and future directions for the field. We intend this review to be an introduction to the statistical methods in multi-region models for computational neuroscientists and experimentalists alike.}
}
@article{20214105,
title = {Introducing new group leaders: Lorenzo Calviello},
journal = {Molecular Cell},
volume = {81},
number = {20},
pages = {4105-4108},
year = {2021},
issn = {1097-2765},
doi = {https://doi.org/10.1016/j.molcel.2021.08.043},
url = {https://www.sciencedirect.com/science/article/pii/S1097276521007334},
abstract = {Lorenzo Calviello tells us about his return to Milan, Italy, to set up his lab, which aims to untangle the complex life of mRNA using a mix of computational and experimental approaches; the kind of environment he hopes to promote as part of a wider scientific culture; and the importance of heavy metal and affordable education.}
}
@article{GIGERENZER200193,
title = {Content-blind norms, no norms, or good norms? A reply to Vranas},
journal = {Cognition},
volume = {81},
number = {1},
pages = {93-103},
year = {2001},
issn = {0010-0277},
doi = {https://doi.org/10.1016/S0010-0277(00)00135-9},
url = {https://www.sciencedirect.com/science/article/pii/S0010027700001359},
author = {Gerd Gigerenzer},
keywords = {Reasoning, Probability, Norms, Rationality, Fallacy, Error},
abstract = {In the psychology of thinking, little thought is given to what constitutes good thinking. Instead, normative solutions to problems have been accepted at face value, thereby determining what counts as a reasoning fallacy. I applaud Vranas (Cognition 76 (2000) 179) for thinking seriously about norms. I do, however, disagree with his attempt to provide post hoc justifications for supposed reasoning fallacies in terms of ‘content-neutral’ norms. Norms need to be constructed for a specific situation, not imposed upon it in a content-blind way. The reason is that content-blind norms disregard relevant structural properties of the given situation, including polysemy, reference classes, and sampling. I also show that content-blind norms can, unwittingly, lead to double standards: the norm in one problem is the fallacy in the next. The alternative to content-blind norms is not no norms, but rather carefully designed norms.}
}
@article{AVEN2002195,
title = {Implementing the Bayesian paradigm in risk analysis},
journal = {Reliability Engineering & System Safety},
volume = {78},
number = {2},
pages = {195-201},
year = {2002},
issn = {0951-8320},
doi = {https://doi.org/10.1016/S0951-8320(02)00161-8},
url = {https://www.sciencedirect.com/science/article/pii/S0951832002001618},
author = {T. Aven and J.T. Kvaløy},
keywords = {Bayesian, Risk analysis, True probabilities},
abstract = {The Bayesian paradigm comprises a unified and consistent framework for analyzing and expressing risk. Yet, we see rather few examples of applications where the full Bayesian setting has been adopted with specifications of priors of unknown parameters. In this paper, we discuss some of the practical challenges of implementing Bayesian thinking and methods in risk analysis, emphasizing the introduction of probability models and parameters and associated uncertainty assessments. We conclude that there is a need for a pragmatic view in order to ‘successfully’ apply the Bayesian approach, such that we can do the assignments of some of the probabilities without adopting the somewhat sophisticated procedure of specifying prior distributions of parameters. A simple risk analysis example is presented to illustrate ideas.}
}
@article{DRESHER1990137,
title = {A computational learning model for metrical phonology},
journal = {Cognition},
volume = {34},
number = {2},
pages = {137-195},
year = {1990},
issn = {0010-0277},
doi = {https://doi.org/10.1016/0010-0277(90)90042-I},
url = {https://www.sciencedirect.com/science/article/pii/001002779090042I},
author = {B.Elan Dresher and Jonathan D. Kaye},
abstract = {One of the major challenges to linguistic theory is the solution of what has been termed the “projection problem”. Simply put, linguistics must account for the fact that starting from a data base that is both unsystematic and relatively small, a human child is capable of constructing a grammar that mirrors, for all intents and purposes, the adult system. In this article we shall address ourselves to the question of the learnability of a postulated subsystem of phonological structure: the stress system. We shall describe a computer program which is designed to acquire this subpart of linguistic structure. Our approach follows the “principles and parameters” model of Chomsky (1981a, b). This model is particularly interesting from both a computational point of view and with respect to the development of learning theories. We encode the relevant aspects of universal grammar (UG) - those aspects of linguistic structure that are presumed innate}
}
@article{KIRGIL2022101668,
title = {“Do your part: Stay apart”: Collective intentionality and collective (in)action in US governor's COVID-19 press conferences},
journal = {Poetics},
volume = {93},
pages = {101668},
year = {2022},
issn = {0304-422X},
doi = {https://doi.org/10.1016/j.poetic.2022.101668},
url = {https://www.sciencedirect.com/science/article/pii/S0304422X22000304},
author = {Z.M. Kirgil and A. Voyer},
keywords = {Collective intentionality, Leadership, Democrat, Republican, COVID-19},
abstract = {This mixed-methods study examines how political leaders mobilize collective intentionality during the COVID-19 pandemic in nine US States, and how collective intentionality differs across republican and democratic administrations. The results of our computational and qualitative analyses show that i) political leaders establish collective intentionality by emphasizing unity, vulnerability, action, and community boundaries; ii) political leaders’ call to collective action clashes with the inaction required by health guidelines; iii) social inequalities received little attention across all states compared to other themes; and iv) collective intentionality in democratic administrations is linked to individuals’ agency and actions, suggesting a bottom-up approach. Conversely, in republican administrations individuals’ contributions are downplayed compared to work and state-level action, indicating a top-down approach. This study demonstrates the theoretical and empirical value of collective intentionality in sociological research, and contributes to a better understanding of leadership and prosociality in times of crisis.}
}
@article{BRIGHT2020187,
title = {Applying computer algebra systems with SAT solvers to the Williamson conjecture},
journal = {Journal of Symbolic Computation},
volume = {100},
pages = {187-209},
year = {2020},
note = {Symbolic Computation and Satisfiability Checking},
issn = {0747-7171},
doi = {https://doi.org/10.1016/j.jsc.2019.07.024},
url = {https://www.sciencedirect.com/science/article/pii/S0747717119300902},
author = {Curtis Bright and Ilias Kotsireas and Vijay Ganesh},
keywords = {Williamson matrices, Boolean satisfiability, SAT solvers, Exhaustive search, Autocorrelation},
abstract = {We employ tools from the fields of symbolic computation and satisfiability checking—namely, computer algebra systems and SAT solvers—to study the Williamson conjecture from combinatorial design theory and increase the bounds to which Williamson matrices have been enumerated. In particular, we completely enumerate all Williamson matrices of even order up to and including 70 which gives us deeper insight into the behaviour and distribution of Williamson matrices. We find that, in contrast to the case when the order is odd, Williamson matrices of even order are quite plentiful and exist in every even order up to and including 70. As a consequence of this and a new construction for 8-Williamson matrices we construct 8-Williamson matrices in all odd orders up to and including 35. We additionally enumerate all Williamson matrices whose orders are divisible by 3 and less than 70, finding one previously unknown set of Williamson matrices of order 63.}
}
@article{FENG2024117540,
title = {Large language models for biomolecular analysis: From methods to applications},
journal = {TrAC Trends in Analytical Chemistry},
volume = {171},
pages = {117540},
year = {2024},
issn = {0165-9936},
doi = {https://doi.org/10.1016/j.trac.2024.117540},
url = {https://www.sciencedirect.com/science/article/pii/S0165993624000220},
author = {Ruijun Feng and Chi Zhang and Yang Zhang},
keywords = {Large language model, Biomolecular analysis, Fine-tuning, Prompt engineering, Parameter-efficient fine-tuning, In-context learning, Protein structure analysis, Protein sequence generation, Gene sequence analysis, Molecular representation learning},
abstract = {Large language models (LLMs) are proving to be very useful in many fields, especially chemistry and biology, because of their amazing capabilities. Biomolecular data is often represented sequentially, much like textual data used to train LLMs. However, developing LLMs from scratch requires a substantial amount of data and computational resources, which may not be feasible for most researchers. A more workable solution to this problem is to change the inputs or parameters so that the previously trained general LLMs can pick up the specific knowledge needed for biomolecular analysis. These adaption strategies lower the amount of data and hardware needed, providing a more affordable option. This review provides the introduction of two popular LLM adaptation techniques: fine-tuning and prompt engineering, along with their uses in the analysis of molecules, proteins, and genes. A thorough overview of current common datasets and pre-trained models is also provided. This review outlines the possible advantages and difficulties of LLMs for biomolecular analysis, opening the door for chemists and biologists to effectively utilize LLMs in their future studies.}
}
@article{SIMS202226,
title = {Externalized memory in slime mould and the extended (non-neuronal) mind},
journal = {Cognitive Systems Research},
volume = {73},
pages = {26-35},
year = {2022},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2021.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S1389041721000954},
author = {Matthew Sims and Julian Kiverstein},
keywords = {Extended mind, Slime mould, Navigational memory, Basal cognition, Stigmergy, Sensorimotor coordination},
abstract = {The hypothesis of extended cognition (HEC) claims that the cognitive processes that materially realise thinking are sometimes partially constituted by entities that are located external to an agent’s body in its local environment. We show how proponents of HEC need not claim that an agent must have a central nervous system, or physically instantiate processes organised in such a way as to play a causal role equivalent to that of the brain if that agent is to be capable of cognition. Focusing on the case of spatial memory, we make our argument by taking a close look at the striking example of Physarum Polycephalum plasmodium (i.e., slime mould) which uses self-produced non-living extracellular slime trails to navigate its environment. We will argue that the use of externalized spatial memory by basal organisms like Physarum is an example of extended cognition. Moreover, it is a possible evolutionary precursor to the use of internal spatial memory and recall in animals thus demonstrating how extended cognition may have emerged early in evolutionary history.}
}
@article{GONZALEZRODRIGUEZ201827,
title = {Self-Organized Linguistic Systems: From traditional AI to bottom-up generative processes},
journal = {Futures},
volume = {103},
pages = {27-34},
year = {2018},
note = {Futures of Society: The Interactions Revolution},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2018.03.003},
url = {https://www.sciencedirect.com/science/article/pii/S0016328717302161},
author = {Diego Gonzalez-Rodriguez and Jose Rodolfo Hernandez-Carrion},
keywords = {Artificial intelligence, Self-organization, Emergence, Constructed languages, Conlangs, Agent-based modelling},
abstract = {This work seeks to explore the potential of bottom-up generative processes in the context of conlang production, aiming to describe the basis of a new field of research: Self-Organized Linguistic Systems or SOLS, specified under the perspective of both self-organized systems and constructed languages. SOLS approach provides a framework for the creation of self-generated artificial languages and may serve as a starting point for the development of context-dependent or domain-specific languages. It acknowledges that the development of conlangs can happen in artificial societies of simple agents, as the output of social interactions in computational simulations under the agent-based modelling paradigm. In the proposed initial SOLS model, automatic generation of lexicon takes place in the context of a digital environment with objects, actions and agents with embodied cognition through peer-to-peer interactions. Specifically, this paper exposes how SOLS can be developed with bi-dimensional games and simulations. An initial work has been done with the xmunch-atomspace and the SciArt simulator, which constitute the first implementations of both our knowledge representation toolbox and our bi-dimensional simulator of P2P Social Dynamics. Non-interactive agent-based SOLS can allow artificial agents to independently evolve emergent languages as part of their self-organizing or adaptation processes.}
}
@article{LOMBAERS1987387,
title = {Computational techniques in operations research: A.M. Andrew Computer Language & Programming Series, Abacus, Tunbridge Wells, 1985, viii + 201 pages, £14.95},
journal = {European Journal of Operational Research},
volume = {31},
number = {3},
pages = {387-388},
year = {1987},
note = {Methodology for Public Decision-Making Interactive Decision Support Systems Queue and Game Theory},
issn = {0377-2217},
doi = {https://doi.org/10.1016/0377-2217(87)90050-6},
url = {https://www.sciencedirect.com/science/article/pii/0377221787900506},
author = {H.J.M. Lombaers}
}
@incollection{YE201842,
title = {1.05 - Open Data and Open Source GIS},
editor = {Bo Huang},
booktitle = {Comprehensive Geographic Information Systems},
publisher = {Elsevier},
address = {Oxford},
pages = {42-49},
year = {2018},
isbn = {978-0-12-804793-4},
doi = {https://doi.org/10.1016/B978-0-12-409548-9.09592-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780124095489095920},
author = {Xinyue Ye},
keywords = {Open data, Open GIS, Open source},
abstract = {The multiple dimensions and scales of emerging open data pose numerous challenges for the application and evaluation of public policies. At the same time, domain researchers have been relatively slow to adopt and implement new spatiotemporally explicit data analysis methods due to the availability of suitable data and the lack of extensible software packages, which becomes a major impediment to the promotion of spatiotemporal thinking and collaboration. In this regard, more attention to open data and open source geographic information system (GIS) is necessary. Free access to the data and source code allows the broader GIS and domain science communities to incorporate additional advances in theoretical perspectives and analytical methods, thus facilitating interdisciplinary collaboration of spatial science and education. A case study of comparative LISA time path is illustrated in the open source GIS context. Additionally, open source implementation of new methods can expedite comparative studies of geographical dynamics.}
}
@article{MANDEVILLE1997397,
title = {The effect of teacher certification and task level on mathematics achievement},
journal = {Teaching and Teacher Education},
volume = {13},
number = {4},
pages = {397-407},
year = {1997},
issn = {0742-051X},
doi = {https://doi.org/10.1016/S0742-051X(96)00031-5},
url = {https://www.sciencedirect.com/science/article/pii/S0742051X96000315},
author = {Garrett K Mandeville and Qiduan Liu},
abstract = {In this study it was hypothesized that the degree of content area preparation of seventh grade mathematics teachers would differentially effect student performance as a function of the level of the mathematics tasks used to assess that performance. More specifically, students of high MATHPREP teachers were hypothesized to outperform those of low MATHPREP teachers on the higher level tasks. The sample consisted of over 9000 seventh grade students from 33 matched pairs of schools whose teachers differed on level of mathematics preparation. Aggregate measures of achievement for the students at each school on test items representing three ordered levels of thinking were obtained. The primary hypothesis was addressed using a single df interaction contrast and produced a statistically and practically significant result in the hypothesized direction. Implications for teacher preparation and the hiring practices of school administrators are considered.}
}
@article{LEE2019325,
title = {Improving process safety: What roles for Digitalization and Industry 4.0?},
journal = {Process Safety and Environmental Protection},
volume = {132},
pages = {325-339},
year = {2019},
issn = {0957-5820},
doi = {https://doi.org/10.1016/j.psep.2019.10.021},
url = {https://www.sciencedirect.com/science/article/pii/S0957582019317057},
author = {John Lee and Ian Cameron and Maureen Hassall},
keywords = {Process safety, Digital twin, Digitalization, Industry 4.0, Models, Life cycle, ISO15926},
abstract = {Process safety and risk management remain a significant challenge for the process and manufacturing industries. Digital systems have been applied over many decades to assist in process safety management throughout the lifecycle of a process plant. There has been much hype in recent years regarding Industry 4.0, digitalization and digital twins regarding the transformative potential that exists within these technologies to improve operational performance and reduce process safety accidents. In this article, a fundamental systems thinking approach is applied to the implementation of the digital twin within the process industries. The importance of having a standardized language and ontology, such as ISO15926, enables the use of reasoning engines and the ability to interconnect models and systems across the process and product lifecycle. We discuss use-cases and forms of the digital twin to improve safety within the process industries. A specific focus shows how an operator training simulator and its embedded dynamic models are applied within this environment. The article concludes with a summary of process safety related opportunities and threats associated with the application of digitalized dynamic models in industry.}
}
@article{LEITEFILHO2024100381,
title = {Evaluating chatbot user experience (UX) through electroencephalography measures: A systematic literature review},
journal = {Computers in Human Behavior Reports},
volume = {13},
pages = {100381},
year = {2024},
issn = {2451-9588},
doi = {https://doi.org/10.1016/j.chbr.2024.100381},
url = {https://www.sciencedirect.com/science/article/pii/S2451958824000149},
author = {Jaime Ranulfo {Leite Filho} and Thiago Adriano Coleti and Marcelo Morandini},
keywords = {User experience, Chatbot, Virtual assistant, Conversational agent, Natural language interface, Brain-computer interface, Electroencephalography, EEG, Evaluating model methodology approach strategy},
abstract = {Brain activity is a biological signal with unique characteristics that can determine important patterns for recording and processing. The electroencephalogram (EEG) is the most used signal because it measures brain electrical activity, offering greater resolution and data accuracy. When associated to brain activities in user-computer interactions, it can provide information that allows analyzing adequacy and user satisfaction. Thus, the objective of this paper is to identify works that specify which information on electroencephalography assessment may be used to compose an analysis of its interactions with conversational systems. The delimited that the research problems are: (1) What information about user experience by electroencephalography can be used to compose an analysis of their interactions with conversational systems? (2) What techniques are used to present user experience information by EEG to individuals? Based on the Systematic Review method, seven studies were identified that examined commercial EEG devices for UX assessment between 2011 and 2022. The current study found that multiple emotional stimuli were used and reported. The most popular technique among researchers is event induced emotional stimulation, in which participants passively perceive emotional stimuli such as images, music and videos to evoke certain emotions.}
}
@incollection{DUCHATEAU2023147,
title = {Chapter 7 - Machine learning and biophysical models: how to benefit each other?},
editor = {Francisco Chinesta and Elías Cueto and Yohan Payan and Jacques Ohayon},
booktitle = {Reduced Order Models for the Biomechanics of Living Organs},
publisher = {Academic Press},
pages = {147-164},
year = {2023},
series = {Biomechanics of Living Organs},
issn = {25890999},
doi = {https://doi.org/10.1016/B978-0-32-389967-3.00009-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780323899673000093},
author = {Nicolas Duchateau and Oscar Camara},
keywords = {Machine learning, Simulations, Biophysical models, Synthetic data, Reduced-order models},
abstract = {Biophysical models and machine learning may be perceived as rather different entities, or on the contrary as very related forms of modelling. In this chapter, we precisely develop the latter idea to provide a didactic and up-to-date overview of some major research tracks where these two fields can collaborate and benefit each other. We specifically articulate contents around two complementary points-of-view on the potential benefits of one field to the other. For biophysical modelling, we focused on accelerating computations, estimating unobservable parameters and examining complex outputs; for machine learning, we laid stress on adding physiologically relevant knowledge and generating synthetic data for training and validation. Throughout this review, we detail specific questions of relevance with examples mostly in the context of computational cardiology, which is our field of interest, and encourage further interaction between these two areas of active research.}
}
@article{KANGASHARJU2022100048,
title = {Lower secondary students’ poetry writing with the AI-based Poetry Machine},
journal = {Computers and Education: Artificial Intelligence},
volume = {3},
pages = {100048},
year = {2022},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2022.100048},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X22000030},
author = {Arja Kangasharju and Liisa Ilomäki and Minna Lakkala and Auli Toom},
keywords = {AI-based learning, Lower secondary education, Technology in education, Poetry writing, AI-supported writing},
abstract = {Despite poetry’s important role in improving linguistic skills and creative thinking, students often find poetry writing to be difficult and boring. This study is an investigation of how the digital Poetry Machine influences students’ poetry writing by applying AI techniques. It uses qualitative and quantitative analysis of the log data of poems that the seventh graders wrote with the Poetry Machine. The results show that the draft poems functioned as affordances, which the students followed as models. The drafts encouraged students to experiment with several different poetic features. The data suggest an association between the number of edited versions and the quality of the final poem. The results suggest that a co-creative AI-based tool inspires and supports those students who engage in the writing process, and the poems are developed from the first versions. More studies regarding the role of AI based digital tools in developing students’ writing competencies would be worthwhile.}
}
@article{SPRUGNOLI201799,
title = {Neural correlates of Eureka moment},
journal = {Intelligence},
volume = {62},
pages = {99-118},
year = {2017},
issn = {0160-2896},
doi = {https://doi.org/10.1016/j.intell.2017.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S0160289616302756},
author = {Giulia Sprugnoli and Simone Rossi and Alexandra Emmendorfer and Alessandro Rossi and Sook-Lei Liew and Elisa Tatti and Giorgio {di Lorenzo} and Alvaro Pascual-Leone and Emiliano Santarnecchi},
keywords = {Insight, , , Cognition, fMRI, EEG, ERPs, Non-invasive brain stimulation, Neuroenhancement, NIBS, Creativity},
abstract = {Insight processes that peak in “unpredictable moments of exceptional thinking” are often referred to as Aha! or Eureka moments. During insight, connections between previously unrelated concepts are made and new patterns arise at the perceptual level while new solutions to apparently insolvable problems suddenly emerge to consciousness. Given its unpredictable nature, the definition, and behavioral and neurophysiological measurement of insight problem solving represent a major challenge in contemporary cognitive neuroscience. Numerous attempts have been made, yet results show limited consistency across experimental approaches. Here we provide a comprehensive overview of available neuroscience of insight, including: i) a discussion about the theoretical definition of insight and an overview of the most widely accepted theoretical models, including those debating its relationship with creativity and intelligence; ii) an overview of available tasks used to investigate insight; iii) an ad-hoc quantitative meta-analysis of functional magnetic resonance imaging studies investigating the Eureka moment, using activation likelihood estimation maps; iv) a review of electroencephalographic evidence in the time and frequency domains, as well as v) an overview of the application of non-invasive brain stimulation techniques to causally assess the neurobiological basis of insight as well as enhance insight-related cognition.}
}
@article{TULVER2025106081,
title = {The road to Aha: A recipe for mental breakthroughs},
journal = {Cognition},
volume = {257},
pages = {106081},
year = {2025},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2025.106081},
url = {https://www.sciencedirect.com/science/article/pii/S0010027725000216},
author = {Kadi Tulver and Karl Kristjan Kaup and Jaan Aru},
keywords = {Insight, Transformation, Problem-solving, Predictive processing, Psychotherapy, Psychedelics, Meditation, Attractor dynamics},
abstract = {We present a novel framework for understanding the diverse spectrum of mental breakthrough events, ranging from problem-solving insights to profound personal transformations. We propose that these events, while varied in expression and impact, share common underlying mechanisms of representational change. We also hypothesise that the differences in phenomenological intensity can be conceptualized along a continuum. Central to our model are three core components – tension, altered salience, and enhanced flexibility – which we identify as essential prerequisites for significant cognitive restructuring. These components interact within an iterative cycle, influencing both the emergence and nature of insight experiences. Drawing on examples from different fields, we explore how a conflict between existing models can trigger this cycle, wherein mechanisms of attention allocation and relaxation of constraints work in tandem to facilitate the emergence of insights. Furthermore, we propose that the intensity of the “aha-moment” and the breadth of its impact are contingent on how central the conflict is within one's conceptual landscape and the extent to which existing mental models are challenged. Thus, the model accounts for both the subtle, momentary insights in problem-solving and the transformative realizations that reshape core beliefs and self-perception. By synthesising insights from various domains, including psychotherapy, contemplative science, and psychedelic research, we present a theoretical account with broad scope, aiming to shed light on the complex processes that can lead to a wide array of mental breakthroughs, thereby contributing to the understanding of insight phenomena across disciplines.}
}
@incollection{JANI202491,
title = {13.05 - Recent advances in simulation in fiber-reinforced polymer composites: Mechanical properties and applications},
editor = {Saleem Hashmi},
booktitle = {Comprehensive Materials Processing (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {91-109},
year = {2024},
isbn = {978-0-323-96021-2},
doi = {https://doi.org/10.1016/B978-0-323-96020-5.00182-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780323960205001825},
author = {Hasan Rafsan Jani and Md Zillur Rahman},
keywords = {Natural fiber, Composites, FRP composites, Hybrid composites, Mechanical properties, Finite element analysis, Simulation, Industrial applications},
abstract = {This study provides an overview of the recent advances in simulation techniques applied to fiber-reinforced polymer (FRP) composites, focusing on analyzing their mechanical properties and applications. FRP composites have gained significant attention in various industries due to their excellent mechanical properties, lightweight nature, and corrosion resistance. Simulating the behavior of FRP composites allows researchers and engineers to optimize their design, understand their mechanical performance, and predict their response under different loading conditions. This study explores different theoretical models for predicting the mechanical properties of composites. This study also acknowledges the challenges of characterizing these heterogeneous materials, making computational techniques such as finite element analysis indispensable for accurate predictions. In addition, simulated mechanical properties of composites with varying fibers and matrices are explored and compared with experimental results. Furthermore, the applications of simulation in different industries for characterizing FRP composites are discussed.}
}
@article{SAMSONOVICH2012100,
title = {On a roadmap for the BICA Challenge},
journal = {Biologically Inspired Cognitive Architectures},
volume = {1},
pages = {100-107},
year = {2012},
issn = {2212-683X},
doi = {https://doi.org/10.1016/j.bica.2012.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S2212683X12000126},
author = {Alexei V. Samsonovich},
keywords = {Human-level AI, Cognitive architectures, Turing test, Newell list, Critical mass},
abstract = {The BICA Challenge is the challenge to create a general-purpose, real-life computational equivalent of the human mind using an approach based on biologically inspired cognitive architectures (BICA). To solve it, we need to understand at a computational level how natural intelligent systems develop their cognitive, metacognitive and learning functions. The solution is expected to lead us to a breakthrough to intelligent agents integrated into the human society as its members. This outcome has the potential to solve many problems of the modern world. The article starts from the roadmap proposed by Dr. James Albus for a national program unifying artificial intelligence, neuroscience and cognitive science. The BICA Challenge is introduced in this context as a waypoint on the expanded roadmap. The gap between the state of the art and challenge demands is analyzed. Specific problems and barriers are identified, an approach to overcoming them is proposed, and an ultimate practical criterion for success is formulated. It is estimated that the BICA Challenge can be solved within a decade.}
}
@incollection{HERNANDEZLEMUS2017251,
title = {Chapter 14 - Handling Big Data in Precision Medicine},
editor = {Mukesh Verma and Debmalya Barh},
booktitle = {Progress and Challenges in Precision Medicine},
publisher = {Academic Press},
pages = {251-268},
year = {2017},
isbn = {978-0-12-809411-2},
doi = {https://doi.org/10.1016/B978-0-12-809411-2.00014-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128094112000143},
author = {E. Hernández-Lemus and J. Espinal-Enríquez and R. García-Herrera},
keywords = {Big data, Cloud computing, Data confidentiality, High-throughput data, Individualized therapy, Meta-data, Precision medicine},
abstract = {Precision medicine looks for the integration of vast information data sets on the molecular and environmental origins of disease for the development of individualized, context-dependent diagnostics and therapies. To build predictive models of complex disease compliant with individual variability on genetic and socially determined conditions, it is necessary to have computationally efficient methods to handle, visualize, and integrate large data sets of different origins, in a multitude of formats and subject to different levels of confidentiality into a single framework. This involves the ability to comply with the big data paradigm under demanding conditions of performance and subject to time, computational power, and bioethical constraints. This is still an open problem; however, we can devise some ways in which big data analytics may join forces with bioinformatics, medical informatics, and computational systems biology in a fast and effective way, motivated with the success of approaches such as the one given by translational bioinformatics.}
}
@incollection{VODOVOTZ201557,
title = {Chapter 3.1 - Towards Translational Systems Biology of Inflammation},
editor = {Yoram Vodovotz and Gary An},
booktitle = {Translational Systems Biology},
publisher = {Academic Press},
address = {Boston},
pages = {57-61},
year = {2015},
isbn = {978-0-12-397884-4},
doi = {https://doi.org/10.1016/B978-0-12-397884-4.00008-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780123978844000082},
author = {Yoram Vodovotz and Gary An},
keywords = {Translational medicine, translational research, systems biology, computational biology, bioinformatics, molecular biology, genomics, proteomics, metabolomics, clinical trials},
abstract = {Having introduced the historical, methodological, procedural, and societal antecedents that have contributed to the Translational Dilemma, in this chapter we propose our strategy to overcoming the challenges associated with the Dilemma, a research program we call Translational Systems Biology. This investigative strategy is predicated on the use of dynamic computational modeling and associated computational methods of data analysis and aggregation to accelerate the Scientific Cycle with an explicit target of generating clinically actionable knowledge. Translational Systems Biology is firmly grounded in the fundamental scientific principles discussed in earlier chapters, with its computational component specifically designed to overcome the numerous factors previously identified as contributing to the Translational Dilemma. These factors include the challenge of integrating and synthesizing mechanistic knowledge in systems with known nonlinear dynamics, utilizing and analyzing high-throughput data in a manner that facilitates the construction and use of dynamic computational models, the use of computational models as means of dynamic knowledge representation to test and falsify mechanistic hypotheses, and the use of computational modeling to bridge experimental biology to clinical application through the execution of in silico clinical trials.}
}
@article{DAHMANI2021128847,
title = {Smart circular product design strategies towards eco-effective production systems: A lean eco-design industry 4.0 framework},
journal = {Journal of Cleaner Production},
volume = {320},
pages = {128847},
year = {2021},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2021.128847},
url = {https://www.sciencedirect.com/science/article/pii/S0959652621030432},
author = {Noureddine Dahmani and Khalid Benhida and Amine Belhadi and Sachin Kamble and Said Elfezazi and Sunil Kumar Jauhar},
keywords = {Lean design, Eco-design, Industry 4.0, Circular economy, Sustainable products, Circular business models, Smart circular product design},
abstract = {For industrial enterprises, the transformation to circular business models can be curbed both by operational tools and the lack of relevant data. Lean thinking has offered great flexibility in production processes and systems by challenging mass production practices, resulting in more “Lean” products with less waste. Lean design and Eco-design, associated with Industry 4.0 technologies, can be an efficient structured and methodological approach in developing products based on the circular economy strategies. Indeed, decisions made during the product design stage can significantly impact the sustainability of products throughout their life cycle. Hence, lean design combined with eco-design and Industry 4.0 represents an innovative model to include sustainability throughout the product life cycle. This paper explores the relationship between lean eco-design and I4.0 strategies for designing eco-efficient products based on a literature review. The proposed framework is based on the synergic use of Lean design, Eco-design, and Industry 4.0. It offers the right formula to deliver better and cleaner products using appropriate processes to support manufacturers in designing products, fulfilling customers' needs and expectations. It provides scholars, designers, and managers with valuable insights into deploying strategies to design sustainable products.}
}
@article{GULOTTA2022106698,
title = {Life Cycle Assessment and Life Cycle Costing of unitized regenerative fuel cell: A systematic review},
journal = {Environmental Impact Assessment Review},
volume = {92},
pages = {106698},
year = {2022},
issn = {0195-9255},
doi = {https://doi.org/10.1016/j.eiar.2021.106698},
url = {https://www.sciencedirect.com/science/article/pii/S0195925521001487},
author = {Teresa Maria Gulotta and Roberta Salomone and Francesco Lanuzza and Giuseppe Saija and Giovanni Mondello and Giuseppe Ioppolo},
keywords = {Unitized regenerative proton exchange membrane fuel cells, Environmental impacts, Economic impacts, Hydrogen technologies, PEM devices},
abstract = {Unitized Regenerative Fuel Cell (URFC) is considered a promising green hydrogen technology for producing clean energy, but further research is needed to make it attractive for a wide range of sectors and applications. In particular, the environmental and economic implications related to the life cycle of this electrochemical device play a fundamental role in determining its attractiveness and potential for improvement, and Life Cycle Thinking (LCT) assessment methods are considered to be the most effective means to improve knowledge about these implications. In this context, the present article provides a systematic and bibliometric literature review analysis of Life Cycle Assessment (LCA) and Life Cycle Costing (LCC) studies applied to URFCs using proton exchange membrane (PEM) devices. The aim is to evaluate the state-of-the-art of implementations of LCT methods to this electrochemical device in order to highlight good practices and critical issues, referred to both technical and methodological data. A reference sample of 44 scientific articles is extracted from the Web of Science (WoS), Scopus, and ScienceDirect databases and analysed using two computational tools: VOS viewer and Microsoft Excel. This group of publications helped establish the development over the last few decades of some key themes: LCC and LCA studies applied on PEM and URFC, also extending the search to its main components (such as fuel cell and electrolyser) and its original shape (i.e., regenerative fuel cell). The results of the analysis are presented quantitatively and qualitatively. Regarding the technical issues, there is significant variability in environmental and economic impacts, given by the selected system boundaries, the final users, and the fuel used by the systems. Regarding the methodological issues, no consensus emerges on how to model the LCT studies according to functional units, system boundaries, type of data selected, or model environmental externalities. The analysis also highlights the strong need for a higher level of transparency and harmonization of LCAs and LCCs applied on PEM technologies in order to improve the comparability of the results of these assessments.}
}
@article{HEMMATIAN202469,
title = {The utilitarian brain: Moving beyond the Free Energy Principle},
journal = {Cortex},
volume = {170},
pages = {69-79},
year = {2024},
issn = {0010-9452},
doi = {https://doi.org/10.1016/j.cortex.2023.11.013},
url = {https://www.sciencedirect.com/science/article/pii/S0010945223003076},
author = {Babak Hemmatian and Lav R. Varshney and Frederick Pi and Aron K. Barbey},
keywords = {Free Energy Principle, Subjective utility, Extended cognition, Decision-making, Cognitive neuroscience, Bayesian Brain Hypothesis},
abstract = {The Free Energy Principle (FEP) is a normative computational framework for iterative reduction of prediction error and uncertainty through perception–intervention cycles that has been presented as a potential unifying theory of all brain functions (Friston, 2006). Any theory hoping to unify the brain sciences must be able to explain the mechanisms of decision-making, an important cognitive faculty, without the addition of independent, irreducible notions. This challenge has been accepted by several proponents of the FEP (Friston, 2010; Gershman, 2019). We evaluate attempts to reduce decision-making to the FEP, using Lucas' (2005) meta-theory of the brain's contextual constraints as a guidepost. We find reductive variants of the FEP for decision-making unable to explain behavior in certain types of diagnostic, predictive, and multi-armed bandit tasks. We trace the shortcomings to the core theory's lack of an adequate notion of subjective preference or “utility”, a concept central to decision-making and grounded in the brain's biological reality. We argue that any attempts to fully reduce utility to the FEP would require unrealistic assumptions, making the principle an unlikely candidate for unifying brain science. We suggest that researchers instead attempt to identify contexts in which either informational or independent reward constraints predominate, delimiting the FEP's area of applicability. To encourage this type of research, we propose a two-factor formal framework that can subsume any FEP model and allows experimenters to compare the contributions of informational versus reward constraints to behavior.}
}
@article{LIANG201291,
title = {Psychological-Physical Force Model for Bicycle Dynamics},
journal = {Journal of Transportation Systems Engineering and Information Technology},
volume = {12},
number = {2},
pages = {91-97},
year = {2012},
issn = {1570-6672},
doi = {https://doi.org/10.1016/S1570-6672(11)60197-9},
url = {https://www.sciencedirect.com/science/article/pii/S1570667211601979},
author = {Xiao LIANG and Baohua MAO and Qi XU},
keywords = {urban traffic, bicycle, micro behavior, dynamics, psychological-physical force model, interaction},
abstract = {The core challenge in modeling bicyclist behavior dynamics is how to tackle the interaction between the lateral and the longitudinal movements. Further the bicycle transportation could be considered as multi-particle self-driven system. The combined dynamic model, psychological-physical force model (PPFM) and trajectories choice model (TCM), is proposed as a multi agent model to describe bicycle microscopic behavior dynamics. The PPFM is a continuous force model, which obeyed to the Newton's second law. By introducing the trajectories choice behavior in the tactical level, the TCM is modeled to describe the ability to individual autonomous thinking and to respond to changes ambient conditions for predefined behavior tank. Through designing computational experiments, the simulation data is collected to calibrate and validate the models. The simulation results show that the fundamental diagram obtained by simulation is dovetail into the empirical data. The PPFM is capable of describing the nonlinear interaction between individuals and the microscopic behavior of the proposed bicycle dynamic model with reasonable traffic.
摘要
自行车微观行为动力学建模的关键是如何描述自行车横向和纵向的运动关系。本文将自行车交通系统视作具有自主性的多粒子系统，提出了由心理生理力模型和轨迹选择模型构成的、描述自行车微观行为动力学特性的多主体模型。心理生理力模型为连续力模型，自行车/骑行者个体被视作是受心理力和生理力作用的、服从牛顿力学的基本粒子。在轨迹选择模型中，通过在行为模型层面引入个体运动的轨迹选择行为，预定义个体面对不同交通状况时的行为库，描述组成自行车群体中的个体独立思考和对周围环境变化做出反应的能力。通过设计计算机模拟实验并收集数据，对模型进行有效性验证。模拟结果表明：模拟得到的自行车交通流的密度-速度关系与实测数据具有良好的一致性，认为本文提出的自行车微观行为动力学模型具有交通上的合理性。}
}
@article{LI2025100894,
title = {A concise review of intelligent game agent},
journal = {Entertainment Computing},
volume = {52},
pages = {100894},
year = {2025},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2024.100894},
url = {https://www.sciencedirect.com/science/article/pii/S1875952124002623},
author = {Hui Li and Xinyi Pang and Bixia Sun and Kexin Liu},
keywords = {Intelligent agent, Artificial intelligence, Monte Carlo tree, Reinforcement learning, Large language models},
abstract = {Intelligent game agents are crafted using AI technologies to mimic player behavior and make decisions autonomously. Over the past decades, the scope of intelligent agents has broadened from chess to encompass content generation, player modeling, and result prediction, reflecting the field’s evolving and multifaceted nature. In this paper, we conduct a systematic review of recent literature on intelligent methods and applications of game agents, along with general game agent frameworks. Our findings suggest that creating general intelligent agents remains a significant challenge, yet it is worthwhile to explore methods that better integrate the strengths of different techniques to build more robust and adaptable intelligent game agents.}
}
@incollection{BURK2023457,
title = {Chapter 24 - Analytics architectures for the 21st century},
editor = {Gary D. Miner and Linda A. Miner and Scott Burk and Mitchell Goldstein and Robert Nisbet and Nephi Walton and Thomas Hill},
booktitle = {Practical Data Analytics for Innovation in Medicine (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
pages = {457-472},
year = {2023},
isbn = {978-0-323-95274-3},
doi = {https://doi.org/10.1016/B978-0-323-95274-3.00017-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780323952743000178},
author = {Scott Burk},
keywords = {Data literacy, center of excellence, culture, data architecture, APIS, microservices, streaming data, data stores, data virtualization, 5v’s of data, master data, reference data, metadata, data governance, data management, exploratory data analysis, data prep, feature engineering, model selection, model evaluation, model deployment, model monitoring, model Ops},
abstract = {Artificial Intelligence (AI) and analytics efforts are too often just extensions or isolated additions to medical practice and research. It could be an extension of research efforts, part of a medical degree curriculum or residency program to extend scholarly pursuits. It could be hospital systems wishing to gain operation efficiency and control. It could be physician groups-of-practice trying to mitigate risk or improve financial performance. While isolated efforts may provide some benefit, leaders understand that exceptional results require a paradigm shift in thinking and the infrastructure to support the new technologies and innovation. In this chapter, we present three pillars that lay the foundation for success in medical research and results in the 21st century. Successful participants will have designed and implemented the following architectures.}
}
@article{RAYBOURN2014471,
title = {A new paradigm for serious games: Transmedia learning for more effective training and education},
journal = {Journal of Computational Science},
volume = {5},
number = {3},
pages = {471-481},
year = {2014},
issn = {1877-7503},
doi = {https://doi.org/10.1016/j.jocs.2013.08.005},
url = {https://www.sciencedirect.com/science/article/pii/S1877750313001014},
author = {Elaine M. Raybourn},
keywords = {Transmedia learning, Serious games, Transmedia campaigns, Storytelling, Social media, Data mining, xAPI, MOOC},
abstract = {Serious games present a relatively new approach to training and education for international organizations such as NATO (North Atlantic Treaty Organization), non-governmental organizations (NGOs), the U.S. Department of Defense (DoD) and the U.S. Department of Homeland Security (DHS). Although serious games are often deployed as stand-alone solutions, they can also serve as entry points into a comprehensive training pipeline in which content is delivered via different media to rapidly scale immersive training and education for mass audiences. The present paper introduces a new paradigm for more effective and scalable training and education called transmedia learning. Transmedia learning leverages several new media trends including the peer communications of social media, the scalability of massively openonline course (MOOCs), and the design of transmedia storytelling used by entertainment, advertising, and commercial game industries to sustain audience engagement. Transmedia learning is defined as the scalable system of messages representing a narrative or core experience that unfolds from the use of multiple media, emotionally engaging learners by involving them personally in the story. In the present paper, we introduce the transmedia learning paradigm as offering more effective use of serious games for training and education. This approach is consistent with the goals of international organizations implementing approaches similar to those described by the Army Learning Model (ALM) to deliver training and education to Soldiers across multiple media. We discuss why the human brain is wired for transmedia learning and demonstrate how the Simulation Experience Design Method can be used to create transmedia learning story worlds for serious games. We describe how social media interactions and MOOCs may be used in transmedia learning, and how data mining social media and experience tracking can inform the development of computational learner models for transmedia learning campaigns. Examples of how the U.S. Army has utilized transmedia campaigns for strategic communication and game-based training are provided. Finally, we provide strategies the reader can use today to incorporate transmedia storytelling elements such as Internet, serious games, video, social media, graphic novels, machinima, blogs, and alternate reality gaming into a new paradigm for training and education: transmedia learning.}
}
@article{ACHARYA2025108652,
title = {EEGConvNeXt: A novel convolutional neural network model for automated detection of Alzheimer's Disease and Frontotemporal Dementia using EEG signals},
journal = {Computer Methods and Programs in Biomedicine},
volume = {262},
pages = {108652},
year = {2025},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2025.108652},
url = {https://www.sciencedirect.com/science/article/pii/S0169260725000690},
author = {Madhav Acharya and Ravinesh C Deo and Prabal Datta Barua and Aruna Devi and Xiaohui Tao},
keywords = {EEGConvNeXt, AD detection, Transformer-like CNN, EEG analysis, Signal processing},
abstract = {Background and objective
Deep learning models have gained widespread adoption in healthcare for accurate diagnosis through the analysis of brain signals. Neurodegenerative disorders like Alzheimer's Disease (AD) and Frontotemporal Dementia (FD) are increasingly prevalent due to age-related brain volume reduction. Despite advances, existing models often lack comprehensive multi-class classification capabilities and are computationally expensive. This study addresses these gaps by proposing EEGConvNeXt, a novel convolutional neural network (CNN) model for detecting AD and FD using electroencephalogram (EEG) signals with high accuracy.
Materials and method
In this research, we employ an open-access EEG signal public dataset containing three distinct classes: AD, FD, and control subjects. We then constructed a newly proposed EEGConvNeXt model comprised of a 2-dimensional CNN algorithm that firstly converts the EEG signals into power spectrogram-based images. Secondly, these images were used as input for the proposed EEGConvNeXt model for automated classification of AD, FD, and a control outcome. The proposed EEGConvNeXt model is therefore a lightweight model that contributes to a new image classification CNN structure based on the transformer model with four primary stages: a stem, a main model, downsampling, and an output stem.
Results
The EEGConvNeXt model achieved a classification accuracy of ∼95.70% for three-class detection (AD, FD, and control), validated using a hold-out strategy. Binary classification cases, such as AD versus FD and FD versus control, achieved accuracies exceeding 98%, demonstrating the model's robustness across scenarios.
Conclusions
The proposed EEGConvNeXt model demonstrates high classification performance with a lightweight architecture suitable for deployment in resource-constrained settings. While the study establishes a novel framework for AD and FD detection, limitations include reliance on a relatively small dataset and the need for further validation on diverse populations. Future research should focus on expanding datasets, optimizing architecture, and exploring additional neurological disorders to enhance the model's utility in clinical applications.}
}
@article{ALI2025103632,
title = {Proposing wavy-shaped legs for performance improvement of thermoelectric generators: energy, exergy, environmental, and mechanical analysis using a CFD-trained machine learning method},
journal = {Thermal Science and Engineering Progress},
volume = {62},
pages = {103632},
year = {2025},
issn = {2451-9049},
doi = {https://doi.org/10.1016/j.tsep.2025.103632},
url = {https://www.sciencedirect.com/science/article/pii/S2451904925004226},
author = {Elimam Abdallah Ali and Hazim Moria},
keywords = {Segmented thermoelectric generator, Wavy Leg Structure, Deep neural network, Energy, Exergy},
abstract = {The innovative wavy-shaped leg proposed in this research enables a longer thermoelectric leg without increasing the total height of the thermoelectric module (see the graphical abstract). A greater leg height enhances TEG’s performance while maintaining the same overall module height. The proposed idea is investigated from energy, exergy, environmental, and mechanical performance perspectives using machine learning trained with data coming from a 3D validated numerical simulations based on the Finite Element Method (FEM) for a segmented thermoelectric generator. The wavy-shaped leg is designed with varying wave amplitude (Aw), number of cycles (N), leg thickness (tl), and height ratio between segments (hr), all of which are evaluated based on their impact on voltage, generated power, efficiency, exergy-based efficiency, CO2 savings, and von Mises stress. The FEM simulations are then used to develop a highly accurate predictive model employing Deep Neural Networks (DNN), enabling rapid and efficient optimization and extrapolation across parameter values. Comprehensive results are identified and reported in this study. According to the results, the wavy structure generates more power while the higher number of cycles the higher generated voltage. Outcomes also confirm that larger Aw, N, hr, and qin values yield considerable improvements in terms of thermoelectric performance. For instance, atqin = 35,000  W/m2 and hr = 0.8, output power reaches 0.0178 W, with respective CO2 savings of 0.03045 kg. Slim legs (tl = 0.1) in combination with larger Aw values maximize output and efficiency but drive von Mises stress over 400 MPa. The DNN model accurately forecasts such trends, with high agreement with FEM (e.g., for Aw = 0.1  mm and tl = 0.1  mm) DNN forecasts CO2 savings of 0.01893 kg, in contrast with 0.01888 kg for FEM). It also yields continuous and smooth extrapolation for intermediate values, such as hr = 0.24032 (voltage = 0.076676 V) and tl = 0.1201 mm (stress = 270.02 MPa). Integration between FEM simulations and prediction with DNN brings computational accuracy and efficiency together, allowing for rapid and efficient optimization of robust and high-performance TEGs.}
}
@article{CEMPEL2013328,
title = {Application of TRIZ approach to machine vibration condition monitoring problems},
journal = {Mechanical Systems and Signal Processing},
volume = {41},
number = {1},
pages = {328-334},
year = {2013},
issn = {0888-3270},
doi = {https://doi.org/10.1016/j.ymssp.2013.07.011},
url = {https://www.sciencedirect.com/science/article/pii/S0888327013003610},
author = {Czesław Cempel},
keywords = {Vibration condition monitoring, TRIZ, Ideal final result –IFR, Engineering parameters, Inventive principles, Contradiction matrix},
abstract = {Up to now machine condition monitoring has not been seriously approached by TRIZ11TRIZ= Russian acronym for Inventive Problem Solving System, created by G. Altshuller ca 50 years ago. users, and the knowledge of TRIZ methodology has not been applied there intensively. However, there are some introductory papers of present author posted on Diagnostic Congress in Cracow (Cempel, in press [11]), and Diagnostyka Journal as well. But it seems to be further need to make such approach from different sides in order to see, if some new knowledge and technology will emerge. In doing this we need at first to define the ideal final result (IFR) of our innovation problem. As a next we need a set of parameters to describe the problems of system condition monitoring (CM) in terms of TRIZ language and set of inventive principles possible to apply, on the way to IFR. This means we should present the machine CM problem by means of contradiction and contradiction matrix. When specifying the problem parameters and inventive principles, one should use analogy and metaphorical thinking, which by definition is not exact but fuzzy, and leads sometimes to unexpected results and outcomes. The paper undertakes this important problem again and brings some new insight into system and machine CM problems. This may mean for example the minimal dimensionality of TRIZ engineering parameter set for the description of machine CM problems, and the set of most useful inventive principles applied to given engineering parameter and contradictions of TRIZ.}
}
@article{MAGNANI2004439,
title = {Reasoning through doing. Epistemic mediators in scientific discovery},
journal = {Journal of Applied Logic},
volume = {2},
number = {4},
pages = {439-450},
year = {2004},
note = {CMSRA},
issn = {1570-8683},
doi = {https://doi.org/10.1016/j.jal.2004.07.004},
url = {https://www.sciencedirect.com/science/article/pii/S1570868304000448},
author = {Lorenzo Magnani},
keywords = {Epistemic mediators, Abduction, Manipulative reasoning, Optical diagrams, Morphodynamics of discovery},
abstract = {The recent epistemological and cognitive studies concentrate on the concept of abduction, as a means to originate and refine new ideas. Traditional cognitive science and computational accounts concerning abduction aim at illustration discovery and creativity processes in terms of theoretical and “internal” aspects, by means of computational simulations and/or abstract cognitive models. I will illustrate in this paper that some typical internal abductive processes are involved in scientific reasoning and discovery (for example through radical innovations). Nevertheless, especially concrete manipulations of the external world constitute a fundamental passage in science: by a process of manipulative abduction it is possible to build prostheses (epistemic mediators) for human minds, by interacting with external objects and representations in a constructive way. In this manner it is possible to create implicit knowledge through doing and to produce various opportunity to find, for example, anomalies and fruitful new risky perspectives. This kind of embodied and unexpressed knowledge holds a key role in the subsequent processes of scientific comprehension and discovery.}
}
@article{MISRA201964,
title = {Do religious and conscious investors make better economic decisions? Evidence from India},
journal = {Journal of Behavioral and Experimental Finance},
volume = {22},
pages = {64-74},
year = {2019},
issn = {2214-6350},
doi = {https://doi.org/10.1016/j.jbef.2019.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S2214635018301217},
author = {Rupali Misra and Sumita Srivastava and D.K. Banwet},
keywords = {Religiosity, Consciousness, Intuition, Rationality, Ambidextrous decision-making, Information processing models, Investor efficacy, Investment decision-making},
abstract = {Investment decision-making in India is different from the world and is affected by social settings. Religion lies at the core of the prevailing socio-cultural environment strengthening social-norms and belief-system, while consciousness is considered core ingredient of life which improves awareness beyond the physical plane, not attributable to cortical processes. Although socio-cultural influence on economic behaviour is quite discernible in countries following eastern-religious traditions, yet studies examining its impact on decision efficacy are scant. Present research addresses this and explores causal role of religiosity and consciousness in shaping investor’s intuitive and analytical abilities. Following a multi-step procedure with qualitative procedure – using engaged scholarship of experts in participative research model and quantitative assessment – surveying investors, the role of these innate behavioural variables in investment decision making has been examined. The paper also validates ambidextrous decision-making style in investment efficacy where intuition improves analytical thinking which further enhances investment efficacy.}
}
@article{WYCKMANS2024100574,
title = {Impact of provoked stress on model-free and model-based reinforcement learning in individuals with alcohol use disorder},
journal = {Addictive Behaviors Reports},
volume = {20},
pages = {100574},
year = {2024},
issn = {2352-8532},
doi = {https://doi.org/10.1016/j.abrep.2024.100574},
url = {https://www.sciencedirect.com/science/article/pii/S2352853224000518},
author = {Florent Wyckmans and Armand Chatard and Charles Kornreich and Damien Gruson and Nemat Jaafari and Xavier Noël},
keywords = {Alcohol Use Disorder, Reinforcement Learning, Model-Based, Model-Free, Stress, Cortisol},
abstract = {Background
From both clinical and theoretical perspectives, understanding the functionality of evaluative reinforcement learning mechanisms (Model-Free, MF, and Model-Based, MB) under provoked stress, particularly in Alcohol Use Disorder (AUD), is crucial yet underexplored. This study aims to evaluate whether individuals with AUD who do not seek treatment show a greater tendency towards retrospective behaviors (MF) rather than prospective and deliberative simulations (MB) compared to controls. Additionally, it examines the impact of induced social stress on these decision-making processes.
Methods
A cohort comprising 117 participants, including 55 individuals with AUD and 62 controls, was examined. Acute social stress was induced through the socially evaluated cold pressor task (SECPT), followed by engagement in a Two-Step Markov task to assess MB and MF learning tendencies. We measured hypothalamic–pituitary–adrenal axis stress response using salivary cortisol levels.
Results
Both groups showed similar baseline cortisol levels and responses to the SECPT. Our findings indicate that participants with AUD exhibit a reduced reliance on MB strategies compared to those without AUD. Furthermore, stress decreases reliance on MB strategies in healthy participants, but this effect is not observed in those with AUD.
Conclusion
An atypical pattern of stress modulation impacting the balance between MB and MF reinforcement learning was identified in individuals with AUD who are not seeking treatment. Potential explanations for these findings and their clinical implications are explored.}
}
@article{DUCH201928,
title = {Mind as a shadow of neurodynamics},
journal = {Physics of Life Reviews},
volume = {31},
pages = {28-31},
year = {2019},
note = {Physics of Mind},
issn = {1571-0645},
doi = {https://doi.org/10.1016/j.plrev.2019.01.023},
url = {https://www.sciencedirect.com/science/article/pii/S1571064519300971},
author = {Włodzisław Duch},
keywords = {Mind models, Neurodynamics, Physics of the mind, Mental spaces, Mental trajectories}
}
@article{MANFRE201612,
title = {Exploiting interactive genetic algorithms for creative humanoid dancing},
journal = {Biologically Inspired Cognitive Architectures},
volume = {17},
pages = {12-21},
year = {2016},
issn = {2212-683X},
doi = {https://doi.org/10.1016/j.bica.2016.07.004},
url = {https://www.sciencedirect.com/science/article/pii/S2212683X16300378},
author = {Adriano Manfré and Agnese Augello and Giovanni Pilato and Filippo Vella and Ignazio Infantino},
keywords = {Robot, Dance, Computational creativity, Music perception, Co-creative tool},
abstract = {The paper discusses an approach aimed at endowing a cognitive architecture with artificial creativity capabilities in order to make a humanoid able to dance in a pleasant manner. The robot associates movements to music perception creating an aesthetically valuable dance by using a Hidden Markov Model with a nonclassical approach. Two matrices mainly influence the model: a Transition matrix TM, and an Emission Matrix EM. The TM matrix rules the transition between two subsequent movements. The EM matrix constitutes the link between a set of movements and the perceived music features. In order to compute the EM matrix, we exploit a genetic algorithm approach. The approach makes use of two kinds of fitness functions. The first one is an internal evaluation fitness that allows the robot to autonomously learn the association between music and movements. The second one depends on the interaction with a human teacher, leading to the determination of different dance styles, which constitute the robot repertoire. The experimental part discusses the effects on the creativity of different distances to compute fitness.}
}
@article{FUSON2009343,
title = {Avoiding misinterpretations of Piaget and Vygotsky: Mathematical teaching without learning, learning without teaching, or helpful learning-path teaching?},
journal = {Cognitive Development},
volume = {24},
number = {4},
pages = {343-361},
year = {2009},
note = {Atypical Development of Numerical Cognition},
issn = {0885-2014},
doi = {https://doi.org/10.1016/j.cogdev.2009.09.009},
url = {https://www.sciencedirect.com/science/article/pii/S0885201409000707},
author = {Karen C. Fuson},
abstract = {This article provides an overview of some perspectives about special issues in classroom mathematical teaching and learning that have stemmed from the huge explosion of research in children's mathematical thinking stimulated by Piaget. It concentrates on issues that are particularly important for less-advanced learners and for those who might be having special difficulties in learning mathematics. A major goal of the article is to develop a framework for understanding what effective mathematics teaching and learning is, because doing so is so important for struggling students and for research about them. Piaget's research had a fundamental influence on the on-going tension between understanding and fluency in the classroom, supporting efforts toward increasing understanding. But in some countries, misinterpretations of Piaget led to practices that are counterproductive for children, especially struggling learners. Such misinterpretations are identified and a more balanced approach that also draws on Vygotsky is described—a learning-path developmentally-appropriate learning/teaching approach.}
}
@article{GUIMARAES2019242,
title = {Extension of Reward-Attention Circuit Model: Alcohol’s Influence on Attentional Focus and Consequences on Autism Spectrum Disorder},
journal = {Neurocomputing},
volume = {325},
pages = {242-253},
year = {2019},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2018.10.034},
url = {https://www.sciencedirect.com/science/article/pii/S0925231218312220},
author = {Karine Guimarães},
keywords = {Alcohol, Dopamine, Attention, Autism spectrum disorder, Computational neuroscience},
abstract = {Attention is a key element that allows us to enhance or decrease the cognitive processing of distinct stimuli, depending on their relevance. In this work we investigate the influence that alcohol exerts on attention focusing, modeling the coupling of reward and thalamocortical circuits. Computer simulations of the reward-attention circuit reflect the spiking behavior of each neuron in the network, under the presence or absence of alcohol. Each neuron in the neural networks that replicate such circuits is described by a carefully designed coupled system of nonlinear differential equations that details essential neurophysiological properties. The computational simulations highlight aspects of clinical inattention symptoms in the autism spectrum disorder. Our results indicate that alcohol may lead to distraction or lack of attentional focus. Also, the simulations suggest why people with ASD might relaxes enhanced attentional focus when exposed to alcohol.}
}
@article{ANTONIOU2024105918,
title = {Realistic simulation of air pollution in an urban area to promote environmental policies},
journal = {Environmental Modelling & Software},
volume = {172},
pages = {105918},
year = {2024},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2023.105918},
url = {https://www.sciencedirect.com/science/article/pii/S1364815223003043},
author = {A. Antoniou and G. Ioannidis and L. Ntziachristos},
keywords = {Optimized visualization, 3D graphics software, Photo-realistic result, Pollutant reduction},
abstract = {Visualizing tools are now capable of synthesizing satellite imagery with Computational Fluid Dynamics (CFD) results. The new method is applied in Augsburg, Germany, and consists of two main phases: pre-processing and post-processing. The pre-processing phase involves creating geometry, mesh, and extracting results from simulations of traffic pollutant dispersion. In the post-processing phase, the results are combined with satellite images to produce visually optimized results. The demonstration of road traffic air pollution is based on real data from local air quality measurements and specific scenarios. The results indicate that using these visualization tools produce understandable and impressive images and videos. This enhances the public's comprehension of scientific results and raises awareness of environmental issues. An increased understanding of scientific results can reinforce the implementation of environmental policies by pressuring responsible authorities to take action. This paper provides a valuable tool for visualizing air pollution and facilitating public engagement with environmental issues.}
}
@incollection{WALTZ1997327,
title = {AI applications of massive parallelism: An experience report},
editor = {James Geller and Hiroaki Kitano and Christian B. Suttner},
series = {Machine Intelligence and Pattern Recognition},
publisher = {North-Holland},
volume = {20},
pages = {327-339},
year = {1997},
booktitle = {Parallel Processing for Artificial Intelligence 3},
issn = {0923-0459},
doi = {https://doi.org/10.1016/S0923-0459(97)80016-X},
url = {https://www.sciencedirect.com/science/article/pii/S092304599780016X},
author = {David L. Waltz},
abstract = {For nearly ten years my group and I at Thinking Machines Corporation worked at selling massively parallel computers for a variety of applications that fall broadly in the area now called “database mining.” We had an amazing team of scientists and engineers, saw trends far ahead of the rest of the world, and developed several great systems. However, we began as novices in the business arena. Sometimes we made sales, sometimes we did not; but we learned a great deal in either case. This chapter recounts the sales process and a brief history, mostly in the form of “war stories” mixed with technical details, and attempts to summarize some messages to take away, based on what we learned.}
}
@article{MOZUNI2017303,
title = {An Introduction to the Morphological Delphi Method for Design: A Tool for Future-Oriented Design Research},
journal = {She Ji: The Journal of Design, Economics, and Innovation},
volume = {3},
number = {4},
pages = {303-318},
year = {2017},
issn = {2405-8726},
doi = {https://doi.org/10.1016/j.sheji.2018.02.004},
url = {https://www.sciencedirect.com/science/article/pii/S2405872617300710},
author = {Mehdi Mozuni and Wolfgang Jonas},
keywords = {Morphological analysis, Systemic design, Future-oriented design, Delphi method, Scenario development, Strategic foresight},
abstract = {Projecting analytical concepts is a difficult, though established process in innovation management. Designers face methodological obstacles, however, when engaging with a future system with rapidly changing factors. First, the system’s users do not yet exist. Second, continuing changes in key factors and their interactions make conceiving of relationships and delivering synthesizable data impossible. The rational core for making projections suffers from a lack of substantiation. Both morphological analysis and the Delphi method are established tools in strategic foresight. We suggest that a morphology-based Delphi method supports the process of projecting future outcomes in innovative, complex projects. In addition, each tool compensates for the other’s theoretical and functional deficits by illustrating transparent, value-based arguments in a modifiable, iterative manner.}
}
@article{DIMARTINO2023138293,
title = {A comprehensive classification of food–energy–water nexus optimization studies: State of the art},
journal = {Journal of Cleaner Production},
volume = {420},
pages = {138293},
year = {2023},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2023.138293},
url = {https://www.sciencedirect.com/science/article/pii/S0959652623024514},
author = {Marcello {Di Martino} and Patrick Linke and Efstratios N. Pistikopoulos},
keywords = {Food–energy–water nexus, Resource supply systems, Process systems engineering, Optimization, Sustainability},
abstract = {To tackle the globally increasing discrepancy between food, energy and water demands and resource availability sustainably, resource supply system models have to incorporate the inter-dependencies and -connectivities to other supply systems. This leads naturally to a food–energy–water nexus (FEWN) approach. The FEWN can be interpreted as the study of the connections between the food, energy and water resource systems, emphasizing how decision-making influences the synergies, conflicts and trade-offs among the various sectors. In recent years, modeling and optimization of FEWN systems has been receiving an increasing interest in the open literature, however, with limited emphasis on how decisions of the FEWN are derived. In this review, FEWN optimization studies are analyzed with focus on the employed objectives, optimization and solution strategies, as well as the selected sub-systems and their corresponding spatial and temporal scales to uncover in detail how decision-making is facilitated. More specifically, FEWN optimization studies are classified according to their modeling and solution strategies. Based on this classification it is uncovered that (i) the decision-making itself has not yet been investigated in detail in FEWN literature, (ii) the incorporation of all aspects of the FEWN is still a challenge, (iii) the interconnection between FEWN systems and society has to be further investigated, and (iv) the implications of uncertainty for the resiliency, robustness and security of process systems is not yet well defined. Additionally, a generic FEWN resource-task network formulation is introduced to illustrate the similarities across the various resource supply sectors. Special interest is placed on how synergies are identified and competition be avoided among resource systems. It is shown that the selected spatial scale as well as the utilized modeling and optimization strategies significantly influence the synergy level of obtained solutions. Furthermore, it is derived that the energy transition has to incorporate FEWN systems thinking for sustainable solution generation. Overall, this review summarizes the different applications and implications of process systems engineering concepts to FEWN systems.}
}
@article{CHAFEE2022R346,
title = {Prefrontal cortex},
journal = {Current Biology},
volume = {32},
number = {8},
pages = {R346-R351},
year = {2022},
issn = {0960-9822},
doi = {https://doi.org/10.1016/j.cub.2022.02.071},
url = {https://www.sciencedirect.com/science/article/pii/S0960982222003414},
author = {Matthew V. Chafee and Sarah R. Heilbronner},
abstract = {Summary
The prefrontal cortex is a well-studied but, in terms of understanding what it is for, deeply divisive part of the brain located at the front of the head. Perhaps the least controversial feature of the prefrontal cortex is its complexity. The prefrontal cortex is anatomically, functionally, and computationally complex. It is anatomically complex, containing a number of subregions each sending and receiving projections to a unique set of other cortical and subcortical areas. This interconnectivity presents a serious challenge to efforts to localize function to prefrontal cortex, because it can seem as though information flows everywhere all at once in prefrontal networks. Perhaps as a result, prefrontal cortex is also computationally complex: working memory, abstraction, sensory attention, value-based decision making, planning, and motor control are all functions that have been attributed to the prefrontal cortex. This diversity of functions is likely to reflect the diversity of brain regions that prefrontal cortex communicates with while carrying out the computations it performs to influence behavior.}
}
@article{GUPTA2024114777,
title = {Replicated multistage interconnection networks: QoS evaluation for parallel and distributed computing},
journal = {Theoretical Computer Science},
volume = {1016},
pages = {114777},
year = {2024},
issn = {0304-3975},
doi = {https://doi.org/10.1016/j.tcs.2024.114777},
url = {https://www.sciencedirect.com/science/article/pii/S0304397524003943},
author = {Shilpa Gupta and G.L. Pahuja},
keywords = {Sensor networks, Distributed system, Real-time data, Efficient computing, Multistage interconnection network (MIN), Shuffle exchange network (SEN), Reliability, Cost},
abstract = {Introduction to big data becomes very important with dealing in high-performance parallel distributed computing, especially in those systems where communication among a number of processors is required. The current paper takes into consideration different topologies for the Shuffle Exchange Network (SEN) in order to attain optimal data transfer among such scenarios. SEN topologies are a key feature in connecting several processors and implementing the data transfer among them where a single processor fails to handle the load. The study hereby reports in the performance, reliability, and cost analysis of these topologies. These topologies, some of which are advocated to have better performance in many studies, include replicated networks. Our research demystifies claims made in earlier papers that replicated networks have inflated reliability and higher costs due to their additional links. Researchers are therefore guided on accurate performance data that will lead them to make optimum choices of SEN topologies for targeted applications. These findings further highlight that the trade-offs between reliability and cost must be carefully considered during network design so as to arrive at better results for big data communication and computation in parallel and distributed systems. This work provides important insights into the correct evaluation of SEN topologies, helping to correct the misleading facts and to have better network selection for various real-time application realizations.}
}