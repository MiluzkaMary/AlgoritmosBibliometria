@article{HONG2024422,
title = {AF-FTTSnet: An end-to-end two-stream convolutional neural network for online quality monitoring of robotic welding},
journal = {Journal of Manufacturing Systems},
volume = {74},
pages = {422-434},
year = {2024},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2024.04.006},
url = {https://www.sciencedirect.com/science/article/pii/S0278612524000724},
author = {Yuxiang Hong and Xingxing He and Jing Xu and Ruiling Yuan and Kai Lin and Baohua Chang and Dong Du},
keywords = {Welding quality monitoring, Visual sensing, Molten pool, Defect prediction, Two-stream network},
abstract = {Online welding quality monitoring (WQM) is crucial for intelligent welding, and deep learning approaches considering spatiotemporal features for WQM tasks show great potential. However, one of the important challenges for existing approaches is to balance the spatiotemporal representation learning capability and computational efficiency, which makes it challenging to adapt welding processes with complex and drastic molten pool dynamic behavior. This paper proposes a novel approach for WQM using molten pool visual sensing and deep learning considering spatiotemporal features, the proposed deep learning network called attention fusion based frame-temporality two-stream network (AF-FTTSnet). Firstly, a passive vision sensor is used to acquire continuous dynamic molten pool images. Meanwhile, temporal difference images are computed to provide novel features and temporal representations. Then, a two-stream feature extraction module is designed to concurrently extract rich spatiotemporal features from molten pool images and temporal difference images. Finally, an attention fusion module with the ability to automatically identify and weight the most relevant features is designed to achieve optimal fusion of the two-stream features. The shop welding experimental results indicate that the proposed AF-FTTSnet model can effectively and robustly recognize five typical welding states during helium arc welding, with an accuracy of 99.26%. This model has been demonstrated to exhibit significant performance improvements compared to mainstream temporal sequence models. Available: https://github.com/Just199806/TSCNN/tree/master.}
}
@article{SALINGER1994139,
title = {Massively parallel finite element computations of three-dimensional, time-dependent, incompressible flows in materials processing systems},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {119},
number = {1},
pages = {139-156},
year = {1994},
issn = {0045-7825},
doi = {https://doi.org/10.1016/0045-7825(94)00081-6},
url = {https://www.sciencedirect.com/science/article/pii/0045782594000816},
author = {Andrew G. Salinger and Qiang Xiao and Yuming Zhou and Jeffrey J. Derby},
abstract = {A parallel implementation of the Galerkin finite element method for three-dimensional, incompressible flows is presented. The inherent element-by-element parallelism of the method is exploited to make efficient use of the architecture of the CM-5 computer. Our implementation features a mixed formulation to expand the primitive variables using triquadratic brick elements with linear, discontinuous pressure basis functions, and the GMRES method with diagonal preconditioning is employed to solve the linear system at each Newton iteration. Transitions among flow states in the classical Taylor-Couette system, which are representative of the complexity of flows found in materials processing systems, are computed as benchmark solutions, and preliminary results are presented for flow in a large-scale, solution crystal growth system. Sustained calculation rates of up to 6 GigaFLOPS are achieved on 512 processors of the CM-5.}
}
@article{LAI2023101343,
title = {Optimization of urban and rural ecological spatial planning based on deep learning under the concept of sustainable development},
journal = {Results in Engineering},
volume = {19},
pages = {101343},
year = {2023},
issn = {2590-1230},
doi = {https://doi.org/10.1016/j.rineng.2023.101343},
url = {https://www.sciencedirect.com/science/article/pii/S259012302300470X},
author = {Yilin Lai},
keywords = {Sustainable development, Spatial planning, Remote sensing images, CNN, GPU},
abstract = {At present, the speed of urbanization in China is constantly accelerating. At the same time, due to the severe situation of tight resource constraints, severe environmental pollution, and ecosystem degradation, vigorously promoting the construction of ecological civilization has become a key planning direction. However, traditional urban and rural ecological spatial planning is influenced by factors such as region, terrain, and spatial scale, which cannot adapt to the current spatial planning requirements. To achieve sustainable urban and rural ecological spatial planning, we propose a method that uses the optimized remote sensing images and convolutional neural networks to achieve spatial planning. In the analysis of the application effect of the usage method, the experimental results show that increasing the amount of data such as image size can improve the execution performance of the computer when the computer is not fully utilizing its resources and its computational volume fails to saturate the computational capacity. The parallel configuration designed in this experiment can accelerate the performance of the computer better, and the acceleration effect becomes more obvious as the difficulty of the algorithm increases. The Faster RCNN algorithm proposed in this experiment has the highest retrieval accuracy in the Flickr30K dataset and MS-COCO dataset compared with other algorithms. In Flickr30k data set, compared with other models in the table, the model used in this paper has the highest retrieval accuracy. The retrieval accuracy of R@1, R@5, R@10 increased by 23.1%, 8.1% and 5.3%, respectively. In MS-COCO data set, the retrieval accuracy increased by 19.2%, 13.1% and 8.3% respectively. The above results confirm that the combination of remote sensing images and convolutional neural network technology can perform simple ecological planning of a city's urban and rural areas, which proves that the method proposed in this experiment has practicality.}
}
@incollection{ASCHEID200711,
title = {Chapter 2 - Opportunities for Application-Specific Processors: The Case of Wireless Communications},
editor = {Paolo Lenne and Rainer Leupers},
booktitle = {Customizable Embedded Processors},
publisher = {Morgan Kaufmann},
address = {Burlington},
pages = {11-37},
year = {2007},
series = {Systems on Silicon},
issn = {18759661},
doi = {https://doi.org/10.1016/B978-012369526-0/50003-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780123695260500036},
author = {Gerd Ascheid and Heinrich Meyr},
abstract = {Publisher Summary
A paradigm change in designing complex systems-on-chip (SoCs) occurs roughly every 12 years because of the exponentially increasing number of transistors on a chip. This paradigm change is characterized by a move to a higher level of abstraction. Instead of thinking in register-transfer level (RTL) blocks and wires, computing elements and interconnect are needed to be thought. The next design discontinuity will lead to different solutions, depending on the application. The following core propositions for wireless communications are made: future SoC for wireless communications will be heterogeneous, reconfigurable Multi-Processor System-on-Chip (MPSoC). They will contain computational elements that cover the entire spectrum, from fixed functionality blocks to domain-specific DSPs and general-purpose processors. A key role will be played by ASIPs. ASIPs exploit the full architectural space (memory, interconnect, instruction set, parallelism), so they are optimally matched to a specific task. The heterogeneous computational elements will communicate via a network-on-chip (NoC), as the conventional bus structures do not scale. These MPSoC platforms will be designed by a cross-disciplinary team. This chapter substantiates this proposition. It begins by analyzing the properties of future wireless communication systems and observes that the systems are computationally demanding. Furthermore, they need innovative architectural concepts to be energy efficient. The chapter discusses the canonical structure of a digital receiver for wireless communication and addresses the design of ASIPs.}
}
@article{LAWRENCE2023100786,
title = {Translational argument technology: Engineering a step change in the argument web},
journal = {Journal of Web Semantics},
volume = {77},
pages = {100786},
year = {2023},
issn = {1570-8268},
doi = {https://doi.org/10.1016/j.websem.2023.100786},
url = {https://www.sciencedirect.com/science/article/pii/S157082682300015X},
author = {John Lawrence and Jacky Visser and Chris Reed},
keywords = {Argumentation, Argument analytics, Argument mining, Argument technology, Argument web, Debate technology},
abstract = {Following the establishment in 2006 of a representational standard for the computational handling of structures of argumentation, the Argument Interchange Format, it became possible to develop a vision for the coherent integration of multifarious services, components and tools that create, consume, navigate, analyse, evaluate and manipulate arguments and debates. This vision was the Argument Web with theoretical foundations laid by Rahwan et al. (2007), and practical engineering work described by Bex et al. (2013). Over the intervening period, the key challenge has been to demonstrate the practical and societal value of the Argument Web by taking its tools and applications to larger audiences. This paper lays out three approaches by which the Argument Web has been scaled up in this way, each in partnership with the BBC, and each with different kinds of evaluation and impact. Transitioning these technologies to large user groups paves the way for broader-scale uptake of the Argument Web and heralds the translation from lab to real-world application for a substantial research community working in argument technology.}
}
@article{JONES2013122,
title = {Understanding the integral: Students’ symbolic forms},
journal = {The Journal of Mathematical Behavior},
volume = {32},
number = {2},
pages = {122-141},
year = {2013},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2012.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S0732312312000612},
author = {Steven R. Jones},
keywords = {Calculus, Integral, Student understanding, Undergraduate mathematics education, Symbolic form, Accumulation},
abstract = {Researchers are currently investigating how calculus students understand the basic concepts of first-year calculus, including the integral. However, much is still unknown regarding the cognitive resources (i.e., stable cognitive units that can be accessed by an individual) that students hold and draw on when thinking about the integral. This paper presents cognitive resources of the integral that a sample of experienced calculus students drew on while working on pure mathematics and applied physics problems. This research provides evidence that students hold a variety of productive cognitive resources that can be employed in problem solving, though some of the resources prove more productive than others, depending on the context. In particular, conceptualizations of the integral as an addition over many pieces seem especially useful in multivariate and physics contexts.}
}
@article{YUAN2025129490,
title = {Global attention network with rain prior for real time single image deraining},
journal = {Neurocomputing},
volume = {625},
pages = {129490},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2025.129490},
url = {https://www.sciencedirect.com/science/article/pii/S0925231225001626},
author = {Yuan Yuan and Xuanbin Guo and Dandan Ma},
keywords = {Rain removal, Deep learning, Attention mechanism, Activation function},
abstract = {Poor visibility caused by rainy image can have a negative impact on the performance of computer vision applications. While several image deraining algorithms have been popularly adopted, most of them suffer from two main limitations: (1) they cannot well handle real and complex rain scenes by only focusing on one type of rain in images (e.g. raindrops or rain streaks) whereas the reality often coexists with both types, (2) they face significant difficulties in practical application because of ignoring the speed of inference. To address the above problems, we propose a global attention network (GANet) that can quickly and effectively separate rain streaks and raindrops. Inspired by the fact that rain in images often appears white, we leverage this prior to obtain an initial rain-free background image to guide neural network-based image deraining. Moreover, a new global attention block (GAB) is designed to simultaneously extract the rain features from spatial and channel dimensions. By cascading multiple GABs, the proposed method can effectively obtain the features of rain streaks and raindrops and progressively separates the rain-free image. Furthermore, owing to the nonlinear properties of GAB, the activation functions are omitted, which can speed up the inference time. And the depth-wise and point-wise convolutions are employed to promote computation efficiency as well. Extensive experiments on raindrop and rain streak datasets demonstrate that our method outperforms state-of-the-art methods, achieving up to 37.53 dB PSNR on Rain100L with an inference speed of 39 FPS, which is 2–30 times faster than competitors.}
}
@article{HUSSAIN2025109490,
title = {A neural network integrated mathematical model to analyze the impact of nutritional status on cognitive development of child},
journal = {Computers in Biology and Medicine},
volume = {185},
pages = {109490},
year = {2025},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2024.109490},
url = {https://www.sciencedirect.com/science/article/pii/S0010482524015750},
author = {Zakir Hussain and Malaya Dutta Borah},
keywords = {Cognitive development, Cognition, Nutritional status, Neural network, Mathematical model},
abstract = {Cognitive development is a crucial developmental aspect of children. It is a concise field of study in psychology and neuroscience that focuses on various developmental aspects of the brain. Among all other factors, nutritional status is believed to play a very important role in cognitive development. The purpose of this work is to analyze the impacts of different nutritional status levels on the child’s cognitive development. This work designs a model that uses a neural network and differential equations. The neural network is applied on a dataset called “Child Birth Weight Dataset” available at IEEE Dataport ( http://dx.doi.org/10.21227/dvd4-3232) for finding the nutritional status of a child. The different levels of nutritional status, such as low-nutritional status, normal-nutritional status, and over-nutritional status are integrated with the formulated differential equations. The model is computationally simulated considering four different sets of parameter values that represent four different perspectives such as ‘only positive’, ‘only negative’, ‘mix and unequal weight’, and ‘mix and equal weight’ of the influencing factors. The experimental results show that normal-nutritional status is the best nutritional status for cognitive development. However, the best cognitive development happens when all other influencing factors like environmental effects, socioeconomic status, heredity, learning opportunities, and use of experiences are given equal importance. The results also depict that the low- and over-nutritional status cannot restrict cognitive development for a long time. After a certain period, the development gets triggered and it happens. It may be slow and not up to the mark of the development under normal-nutritional status, but it happens. Simply it can be said that nutritional status alone does not have control over the cognitive development of a child. Along with nutritional status, other influencing factors are important too.}
}
@article{ADHIKARI20121374,
title = {Multi-commodity network flow models for dynamic energy management – Smart Grid applications},
journal = {Energy Procedia},
volume = {14},
pages = {1374-1379},
year = {2012},
note = {2011 2nd International Conference on Advances in Energy Engineering (ICAEE)},
issn = {1876-6102},
doi = {https://doi.org/10.1016/j.egypro.2011.12.1104},
url = {https://www.sciencedirect.com/science/article/pii/S1876610211045243},
author = {R.S. Adhikari and N. Aste and M. Manfren},
keywords = {Dynamic energy management, Smart Grid, Multi-commodity network flow models},
abstract = {The strong interconnection between human activities, energy use and pollution reduction strategies in contemporary society has determined the necessity of collecting scientific knowledge from different fields to provide useful methods and models to foster the transition towards more sustainable energy systems. This is a challenging task in particular for contemporary communities where an increasing demand for services is combined with rapidly changing lifestyles and habits. The Smart Grid concept is the result of a confluence of issues and a convergence of objectives, which include national energy security, climate change, pollution reduction, grid reliability, etc. While thinking about a paradigm shift in energy systems, drivers, characteristics, market segments, applications and other interconnected aspects must be taken into account simultaneously. In this context, the use of multi-commodity network flow models for dynamic energy management aims at finding a compromise between model usefulness, accuracy, flexibility, solvability and scalability in Smart Grid applications.}
}
@article{UDDIN2021106,
title = {Application of Theory in Chronic Pain Rehabilitation Research and Clinical Practice},
journal = {The Open Sports Sciences Journal},
volume = {14},
pages = {106-113},
year = {2021},
issn = {1875-399X},
doi = {https://doi.org/10.2174/1875399X02114010106},
url = {https://www.sciencedirect.com/science/article/pii/S1875399X2100013X},
author = {Zakir Uddin and Joy C. MacDermid and Fatma A. Hegazy and Tara L. Packham},
keywords = {Chronic pain , Hypersensitivity , Theory , Rehabilitation , Disability , T-cell},
abstract = {Introduction
Chronic pain has multiple aetiological factors and complexity. Pain theory helps us to guide and organize our thinking to deal with this complexity. The objective of this paper is to critically review the most influential theory in pain science history (the gate control theory of pain) and focus on its implications in chronic pain rehabilitation to minimize disability.
Methods
In this narrative review, all the published studies that focused upon pain theory were retrieved from Ovoid Medline (from 1946 till present), EMBAS, AMED and PsycINFO data bases.
Results
Chronic pain is considered a disease or dysfunction of the nervous system. In chronic pain conditions, hypersensitivity is thought to develop from changes to the physiological top-down control (inhibitory) mechanism of pain modulation according to the pain theory. Pain hypersensitivity manifestation is considered as abnormal central inhibitory control at the gate controlling mechanism. On the other hand, pain hypersensitivity is a prognostic factor in pain rehabilitation. It is clinically important to detect and manage hypersensitivity responses and their mechanisms.
Conclusion
Since somatosensory perception and integration are recognized as a contributor to the pain perception under the theory, then we can use the model to direct interventions aimed at pain relief. The pain theory should be leveraged to develop and refine measurement tools with clinical utility for detecting and monitoring hypersensitivity linked to chronic pain mechanisms.}
}
@incollection{MARR1988534,
title = {A computational theory of human stereo vision††M.I.T. Psychology Department, 79 Amherst Street, Cambridge Ma 02139, U.S.A.},
editor = {Allan Collins and Edward E. Smith},
booktitle = {Readings in Cognitive Science},
publisher = {Morgan Kaufmann},
pages = {534-547},
year = {1988},
isbn = {978-1-4832-1446-7},
doi = {https://doi.org/10.1016/B978-1-4832-1446-7.50046-7},
url = {https://www.sciencedirect.com/science/article/pii/B9781483214467500467},
author = {D. MARR and T. POGGIO}
}
@article{FALZON2006629,
title = {Using Bayesian network analysis to support centre of gravity analysis in military planning},
journal = {European Journal of Operational Research},
volume = {170},
number = {2},
pages = {629-643},
year = {2006},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2004.06.028},
url = {https://www.sciencedirect.com/science/article/pii/S0377221704005156},
author = {Lucia Falzon},
keywords = {Military, Decision analysis, Probabilistic models, Bayesian networks},
abstract = {Centre of gravity (COG) analysis is an integral and cognitively demanding aspect of military operational planning. It involves identifying the enemy and friendly COG and subsequently determining the critical vulnerabilities that have to be degraded or negated to influence the COG of each side. This paper describes a modelling framework based on the causal relationships among the critical capabilities and requirements for an operation. The framework is subsequently used as a basis for the construction, population and analysis of Bayesian networks to support a rigorous and systematic approach to COG analysis. The importance of this work is that it uses existing planning process concepts to facilitate the construction of comprehensive models in which uncertainties and subjective judgements are clearly represented, thus enabling future re-use and traceability. The visual representation of the COG causal structure helps to clarify thinking and provides a way to record and impart this thinking. Moreover, it gives planners the capability to perform impact analysis, that is, to determine which actions are most likely to achieve a desirable end-state. The paper discusses the methodology, development and implementation of the COG Network Effects Tool (COGNET) suite for model population and model checking as well as impact analysis.}
}
@article{LUO2023101957,
title = {A design model of FBS based on interval-valued Pythagorean fuzzy sets},
journal = {Advanced Engineering Informatics},
volume = {56},
pages = {101957},
year = {2023},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2023.101957},
url = {https://www.sciencedirect.com/science/article/pii/S147403462300085X},
author = {Yuhan Luo and Minna Ni and Feng Zhang},
keywords = {FBS model, Pythagorean fuzzy sets, AHP, HOQ},
abstract = {The FBS (Function-Behaviour-Structure) model is a research model that stimulates creative thinking of designers in the design process. In order to reduce the influence of user requirement ambiguity on design results in the product design process and improve the accuracy of user requirements in the function-behavior-structure (FBS) design model, this paper proposes an interval-valued Pythagorean fuzzy set-based FBS model integrating AHP and HOQ methods. Firstly, the design model will use IVPF-AHP method to study user requirements and use interval-valued Pythagorean linguistic terms to replace the traditional scoring method of AHP to get the weight of each user requirement. Secondly, the conversion between user requirements and functions will be realized by IVPF-HOQ method, which converts customer requirements into functional characteristics and calculates the weights of each functional characteristic. Finally, the design focus will be filtered according to the order of importance of the functional characteristics, which will be used as functions to guide the development of the FBS model. In this paper, the feasibility and effectiveness of the proposed method will be verified by an application example of a hand-held fluorescence spectrometer. The results show that the proposed FBS model can effectively reduce the subjectivity and ambiguity in the decision-making process, improve the accuracy and information richness of user requirements, and effectively highlight the focus of the design study. The innovation of the proposed method is to provide a more objective and accurate innovative design method for user requirements through the integration of AHP, HOQ and FBS to effectively explore and analyze user requirements. The use of IVPFS to deal with fuzzy information in the design process in a more flexible manner can reduce the ambiguity of requirements when user data is small, and effectively improve the limitations of the FBS design model which is more subjective.}
}
@article{BACHMANN2020102937,
title = {Account of consciousness by Christof Koch: Review and questions},
journal = {Consciousness and Cognition},
volume = {82},
pages = {102937},
year = {2020},
issn = {1053-8100},
doi = {https://doi.org/10.1016/j.concog.2020.102937},
url = {https://www.sciencedirect.com/science/article/pii/S1053810020300143},
author = {Talis Bachmann},
keywords = {Consciousness, Integrated information, Cognitive computation, Microgenesis, Phenomenal experience},
abstract = {This review is set to present the gist of the theoretical account of consciousness recently presented by Christof Koch and pose a couple of questions instigated by this account. The expected answers to these questions would hopefully help to advance our understanding of the basic nature of the conscious mind.}
}
@article{HELBING2023102061,
title = {Democracy by Design: Perspectives for Digitally Assisted, Participatory Upgrades of Society},
journal = {Journal of Computational Science},
volume = {71},
pages = {102061},
year = {2023},
issn = {1877-7503},
doi = {https://doi.org/10.1016/j.jocs.2023.102061},
url = {https://www.sciencedirect.com/science/article/pii/S1877750323001217},
author = {Dirk Helbing and Sachit Mahajan and Regula Hänggli Fricker and Andrea Musso and Carina I. Hausladen and Cesare Carissimo and Dino Carpentras and Elisabeth Stockinger and Javier {Argota Sanchez-Vaquerizo} and Joshua C. Yang and Mark C. Ballandies and Marcin Korecki and Rohit K. Dubey and Evangelos Pournaras},
keywords = {Computational diplomacy, Digital democracy, Participation, Collective intelligence, Value-based engineering},
abstract = {The technological revolution, particularly the availability of more data and more powerful computational tools, has led to the emergence of a new scientific field called “Computational Diplomacy”. Our work tries to define its scope and focuses on a popular subarea of it, namely “Digital Democracy”. In recent years, there has been a surge of interest in using digital technologies to promote more participatory forms of democracy. While there are numerous potential benefits to using digital tools to enhance democracy, significant challenges must be addressed. It is essential to ensure that digital technologies are used in an accessible, equitable, and fair manner rather than reinforcing existing power imbalances. This paper investigates how digital tools can be used to help design more democratic societies by investigating three key research areas: (1) the role of digital technologies for facilitating civic engagement in collective decision-making; (2) the use of digital tools to improve transparency and accountability in governance; and (3) the potential for digital technologies to enable the formation of more inclusive and representative democracies. We argue that more research on how digital technologies can be used to support democracy upgrade is needed. Along these lines, we lay out a research agenda for the future.}
}
@article{HASSABIS2007299,
title = {Deconstructing episodic memory with construction},
journal = {Trends in Cognitive Sciences},
volume = {11},
number = {7},
pages = {299-306},
year = {2007},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2007.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S1364661307001258},
author = {Demis Hassabis and Eleanor A. Maguire},
abstract = {It has recently been observed that the brain network supporting recall of episodic memories shares much in common with other cognitive functions such as episodic future thinking, navigation and theory of mind. It has been speculated that ‘self-projection’ is the key common process. However, in this Opinion article, we note that other functions (e.g. imagining fictitious experiences) not explicitly connected to either the self or a subjective sense of time, activate a similar brain network. Hence, we argue that the process of ‘scene construction’ is better able to account for the commonalities in the brain areas engaged by an extended range of disparate functions. In light of this, we re-evaluate our understanding of episodic memory, the processes underpinning it and other related cognitive functions.}
}
@article{CRANFORD2022100638,
title = {Navigating the “Kessel Run” of digital materials acceleration},
journal = {Patterns},
volume = {3},
number = {11},
pages = {100638},
year = {2022},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2022.100638},
url = {https://www.sciencedirect.com/science/article/pii/S2666389922002707},
author = {Steve Cranford},
abstract = {Computational methods such as machine learning, artificial intelligence, and big data in physical sciences, particularly materials science, have been exponentially growing in terms of progress, method development, and number of studies and related publications. This aggregate momentum of the community is palpable, and many exciting discoveries are likely on the horizon. But, like all endeavors, some thought should be given to the current trajectory of the field, ensuring the full potential of the new digital space.}
}
@article{AGNOLI2020116385,
title = {Predicting response originality through brain activity: An analysis of changes in EEG alpha power during the generation of alternative ideas},
journal = {NeuroImage},
volume = {207},
pages = {116385},
year = {2020},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2019.116385},
url = {https://www.sciencedirect.com/science/article/pii/S1053811919309760},
author = {Sergio Agnoli and Marco Zanon and Serena Mastria and Alessio Avenanti and Giovanni Emanuele Corazza},
keywords = {EEG, Alpha power, Originality, Idea generation, Divergent-thinking, Temporal dynamics, Creativity},
abstract = {Growing neurophysiological evidence points to a role of alpha oscillations in divergent thinking (DT). In particular, studies have shown a consistent EEG alpha synchronization during performance on the Alternative Uses Task (AUT), a well-established DT task. However, there is a need for investigating the brain dynamics underlying the production of a sequence of multiple, alternative ideas at the AUT and their relationship with idea originality. In twenty young adults, we investigated changes in alpha power during performance on a structured version of the AUT, requiring to ideate four alternative uses for conventional objects in distinct and sequentially balanced time periods. Data analysis followed a three-step approach, including behaviour aspects, physiology aspects, and their mutual relationship. At the behavioural level, we observed a typical serial order effect during DT production, with an increase of originality associated with an increase in ideational time and a decrease in response percentage over the four responses. This pattern was paralleled by a shift from alpha desynchronization to alpha synchronization across production of the four alternative ideas. Remarkably, alpha power changes were able to explain response originality, with a differential role of alpha power over different sensor sites. In particular, alpha synchronization over frontal, central, and temporal sites was able to predict the generation of original ideas in the first phases of the DT process, whereas alpha synchronization over centro-parietal sites persistently predicted response originality during the entire DT production. Moreover, a bilateral hemispheric effect in frontal sites and a left-lateralized effect in central, temporal, and parietal sensor sites emerged as predictors of the increase in response originality. These findings highlight the temporal dynamics of DT production across the generation of alternative ideas and support a partially distinct functional role of specific cortical areas during DT.}
}
@incollection{GARDNER2024103,
title = {Chapter 5 - Smart design for socially engaging environments},
editor = {Nicole Gardner},
booktitle = {Scaling the Smart City},
publisher = {Elsevier},
pages = {103-128},
year = {2024},
series = {Smart Cities},
isbn = {978-0-443-18452-9},
doi = {https://doi.org/10.1016/B978-0-443-18452-9.00006-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780443184529000069},
author = {Nicole Gardner},
keywords = {Cyber-physical system, Design, Interaction, Physical computing, Play, Smart city, Smart cities, Social engagement, Social capital, Social interaction, Urban play, Urban technology},
abstract = {Smart city initiatives typically aim to optimize the efficiency of essential urban infrastructure and urban service delivery and performance. This chapter considers how smart technologies can also be deployed in ways to catalyze social interactions among citizens in urban public realm spaces to create socially engaging environments. Drawing on a range of concepts such as social cohesion, social capital, and object-centered sociality, this chapter considers how existing and speculative urban technology projects that combine spatial design thinking and physical computing can scaffold and amplify opportunities for social engagement. It considers how urban technology projects that mobilize tactics of proximity, curiosity, and play can create new and different ways for people to relate to each other in urban space.}
}
@incollection{SANTOS2024,
title = {Data analysis on Decision-Making},
booktitle = {Reference Module in Social Sciences},
publisher = {Elsevier},
year = {2024},
isbn = {978-0-443-15785-1},
doi = {https://doi.org/10.1016/B978-0-443-13701-3.00018-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780443137013000189},
author = {Eulália Santos and Margarida F. Oliveira},
keywords = {Artificial intelligence, Business, Data analysis, Decision making, Logistics, Machine learning, Mathematical modeling operations research, Mathematical programming, Optimization, Statistic, Strategic management, Technology},
abstract = {Today, data analysis plays a vital role in identifying market trends and supporting strategic decision-making in organizations. To make an effective decision in order to obtain positive results, it is necessary not only to carefully analyze various pieces of information but also to use artificial intelligence and critical thinking. Mathematics plays an essential role in making effective decisions and providing tools and methods for analyzing, modeling and solving both simple and more complex problems.}
}
@article{GRANJO202021,
title = {Enhancing the autonomy of students in chemical engineering education with LABVIRTUAL platform},
journal = {Education for Chemical Engineers},
volume = {31},
pages = {21-28},
year = {2020},
issn = {1749-7728},
doi = {https://doi.org/10.1016/j.ece.2020.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S174977282030018X},
author = {José F.O. Granjo and Maria G. Rasteiro},
keywords = {Web platform, Virtual labs, Chemical processes, Autonomous learning},
abstract = {Engineering educators have been developing different approaches to supplement scientific background and further develop the ability for autonomous and critical thinking in students. In 2009, the University of Coimbra has made available on-line a virtual platform with a wide scope, directed towards Chemical Engineering education. The platform is divided into four different educational topics: Unit Operations and Separations, Chemical Reaction, Process Systems Engineering and Biological Processes. These sections include simulators, applications, and case studies to help understanding chemical/biochemical processes and improve their autonomy. This paper presents an assessment of the use of that platform by two different groups of students in the school years of 2015/2016 and 2018/2019: a group from the 3rd-year of Chemical Engineering, and another one from a Project Design course (2nd cycle, MSc of Chemical Engineering). A case study addressing the synthesis of phthalic anhydride by o-xylene oxidation on a fixed-bed catalytic reactor is also given to show the use of existing simulators in LABVIRTUAL.}
}
@incollection{KURGANSKAYA2024760,
title = {Multi-scale modeling of crystal-fluid interactions: State-of-the-art, challenges and prospects},
editor = {Klaus Wandelt and Gianlorenzo Bussetti},
booktitle = {Encyclopedia of Solid-Liquid Interfaces (First Edition)},
publisher = {Elsevier},
edition = {First Edition},
address = {Oxford},
pages = {760-792},
year = {2024},
isbn = {978-0-323-85670-6},
doi = {https://doi.org/10.1016/B978-0-323-85669-0.00034-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780323856690000349},
author = {I. Kurganskaya and R.D. Rohlfs and A. Luttge},
keywords = {Crystal-water interface, Electric double layer, Grand canonical Monte Carlo, Kinetic Monte Carlo, Kinetics, Mineral–water interface, Parameterization, Reaction pathways, Reaction probability, Reaction rates, Statistical mechanics of interfaces, Stepwave, Stochastic model, Upscaling, Voronoi},
abstract = {We describe theoretical and conceptual approaches to treat crystal-fluid interactions across the scales in the communities studying mineral-fluid interactions for a variety of purposes, from understanding fundamental principles to geological reservoir characterization and environmental mitigation. We delineate basics of theory, recent breakthroughs, and challenges in modeling approaches from the atomistic scale to the mesoscale. Quantum Mechanics, Molecular Dynamics, Kinetic Monte Carlo and Voronoi computational geometry are covered. We discuss possible theoretical and conceptual developments to overcome those challenges toward more reliable predictive models. A special attention is given to the development of interfaces between the techniques addressing different scales.}
}
@article{SZYMANSKI2021,
title = {Words Are Essential, but Underexamined, Research Tools for Microbes and Microbiomes},
journal = {mSystems},
volume = {6},
number = {4},
year = {2021},
issn = {2379-5077},
doi = {https://doi.org/10.1128/msystems.00769-21},
url = {https://www.sciencedirect.com/science/article/pii/S2379507721002683},
author = {Erika Szymanski},
keywords = {discourse, engineering, metaphor, microbiome, science and technology studies, synthetic biology, synthetic yeast},
abstract = {Language constitutes an essential set of scientific construction tools, not only for communicating knowledge, but for conceptualizing the world. Metaphors in particular, as conventions that guide and reproduce analogical reasoning, merit attention that they largely do not receive.
ABSTRACT
Language constitutes an essential set of scientific construction tools, not only for communicating knowledge, but for conceptualizing the world. Metaphors in particular, as conventions that guide and reproduce analogical reasoning, merit attention that they largely do not receive. My research addresses this deficit by examining how metaphors for handling microbes shape possibilities for working with yeast and bacteria in synthetic biology, microbiome research, and other fields that reconfigure what microbes can be. Though poised to reexamine assumptions, these fields routinely rest on metaphors and other language tools that quietly embed ways of thinking that may work against wider aims—for example, imagining bacteria as imperfect machines that should therefore be rendered increasingly passive and controllable. Researchers, therefore, need to examine how language tools structure their observations and expectations so that the tools they choose are appropriate for the work they want to do.}
}
@article{RANDALL1991219,
title = {Review of linear least squares computations: by R.W. Farebrother},
journal = {Linear Algebra and its Applications},
volume = {153},
pages = {219-223},
year = {1991},
issn = {0024-3795},
doi = {https://doi.org/10.1016/0024-3795(91)90221-H},
url = {https://www.sciencedirect.com/science/article/pii/002437959190221H},
author = {John H. Randall}
}
@article{GARDECKI2018138,
title = {Innovative Internet of Things-reinforced Human Recognition for Human-Machine Interaction Purposes},
journal = {IFAC-PapersOnLine},
volume = {51},
number = {6},
pages = {138-143},
year = {2018},
note = {15th IFAC Conference on Programmable Devices and Embedded Systems PDeS 2018},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2018.07.143},
url = {https://www.sciencedirect.com/science/article/pii/S2405896318308875},
author = {Arkadiusz Gardecki and Michal Podpora and Aleksandra Kawala-Janik},
keywords = {Human-Machine Interaction, Internet of Things, Human Recognition, Humanoid Robots, Human Identification},
abstract = {Accurate and reliable human recognition and parametrisation have always been an important challenge in efficient Man-Machine Interaction. A humanoid robot is able to offer a much richer and more natural behaviour and human-like communication, but only if the robot possesses sufficient knowledge about the interlocutor, such as inter alia: gender, age, mood, behaviour data, interaction history. In this paper authors introduced an innovative conception in Human-Machine Interaction, where instead of thinking about an interaction as an event (which uses and produces information) an innovative point of view was proposed, where the interaction is just an event in a continuous flow of information. The difference, once perceived, results in an astounding change of conception, as well as a whole new set of ideas. The human detection, information acquisition, human recognition – can be performed earlier, before a human reaches the humanoid robot, also the history of interactions and possible interests of the interlocutor can be predicted before they would even start the conversation. This paper contains a detailed analysis of the proposed environment-based approach to interaction, as well as the Internet of Things-reinforced information acquisition.}
}
@article{NSSSN2024106769,
title = {VNSMAS: A constraint-based portfolio profit maximization},
journal = {Computers & Operations Research},
volume = {170},
pages = {106769},
year = {2024},
issn = {0305-0548},
doi = {https://doi.org/10.1016/j.cor.2024.106769},
url = {https://www.sciencedirect.com/science/article/pii/S0305054824002417},
author = {Usha Devi N.S.S.S.N. and R. Mohan},
keywords = {GAN, Reinforcement learning, Stock, Fuzzy},
abstract = {Stock trading has a more significant influence on the global economy. Stock trading with portfolio optimization became challenging due to the complexity of analyzing the high variance in time series stock data. Efficient portfolio management increases profit and avoids risky situations when investing. The present work aims to model a Variable Neighborhood Search Multi-Agent System for Portfolio Optimization (VNSMASPPO) to optimize the profit on defined trading constraints on buying, selling, and holding trading decisions. This work proposes a novel Variable Neighborhood Search-based Multi-Agent System (VNASMAS) algorithm for profit computation with a constraint-based multi-agent system. The stock price history experimental data sets are collected from 8th August 2016 to 31st March 2023 with 14,567 records. The proposed model achieved an RMSE of 10.11, MAE of 2.75, and MAPE of 0.017, outperforming the literature models. VNSMASPPO maximizes the portfolio profit and is a reliable, adaptable approach.}
}
@incollection{CALVERT201369,
title = {Chapter 3 - Social Dimensions of Microbial Synthetic Biology},
editor = {Colin Harwood and Anil Wipat},
series = {Methods in Microbiology},
publisher = {Academic Press},
volume = {40},
pages = {69-86},
year = {2013},
booktitle = {Microbial Synthetic Biology},
issn = {0580-9517},
doi = {https://doi.org/10.1016/B978-0-12-417029-2.00003-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780124170292000030},
author = {Jane Calvert and Emma Frow},
keywords = {Anticipation, Governance, Public engagement, Public good, Responsible innovation, Social dimensions, Science and technology studies, Synthetic biology},
abstract = {In this chapter, we outline a number of foundational ideas that underpin our approach to the study of the social, ethical, legal and philosophical dimensions of synthetic biology. We describe these through a series of important shifts that have taken place over the past few decades of social science research. We suggest a move away from discussions centred around ethical ‘implications’, speculations about the future and concerns about risk, regulation and public acceptance, towards a conversation that talks in terms of social ‘dimensions’, anticipating the future, managing uncertainty, tools of governance and research for the public good. We argue that these seemingly subtle changes in vocabulary open up a new and productive space for thinking about the social dimensions of synthetic biology.}
}
@article{VASILE201177,
title = {Entry points, interests and attitudes. An integrative approach of learning},
journal = {Procedia - Social and Behavioral Sciences},
volume = {11},
pages = {77-81},
year = {2011},
note = {Teachers for the Knowledge Society},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2011.01.037},
url = {https://www.sciencedirect.com/science/article/pii/S1877042811000395},
author = {Cristian Vasile},
keywords = {multiple intelligence, entry points, personality, interests},
abstract = {The relationship between personality and intelligence is of a major importance in the learning process. Interests and attitudes are related to the entry points on emotional ground. In some educational systems the focus on cognitive abilities and cognitive functions increased, amplified by the neuroscience and the computational approach. The cognitive approach should be enriched with major aspects from the global human psychological system like interests/motivation, emotional profile, attitudes and so on. The focus on cognition only, or the computational view should be completed with personality approaches and behavior regulation, all of these influencing without doubt the intelligence.}
}
@article{FOSGERAU2021109911,
title = {Some remarks on CCP-based estimators of dynamic models},
journal = {Economics Letters},
volume = {204},
pages = {109911},
year = {2021},
issn = {0165-1765},
doi = {https://doi.org/10.1016/j.econlet.2021.109911},
url = {https://www.sciencedirect.com/science/article/pii/S0165176521001889},
author = {Mogens Fosgerau and Emerson Melo and Matthew Shum and Jesper R.-V. Sørensen},
keywords = {Dynamic discrete choice, Random utility, Linear programming, Convex analysis, Convex optimization},
abstract = {This note provides several remarks relating to the conditional choice probability (CCP) based estimation approaches for dynamic discrete-choice models. Specifically, the Arcidiacono and Miller (2011) estimation procedure relies on the ”inverse-CCP” mapping ψp from CCPs to choice-specific value functions. Exploiting the convex-analytic structure of discrete choice models, we discuss two approaches for computing this mapping, using either linear or convex programming, for models where the utility shocks can follow arbitrary parametric distributions. Furthermore, the ψ function is generally distinct from the ”selection adjustment” term (i.e. the expectation of the utility shock for the chosen alternative), so that computational approaches for computing the latter may not be appropriate for computing ψ.}
}
@article{BEYNON2008476,
title = {Experimenting with computing},
journal = {Journal of Applied Logic},
volume = {6},
number = {4},
pages = {476-489},
year = {2008},
note = {The Philosophy of Computer Science},
issn = {1570-8683},
doi = {https://doi.org/10.1016/j.jal.2008.09.008},
url = {https://www.sciencedirect.com/science/article/pii/S157086830800044X},
author = {Meurig Beynon and Steve Russ},
keywords = {Empirical Modelling, Observation, Experiment, Computing, Theory, Radical empiricism, Dependency, Agency},
abstract = {We distinguish two kinds of experimental activity: post-theory and exploratory. Post-theory experiment enjoys computer support that is well-aligned to the classical theory of computation. Exploratory experiment, in contrast, arguably demands a broader conception of computing. Empirical Modelling (EM) is proposed as a more appropriate conceptual framework in which to provide computational support for exploratory experiment. In the process, it promises to provide integrated computational support for both exploratory and post-theory experiment. We first sketch the motivation for EM and illustrate its potential for supporting experimentation, then briefly highlight the semantic challenge it poses and the philosophical implications.}
}
@article{MARTINI2022105446,
title = {R_IC: A novel and versatile implementation of the index of connectivity in R},
journal = {Environmental Modelling & Software},
volume = {155},
pages = {105446},
year = {2022},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2022.105446},
url = {https://www.sciencedirect.com/science/article/pii/S1364815222001529},
author = {Lorenzo Martini and Tommaso Baggio and Loris Torresani and Stefano Crema and Marco Cavalli},
keywords = {Sediment connectivity, Geomorphometry, R_IC, Open-source},
abstract = {Sediment connectivity is the capability of a system to regulate the exchange of sediment in catchments. The Index of Connectivity (IC) has become a widely used tool, offering a practical way to assess sediment connectivity from hillslopes to downstream channels. We present a novel implementation of IC in R environment to expand the audience of users and encourage alternative applications of the index. The R_IC is an open-source and freely available tool composed by three codes. Standard R_IC runs the IC and it represents the core of the other variants. Custom R_IC offers a more flexible script, allowing the computation of alternative weighting factors and the possibility of running a further profile IC analysis. Batch R_IC performs batch processing of the index. For each code variant, a geomorphological application is presented to illustrate how the R_IC could be used in watershed management and practical issues related to sediment dynamics.}
}
@article{BLAND2025,
title = {Quantal response equilibrium as a structural model for estimation: The missing manual},
journal = {Games and Economic Behavior},
year = {2025},
issn = {0899-8256},
doi = {https://doi.org/10.1016/j.geb.2025.02.008},
url = {https://www.sciencedirect.com/science/article/pii/S0899825625000211},
author = {James R. Bland and Theodore L. Turocy},
keywords = {Quantal response, Estimation, Computation, Experiments},
abstract = {One of the original objectives of the (logit) quantal response equilibrium (LQRE) model was to provide a method for structural estimation of behavior in games, when behavior deviated from Nash equilibrium predictions. To date, only Chapter 6 of the book on quantal response equilibrium by Goeree et al. (2016) focuses on how such estimation can be implemented. We build on that chapter to provide here a more detailed treatment of the methodological issues of implementing maximum likelihood estimation of QRE. We compare the equilibrium correspondence and empirical payoff approaches to estimation, and identify some considerations in interpreting the results of those approaches when applied to the same data on the same game. We also provide a more detailed “field guide” to using numerical continuation methods to accomplish estimation, including guidance on how to tailor implementations to games with different structures.}
}
@article{MUTHUSAMY2025112916,
title = {High-precision malware detection in android apps using quantum explainable hierarchical interaction network},
journal = {Knowledge-Based Systems},
volume = {310},
pages = {112916},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.112916},
url = {https://www.sciencedirect.com/science/article/pii/S0950705124015508},
author = {Ramnath Muthusamy and Yesubai Rubavathi Charles},
keywords = {Recommender system, Android applications, Real or fake app detection, Privacy, Feature interaction, Quantum superposition and entanglement},
abstract = {The exponential growth of Android applications has increased the prevalence of fraudulent and malicious apps, posing significant risks to user security and privacy. Existing detection methodologies often struggle with poor interpretability, scalability, and computational complexity, limiting their effectiveness. To address these challenges, this study introduces the Quantum Explainable Hierarchical Interaction Network (QEHIN), a novel framework designed to detect real and fake Android applications with superior accuracy and interpretability. QEHIN incorporates quantum computing principles such as superposition and entanglement to model high-order feature interactions effectively. Its innovative architecture includes a Quantum Embedding Layer for transforming input features into quantum states, a Quantum Hierarchical Interaction Network (QHIN) for capturing complex dependencies, a Quantum Deep Neural Network (QDNN) for enhanced feature processing, and a Quantum Cross-Hierarchical Unit (QCHU) to ensure seamless integration across hierarchical levels. This design achieves precise, transparent, and scalable detection of malicious applications, addressing the shortcomings of traditional methods. Evaluation on the Google Play Store Reviews, MobileRec, and Android-App-Recommendation datasets demonstrates the novelty and effectiveness of QEHIN. It achieves an accuracy of 98.86 %, precision of 98.78 %, recall of 98.82 %, and a kappa score of 98.54 %, significantly outperforming existing approaches.}
}
@article{PANULAONTTO2019292,
title = {The AXIOM approach for probabilistic and causal modeling with expert elicited inputs},
journal = {Technological Forecasting and Social Change},
volume = {138},
pages = {292-308},
year = {2019},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2018.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S0040162518305870},
author = {Juha Panula-Ontto},
keywords = {Systems modeling, Modeling techniques, Decision support, Cross-impact analysis, Belief networks, Expert elicitation},
abstract = {Expert informants can be used as the principal information source in the modeling of socio-techno-economic systems or problems to support planning, foresight and decision-making. Such modeling is theory-driven, grounded in expert judgment and understanding, and can be contrasted with data-driven modeling approaches. Several families of approaches exist to enable expert elicited systems modeling with varying input information requirements and analytical ambitions. This paper proposes a novel modeling language and computational process, which combines aspects from various other approaches in an attempt to create a flexible and practical systems modeling approach based on expert elicitation. It is intended to have high fitness in modeling of systems that lack statistical data and exhibit low quantifiability of important system characteristics. AXIOM is positioned against Bayesian networks, cross-impact analysis, structural analysis, and morphological analysis. The modeling language and computational process are illustrated with a small example model. A software implementation is also presented.}
}
@article{HERNANDEZRAMIREZ2024414,
title = {The Future End of Design Work: A Critical Overview of Managerialism, Generative AI, and the Nature of Knowledge Work, and Why Craft Remains Relevant},
journal = {She Ji: The Journal of Design, Economics, and Innovation},
volume = {10},
number = {4},
pages = {414-440},
year = {2024},
issn = {2405-8726},
doi = {https://doi.org/10.1016/j.sheji.2024.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S2405872624000960},
author = {Rodrigo Hernández-Ramírez and João Batalheiro Ferreira},
keywords = {creativity, design work, generative artificial intelligence (GenAI), knowledge work, managerialism},
abstract = {This article examines the transformation of design work under the influence of managerialism and the rise of Generative Artificial Intelligence (GenAI). Drawing on John Maynard Keynes’s projections of technological unemployment and the evolving nature of work, it argues that despite advancements in automation, work has not diminished but rather devalued. Design, understood as a type of knowledge work, faces an apparent existential crisis. GenAI grows adept at mimicking the output of creative processes. The article explores how the fear of the end of design work fueled by the rise of GenAI is rooted in a misunderstanding of design work. This misunderstanding is driven by managerialism—an ideology that prioritizes efficiency and quantifiable outcomes over the intrinsic value of work. Managerialism seeks to instrumentalize and automate design, turning it into a controllable procedure to generate quantifiable creative outputs. The article argues why design work cannot be turned into a procedure and automated using GenAI. Advocates of these systems claim they enhance productivity and open new opportunities. However, evidence so far shows that flawed GenAI models produce disappointing outcomes while operating at a significant environmental cost. The article concludes by arguing for a robust theory of design—one that acknowledges the unique ontological and epistemic boundaries of design work and underscores why design cannot be reduced to a procedural output.}
}
@incollection{KOSSLYN1988615,
title = {Seeing and Imagining in the Cerebral Hemispheres: A Computational Approach},
editor = {Allan Collins and Edward E. Smith},
booktitle = {Readings in Cognitive Science},
publisher = {Morgan Kaufmann},
pages = {615-642},
year = {1988},
isbn = {978-1-4832-1446-7},
doi = {https://doi.org/10.1016/B978-1-4832-1446-7.50052-2},
url = {https://www.sciencedirect.com/science/article/pii/B9781483214467500522},
author = {Stephen M. Kosslyn}
}
@article{PUTICA2024105836,
title = {Reconceptualizing complex posttraumatic stress disorder: A predictive processing framework for mechanisms and intervention},
journal = {Neuroscience & Biobehavioral Reviews},
volume = {164},
pages = {105836},
year = {2024},
issn = {0149-7634},
doi = {https://doi.org/10.1016/j.neubiorev.2024.105836},
url = {https://www.sciencedirect.com/science/article/pii/S0149763424003051},
author = {Andrea Putica and James Agathos},
keywords = {Complex Posttraumatic Stress Disorder (C-PTSD), Predictive processing, Trauma, Interoceptive inference, Active inference},
abstract = {In this article, we introduce a framework for interpreting Complex Posttraumatic Stress Disorder (C-PTSD) through predictive processing, a neuroscience concept explaining the brain’s interpretation and prediction of sensory information. While closely related to PTSD, C-PTSD encompasses additional symptom clusters marked by disturbances in self-organization (DSO), such as negative self-concept, affect dysregulation, and relational difficulties, typically resulting from prolonged traumatic stressors. Our model leverages advances in computational psychiatry and neuroscience, offering a mechanistic explanation for these symptoms by illustrating how prolonged trauma disrupts the brain's predictive processing. Specifically, altered predictive mechanisms contribute to C-PTSD's symptomatology, focusing on DSO: (1) Negative self-concept emerges from maladaptive priors that bias perception towards self-criticism, misaligning expected and actual interoceptive states; (2) Misalignment between predicted and actual interoceptive signals leads to affect dysregulation, with sensitivity to bodily cues; and (3) Relationship challenges arise from skewed social prediction errors, fostering mistrust and withdrawal. This precision-focused approach sheds light on the dynamics underpinning C-PTSD and highlights potential intervention targets aimed at recalibrating the predictive processing system.}
}
@article{CAGNAC2023,
title = {Codes and methods improvements for safety assessment and LTO: varied approaches},
journal = {EPJ - Nuclear Sciences & Technologies},
volume = {9},
year = {2023},
issn = {2491-9292},
doi = {https://doi.org/10.1051/epjn/2023001},
url = {https://www.sciencedirect.com/science/article/pii/S2491929223000109},
author = {Albannie Cagnac and Denis Verrier and Vladislav Pištora},
abstract = {Nuclear safety has always been at the heart of the concerns of nuclear power plant operators and developers, as well as of various nuclear research organizations and regulatory authorities. Over the last decades, all these nuclear actors have developed and integrated a large number of calculation codes and other tools into their safety work. From the system approach to the local understanding of a phenomenon on a given component, from neutronics to operation optimization for long-term operation, these methods and codes have been constantly evolving since their appearance, in order to be able to integrate new plant designs and components, to improve the results of modeling physical phenomena or quantify and thus reduce the uncertainties on these results. Currently, several H2020 Euratom projects are working on the improvement of these codes and methods. This article will focus on three of these projects: CAMIVVER (Codes And Methods Improvements for VVER comprehensive safety assessment), APAL (Advanced PTS Analysis for LTO), and sCO2-4-NPP (innovative SCO2-based heat removal technology for an increased level of safety of Nuclear Power Plants) in order to illustrate our thinking on the improvement of calculation frameworks. First, we will present the work and the approach adopted with regard to the different calculation codes and methods used in each of these three projects. We will then conclude with an overall analysis of these three approaches, highlighting the difficulties and successes of these three projects, and identifying areas of work for the general improvement of the calculation codes.}
}
@article{DAI2024108354,
title = {Leveraging artificial intelligence (AI) in English as a foreign language (EFL) classes: Challenges and opportunities in the spotlight},
journal = {Computers in Human Behavior},
volume = {159},
pages = {108354},
year = {2024},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2024.108354},
url = {https://www.sciencedirect.com/science/article/pii/S074756322400222X},
author = {Kun Dai and Quanguo Liu},
keywords = {Artificial intelligence (AI), AI-Powered instruments, Challenges and opportunities, English as a foreign language (EFL) classes, EFL students},
abstract = {The widespread use of Artificial Intelligence (AI) in language education contexts has motivated several scholars around the world to uncover the advantages and disadvantages of AI and AI-powered instruments in different language classrooms. Yet, as the review of earlier investigations revealed, few inquiries have been carried out to divulge the pros and cons of leveraging AI in EFL classes. To narrow this gap, using the phenomenological approach, this inquiry investigated the opportunities and challenges of implementing AI in EFL classes from the perspective of Chinese EFL students. To do so, through the criterion sampling technique, a total of 45 EFL students was recruited from different educational institutions in China. To collect the dataset, participants were asked to complete an open-ended questionnaire. For the sake of triangulation, among the 45 participants, 15 were randomly selected to engage in a follow-up interview session. With the aid of MAXQDA software (version 2023), participants’ perceptions of AI opportunities and challenges were carefully analyzed. Overall, the analysis findings uncovered that leveraging AI in EFL classes can bring numerous opportunities for EFL students, including individualized learning, timely and immediate feedback, rich educational resources, and an interactive learning atmosphere. However, as demonstrated by the analysis outcomes, implementing AI in EFL courses may also face students with a range of challenges and problems. The research outcomes would be of great help to teachers and educational leaders in mitigating the challenges of leveraging AI in language classrooms.}
}
@article{THOMPSON2024939,
title = {Leveraging marine biotechnology for an All-Atlantic sustainable blue economy},
journal = {Trends in Biotechnology},
volume = {42},
number = {8},
pages = {939-941},
year = {2024},
issn = {0167-7799},
doi = {https://doi.org/10.1016/j.tibtech.2023.12.011},
url = {https://www.sciencedirect.com/science/article/pii/S0167779923003670},
author = {Cristiane Thompson and Alice C. Ortmann and Thulani Makhalanyane and Fabiano Thompson},
keywords = {All Atlantic, food security, biotechnology, low-carbon aquaculture, integrated multitrophic aquaculture, biofloc technology},
abstract = {Despite the lack of research, development, and innovation funds, especially in South Atlantic countries, the Atlantic is suited to supporting a sustainable marine bioeconomy. Novel low-carbon mariculture systems can provide food security, new drugs, and climate mitigation. We suggest how to develop this sustainable marine bioeconomy across the entire Atlantic.}
}
@article{PAIVIO2014141,
title = {Intelligence, dual coding theory, and the brain},
journal = {Intelligence},
volume = {47},
pages = {141-158},
year = {2014},
issn = {0160-2896},
doi = {https://doi.org/10.1016/j.intell.2014.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S0160289614001305},
author = {Allan Paivio},
keywords = {IQ theories, IQ tests, Conceptual/empirical flaw, DCT a unified theory, IQ neuroscience},
abstract = {The distinction between verbal and nonverbal cognitive abilities has been a defining feature of psychometric theories of intelligence for the past century. Despite their popularity, however, these theories have not included functional connections between verbal and nonverbal systems that are necessary if they are to explain performance in intellectual tasks involving interactions between language and nonverbal knowledge. This functional gap limits the capacity of psychometric theories to explain and predict fundamental aspects of individual differences in cognitive abilities that have long been studied experimentally. This article summarizes the history, nature, and possible causes of the problem, and then concludes with a neuroscientifically-enhanced, multimodal dual coding approach to intelligence that focuses on the synergistic interactivity of verbal and nonverbal systems.}
}
@article{HUHN201642,
title = {Cognitive framing in action},
journal = {Cognition},
volume = {151},
pages = {42-51},
year = {2016},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2016.02.015},
url = {https://www.sciencedirect.com/science/article/pii/S0010027716300439},
author = {John M. Huhn and Cory Adam Potts and David A. Rosenbaum},
keywords = {Action, Cognitive framing, Heuristics, Object manipulation, Motor control, Bimanual actions},
abstract = {Cognitive framing effects have been widely reported in higher-level decision-making and have been ascribed to rules of thumb for quick thinking. No such demonstrations have been reported for physical action, as far as we know, but they would be expected if cognition for physical action is fundamentally similar to cognition for higher-level decision-making. To test for such effects, we asked participants to reach for a horizontally-oriented pipe to move it from one height to another while turning the pipe 180° to bring one end (the “business end”) to a target on the left or right. From a physical perspective, participants could have always rotated the pipe in the same angular direction no matter which end was the business end; a given participant could have always turned the pipe clockwise or counter-clockwise. Instead, our participants turned the business end counter-clockwise for left targets and clockwise for right targets. Thus, the way the identical physical task was framed altered the way it was performed. This finding is consistent with the hypothesis that cognition for physical action is fundamentally similar to cognition for higher-level decision-making. A tantalizing possibility is that higher-level decision heuristics have roots in the control of physical action, a hypothesis that accords with embodied views of cognition.}
}
@article{AUGIER2001307,
title = {Sublime Simon: The consistent vision of economic psychology's Nobel laureate},
journal = {Journal of Economic Psychology},
volume = {22},
number = {3},
pages = {307-334},
year = {2001},
issn = {0167-4870},
doi = {https://doi.org/10.1016/S0167-4870(01)00036-8},
url = {https://www.sciencedirect.com/science/article/pii/S0167487001000368},
author = {Mie Augier},
keywords = {Herbert Simon, Bounded rationality, Carnegie Mellon University, Economics and psychology},
abstract = {This essay contains a study of some of Herbert Simon's ideas, with particular emphasis on the role of bounded rationality in Simon's thinking and his contributions to economics and psychology. I describe Simon's visions for challenging rational choice theory, through limited rationality, and for bringing psychology into economics, putting this in perspective by describing the evolution of some of this thoughts, focusing on the continuity in his work.}
}
@incollection{ZIEGLERRODRIGUEZ2025169,
title = {Chapter 6 - Life cycle assessment of constructed wetlands: measuring their contribution to sustainable development},
editor = {Asheesh Kumar Yadav and Jan Vymazal and Yaqian Zhao and Pratiksha Srivastava},
booktitle = {Emerging Developments in Constructed Wetlands},
publisher = {Elsevier},
pages = {169-193},
year = {2025},
isbn = {978-0-443-14078-5},
doi = {https://doi.org/10.1016/B978-0-443-14078-5.00006-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780443140785000064},
author = {Kurt Ziegler-Rodriguez and Marianna Garfí},
keywords = {Sustainable development, life cycle assessment, constructed wetland, biological waste treatment, water management},
abstract = {Life cycle thinking has led to the development of a series of methodologies that evaluate the sustainability of any process, product, or activity, considering the three aspects of sustainable development: the environmental, economic, and social pillars. These methodologies called the (Environmental) Life Cycle Assessment, the Social Life Cycle Assessment and the Life Cycle Costing, have the peculiarity to consider the whole life cycle of a product or process, from the extraction of raw materials to their end of life. At the same time, sustainable development has led to the strengthening of disciplines and novel technologies such as circular bioeconomy, industrial ecology, and nature-based solutions. In this context, constructed wetlands have been gaining popularity since they are a low-cost alternative for urban and industrial wastewater treatment in small communities. The performed life cycle assessments of these technologies have shown that, regardless of the model, configuration, or type of waste treated, they have low environmental impacts compared with conventional solutions (e.g., activated sludge system) due to low energy requirements, no chemicals consumption, and avoidance of off-site management and transportation practices. In terms of costs, constructed wetlands can drastically reduce the costs associated with wastewater treatment and management. However, more efforts should be made in order to define the social benefits of this technology (e.g., local employment generation, landscape improvement) and the quality of the recovered resources (e.g., treated water, fertilizer).}
}
@article{ASOULIN201998,
title = {Phrase structure grammars as indicative of uniquely human thoughts},
journal = {Language Sciences},
volume = {74},
pages = {98-109},
year = {2019},
issn = {0388-0001},
doi = {https://doi.org/10.1016/j.langsci.2019.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S0388000119300117},
author = {Eran Asoulin},
keywords = {Thought, Cognition, Phrase structure grammars, Chomsky hierarchy, Animal cognition},
abstract = {I argue that the ability to compute phrase structure grammars is indicative of a particular kind of thought. This type of thought that is only available to cognitive systems that have access to the computations that allow the generation and interpretation of the structural descriptions of phrase structure grammars. The study of phrase structure grammars, and formal language theory in general, is thus indispensable to studies of human cognition, for it makes explicit both the unique type of human thought and the underlying mechanisms in virtue of which this thought is made possible.}
}
@article{1985174,
title = {Learning to use a word processor: by doing, by thinking, and by knowing: John M. Carroll and Robert L. Mack: Rep. RC 9481, IBM T.J. Watson Research Center, Yorktown Heights, New York 10598, U.S.A., (July 1982)},
journal = {Decision Support Systems},
volume = {1},
number = {2},
pages = {174},
year = {1985},
issn = {0167-9236},
doi = {https://doi.org/10.1016/0167-9236(85)90071-5},
url = {https://www.sciencedirect.com/science/article/pii/0167923685900715}
}
@incollection{VANLOAN1992247,
title = {Chapter 6 A survey of matrix computations},
series = {Handbooks in Operations Research and Management Science},
publisher = {Elsevier},
volume = {3},
pages = {247-321},
year = {1992},
booktitle = {Computing},
issn = {0927-0507},
doi = {https://doi.org/10.1016/S0927-0507(05)80203-8},
url = {https://www.sciencedirect.com/science/article/pii/S0927050705802038},
author = {Charles {Van Loan}},
abstract = {Publisher Summary
This chapter presents three-level introduction to the field of matrix computations. The chapter discusses analytic and computational tools that underpin numerical linear algebra. Low dimension examples are the rule with appropriate generalizations to follow. The central themes include (a) the language of matrix factorizations, (b) the art of introducing zeros into a matrix, (c) the exploitation of structure, and (d) the distinction between problem sensitivity and algorithmic stability. Matrix factorizations that play a central role in numerical linear algebra are also presented in the chapter. The chapter also discusses factorization. For each factorization, algorithms are surveyed, associated mathematical properties, and applications are discussed. One factorization (Chotesky) is used to illustrate various aspects of high performance matrix computations. Successful computing requires the design of codes that pay careful attention to the flow of data during execution.}
}
@article{YIN2022109800,
title = {Deep learning-accelerated optimization algorithm for controller parameters optimization of doubly-fed induction generators},
journal = {Applied Soft Computing},
volume = {131},
pages = {109800},
year = {2022},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2022.109800},
url = {https://www.sciencedirect.com/science/article/pii/S1568494622008493},
author = {Linfei Yin and Xinghui Cao and Senlin Wang},
keywords = {Deep fully connected models, Gray wolf optimizer, Adaptive differential evolution, Global search, Parameter optimization},
abstract = {In this work, a cooperative Gray wolf Optimizer with adaptive differential Evolution (GOE) is proposed for the multimodal controller parameters optimization of doubly-fed induction generators (DFIGs) based on maximum power point tracking (MPPT) strategies. Moreover, the optimization process of the GOE is accelerated by a deep fully connected model (DFCM). The GOE contains a cooperative gray wolf optimizer (GWO) and adaptive differential evolution (ADE). The cooperative GWO contains alpha, beta, delta, and omega wolves to explore and exploit optimization problems and achieves optimization tasks wider and deeper than GWO. The ADE cooperates with the cooperative GWO to solve global optimization over continuous spaces. The simulation results on seven uni-model benchmark functions show that the GOE accelerated by DFCM obtains acceptable fitness values with 39.99% lesser computation time than the symmetry adapted stochastic search (SASS) algorithm and 80.72% lesser computation time than the Lévy flights-success-history based adaptive differential evolution with constraint handling technique (COLSHADE) algorithm, which are the winners of the CEC2020 Competition on Real-World Single Objective Constrained Optimization. Furthermore, the simulation results on DFIG with MPPT strategies in three real-world cases verify that the GOE accelerated by DFCM can effectively obtain global optimization solutions for non-smooth problems with 99.51% lesser average computation time than the SASS algorithm, 99.63% less than the COLSHADE algorithm, and 89.52% less than other methods. In addition, the accelerated GOE algorithm by DFCM has the feature of faster convergence.}
}
@incollection{BALANAY201949,
title = {3 - Tools for circular economy: Review and some potential applications for the Philippine textile industry},
editor = {Subramanian Senthilkannan Muthu},
booktitle = {Circular Economy in Textiles and Apparel},
publisher = {Woodhead Publishing},
pages = {49-75},
year = {2019},
series = {The Textile Institute Book Series},
isbn = {978-0-08-102630-4},
doi = {https://doi.org/10.1016/B978-0-08-102630-4.00003-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780081026304000030},
author = {Raquel Balanay and Anthony Halog},
keywords = {Circular economy, Industrial sustainability, Life cycle thinking, Sustainable development, Systems modelling, Textile industry},
abstract = {Instituting circular economy (CE) for sustainability is the aim of taking stock of various analytical/assessment tools. A review of these tools reveals a continuing endeavor to incorporate in the procedures the systems and life cycle thinking and the triple bottom-line framework of sustainable development (economic, social, and environmental). Over time, the CE tools have been modified with the incorporation of some unique attributes in the cases being studied. Life cycle assessment (LCA) remains the popular and the only standardized procedure to analyze CE issues in industries, specifically in the environmental aspect. However, consistency, measurement, and aggregation issues are the major setbacks of having an integrated LCA for economic, social, and environmental impacts. The alternative tools used across the world to study the economic, social, and environmental aspects of CE have increased in both number and sophistication. Optimization and systems models have been increasingly used on a case-based format. Although the downside is the less standardized approach with less chances of comparability in terms of results, these models have been designed appropriately to tackle challenges associated with intricate, multifaceted, and encompassing sustainability and CE issues to improve policy development. In the textile industry, LCA as a popular tool is only used for environmental sustainability assessment but not much in social and economic aspects. The Philippine textile industry still has to catch up in the application of those tools for sustainability assessment. A framework has been suggested for the country's roadmap/guide to attain circularity in textile industry operations.}
}
@article{SIMS1991383,
title = {Computers and experiments in stress analysis: Eds G. M. Carlomagno and C. A. Brebbia Computational Mechanics Publications, Southampton, UK},
journal = {Engineering Structures},
volume = {13},
number = {4},
pages = {383-384},
year = {1991},
issn = {0141-0296},
doi = {https://doi.org/10.1016/0141-0296(91)90027-A},
url = {https://www.sciencedirect.com/science/article/pii/014102969190027A},
author = {P. Sims}
}
@article{LI2024124918,
title = {A method of dense point cloud SLAM based on improved YOLOV8 and fused with ORB-SLAM3 to cope with dynamic environments},
journal = {Expert Systems with Applications},
volume = {255},
pages = {124918},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.124918},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424017858},
author = {Yanke Li and Huabo Shen and Yaping Fu and Kai Wang},
keywords = {SLAM, VSLAM, Neural Network, Deep learning},
abstract = {With the development of society and the advancement of technology, intelligent robots have been widely used in various fields. At the same time, Simultaneous Localization and Mapping (SLAM) technology is a key technology in the research field of intelligent robots. However, in dynamic environments, achieving accurate and robust visual SLAM remains a major challenge. In this paper, we propose a method based on improved YOLOv8 fused with ORB-SLAM3 to address dense point cloud SLAM in dynamic environments. Our proposed method successfully integrates real-time object detection and image segmentation technologies of YOLOv8 into the ORB-SLAM3 framework, achieving high-precision and robust visual SLAM in dynamic environments. In the YOLOv8 framework, we use a balanced convolution method, GSConv, instead of some traditional convolution layers (Conv), which balances accuracy with computational load. Based on the GSConv convolution method, we adopt a new feature fusion module, VoVGSCSP, to replace traditional C2f feature fusion modules, thereby improving the Neck structure of YOLOv8 and achieving a lightweight network model. We compare our proposed method with ORB-SLAM3 and some computer vision algorithms on the TUM dataset. Experimental data confirms that our method outperforms existing visual SLAM algorithms in dynamic environments. In fast-moving dynamic environments, the RMSE of absolute pose estimation of our method is 96.28% lower than that of ORB-SLAM3, and the RMSE of relative pose estimation is 51.57% lower than that of ORB-SLAM3. The experimental results demonstrate that our method significantly improves the accuracy of pose estimation in dynamic environments and greatly enhances the performance compared to ORB-SLAM3.}
}
@article{LIU2025113288,
title = {Knowledge-based natural answer generation via effective graph learning},
journal = {Knowledge-Based Systems},
volume = {316},
pages = {113288},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.113288},
url = {https://www.sciencedirect.com/science/article/pii/S0950705125003351},
author = {Zedong Liu and Jianxin Li and Yongle Huang and Ningning Cui and Lili Pei},
keywords = {Natural answer generation, Adaptive multi-hop retrieval, Graph-based Mamba, Prompt optimization},
abstract = {Objectives:
Natural Answer Generation (NAG) aims to generate natural and fluent answers to user questions. Existing NAG methods typically employ fixed-hop retrieval to construct knowledge graphs and utilize attention-based networks for answer generation. However, these approaches lack interpretability, struggle to filter out redundant information in the graph, and are computationally intensive.
Methods:
To address these issues, this paper introduces an innovative approach AdaptQA model. Initially, AdaptQA constructs a knowledge graph from the knowledge base (KB) using an adaptive multi-hop retrieval algorithm. Subsequently, it generates answers through the Graph-based Mamba module (GBM), effectively filtering out redundant information. Finally, the answers are optimized using a pre-trained large language model to enhance their fluency and accuracy.
Novelty:
The proposed AdaptQA model introduces a new approach to NAG by improving the completeness of the knowledge graph and optimizing question answers. This method overcomes the limitations of existing NAG techniques by reducing the complexity of model inference.
Findings:
Through extensive experiments on two benchmark datasets, HotpotQA and WikiHop, AdaptQA demonstrates superior performance, significantly outperforming existing NAG methods. Specifically, AdaptQA achieves an accuracy of 94.47% on the HotpotQA dataset and 91.38% on the WikiHop dataset.}
}
@article{RABOY201736,
title = {An introductory microeconomics in-class experiment to reinforce the marginal utility/price maximization rule and the integration of modern theory},
journal = {International Review of Economics Education},
volume = {24},
pages = {36-49},
year = {2017},
issn = {1477-3880},
doi = {https://doi.org/10.1016/j.iree.2016.10.003},
url = {https://www.sciencedirect.com/science/article/pii/S1477388016300494},
author = {David G. Raboy},
keywords = {Experimental economics, Modern microeconomics, Principles classes, Alternative pedagogy},
abstract = {This paper presents an in-class experiment used as a teaching tool in an introductory microeconomics class at the undergraduate college level. It is directed at a critical but challenging concept for principles students—constrained utility maximization and a methodology to intuit preferences. The experimental project is nested in the literature pertaining to the current transition in microeconomic theory motivated by contributions from behavioral economics and transactions-cost economics, among other elements; modern pedagogical models; experimental economics; and experiments as in-classroom teaching tools. While not dispositive as to the general efficacy of in-class experiments, the paper provides an example of an alternative instructional approach which is helpful to principles students under strictly defined protocols. The benefits to students include heightened understanding of the core subject topic, greater interest in the subject matter, a closer connection to real-world economics, and enhanced critical thinking capabilities.}
}
@incollection{NICHELLI2016379,
title = {Chapter 23 - Consciousness and Aphasia},
editor = {Steven Laureys and Olivia Gosseries and Giulio Tononi},
booktitle = {The Neurology of Conciousness (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {San Diego},
pages = {379-391},
year = {2016},
isbn = {978-0-12-800948-2},
doi = {https://doi.org/10.1016/B978-0-12-800948-2.00023-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128009482000236},
author = {Paolo Nichelli},
keywords = {language impairment, anarthria, dynamic aphasia, fMRI, neurophysiological measures},
abstract = {Different language impairments allow us to investigate how much the use of language can influence the content of conscious awareness and therefore of thinking and reasoning. Pure anarthria (different from mutism) and verbal short-term memory deficits are associated with an impairment of the effect of covert speech on the content of working memory. Dynamic aphasia impairs the processes involved in the transition between thinking and speaking. However, even the most severe agrammatic patients can retain reasoning about others’ beliefs that according to some theories can only take place in explicit sentences of a natural language. Error monitoring is also impaired in many aphasic patients and in some of them is associated with complete lack of error awareness (anosognosia for aphasia). In patients with impaired consciousness whenever language examination is impossible or unreliable, fMRI and neurophysiological measures such as event-related potentials can provide a window for examining residual language capabilities.}
}
@incollection{SHAYNAROSENBAUM201787,
title = {2.06 - Episodic and Semantic Memory},
editor = {John H. Byrne},
booktitle = {Learning and Memory: A Comprehensive Reference (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {Oxford},
pages = {87-118},
year = {2017},
isbn = {978-0-12-805291-4},
doi = {https://doi.org/10.1016/B978-0-12-809324-5.21037-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128093245210377},
author = {R. {Shayna Rosenbaum} and Alice S.N. Kim and Stevenson Baker},
keywords = {Aging, Amnesia, Autobiographical memory, Autonoetic consciousness, Child development, Default mode network, Familiarity, fMRI, Future imagining, Hippocampus, Medial temporal lobe, Mental time travel, Patient K.C., Personal semantic memory, Recollection, Spatial memory, Temporal neocortex},
abstract = {Much of the richness in human life derives from episodic memory, mental representations of detailed experiences from our personal pasts. To make sense of those experiences, knowledge about the world and oneself must also exist in a form that is free of context – known as semantic memory. This chapter revisits and builds on Tulving's distinction between episodic and semantic memory, with a focus on their differences, similarities, and interactions, informed by cognitive, neuropsychological, and neuroimaging studies. Extensions of this distinction into spatial memory, and beyond memory into future thinking, are considered in the context of process views of memory organization.}
}
@incollection{CATTANEO2015220,
title = {Mental Imagery: Visual Cognition},
editor = {James D. Wright},
booktitle = {International Encyclopedia of the Social & Behavioral Sciences (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {220-227},
year = {2015},
isbn = {978-0-08-097087-5},
doi = {https://doi.org/10.1016/B978-0-08-097086-8.57024-X},
url = {https://www.sciencedirect.com/science/article/pii/B978008097086857024X},
author = {Zaira Cattaneo and Juha Silvanto},
keywords = {Brain stimulation, Creative thinking, Depictivist, Imagery, Imagery debate, Mathematics, Memory, Neuroimaging, Occipital cortex, Perception, Propositional, Reasoning, Vision, Visual cognition, Working memory},
abstract = {Mental imagery can be defined as a quasi-perceptual experience occurring in the absence of perceptual input. The present article provides a review of the key processes involved in mental imagery, the relationship of imagery to working memory, and of the debate on the underlying format of mental images. We also review the functional significance of imagery in a range of cognitive processes, such as memory, creative thinking, reasoning, and problem solving. Finally, the brain basis of mental imagery and its overlap with the cortical regions involved in visual perception are discussed.}
}
@article{MEARA2000345,
title = {Vocabulary and neural networks in the computational assessment of texts written by second-language learners},
journal = {System},
volume = {28},
number = {3},
pages = {345-354},
year = {2000},
issn = {0346-251X},
doi = {https://doi.org/10.1016/S0346-251X(00)00016-6},
url = {https://www.sciencedirect.com/science/article/pii/S0346251X00000166},
author = {Paul Meara and Catherine Rodgers and Gabriel Jacobs},
keywords = {Neural network, Computational assessment, Vocabulary, French},
abstract = {This paper explores the potential of a neural network in language assessment. Many examination systems rely on subjective judgments made by examiners as a way of grading the writing of non-native speakers. Some research (e.g. Engber, 1995. The relationship of lexical proficiency to the quality of ESL compositions. Journal of Second Language Writing 4(2), 139–155) has shown that these subjective judgements are influenced to a very large extent by the lexical choices made by candidates. We took Engber's basic model, but automated the evaluation of lexical content. A group of non-native speakers of French were asked to produce a short text in response to a picture stimulus. The texts were graded by French native speaker teachers. We identified a number of words which occurred in about half the texts, and coded each text for the occurrence and non-occurrence of each word. We then trained a neural network to grade the texts on the basis of these codings. The results suggest that it might be possible to teach a neural network to mimic the judgements made by human markers.}
}
@article{WHITACRE201747,
title = {Integer comparisons across the grades: Students’ justifications and ways of reasoning},
journal = {The Journal of Mathematical Behavior},
volume = {45},
pages = {47-62},
year = {2017},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2016.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S0732312316301742},
author = {Ian Whitacre and Beti Azuz and Lisa L.C. Lamb and Jessica Pierson Bishop and Bonnie P. Schappelle and Randolph A. Philipp},
keywords = {Integers, Negative numbers, Children’s mathematical thinking, Order, Magnitude},
abstract = {This study is an investigation of students’ reasoning about integer comparisons—a topic that is often counterintuitive for students because negative numbers of smaller absolute value are considered greater (e.g., −5>−6). We posed integer-comparison tasks to 40 students each in Grades 2, 4, and 7, as well as to 11th graders on a successful mathematics track. We coded for correctness and for students’ justifications, which we categorized in terms of 3 ways of reasoning: magnitude-based, order-based, and developmental/other. The 7th graders used order-based reasoning more often than did the younger students, and it more often led to correct answers; however, the college-track 11th graders, who responded correctly to almost every problem, used a more balanced distribution of order- and magnitude-based reasoning. We present a framework for students’ ways of reasoning about integer comparisons, report performance trends, rank integer-comparison tasks by relative difficulty, and discuss implications for integer instruction.}
}
@article{CHRISTENSEN2025102467,
title = {perms: Likelihood-free estimation of marginal likelihoods for binary response data in Python and R},
journal = {Journal of Computational Science},
volume = {84},
pages = {102467},
year = {2025},
issn = {1877-7503},
doi = {https://doi.org/10.1016/j.jocs.2024.102467},
url = {https://www.sciencedirect.com/science/article/pii/S1877750324002606},
author = {Dennis Christensen and Per August Jarval Moen},
keywords = {Binary classification, Bioassay, Marginal likelihood estimation, Permutation counting},
abstract = {In Bayesian statistics, the marginal likelihood (ML) is the key ingredient needed for model comparison and model averaging. Unfortunately, estimating MLs accurately is notoriously difficult, especially for models where posterior simulation is not possible. Recently, the idea of permutation counting was introduced, which provides an estimator which can accurately estimate MLs of models for exchangeable binary responses. Such data arise in a multitude of statistical problems, including binary classification, bioassay and sensitivity testing. Permutation counting is entirely likelihood-free and works for any model from which a random sample can be generated, including nonparametric models. Here we present perms, a package implementing permutation counting. Following optimisation efforts, perms is computationally efficient and can handle large data problems. It is available as both an R package and a Python library. A broad gallery of examples illustrating its usage is provided, which includes both standard parametric binary classification and novel applications of nonparametric models, such as changepoint analysis. We also cover the details of the implementation of perms and illustrate its computational speed via a simple simulation study.}
}
@article{MALEKSHAHIAN2025,
title = {Bridging the Skills Gap: Enhancing Employability for Chemical Engineering Graduates},
journal = {Education for Chemical Engineers},
year = {2025},
issn = {1749-7728},
doi = {https://doi.org/10.1016/j.ece.2025.04.005},
url = {https://www.sciencedirect.com/science/article/pii/S174977282500017X},
author = {Maryam Malekshahian and Jessica Dautelle and Salman Shahid},
keywords = {Employability Skills, Chemical Engineering Education, Transferable Skills, Curriculum Development, Industry Readiness},
abstract = {Extensive research underscores a persistent skills gap among graduates across various disciplines. However, identifying the precise skill gaps in engineering education remains challenging due to inconsistencies in existing research, and studies specifically addressing employability skills in chemical engineering are limited. This study aims to address these knowledge gaps by identifying the critical employability skills necessary for chemical engineering graduates. The study employs a multi-method approach, incorporating a systematic literature review, surveys of students, alumni, and employers, and a statistical analysis of job advertisements for graduate positions. The objective is to establish a comprehensive understanding of required competencies and evaluate the alignment between employer expectations and graduate competencies. A structured skill framework was developed, encompassing 15 primary skill groups and over 75 sub-skills. Comparative analysis of employer perceptions and job advertisement data highlighted discrepancies in perceived versus stated skill priorities. However, competencies such as communication, interpersonal skills, self-management, and adaptability were consistently recognised as essential across sectors. Significant skill gaps were observed in areas such as communication, problem-solving, literacy, interpersonal, self-management, and business acumen. Survey findings indicate that engineering students often overestimate their technical proficiency while underestimating the importance of transferable skills such as resilience, ethics, and integrity. Conversely, employers consistently emphasise the need for a well-rounded skillset that integrates technical expertise with strong communication and management capabilities. This disconnect underscores the need for educational programmes to promote greater self-awareness among students and ensure their skill development aligns with industry demands. These results align with existing literature, reinforcing the importance of embedding transferable skills within engineering curricula to better prepare graduates for professional success.}
}
@incollection{JAMES2025,
title = {Genetic Basis of Adaptation},
booktitle = {Reference Module in Life Sciences},
publisher = {Elsevier},
year = {2025},
isbn = {978-0-12-809633-8},
doi = {https://doi.org/10.1016/B978-0-443-15750-9.00112-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780443157509001129},
author = {Maddie E. James and Daniel Ortiz-Barrientos},
keywords = {Genetic architecture, Genotype-phenotype-fitness landscape, Natural selection, Adaptive evolution, Phenotypic variation, Monogenic, Oligogenic, Polygenic and omnigenic adaptation, Comparative and functional genomics, Genetic mapping and functional validation, Experimental evolution and field transplants, Predictive evolutionary genomics},
abstract = {This chapter provides an overview of the genetic basis of adaptation, from understanding variation in nature to implementing cutting-edge methodologies that uncover genetic complexities. It highlights the importance of linking genotypes to phenotypes to fitness and overviews the diverse genetic architectures underlying traits. The chapter outlines critical concepts such as trait correlations and genetic redundancy while emphasizing the complementary approaches of comparative and functional genomics, genetic mapping, functional validation, and manipulative experiments to understand adaptation in natural systems. Through examples across diverse organisms, knowledge from this chapter will help readers expand their evolutionary thinking of evolutionary change with implications for conservation, agriculture, and medicine.}
}
@article{KERREN2025,
title = {Exploring the role of dimensionality transformation in episodic memory},
journal = {Trends in Cognitive Sciences},
year = {2025},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2025.01.007},
url = {https://www.sciencedirect.com/science/article/pii/S136466132500021X},
author = {Casper Kerrén and Daniel Reznik and Christian F. Doeller and Benjamin J. Griffiths},
keywords = {episodic memory, dimensionality reduction and dimensionality expansion, neural oscillations, corticohippocampal connectivity, neural representations},
abstract = {Episodic memory must accomplish two adversarial goals: encoding and storing a multitude of experiences without exceeding the finite neuronal structure of the brain, and recalling memories in vivid detail. Dimensionality reduction and expansion (‘dimensionality transformation’) enable the brain to meet these demands. Reduction compresses sensory input into simplified, storable codes, while expansion reconstructs vivid details. Although these processes are essential to memory, their neural mechanisms for episodic memory remain unclear. Drawing on recent insights from cognitive psychology, systems neuroscience, and neuroanatomy, we propose two accounts of how dimensionality transformation occurs in the brain: structurally (via corticohippocampal pathways) and functionally (through neural oscillations). By examining cross-species evidence, we highlight neural mechanisms that may support episodic memory and identify crucial questions for future research.}
}
@article{SELESNICK2012115,
title = {Quantum-like logics and schizophrenia},
journal = {Journal of Applied Logic},
volume = {10},
number = {1},
pages = {115-126},
year = {2012},
note = {Special issue on Automated Specification and Verification of Web Systems},
issn = {1570-8683},
doi = {https://doi.org/10.1016/j.jal.2011.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S1570868311000656},
author = {S.A. Selesnick and G.S. Owen},
keywords = {Logic, Quantum logic, Linear logic, Schizophrenia},
abstract = {Many researchers in different disciplines have independently concluded that brains are, possibly among other things, vector processing devices. In this paper we offer support for this hypothesis coming from a new perspective. Namely, we test it against some known anomalies in the processing by schizophrenic patients of certain logical tasks: they perform better at them than normal controls, despite the observation that they do not generally employ “normal” or “commonsense” logic. On the assumption that they are compelled to use the intrinsic logic of the brain instead of commonsense logic, and that this logic is linear or quantum-like, we are able to resolve these and other anomalies. Our conclusions support the idea that human brains (at least) perform intrinsic logical operations according to the dictates of a linear (or Grassmannian, or quantum-like) logic rather than “classical” or Aristotelian logic (which seems not to be intrinsic to brains, these having evolved under the pressure of different constraints). If this is the case, then commonsense logic must be acquired through experience and the construction of contexts, an ability schizophrenic patients seem to lack, and who are consequently compelled to rely on the intrinsic logic, which is quantum-like and more efficient at certain tasks. Moreover, the proclivity toward errors of von Domarus type (namely the inference that shared attributes imply identity), which seems to be endemic to human thinking and has been discussed in connection with schizophrenia, is also explained on this basis.}
}
@incollection{TAYLOR1995227,
title = {Chapter 13 - Computational needs for process tomography},
editor = {R.A. Williams and M.S. Beck},
booktitle = {Process Tomography},
publisher = {Butterworth-Heinemann},
address = {Oxford},
pages = {227-249},
year = {1995},
isbn = {978-0-08-093801-1},
doi = {https://doi.org/10.1016/B978-0-08-093801-1.50017-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780080938011500174},
author = {R.W. Taylor}
}
@article{SANCHIS2023102162,
title = {Towards a general equilibrium theory of allocation of time for the digital revolution era},
journal = {Technology in Society},
volume = {72},
pages = {102162},
year = {2023},
issn = {0160-791X},
doi = {https://doi.org/10.1016/j.techsoc.2022.102162},
url = {https://www.sciencedirect.com/science/article/pii/S0160791X22003037},
author = {Raúl G. Sanchis},
keywords = {Household economics, Time allocation, Consumer behaviour, Firm behaviour, General equilibrium},
abstract = {The Digital Revolution we are witnessing has started a new era in modern societies and economies. Time inputs, whether these are from human beings and non-human, electronical or mechanical devices are increasingly more important, especially –but not uniquely– in most advanced economies and societies. Existing economic theory strives to accommodate time inputs into mainstream economic theory. This paper contributes to the existing literature on time allocation theoretical models by suggesting a general equilibrium framework likely to respond to some existing challenges in modern economies. In the general equilibrium modelling process, some improvements are made to time allocation models from the consumer side which concern the inclusion of non-human time inputs and multitasking, and a novel development on a producer theory of allocation of time is designed to determine the underpinnings of a computationally tractable general equilibrium theory of allocation of time. Both the solution and usefulness of this work will require the help of cutting-edge computational techniques in future work.}
}
@article{AKANDA2025112329,
title = {Understanding comment practices in Scratch: A study of comments in a block-based visual programming language},
journal = {Journal of Systems and Software},
volume = {222},
pages = {112329},
year = {2025},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2024.112329},
url = {https://www.sciencedirect.com/science/article/pii/S016412122400373X},
author = {Wahiduzzaman Akanda and James Clause},
keywords = {Comment, Text-based programming, Visual programming, Scratch, Taxonomy},
abstract = {Comments are vital for software documentation. They provide necessary insights and assist developers in understanding and maintaining the software. Due to their importance, comments have been extensively studied, and much has been learned about them. These existing studies have predominantly focused on text-based languages. Conversely, block-based visual programming languages, particularly Scratch, are becoming increasingly popular. Some studies regarding comments related to the Scratch online community focus on topics such as fostering online community and engagement, sentiment analysis, etc. However, they overlook the visual aspects and the qualitative analysis of comments within code in Scratch projects. This is a meaningful limitation, and this research project studies comments and their pattern in Scratch projects from both textual and visual perspectives. We examined comments collected from different Scratch projects. Each comment was manually annotated based on textual and visual attributes, producing a taxonomy model of comments for a visual programming language. The classification results were analyzed to understand better the practice of commenting in Scratch. Our result revealed that Scratch projects produced noisier(i.e., less understandable) comments than text-based programming languages like Java. In addition, the study also revealed several limitations and shortcomings that could be addressed to improve the commenting experience in Scratch.}
}
@article{SHI2023926,
title = {Decoding Human Biology and Disease Using Single-cell Omics Technologies},
journal = {Genomics, Proteomics & Bioinformatics},
volume = {21},
number = {5},
pages = {926-949},
year = {2023},
issn = {1672-0229},
doi = {https://doi.org/10.1016/j.gpb.2023.06.003},
url = {https://www.sciencedirect.com/science/article/pii/S1672022923001043},
author = {Qiang Shi and Xueyan Chen and Zemin Zhang},
keywords = {Single-cell omics, Computational method, Cellular heterogeneity, Disease, Cancer research},
abstract = {Over the past decade, advances in single-cell omics (SCO) technologies have enabled the investigation of cellular heterogeneity at an unprecedented resolution and scale, opening a new avenue for understanding human biology and disease. In this review, we summarize the developments of sequencing-based SCO technologies and computational methods, and focus on considerable insights acquired from SCO sequencing studies to understand normal and diseased properties, with a particular emphasis on cancer research. We also discuss the technological improvements of SCO and its possible contribution to fundamental research of the human, as well as its great potential in clinical diagnoses and personalized therapies of human disease.}
}
@incollection{YELLA2022770,
title = {2.32 - Magic bullets: Drug repositioning and drug combinations},
editor = {Terry Kenakin},
booktitle = {Comprehensive Pharmacology},
publisher = {Elsevier},
address = {Oxford},
pages = {770-788},
year = {2022},
isbn = {978-0-12-820876-2},
doi = {https://doi.org/10.1016/B978-0-12-820472-6.00116-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012820472600116X},
author = {Jaswanth K. Yella and Anil G. Jegga},
keywords = {Artificial intelligence, Computer-aided drug synthesis, COVID-19, De novo drug discovery, Drug combinations, Drug repurposing, Drug synergy, Machine learning, Network analysis},
abstract = {Discovery and development of novel pharmaceuticals continue to be a very costly, time-consuming and uncertain process impacting negatively not only the research and development of pharmaceutical industry but also health care. Although the number of novel drugs approved each year has grown over by as much as 60% compared to the past decade, there are still many diseases that do not have any approved drug. Recent technological advances in the biomedical, genomics, and computational science domains accompanied by multisource and multidimensional data opened new opportunities and challenges. The drug discovery paradigm is increasingly shifting from hypothesis-driven to data-driven approaches. While the search for the magic bullets of medicine continues, the magic—crunching the data deluge into knowledge and hypotheses nuggets—is mostly driven by machines and machine intelligence. This review will primarily focus on three facets of computational drug discovery approaches, namely, drug repositioning, de novo drug discovery, and drug combinations, and reflect on computational approaches which are reproducible and seem most promising for the machine learning-driven drug discovery. Finally, using COVID-19 as an example, we discuss how the computational approaches are aiding and accelerating the process of discovery of magic bullet(s) for this dreadful pandemic.}
}
@article{SELVERSTON1988109,
title = {A consideration of invertebrate central pattern generators as computational data bases},
journal = {Neural Networks},
volume = {1},
number = {2},
pages = {109-117},
year = {1988},
issn = {0893-6080},
doi = {https://doi.org/10.1016/0893-6080(88)90013-5},
url = {https://www.sciencedirect.com/science/article/pii/0893608088900135},
author = {Allen I Selverston},
abstract = {The essential features of real neural networks are discussed with respect to their usefulness for connectionist modeling. These features are broken down into cellular and synaptic properties and related to a form of neural circuit known as central pattern generators. The gastric and pyloric rhythm of the lobster stomatogastric system are presented as possible computational data bases for modeling studies.}
}
@article{ROTHMCDUFFIE2018173,
title = {Middle school mathematics teachers’ orientations and noticing of features of mathematics curriculum materials},
journal = {International Journal of Educational Research},
volume = {92},
pages = {173-187},
year = {2018},
issn = {0883-0355},
doi = {https://doi.org/10.1016/j.ijer.2018.09.019},
url = {https://www.sciencedirect.com/science/article/pii/S0883035518305512},
author = {Amy {Roth McDuffie} and Jeffrey Choppin and Corey Drake and Jon Davis},
keywords = {Curriculum, Curriculum analysis, Teacher orientation, Middle school mathematics, Teacher noticing},
abstract = {We report findings on teachers’ noticing of features in the teacher resources of mathematics curriculum programs. Based on prior analysis, we selected teachers using one of two curriculum types: delivery mechanism or thinking device. The participating teachers and the curriculum programs aimed to align with the Common Core Standards for Mathematics, and thus, they ostensibly held a common aim for instruction. We analyzed 147 lesson planning interviews with 20 middle school mathematics teachers. We found that teachers attended to similar features of teacher resources; however, patterns for interpreting and planning decisions varied based on teachers’ orientations and curriculum type.}
}
@article{KUZNETSOV20206378,
title = {Harmonic balance analysis of pull-in range and oscillatory behavior of third-order type 2 analog PLLs},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {2},
pages = {6378-6383},
year = {2020},
note = {21st IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.12.1773},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320323818},
author = {N.V. Kuznetsov and M.Y. Lobachev and M.V. Yuldashev and R.V. Yuldashev and G. Kolumbán},
keywords = {Phase-locked loop, third-order PLL, type 2 PLL, nonlinear analysis, harmonic balance method, describing function, global stability, birth of oscillations, hold-in range, pull-in range, lock-in range, Egan conjecture},
abstract = {The most important design parameters of each phase-locked loop (PLL) are the local and global stability properties, and the pull-in range. To extend the pull-in range, engineers often use type 2 PLLs. However, the engineering design relies on approximations which prevent a full exploitation of the benefits of type 2 PLLs. Using an exact mathematical model and relying on a rigorous mathematical thinking this problem is revisited here and the stability and pull-in properties of the third-order type 2 analog PLLs are determined. Both the local and global stability conditions are derived. As a new idea, the harmonic balance method is used to derive the global stability conditions. That approach offers an extra advantage, the birth of unwanted oscillations can be also predicted. As a verification it is shown that the sufficient conditions of global stability derived by the harmonic balance method proposed here and the well-known direct Lyapunov approach coincide with each other, moreover, the harmonic balance predicts the birth of oscillations in the gap between the local and global stability conditions. Finally, an example when the conditions for local and global stability coincide, is considered.}
}
@article{AIROLDI2024101864,
title = {The nested relationality of perceived legitimacy: Mapping taste hierarchies with granular digital traces},
journal = {Poetics},
volume = {102},
pages = {101864},
year = {2024},
issn = {0304-422X},
doi = {https://doi.org/10.1016/j.poetic.2024.101864},
url = {https://www.sciencedirect.com/science/article/pii/S0304422X24000032},
author = {Massimo Airoldi},
keywords = {Taste, Cultural hierarchies, Music classification, Youtube, Digital traces},
abstract = {The article has a double purpose. On the one hand, it contributes to theories of cultural legitimacy and classification. Based on data about consumers’ music evaluations, it shows that taste hierarchies are configured as nested and relational classificatory systems. Nested, because rank systems of symbolic value are collectively recognized, reproduced, and negotiated by consumers not only at the level of genres, but also at lower, nested levels – e.g., sub-genre, artist, single artwork; relational, because the value attributed to music by consumers is ordinarily assessed and constructed through analogies and comparisons, and partly depends on the classifier's relative position in the social space. On the other hand, this paper makes a key methodological contribution: by analyzing large amounts of YouTube data through computational methods and in combination with survey data, it illustrates how the granularity of digital traces can advance sociological research on cultural categories, meaning structures and symbolic imaginaries.}
}
@incollection{ERNST2021265,
title = {Chapter 22 - Pharmaceutical toxicology},
editor = {Martin Wehling},
booktitle = {Principles of Translational Science in Medicine (Third Edition)},
publisher = {Academic Press},
edition = {Third Edition},
address = {Boston},
pages = {265-279},
year = {2021},
isbn = {978-0-12-820493-1},
doi = {https://doi.org/10.1016/B978-0-12-820493-1.00008-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128204931000088},
author = {Steffen W. Ernst and Richard Knight and Jenny Royle and Laura Stephenson},
keywords = {Regulatory toxicology, discovery toxicology, dose-resonse relationship, pharmaceutial safety, drug develoment, risk assessment},
abstract = {This chapter aims to highlight the core principles of pharmaceutical toxicology. It is an interrelated discipline that needs to be applied at all stages of the drug development process to appropriately characterise the safety profile of a drug compound and acknowledge the uncertainties associated with models available. With a strategic mindset, the preclinical safety activities aim to build a comprehensive profile of the drug so that potential hazards can be identified and the risks for healthy trial subjects or patients quantified, and, if necessary, suitable means for eliminating or reducing unacceptable risks can be put in place. We focus on the 2 distinct phases of pharmaceutical toxicology:  Discovery toxicology and regulatory toxicology, to explain how the thinking is built upon at each stage and how the mindset shifts from enabling the selection of an optimally derisked clinical candidate through to thorough risk characterisation and management of those risks for clinical development. Considering attrition due to safety reasons, whether clinical or preclinical, is one of the main reasons for drug project failure, safety assessments should be viewed with equal importance as drug efficacy assessments.}
}
@incollection{GOI2024353,
title = {13 - Perspective on photonic neuromorphic computing},
editor = {Min Gu and Elena Goi and Yangyundou Wang and Zhengfen Wan and Yibo Dong and Yuchao Zhang and Haoyi Yu},
booktitle = {Neuromorphic Photonic Devices and Applications},
publisher = {Elsevier},
pages = {353-375},
year = {2024},
series = {Photonic Materials and Applications Series},
isbn = {978-0-323-98829-2},
doi = {https://doi.org/10.1016/B978-0-323-98829-2.00009-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780323988292000098},
author = {Elena Goi and Min Gu},
keywords = {Neuromorphic photonics, photonic memories, all-optical AI microscopy, hybrid platforms},
abstract = {Bioinspired neuromorphic algorithms can process information more rapidly and more accurately than conventional algorithms, in the attempt to achieve brain-like capacity and efficiency in tasks that are challenging for traditional computers but easy for humans. With the development of applications more performing than ever, the computational requirements for running neuromorphic models are increasing exponentially, motivating efforts to develop new, specialized hardware for fast and efficient execution. Neuromorphic photonics, the implementation of neuromorphic information processing with optoelectronic hardware, is a new computational paradigm based on photons aiming to achieve brain-like information processing in the optical domain, and an interdisciplinary field that is expanding in a multitude of directions. In this chapter, we first revise what we believe are currently the main theoretical and technical challenges in the field and then give a broad perspective on the new directions and opportunities that, in our opinion, represent the current frontiers of neuromorphic photonics.}
}
@article{YANG2025105265,
title = {Harmony in diversity: Digital literacy research in a multidisciplinary landscape},
journal = {Computers & Education},
volume = {230},
pages = {105265},
year = {2025},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2025.105265},
url = {https://www.sciencedirect.com/science/article/pii/S0360131525000338},
author = {Feng Yang and Ruiyang Yao and Yunyue Ren and Luxuan Guo},
keywords = {Information literacy, Interdisciplinary projects, Applications in subject areas, Bibliometrics},
abstract = {The advent of the digital era has significantly heightened interest in digital literacy across multidisciplinary backgrounds and has endowed these fields with interdisciplinary and integrative characteristics. In this study, we employed VOSviewer and Bibliometrix for bibliometric and descriptive analyses of digital literacy, and we analyzed 3005 records from the Social Science Citation Index and Science Citation Index. We constructed keyword co-occurrence time networks across five distinct research areas and supplemented them with keyword co-occurrence frequencies to examine similarities and differences between research themes from diverse disciplinary perspectives. The findings of this study indicate that although various fields recognize the significance of digital literacy, different fields prioritize different aspects. As the main field of research, Education & Educational Research focus primarily on the pedagogical practices of cultivating digital literacy, whereas Communication emphasizes the cultivation of digital literacy to address challenges in information dissemination. Information Science & Library Science typically view libraries as central to digital literacy. Moreover, Computer Science research emphasizes the leveraging of technology, whereas Psychology explores the connection between digital literacy and cognitive processes. Analyzing the differences between different disciplines and drawing new ideas from them is of great significance for Education & Educational Research regarding how to deepen digital literacy education content, construct digital literacy education contexts, integrate digital literacy education resources, narrow the digital divide, and promote educational equity in the future.}
}
@article{ZHANG20211358,
title = {Deep learning-based evaluation of factor of safety with confidence interval for tunnel deformation in spatially variable soil},
journal = {Journal of Rock Mechanics and Geotechnical Engineering},
volume = {13},
number = {6},
pages = {1358-1367},
year = {2021},
issn = {1674-7755},
doi = {https://doi.org/10.1016/j.jrmge.2021.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S1674775521001268},
author = {Jinzhang Zhang and Kok Kwang Phoon and Dongming Zhang and Hongwei Huang and Chong Tang},
keywords = {Deep learning, Convolutional neural network (CNN), Tunnel safety, Confidence interval, Random field},
abstract = {The random finite difference method (RFDM) is a popular approach to quantitatively evaluate the influence of inherent spatial variability of soil on the deformation of embedded tunnels. However, the high computational cost is an ongoing challenge for its application in complex scenarios. To address this limitation, a deep learning-based method for efficient prediction of tunnel deformation in spatially variable soil is proposed. The proposed method uses one-dimensional convolutional neural network (CNN) to identify the pattern between random field input and factor of safety of tunnel deformation output. The mean squared error and correlation coefficient of the CNN model applied to the newly untrained dataset was less than 0.02 and larger than 0.96, respectively. It means that the trained CNN model can replace RFDM analysis for Monte Carlo simulations with a small but sufficient number of random field samples (about 40 samples for each case in this study). It is well known that the machine learning or deep learning model has a common limitation that the confidence of predicted result is unknown and only a deterministic outcome is given. This calls for an approach to gauge the model's confidence interval. It is achieved by applying dropout to all layers of the original model to retrain the model and using the dropout technique when performing inference. The excellent agreement between the CNN model prediction and the RFDM calculated results demonstrated that the proposed deep learning-based method has potential for tunnel performance analysis in spatially variable soils.}
}
@article{MORETTI1980145,
title = {Computational aerodynamics using mini computers},
journal = {Computers & Fluids},
volume = {8},
number = {1},
pages = {145-153},
year = {1980},
note = {Special Issue: Computers in Aerodynamics},
issn = {0045-7930},
doi = {https://doi.org/10.1016/0045-7930(80)90037-7},
url = {https://www.sciencedirect.com/science/article/pii/0045793080900377},
author = {Gino Moretti},
abstract = {The importance of minicomputers as a research tool in gasdynamics is explained, and a few examples are given to show their efficiency.}
}
@article{FISCHLER1987257,
title = {Parallel guessing: A strategy for high-speed computation},
journal = {Pattern Recognition},
volume = {20},
number = {2},
pages = {257-263},
year = {1987},
issn = {0031-3203},
doi = {https://doi.org/10.1016/0031-3203(87)90059-8},
url = {https://www.sciencedirect.com/science/article/pii/0031320387900598},
author = {M.A. Fischler and O. Firschein},
keywords = {Parallel processing, Image analysis algorithms, Image processing, Architectures},
abstract = {Conventional approaches to speeding up image understanding computation involving conventional serial algorithms attempt to decompose these algorithms into portions that can be computed in parallel. Because many classes of algorithms do not readily decompose, one seeks some other basis for parallelism. In this paper we argue that “parallel guessing” for image analysis is a useful approach, and that several recent scene analysis algorithms are based on this concept. Problems suitable for this approach have the characteristic that either “distance” from a true solution, or the correctness of a guess, can be readily checked. We review image analysis algorithms that have a parallel guessing or randomness flavor.}
}
@article{HENNE2019157,
title = {A counterfactual explanation for the action effect in causal judgment},
journal = {Cognition},
volume = {190},
pages = {157-164},
year = {2019},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2019.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S0010027719301301},
author = {Paul Henne and Laura Niemi and Ángel Pinillos and Felipe {De Brigard} and Joshua Knobe},
keywords = {Action effect, Omissions, Omission effect, Causal reasoning, Counterfactual thinking, Causation by omission},
abstract = {People’s causal judgments are susceptible to the action effect, whereby they judge actions to be more causal than inactions. We offer a new explanation for this effect, the counterfactual explanation: people judge actions to be more causal than inactions because they are more inclined to consider the counterfactual alternatives to actions than to consider counterfactual alternatives to inactions. Experiment 1a conceptually replicates the original action effect for causal judgments. Experiment 1b confirms a novel prediction of the new explanation, the reverse action effect, in which people judge inactions to be more causal than actions in overdetermination cases. Experiment 2 directly compares the two effects in joint-causation and overdetermination scenarios and conceptually replicates them with new scenarios. Taken together, these studies provide support for the new counterfactual explanation for the action effect in causal judgment.}
}
@article{KHISTY200577,
title = {Possibilities of steering the transportation planning process in the face of bounded rationality and unbounded uncertainty},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {13},
number = {2},
pages = {77-92},
year = {2005},
note = {Handling Uncertainty in the Analysis of Traffic and Transportation Systems (Bari, Italy, June 10–13 2002)},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2005.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X05000161},
author = {C. Jotin Khisty and Turan Arslan},
keywords = {Paradigm shift, Planning, Rationality, Systemicity, Transportation, Uncertainty},
abstract = {This paper describes and discusses the possibilities of steering the transportation planning process in the face of bounded rationality and unbounded uncertainty: (a) through the introduction of the concept of ‘systemicity’; (b) by expanding the spectrum of the existing planning paradigm currently in use; (c) by reducing complexity through the application of tests of adequacy, dependency, suitability, and adaptability; (d) through the introduction of soft systems thinking; and (e) by using ‘abductive’ in addition to deductive and inductive inferencing. It is concluded that the application of these strategies, adjustments, and tests to the existing planning procedure will hopefully enrich and strengthen our planning effort and make it more robust.}
}
@article{JIN2025127620,
title = {LDBMamba: Language-guided Dual-Branch Mamba for hyperspectral image domain generalization},
journal = {Expert Systems with Applications},
volume = {280},
pages = {127620},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.127620},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425012424},
author = {Chenyang Jin and Xuyang Teng and Minghui Chu and Yuqi Hao and Senlong Qin and Xiaohui Li and Xiaodong Yu},
keywords = {Mamba, Cross-scene, Hyperspectral image classification, Domain generalization, Multimodal},
abstract = {Recent advancements in domain generalization (DG) for cross-scene hyperspectral image (HSI) classification have succeeded considerably with CNN-based and ViT-based methods. However, CNNs often fail to capture global features, and ViTs, although effective, are hindered by high computational demands and slow inference speeds. These problems limit their practicality in cross-scene HSI applications. The recently introduced Mamba model, which is developed from space-state model, strikes a compelling balance between training efficiency and inference speed, offering a promising alternative for HSI classification. Motivated by Mamba, we propose the Language-guided Dual-Branch Mamba (LDBMamba), specifically designed for cross-scene HSI classification. LDBMamba integrates four key components: Spatial Local-Global Scan (SLGS), Spectral Limited-Board Scan (SLBS), Spatial-Spectral-Star-Fusion Inhibited Mamba (S3FIM), and Contrastive Learning with Label-based and Text-based Prior Knowledge Generation Rule (LTR). SLGS and SLBS transform HSI image blocks into spatial and spectral sequences respectively, which can be effectively modeled by the IM module within S3FIM. The S3F component then fuses and enhances these features. In addition, the incorporation of prior knowledge generated by the LTR can enhance the model’s ability to learn domain-invariant representations through contrastive learning. Comprehensive experiments across three benchmark datasets demonstrate that our model outperforms state-of-the-art CNN-based and ViT-based DG methods and provide a promising direction for cross-scene HSI classification.}
}
@article{REN2025109484,
title = {A multi-criteria decision-making method based on discrete Z-numbers and Aczel-Alsina aggregation operators and its application on early diagnosis of depression},
journal = {Engineering Applications of Artificial Intelligence},
volume = {139},
pages = {109484},
year = {2025},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2024.109484},
url = {https://www.sciencedirect.com/science/article/pii/S0952197624016427},
author = {Dong Ren and Xiuqin Ma and Hongwu Qin and Siyue Lei and Xuli Niu},
keywords = {Multi-criteria decision-making, Fuzzy sets, Discrete Z-numbers, Aczel-alsina aggregation operator},
abstract = {In mental health diagnostics, the questionnaire is an effective and cost-effective method. However, the traditional questionnaire test methods for depression and anxiety have great ambiguity. The discrete Z-numbers (DZs) provide solutions for describing and resolving complex fuzzy issues in the intelligent multi-criteria decision-making (MCDM) process. However, large-scale datasets are not suited for the present MCDM techniques due to their extremely high computational cost. Additionally, these techniques are less stable and flexible. To address the above issues, a novel MCDM method is introduced, which is based on the DZs theory and the Aczel-Alsina (AA) aggregation operator (AO) for large-scale datasets. To begin with, centroid points are calculated for DZs, and a series of novel AOs are introduced. And then a score function with a parameter is introduced to balance the influence between the possibility restriction and the fuzzy restriction of DZs. Thirdly, a new MCDM method under DZs is presented based on the proposed AA AOs and score function. Finally, to support the early diagnosis of depression and anxiety, we apply our method to the real-life online Depression, Anxiety, and Stress Scale (DASS) which can be transformed into DZs by our proposed preprocessing method. According to experimental results, our method is applicable to large-scale datasets and has much lower complexity as well as higher flexibility and stability.}
}
@article{JAHEL2023122624,
title = {The future of social-ecological systems at the crossroads of quantitative and qualitative methods},
journal = {Technological Forecasting and Social Change},
volume = {193},
pages = {122624},
year = {2023},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2023.122624},
url = {https://www.sciencedirect.com/science/article/pii/S0040162523003098},
author = {Camille Jahel and Robin Bourgeois and Jérémy Bourgoin and William's Daré and Marie {De Lattre-Gasquet} and Etienne Delay and Patrice Dumas and Christophe {Le Page} and Marc Piraux and Rémi Prudhomme},
keywords = {Quantitative, Qualitative, Anticipation, Foresight, Power relationship, Discontinuities},
abstract = {Urgent calls to transform societies toward more sustainability make the practice of anticipation more and more necessary. The progressive development of computational technologies has opened room for a growing use of quantitative methods to explore the future of social-ecological systems, in addition to qualitative methods. This warrants investigating issues of power relationships and discontinuities and unknowns that arise when mingling quantitative and qualitative anticipatory methods. We first reflected on the semantics attached to these methods. We then conducted a comparative analysis on the way the articulation of quantitative and qualitative methods was conducted, based on an in-depth analysis of a set of eleven anticipatory projects completed by several external case studies. We propose insights to classify projects according to the timing (successive, iterative or convergent) and the purpose of the articulation (imagination, refinement, assessment and awareness raising). We use these insights to explore methodological implications and power relationships and then discuss the ways to inform or frame anticipatory projects that seek to combine these methods.}
}
@article{ESCOUFLAIRE2024129,
title = {Automated text classification of opinion vs. news French press articles. A comparison of transformer and feature-based approaches},
journal = {Language & Communication},
volume = {99},
pages = {129-140},
year = {2024},
issn = {0271-5309},
doi = {https://doi.org/10.1016/j.langcom.2024.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S0271530924000624},
author = {Louis Escouflaire and Antonin Descampe and Cédrick Fairon},
keywords = {Subjectivity, Transformers, Feature-based model, Text classification, Discourse analysis, Explainability},
abstract = {This study explores Natural Language Processing (NLP) methods for distinguishing between press articles belonging to the journalistic genres of ‘objective’ news and ‘subjective’ opinion. Two classification models are compared: CamemBERT, a French transformer model fine-tuned for the task, and a machine learning model using 32 linguistic features. Trained on 8000 Belgian French articles, both models are evaluated on 1000 Canadian French articles. Results show CamemBERT’s superiority but highlight potential for hybrid approaches and emphasizes the need for robust and transparent methods in NLP. The research contributes to understanding NLP’s role in journalism by addressing challenges of point of view detection in press discourse.}
}
@article{TERAN2017384,
title = {Dynamic Profiles Using Sentiment Analysis for VAA's Recommendation Design},
journal = {Procedia Computer Science},
volume = {108},
pages = {384-393},
year = {2017},
note = {International Conference on Computational Science, ICCS 2017, 12-14 June 2017, Zurich, Switzerland},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2017.05.265},
url = {https://www.sciencedirect.com/science/article/pii/S187705091730902X},
author = {Luis Terán and Jose Mancera},
keywords = {Voting Advice Applications, Dynamic Profiles, Recommender Systems, Decision-Making, Elections},
abstract = {In the context of elections, the Internet opens new and promising possibilities for parties and candidates looking for a better political strategy and visibility. In this way they can also organize their election campaign to gather funds, to mobilize support, and to enter into a direct dialogue with the electorate. This paper presents an ongoing research of recommender systems applied on e-government, particularly it is an extension of so-called voting advice applications (VAA’s). VAA’s are Web applications that support voters, providing relevant information on candidates and political parties by comparing their political interests with parties or candidates on different political issues. Traditional VAA’s provide recommendations of political parties and candidates focusing on static profiles of users. The goal of this work is to develop a candidate profile based on different parameters, such as the perspective of voters, social network activities, and expert opinions, to construct a more accurate dynamic profile of candidates. Understanding the elements that compose a candidate profile will help citizens in the decision-making process when facing a lack of information related to the behavior and thinking of future public authorities. At the end of this work, a fuzzy-based visualization approach for a VAA design is given using as a case study the National Elections of Ecuador in 2013.}
}
@article{B2021107538,
title = {A survey on genomic data by privacy-preserving techniques perspective},
journal = {Computational Biology and Chemistry},
volume = {93},
pages = {107538},
year = {2021},
issn = {1476-9271},
doi = {https://doi.org/10.1016/j.compbiolchem.2021.107538},
url = {https://www.sciencedirect.com/science/article/pii/S1476927121001055},
author = {Abinaya B. and Santhi S.},
keywords = {Data sharing, Data access and storage, Data computation, Outsourcing, Privacy-preserving techniques},
abstract = {Nowadays, the purpose of human genomics is widely emerging in health-related problems and also to achieve time and cost-efficient healthcare. Due to advancement in genomics and its research, development in privacy concerns is needed regarding querying, accessing and, storage and computation of the genomic data. While the genomic data is widely accessible, the privacy issues may emerge due to the untrusted third party (adversaries/researchers), they may reveal the information or strategy plans regarding the genome data of an individual when it is requested for research purposes. To mitigate this problem many privacy-preserving techniques are used along with cryptographic methods are briefly discussed. Furthermore, efficiency and accuracy in a secure and private genomic data computation are needed to be researched in future.}
}
@article{CORTESE2024108397,
title = {Applications of genome-scale metabolic models to the study of human diseases: A systematic review},
journal = {Computer Methods and Programs in Biomedicine},
volume = {256},
pages = {108397},
year = {2024},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2024.108397},
url = {https://www.sciencedirect.com/science/article/pii/S0169260724003900},
author = {Nicola Cortese and Anna Procopio and Alessio Merola and Paolo Zaffino and Carlo Cosentino},
keywords = {Genome-scale metabolic networks, Constraint-based modeling, Systems biology, Simulation, Systematic literature review},
abstract = {Background and Objectives:
Genome-scale metabolic networks (GEMs) represent a valuable modeling and computational tool in the broad field of systems biology. Their ability to integrate constraints and high-throughput biological data enables the study of intricate metabolic aspects and processes of different cell types and conditions. The past decade has witnessed an increasing number and variety of applications of GEMs for the study of human diseases, along with a huge effort aimed at the reconstruction, integration and analysis of a high number of organisms. This paper presents a systematic review of the scientific literature, to pursue several important questions about the application of constraint-based modeling in the investigation of human diseases. Hopefully, this paper will provide a useful reference for researchers interested in the application of modeling and computational tools for the investigation of metabolic-related human diseases.
Methods:
This systematic review was conducted according to the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines. Elsevier Scopus®, National Library of Medicine PubMed® and Clarivate Web of Science™ databases were enquired, resulting in 566 scientific articles. After applying exclusion and eligibility criteria, a total of 169 papers were selected and individually examined.
Results:
The reviewed papers offer a thorough and up-to-date picture of the latest modeling and computational approaches, based on genome-scale metabolic models, that can be leveraged for the investigation of a large variety of human diseases. The numerous studies have been categorized according to the clinical research area involved in the examined disease. Furthermore, the paper discusses the most typical approaches employed to derive clinically-relevant information using the computational models.
Conclusions:
The number of scientific papers, utilizing GEM-based approaches for the investigation of human diseases, suggests an increasing interest in these types of approaches; hopefully, the present review will represent a useful reference for scientists interested in applying computational modeling approaches to investigate the aetiopathology of human diseases; we also hope that this work will foster the development of novel applications and methods for the discovery of clinically-relevant insights on metabolic-related diseases.}
}
@article{ZOU2025106959,
title = {LCFFNet: A Lightweight Cross-scale Feature Fusion Network for human pose estimation},
journal = {Neural Networks},
volume = {183},
pages = {106959},
year = {2025},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2024.106959},
url = {https://www.sciencedirect.com/science/article/pii/S0893608024008888},
author = {Xuelian Zou and Xiaojun Bi},
keywords = {Human pose estimation, 2d dynamic multi-scale convolution, Contextual semantic information, Adaptive feature fusion},
abstract = {Human pose estimation is one of the most critical and challenging problems in computer vision. It is applied in many computer vision fields and has important research significance. However, it is still a difficult challenge to strike a balance between the number of parameters and computing load of the model and the accuracy of human pose estimation. In this study, we suggest a Lightweight Cross-scale Feature Fusion Network (LCFFNet) to strike a balance between accuracy and computational load and parameter volume. The Lightweight HRNet-Like (LHRNet) network, Cross-Resolution-Aware Semantics Module (CRASM), and Adapt Feature Fusion Module (AFFM) make up LCFFNet. To be more precise, first, we suggest a lightweight LHRNet network that includes Dynamic Multi-scale Convolution Basic (DMSC-Basic block) block, Basic block, and DMSC-Basic block submodules in the network’s three high-resolution subnetwork stages. The proposed dynamic multi-scale convolution in DMSC-Basic block can reduces the amount of model parameters and complexity of the LHRNet network, and has the ability to extract variable pose features. In order to maintain the model’s ability to express features, the Basic block is introduced. As a result, the LHRNet network not only makes the model more lightweight but also enhances its feature expression capabilities. Second, we propose a CRASM module to enhance contextual semantic information while reducing the semantic gap between different scales by fusing features from different scales. Finally, the augmented semantic feature map’s spatial resolution is finally restored from bottom to top using our suggested AFFM, and adaptive feature fusion is used to increase the positioning accuracy of important sites. Our method successfully predicts keypoints with 74.2 % AP, 89.9 % PCKh@0.5 and 66.9 % AP on the MSCOCO 2017, MPII and Crowdpose datasets, respectively. Our model reduces the number of parameters by 89.0 % and the computational complexity by 87.5 % compared with HRNet. The proposed network performs as well as current large-model human pose estimation networks while outperforming state-of the-art lightweight networks.}
}
@article{OUDMAN2018214,
title = {Effects of different cue types on the accuracy of primary school teachers' judgments of students' mathematical understanding},
journal = {Teaching and Teacher Education},
volume = {76},
pages = {214-226},
year = {2018},
issn = {0742-051X},
doi = {https://doi.org/10.1016/j.tate.2018.02.007},
url = {https://www.sciencedirect.com/science/article/pii/S0742051X17302305},
author = {Sophie Oudman and Janneke {van de Pol} and Arthur Bakker and Mirjam Moerbeek and Tamara {van Gog}},
keywords = {Teacher judgment, Judgment accuracy, Cue utilization, Primary education, Mathematics education, Decimals},
abstract = {To gain insight into how teachers' judgment accuracy can be improved, we investigated effects of cue-type availability. While thinking aloud, 21 teachers judged their fourth grade students' (n = 176) decimal magnitude understanding. Sensitivity (correctly judging what students did understand) did not improve from availability of both answer cues (students' answers to prior practice problems) and student cues (knowledge of students triggered by knowing their names), and was lower when only answer cues were available, compared to only student cues. Specificity (correctly judging what students did not understand) was higher when only answer cues were available, compared to only student cues or both student and answer cues.}
}
@incollection{MILLER2017141,
title = {8 - Doctoral and professional programs},
editor = {Susan M. Miller and Walter H. Moos and Barbara H. Munk and Stephen A. Munk},
booktitle = {Managing the Drug Discovery Process},
publisher = {Woodhead Publishing},
address = {Boston},
pages = {141-169},
year = {2017},
isbn = {978-0-08-100625-2},
doi = {https://doi.org/10.1016/B978-0-08-100625-2.00008-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780081006252000088},
author = {Susan M. Miller and Walter H. Moos and Barbara H. Munk and Stephen A. Munk},
keywords = {Critical thinking, Basic/applied/clinical, Problem identification, Research design, Teams, PhD/PharmD, Postdoc/postdoctoral, Writing/publishing.},
abstract = {In this chapter on graduate and professional education, we explore Doctoral and professional programs, posing a number of key questions you should ask yourself. Where to apply to graduate school or a postdoc, and why? With whom should you work? A PhD or a PharmD, and to postdoc or not? What must you do to be successful? Moreover, we touch on traits important to becoming an independent researcher and ask whether success in graduate school or a postdoctoral fellowship requires different skills than undergraduate degrees. Critical thinking habits and skills underpin this discussion. We outline possible career choices, touching on the knowledge and expertise used by drug hunters, and also ask what might be of most value to potential employers. Each of us is different, and what's best for you is something you will have to decipher, but hopefully only after you consult with family, friends, and advisors or mentors. Regardless, “the big leap” is coming, so get ready.}
}
@article{CARPENTER1992457,
title = {Chapter 4 Cognitively guided instruction: Building on the knowledge of students and teachers},
journal = {International Journal of Educational Research},
volume = {17},
number = {5},
pages = {457-470},
year = {1992},
issn = {0883-0355},
doi = {https://doi.org/10.1016/S0883-0355(05)80005-9},
url = {https://www.sciencedirect.com/science/article/pii/S0883035505800059},
author = {Thomas P. Carpenter and Elizabeth Fennema},
abstract = {This chapter summarizes the results of a series of correlational, experimental, and case studies on Cognitively Guided Instruction (CGI), a program designed to help teachers understand children's thinking and use this knowledge to make instructional decisions. Results of the studies show that teachers' knowledge and beliefs about students' thinking are related to students' achievement. There were significant differences between CGI classes and control classes on the emphasis on problem solving and low level skills, the freedom given to students to construct their own strategies for solving problems, the teachers' knowledge of their students thinking, and the students' achievement in both problem solving and skills.}
}
@article{EVANS201123659,
title = {Advancing Science through Mining Libraries, Ontologies, and Communities*},
journal = {Journal of Biological Chemistry},
volume = {286},
number = {27},
pages = {23659-23666},
year = {2011},
issn = {0021-9258},
doi = {https://doi.org/10.1074/jbc.R110.176370},
url = {https://www.sciencedirect.com/science/article/pii/S0021925819487164},
author = {James A. Evans and Andrey Rzhetsky},
keywords = {Biophysics, Computation, Computer Modeling, Drug Design, Epigenetics, Computational Biology, Information Cascade, Sociology of Science, Text Mining},
abstract = {Life scientists today cannot hope to read everything relevant to their research. Emerging text-mining tools can help by identifying topics and distilling statements from books and articles with increased accuracy. Researchers often organize these statements into ontologies, consistent systems of reality claims. Like scientific thinking and interchange, however, text-mined information (even when accurately captured) is complex, redundant, sometimes incoherent, and often contradictory: it is rooted in a mixture of only partially consistent ontologies. We review work that models scientific reason and suggest how computational reasoning across ontologies and the broader distribution of textual statements can assess the certainty of statements and the process by which statements become certain. With the emergence of digitized data regarding networks of scientific authorship, institutions, and resources, we explore the possibility of accounting for social dependences and cultural biases in reasoning models. Computational reasoning is starting to fill out ontologies and flag internal inconsistencies in several areas of bioscience. In the not too distant future, scientists may be able to use statements and rich models of the processes that produced them to identify underexplored areas, resurrect forgotten findings and ideas, deconvolute the spaghetti of underlying ontologies, and synthesize novel knowledge and hypotheses.}
}
@article{AI2022631,
title = {Reconsidering autistic ‘camouflaging’ as transactional impression management},
journal = {Trends in Cognitive Sciences},
volume = {26},
number = {8},
pages = {631-645},
year = {2022},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2022.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S1364661322001061},
author = {Wei Ai and William A. Cunningham and Meng-Chuan Lai},
keywords = {autism, camouflaging, impression management, predictive coding, social alignment, wellbeing},
abstract = {Social performances pervade human interactions. Some autistic people describe their social performances as ‘camouflaging’ and engage in these performances to mitigate social challenges and survive in the neurotypical world. Here, we reconsider autistic camouflaging under the unifying framework of impression management (IM) by examining overlapping and unique motivations, neurocognitive mechanisms, and consequences. Predictive coding and Bayesian principles are synthesized into a computational model of IM that applies to autistic and neurotypical people. Throughout, we emphasize the inherently transactional, context-dependent nature of IM, the distinct computational challenges faced by autistic people, and the psychological toll that compelled IM can take. Viewing camouflaging through this lens highlights the pressing needs to change societal attitudes, destigmatize autism, refine social skills-building programs for autistic individuals, and integrate these programs with environment-focused support.}
}
@incollection{SCHNEEGANS2008241,
title = {13 - Dynamic Field Theory as a Framework for Understanding Embodied Cognition},
editor = {Paco Calvo and Antoni Gomila},
booktitle = {Handbook of Cognitive Science},
publisher = {Elsevier},
address = {San Diego},
pages = {241-271},
year = {2008},
series = {Perspectives on Cognitive Science},
issn = {15564495},
doi = {https://doi.org/10.1016/B978-0-08-046616-3.00013-X},
url = {https://www.sciencedirect.com/science/article/pii/B978008046616300013X},
author = {Sebastian Schneegans and Gregor Schöner},
abstract = {Publisher Summary
Embodied cognition is an approach to cognition that has roots in motor behavior. This approach emphasizes that cognition typically involves acting with a physical body on an environment in which that body is immersed. The approach of embodied cognition postulates that understanding cognitive processes entails understanding their close link to the motor surfaces that may generate action and to the sensory surfaces that provide sensory signals about the environment. To a certain extent, the embodiment stance implies a mistrust of the abstraction inherent in much information processing thinking, in which the interface between cognitive processes and their sensorimotor support is drawn at a level that is quite removed from both the sensory and the motor systems. New theoretical tools are needed to address cognition within the embodiment perspective. This chapter reviews one set of theoretical concepts which is believed to be particularly suited to address the constraints of embodiment and situatedness. It refers to this set of concepts as Dynamical Systems Thinking.}
}
@article{TALANOV2018473,
title = {Simulation of serotonin mechanisms in NEUCOGAR cognitive architecture},
journal = {Procedia Computer Science},
volume = {123},
pages = {473-478},
year = {2018},
note = {8th Annual International Conference on Biologically Inspired Cognitive Architectures, BICA 2017 (Eighth Annual Meeting of the BICA Society), held August 1-6, 2017 in Moscow, Russia},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.01.072},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918300735},
author = {Max Talanov and Fail Gafarov and Jordi Vallverdú and Sergey Ostapenko and Marat Gazizov and Alexander Toschev and Alexey Leukhin and Salvatore Distefano},
keywords = {Serotonin, dopamine, disgust, artificial intelligence, simulation, affective computing, emotion modelling, neuromodulation},
abstract = {This work aims at demonstrating that the neuromodulatory mechanisms that control the emotional states of mammals (specifically rat’s brains) can be represented and re-implemented in a computational model processed by a machine. In particular we specifically focus on two neuro-transmitters, serotonin and dopamine, starting from their fundamental role in basic cognitive processes. In our specific implementation, we represent the simulation of the ‘disgust-like’ state based on the three dimensional neuromodulatory model of affects or emotions, according to the ‘cube of emotions’. These functional mechanisms can be transferred into an artificial cognitive system: inhibition, for example, can elicit a blocking behaviour that, depending on its intensity and duration, can push the system to a general emotional state. We have simulated 1000 milliseconds of the serotonin and dopamine systems using NEST Neural Simulation Tool with the rat brain as the model to artificially reproduce this mechanism on a computational system.}
}
@incollection{STEIN202139,
title = {Chapter 2 - Brain–minds: What’s the best metaphor?},
editor = {Dan J. Stein},
booktitle = {Problems of Living},
publisher = {Academic Press},
pages = {39-59},
year = {2021},
isbn = {978-0-323-90239-7},
doi = {https://doi.org/10.1016/B978-0-323-90239-7.00005-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780323902397000055},
author = {Dan J. Stein},
keywords = {Psychiatry, Philosophy, Realism, Psychiatric classification, Pluralism, Erklären, Verstehen, Epistemic humility, Practical wisdom},
abstract = {This chapter addresses the question of how best to think about the brain–mind from both philosophical and psychiatric perspectives. The section on philosophy of mind notes the positions of physicalism, dualism, and functionalism, and proposes that emergent materialism has particular advantages. The section on psychiatry notes the positions of behaviourism and existentialism. Two key metaphors of the brain–mind are then critiqued: the hydraulic model of psychoanalysis, and the computational model of cognitive science. A third metaphor, that of ‘wetware’, which emphasizes that the brain–mind cannot simply be divided into hardware and software, but rather that it must be approached as a complex psychobiological phenomenon, is proposed. Several advantages of this metaphor are discussed, including that it is consistent with emergent materialism and a view of the brain–mind as embodied and embedded in social activity, as well as with current cognitive-affective and psychiatric science.}
}
@article{VANOPHEUSDEN2019127,
title = {Tasks for aligning human and machine planning},
journal = {Current Opinion in Behavioral Sciences},
volume = {29},
pages = {127-133},
year = {2019},
note = {Artificial Intelligence},
issn = {2352-1546},
doi = {https://doi.org/10.1016/j.cobeha.2019.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S2352154619300622},
author = {Bas {van Opheusden} and Wei Ji Ma},
abstract = {Research on artificial intelligence and research on human intelligence rely on similar conceptual foundations and have long inspired each other [1,2•]. However, achieving concrete synergy has been difficult, with one obstacle being a lack of alignment of the tasks used in both fields. Artificial intelligence research has traditionally focused on tasks that are challenging to solve, often using human performance as a benchmark to surpass [3, 4, 5, 6, 7]. By contrast, cognitive science and psychology have moved towards tasks that are simple enough to allow for detailed computational modeling of people’s choices. These divergent objectives have led to a divide in the complexity of tasks studied, both in perception and cognition. The purpose of this paper is to explore the middle ground: are there tasks that are reasonably attractive to both fields and could provide fertile ground for synergy?}
}
@incollection{BLISS19921,
title = {REASONING SUPPORTED BY COMPUTATIONAL TOOLS},
editor = {MICHAEL R. KIBBY and J. ROGER HARTLEY},
booktitle = {Computer Assisted Learning: Selected Contributions from the CAL '91 Symposium},
publisher = {Pergamon},
address = {Amsterdam},
pages = {1-9},
year = {1992},
isbn = {978-0-08-041395-2},
doi = {https://doi.org/10.1016/B978-0-08-041395-2.50007-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780080413952500078},
author = {JOAN BLISS and JON OGBORN and RICHARD BOOHAN and JONATHAN BRIGGS and TIM BROSNAN and DEREK BROUGH and HARVEY MELLAR and ROB MILLER and CAROLINE NASH and CATHY RODGERS and BABIS SAKONIDIS},
abstract = {Abstract
This paper sets out the work of the Tools for Exploratory Learning Programme within the ESRC Initiative Information Technology in Education. The research examines young secondary children's reasoning with computational tools. We distinguish between exploratory and expressive modes of learning, that is, interaction with another's model and creation of one's own model, respectively. The research focuses on reasoning, rather than learning, along three dimensions: quantitative, qualitative, and semi-quantitative. It provides a 3 × 2 classification of tasks according to modes of learning and types of reasoning. Modelling tools were developed for the study and descriptions of these are given. The research examined children's reasoning with tools in all three dimensions looking more exhaustively at the semi-quantitative. Pupils worked either in an exploratory mode or an expressive mode on one of the following topics: Traffic, Health and Diet, and Shops and Profits. They spent 3-4 h individually with a researcher over 2 weeks, carrying out four different activities: reasoning without the computer; learning to manipulate first the computer then later the tool and finally carrying out a task with the modelling tool. Pupils were between 12 and 14 yr. Research questions both about children's reasoning when working with or creating models and about the nature of the tools used are discussed. Finally an analytic scheme is set out which describes the nature of the causal and non-causal reasoning observed together with some tentative results.}
}
@article{GOVIL2022103125,
title = {Validation of agile methodology as ideal software development process using Fuzzy-TOPSIS method},
journal = {Advances in Engineering Software},
volume = {168},
pages = {103125},
year = {2022},
issn = {0965-9978},
doi = {https://doi.org/10.1016/j.advengsoft.2022.103125},
url = {https://www.sciencedirect.com/science/article/pii/S0965997822000357},
author = {Nikhil Govil and Ashish Sharma},
keywords = {Software Development Process, Decision support system, Fuzzy logic, Agile Software Development, Fuzzy TOPSIS, Multi-Criteria Decision Making},
abstract = {Agile methodologies have been an emerging choice of software professionals for the past decade and a half. However, apart from this, some other SDLC models are also available for selection in front of software developers to develop any software. Usually, project managers select any of these models to develop software through their past experiences. There is no logical basis for this selection to be completely correct, as a result of which there is always a risk of software failure or over budget if an inappropriate model has opted. Keeping this problem of software industries in mind, an ideal SDLC model has been identified mathematically in this article. In this article, we applied the Fuzzy TOPSIS method that validates Agile software development as an ideal choice. We have taken a total of six software development processes that are being applied globally. Feedback from five experienced decision-makers has been taken in the form of linguistic terms and further converted into fuzzy values to perform the computation of the closeness coefficient rank of each experimented alternative software development process.}
}
@article{SHIN2023104897,
title = {Pedagogical discourse markers in online algebra learning: Unraveling instructor's communication using natural language processing},
journal = {Computers & Education},
volume = {205},
pages = {104897},
year = {2023},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2023.104897},
url = {https://www.sciencedirect.com/science/article/pii/S0360131523001744},
author = {Jinnie Shin and Renu Balyan and Michelle P. Banawan and Tracy Arner and Walter L. Leite and Danielle S. McNamara},
keywords = {Pedagogical communication, Online learning, Video lectures, Natural language processing},
abstract = {Despite the proliferation of video-based instruction and its benefits—such as promoting student autonomy and self-paced learning—the complexities of online teaching remain a challenge. To be effective, educators require extensive training in digital teaching methodologies. As such, there's a pressing need to examine and comprehend the intricacies of instructors' communication patterns within this context. This research addresses the pressing need to understand pedagogical discourse in online video lectures in Algebra classes by employing computational linguistic tools and natural language processing (NLP). Using transcripts from 125 Algebra 1 video lectures—comprising 4962 instances of pedagogical discourse—from five instructors at Math Nation, a virtual math learning environment, we analyzed the conveyance of linguistic, attitudinal, and emotional nuances. With the aid of 26 Coh-Metrix and SÉANCE features, we classified educators' language choices, achieving an accuracy of 86.7%. Furthermore, variations in language choices, as signified by discourse markers, were examined through a K-means clustering approach. The resulting 17 clusters were grouped into interpersonal, structural, and cognitive pedagogic functions. Through this exploration, we demonstrate the promising potential of NLP in efficiently deciphering pedagogical communication patterns in video lectures. These insights open a new avenue for research, aimed at assessing the efficacy of digital instruction by scrutinizing pedagogical discourse characteristics in computer-based learning environments.}
}