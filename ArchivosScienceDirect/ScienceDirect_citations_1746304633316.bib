@article{GENT202238,
title = {Making a mind},
journal = {New Scientist},
volume = {253},
number = {3374},
pages = {38-41},
year = {2022},
issn = {0262-4079},
doi = {https://doi.org/10.1016/S0262-4079(22)00294-9},
url = {https://www.sciencedirect.com/science/article/pii/S0262407922002949},
author = {Edd Gent},
abstract = {In the push to make artificial intelligence that thinks like humans, many researchers are focused on fresh insights from neuroscience. Should they be looking to psychology instead, asks Edd Gent}
}
@incollection{LACA2019994,
title = {2.68 - Life Cycle Assessment in Biotechnology☆},
editor = {Murray Moo-Young},
booktitle = {Comprehensive Biotechnology (Third Edition)},
publisher = {Pergamon},
edition = {Third Edition},
address = {Oxford},
pages = {994-1006},
year = {2019},
isbn = {978-0-444-64047-5},
doi = {https://doi.org/10.1016/B978-0-444-64046-8.00109-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780444640468001099},
author = {Adriana Laca and Amanda Laca and Mónica Herrero and Mario Díaz},
keywords = {Biofuels, Biopolymers, Biotechnology, Environmental impacts, LCA, Life cycle assessment, Sustainability},
abstract = {The life cycle assessment (LCA) is a technique for assessing environmental impacts in order to identify the key points of a process/product/service as well as for suggesting alternatives to improve its environmental performance. The LCA methodology allows a "cradle to grave" perspective, considering that all the stages involved in the life cycle of a product or activity have a responsibility on the environmental consequences of it. Related terms as Life Cycle Engineering, Social Life Cycle and Life Cycle Thinking are emerging with a wide perspective. In this article, an overview of the main aspects of the used methodology is provided. Applications of LCA in food biotechnology, pharmaceuticals, biopolymers, biofuels and waste management of the biodegradable fractions are reviewed. Not only the utility of LCA to the biotechnological processes but also its main limitations are presented. Current tendencies in the LCA development highlight the need to update tools applicable to different areas with increasing demand of more accurate environmental information. Main footprints commonly used as LCA indicators have been recently combined with the final goal of obtaining a single indicator useful for decision-making. The interest of LCA for product design and the interactions of LCA with other environmental tools are also commented.}
}
@article{STEPHAN2024111077,
title = {EPiC grasshopper: A bottom-up parametric tool to quantify life cycle embodied environmental flows of buildings and infrastructure assets},
journal = {Building and Environment},
volume = {248},
pages = {111077},
year = {2024},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2023.111077},
url = {https://www.sciencedirect.com/science/article/pii/S0360132323011046},
author = {André Stephan and Fabian Prideaux and Robert H. Crawford},
keywords = {Hybrid life cycle assessment, Embodied energy, Embodied carbon, Grasshopper, Buildings, Infrastructure, LCA, Python, Rhinoceros, Design},
abstract = {Reducing the embodied environmental flows of built assets is becoming increasingly important and is a key priority for actors in the built environment to improve life cycle environmental performance. Policies and related targets for embodied environmental flow reductions are emerging. Despite this, tools for quantifying the life cycle embodied environmental flows of built assets are limited in variety and scope. Parametric life cycle assessment (LCA) tools have emerged to address some of these limitations. These tools can enhance decision making, be embedded directly into CAD programs, and offer real-time LCA calculations across multiple design variations. Yet, existing parametric tools for LCA rely on process-based material environmental flow data, limited geometries, limited real-time data visualisation capacity, and often require specialised technical expertise to use. These gaps limit their ability to provide transparent, robust, and rapid assessments. This paper introduces EPiC Grasshopper, an open-source, open-access, bottom-up, parametric tool that enables the quantification of life cycle embodied environmental flows at the early stages of built asset design, bridging the aforementioned gaps. The key characteristics and functionalities of the tool are described, followed by verification (checking that calculations are correct), validation (checking that results are representative of reality), and demonstration of its application to two built asset case studies, i.e. parametrically-defined Australian house and road. The paper shows how the tool can be used to generate designs to meet specific embodied environmental flow targets as well as streamline and increase the uptake of embodied environmental flow assessment and considerations in built asset design workflows.}
}
@article{MUNUZURI202264,
title = {Unified representation of Life's basic properties by a 3-species Stochastic Cubic Autocatalytic Reaction-Diffusion system of equations},
journal = {Physics of Life Reviews},
volume = {41},
pages = {64-83},
year = {2022},
issn = {1571-0645},
doi = {https://doi.org/10.1016/j.plrev.2022.03.003},
url = {https://www.sciencedirect.com/science/article/pii/S1571064522000185},
author = {Alberto P. Muñuzuri and Juan Pérez-Mercader},
keywords = {Living systems, Turing instability, Top-down approach, Bottom-up approach, Non-linear reaction-diffusion equations, Properties of life},
abstract = {Today we can use physics to describe in great detail many of the phenomena intervening in the process of life. But no analogous unified description exists for the phenomenon of life itself. In spite of their complexity, all living creatures are out of equilibrium chemical systems sharing four fundamental properties: they (1) handle information, (2) metabolize, (3) self-reproduce and (4) evolve. This small number of features, which in terran life are implemented with biochemistry, point to an underlying simplicity that can be taken as a guide to motivate and implement a theoretical physics style unified description of life using tools from the non-equilibrium physical-chemistry of extended systems. Representing a system with general rules is a well stablished approach to model building and unification in physics, and we do this here to provide an abstract mathematical description of life. We start by reviewing the work of previous authors showing how the properties in the above list can be individually represented with stochastic reaction-diffusion kinetics using polynomial reaction terms. These include “switches” and computation, the kinetic representation of autocatalysis, Turing instability and adaptation in the presence of both deterministic and stochastic environments. Thinking of these properties as existing on a space-time lattice each of whose nodes are subject to a common mass-action kinetics compatible with the above, leads to a very rich dynamical system which, just as natural life, unifies the above properties and can therefore be interpreted as a high level or “outside-in” theoretical physics representation of life. Taking advantage of currently available advanced computational techniques and hardware, we compute the phase plane for this dynamical system both in the deterministic and stochastic cases. We do simulations and show numerically how the system works. We review how to extract useful information that can be mapped into emergent physical phenomena and attributes of importance in life such as the presence of a “membrane” or the time evolution of an individual system's negentropy or mass. Once these are available, we illustrate how to perform some basic phenomenology based on the model's numerical predictions. Applying the above to the idealization of the general Cell Division Cycle (CDC) given almost 25 years ago by Hunt and Murray, we show from the numerical simulations how this system executes a form of the idealized CDC. We also briefly discuss various simulations that show how other properties of living systems such as migration towards more favorable regions or the emergence of effective Lotka-Volterra populations are accounted for by this general and unified view from the “top” of the physics of life. The paper ends with some discussion, conclusions, and comments on some selected directions for future research. The mathematical techniques and powerful simulation tools we use are all well established and presented in a “didactical” style. We include a very rich but concise SI where the numerical details are thoroughly discussed in a way that anyone interested in studying or extending the results would be able to do so.}
}
@article{KHALID2025100900,
title = {Ludogenic innovation: How playing incentivizes technological innovation},
journal = {Entertainment Computing},
volume = {52},
pages = {100900},
year = {2025},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2024.100900},
url = {https://www.sciencedirect.com/science/article/pii/S1875952124002684},
author = {Mohd Nor Akmal Khalid},
keywords = {Play, Incentives, Innovation, Games, Artificial intelligence},
abstract = {Play has been an essential element of culture. It comprises forms and parts that intertwine to form complex and meaningful interactions. As an artifact, in the form of games (or video games), it created a massive following and interests, both on personal and organizational levels. An introduction to the term Ludogenic, which blends “ludus” (Latin for play) with “genic” (producing or generating), suggesting the creation or generation of playful and innovative experiences, was proposed as a new paradigm for future technological development in the age of artificial intelligence (AI). This article explores how play and games influence and impact technological advancement and contemporary innovation, eliciting the transformative potential of ludogenic innovation in reshaping technological development, offering insights into future directions for research and application in addressing complex global challenges, and highlighting the enduring impact of play on technological innovation.}
}
@article{VENKATESWARLU20201093,
title = {Modeling and fabrication of catalytic converter for emission reduction},
journal = {Materials Today: Proceedings},
volume = {33},
pages = {1093-1099},
year = {2020},
note = {International Conference on Future Generation Functional Materials and Research 2020},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2020.07.125},
url = {https://www.sciencedirect.com/science/article/pii/S2214785320352159},
author = {K. Venkateswarlu and Revuri Ajay Kumar and Ram Krishna and M. Sreenivasan},
keywords = {Finite element analysis, Catalytic converter, CFD analysis, Thermal analysis, CO, HC, NOx},
abstract = {Automobiles throughout the world are the primary consumers of fossil fuels, which emit toxic gases when burnt; including HC, CO and NOx. Catalytic converters were developed to detoxify these gases into less harmful gases such as carbon dioxide and H2O.In this paper, the development of a catalytic converter for efficient emission reduction is presented. The results are presented after performing Computational Fluid Dynamics (CFD) on the proposed catalytic converter. In this catalytic converter Two Different materials used they are stainless steel wire mesh and ceramic stones at time and we are conducted tests with catalytic converter at different blended fluids and engine speeds such as methane, ethane and 1700,1900 RPM.3D modeling done by CATIA parametric software And analysis done in ANSYS software.}
}
@article{KEESTRA2009531,
title = {Foundationalism and neuroscience; silence and language},
journal = {Language Sciences},
volume = {31},
number = {4},
pages = {531-552},
year = {2009},
issn = {0388-0001},
doi = {https://doi.org/10.1016/j.langsci.2007.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S0388000107001040},
author = {Machiel Keestra and Stephen J. Cowley},
keywords = {Distributed cognition, Foundationalism and coherentism, Language and cognition, Mereology and dualism, Neuroscience and philosophy},
abstract = {Neuroscience offers more than new empirical evidence about the details of cognitive functions such as language, perception and action. Since it also shows many functions to be highly distributed, interconnected and dependent on mechanisms at different levels of processing, it challenges concepts that are traditionally used to describe these functions. The question is how to accommodate these concepts to the recent evidence. A recent proposal, made in Philosophical Foundations of Neuroscience (2003) by Bennett and Hacker, is that concepts play a foundational role in neuroscience, that empirical research needs to presuppose them and that changing concepts is a philosophical task. In defending this perspective, PFN shows much neuroscientific writing to be dualistic in nature due to our poor grasp of its foundations. In our review article we take a different approach. Instead of foundationalism we plead for a mild coherentism, which allows for a gradual and continuous alteration of concepts in light of new evidence. Following this approach it is also easier to deal with some neurological conditions (like blindsight, synaesthesia) that pose difficulties for our concepts. Finally, although words and concepts seem to seduce us to thinking that many skills and tasks function separately, it is language skill that – as neuroscientific evidence shows – co-emerges with action/perception cycles and thus seems to require revision of some of our central concepts.}
}
@article{KROGER200886,
title = {Distinct neural substrates for deductive and mathematical processing},
journal = {Brain Research},
volume = {1243},
pages = {86-103},
year = {2008},
issn = {0006-8993},
doi = {https://doi.org/10.1016/j.brainres.2008.07.128},
url = {https://www.sciencedirect.com/science/article/pii/S000689930801929X},
author = {James K. Kroger and Leigh E. Nystrom and Jonathan D. Cohen and Philip N. Johnson-Laird},
keywords = {Logic, Reasoning, Math, Cortex, fMRI, Frontal pole},
abstract = {In an effort to clarify how deductive reasoning is accomplished, an fMRI study was performed to observe the neural substrates of logical reasoning and mathematical calculation. Participants viewed a problem statement and three premises, and then either a conclusion or a mathematical formula. They had to indicate whether the conclusion followed from the premises, or to solve the mathematical formula. Language areas of the brain (Broca's and Wernicke's area) responded as the premises and the conclusion were read, but solution of the problems was then carried out by non-language areas. Regions in right prefrontal cortex and inferior parietal lobe were more active for reasoning than for calculation, whereas regions in left prefrontal cortex and superior parietal lobe were more active for calculation than for reasoning. In reasoning, only those problems calling for a search for counterexamples to conclusions recruited right frontal pole. These results have important implications for understanding how higher cognition, including deduction, is implemented in the brain. Different sorts of thinking recruit separate neural substrates, and logical reasoning goes beyond linguistic regions of the brain.}
}
@article{CONG2025103131,
title = {Enhancing novel product iteration: An integrated framework for heuristic ideation via interpretable conceptual design knowledge graph},
journal = {Advanced Engineering Informatics},
volume = {65},
pages = {103131},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103131},
url = {https://www.sciencedirect.com/science/article/pii/S1474034625000242},
author = {Yangfan Cong and Suihuai Yu and Jianjie Chu and Yuexin Huang and Ning Ding and Cong Fang and Stephen Jia Wang},
keywords = {Novel product iteration, Conceptual product design, Design knowledge, Interpretable knowledge graph, Heuristic product ideation},
abstract = {Novel products emerge over time to survive the competitive landscape as no existing product can perpetually satisfy all evolving customer expectations. These products are often characterized by groundbreaking solutions previously unavailable on the market. However, the swift imitation of successful novel products by competitors underscores the need for sustained iteration and continuous improvement. Designers increasingly face challenges in keeping up to date with the growing volume and fragmented nature of design information from diverse sources. While knowledge graphs show promise in structuring and organizing complex design information, their effective application in the ideation process remains limited due to difficulties in automatic knowledge extraction and the lack of interpretability aligned well with designers’ cognitive processes. This study proposes an integrated method to construct an interpretable conceptual design knowledge graph (I-CDKG) that features both inherent and acquired interpretability for heuristic product ideation. First, the schema layer models product design knowledge and governs the semantic connection of design information reinforced by design cognition principles to create a reasonable organizational framework to foster intuitive knowledge exploration. Second, the data layer mainly fulfills automatic and smooth design knowledge extraction for I-CDKG construction through the deep learning ERNIE-BiGRU-CRF model combined with BIESO labeling mode and triple-extracting algorithm. Third, the application layer empowers designers to visually delve into interpretable design knowledge to locate inspiration from cluster, relation, and nest levels and enable constant I-CDKG expansion as design schemes proliferate. A case study on the smart cat litter box demonstrates the feasibility of the proposed methodology. The evaluation results confirm the I-CDKG’s advantages as a productive design tool for inspiring creative, practical, and cost-effective product ideations, thereby empowering the iterative development of competitive novel products.}
}
@article{SHARMA2022100694,
title = {eFeed-Hungers 2.0: Pervasive computing, sustainable feeding to purge global hunger},
journal = {Sustainable Computing: Informatics and Systems},
volume = {35},
pages = {100694},
year = {2022},
issn = {2210-5379},
doi = {https://doi.org/10.1016/j.suscom.2022.100694},
url = {https://www.sciencedirect.com/science/article/pii/S2210537922000361},
author = {Sugam Sharma and Ritu Shandilya and Kihwan Kim and Debasis Mandal and U. Sunday Tim and Johnny Wong},
keywords = {Pervasive, Hunger, Sustainable, Feed, Food waste, Mitigate, Computing, eFeed-Hungers},
abstract = {In this paper, we further strengthen our eFeed-Hungers research efforts in fighting the hunger using the advanced computer technologies. According to the United Nations hunger reports, the amount of the globally produced food is adequate enough to serve the whole world of about 7.5 billion people. Despite this, the world is globally experiencing the growing hunger and growing food waste issues and consequently, the food-deprived suffering. The primary cause of this is dubbed as the unavailability of the proper conduits to efficiently and effectively channel the food waste to the food-deprived population. The eFeed-Hungers is one such globally popular computational framework that provenly addresses these concerns and challenges and smoothly connects the edible food waste to needy. In this research, we have extended the strength of eFeed-Hungers platform to smartly address the mass donations at one location. Further, the required technical artifacts are appropriately implemented to expand the horizon of eFeed-Hungers operations to respond to the global communities with disparate time zones. Additionally, the eFeed-Hungers ecosystem is equipped with the verification and validation system for the incoming food donations to further ensure the consumer’s safely and security. The system is also developed to be responsive especially for the basic pervasive computing devices - mobiles or smartphones of any dimensions to equip the donors as well as consumers to respectively donate and search the food pervasively.}
}
@article{RODRIGUEZPLANAS2022429,
title = {Gender norms in high school: Impacts on risky behaviors from adolescence to adulthood},
journal = {Journal of Economic Behavior & Organization},
volume = {196},
pages = {429-456},
year = {2022},
issn = {0167-2681},
doi = {https://doi.org/10.1016/j.jebo.2022.01.015},
url = {https://www.sciencedirect.com/science/article/pii/S016726812200021X},
author = {Nuria Rodríguez-Planas and Anna Sanz-de-Galdeano and Anastasia Terskaya},
keywords = {Gender norms, short-, medium- and long-run effects, risky behaviors and labor market outcomes, Add health},
abstract = {Engagement in risky behaviors is traditionally more prevalent among males than females, and the gap increases as youths move from adolescence to adulthood. Using the National Longitudinal Study of Adolescent to Adult Health, we identify a causal effect of exposure to high-school grade-mates with mothers who think that important skills for both boys and girls to possess are traditionally masculine ones (such as to think for oneself or work hard) as opposed to traditionally feminine ones (namely, to be well-behaved, popular, or help others) on the gender gap in teenagers’ engagement in risky behaviors. We find that a higher proportion of grade-mates’ mothers with non-traditional or non-stereotypical gender views who believe that independent thinking and working hard matter for either gender is associated with a reduction of the gender gap in risky behaviors both in the short and medium run. These results are driven by males curbing risky behaviors, suggesting that the relaxation of gender stereotypes results in boys behaving “more like girls”. In the long run, being exposed to grade-mates whose mothers have non-stereotypical gender beliefs reduces the gender gap in labor market outcomes by improving women's performance. This evidence, together with our exploration of several potential mechanisms, suggests that the transmission of gender norms is driving our results.}
}
@incollection{OKEKE2024,
title = {Responsible Consumption and Production},
booktitle = {Reference Module in Social Sciences},
publisher = {Elsevier},
year = {2024},
isbn = {978-0-443-15785-1},
doi = {https://doi.org/10.1016/B978-0-443-13701-3.00574-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780443137013005740},
author = {Austin Okeke and Juila J. Nobari and Mahtab Morovat and Yashar Salamzadeh},
keywords = {Responsible consumption, Responsible production, SDG goals, Sustainability},
abstract = {This chapter tries to share a clear and easy to understand view of “responsible consumption and production”. The first section of the chapter defines the importance of responsible consumption and responsible production and links it to SDG goals and sustainability. The second section of the chapter shares some main models of responsible consumption and production including the SURESCOM model, circular Business Models, Soft Consumption Decision-Making Models, Behavioral Models of Responsible Consumption and finally, Environmental Collaboration Frameworks. This short and comprehensive review helps both practitioners and researchers to get a comprehensive understanding of this important topic.}
}
@article{HAVENS2020104571,
title = {Automated Water Supply Model (AWSM): Streamlining and standardizing application of a physically based snow model for water resources and reproducible science},
journal = {Computers & Geosciences},
volume = {144},
pages = {104571},
year = {2020},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2020.104571},
url = {https://www.sciencedirect.com/science/article/pii/S0098300420305598},
author = {Scott Havens and Danny Marks and Micah Sandusky and Andrew Hedrick and Micah Johnson and Mark Robertson and Ernesto Trujillo},
keywords = {Hydrology, Computational method, Software engineering, Data assimilation},
abstract = {Reproducible science requires a shift in thinking and application for how data, code and analysis are shared. Now, scientists must act more like software engineers to design models and perform analysis that use principles and techniques pioneered by software developers. Creating reproducible models that are easy to use and understand is in the best interest for the snow and hydrology community, enabling studies by other researchers and facilitating technology transfer to operational applications. Here, we present the Automated Water Supply Model (AWSM) that streamlines and standardizes the workflow of a physically based snow model to create fully reproducible model simulations that can be utilized by researchers and operational water resource managers. AWSM orchestrates four core components that historically required significant, ad-hoc modeler interaction to load the input data, spatially interpolate to the modeling domain, run the models and process the outputs. Because AWSM was developed using principles and techniques from software engineering, users can quickly perform reproducible simulations on any operating system, from a laptop to the cloud. The three fully reproducible example case studies showcase the simplicity and flexibility of using AWSM to perform simulations from small research catchments to simulations that aid in real time water management decisions.}
}
@article{BRIAN2023129074,
title = {Introducing mindset streams to investigate stances towards STEM in high school students and experts},
journal = {Physica A: Statistical Mechanics and its Applications},
volume = {626},
pages = {129074},
year = {2023},
issn = {0378-4371},
doi = {https://doi.org/10.1016/j.physa.2023.129074},
url = {https://www.sciencedirect.com/science/article/pii/S0378437123006295},
author = {Kieran Brian and Massimo Stella},
keywords = {Cognitive modelling, Network science, Cognitive network science, Mindset reconstruction, STEM learning},
abstract = {We introduce mindset streams for assessing ways of bridging two target concepts in concept maps. We focus on behavioural forma mentis networks (BFMN), which map the associative and affective dimensions of memory recalls. Inspired by trains of thoughts taking several paths to link ideas, mindset streams are defined as BFMN subgraphs induced by all shortest paths between two target concepts, e.g. all recalls in shortest paths bridging “math” and “learning”. These streams quantify the following features of the mindset encoded in a BFMN: (i) semantic content (i.e. which ideas mediate connections between targets?), (ii) valence coherence/conflict (i.e. are connections mediated by entwining ideas perceived negatively, positively or neutrally?), and (iii) semantic relevance (i.e. are the bridges between targets peripheral or central for the connectivity/betweenness of the BFMN?). We investigate mindset streams between ‘maths”/“physics” and key motivational aspects of learning (“fun”, “work”, “failure”) in two BFMNs, encoding how 159 students and 59 experts perceived and associated concepts about Science Technology Engineering and Maths (STEM), respectively. Statistical comparisons against configuration models show that high schoolers bridge “maths” and “fun” only through overabundant levels of valence-conflicting associations, contrasting negatively perceived domain knowledge with peer-related positive experiences. This conflict is absent in the researchers’ mindset stream, which rather bridges “math” and “fun” through positive, science-related associations. The mindset streams of both groups bridge “maths” and “physics” to “work” through mostly positive career-related jargon. Students’ mindset streams of “failure” and “math”/“physics” are dominated by negative associations with test anxiety, whereas researchers integrate “failure” and “math”/“physics” in semantically richer and more positive contexts, denoting failure itself as a cornerstone of STEM learning. We discuss our findings and future research directions in view of relevant psychology/education literature.}
}
@incollection{CANAVERO202221,
title = {Chapter 2 - Cerebral: surface},
editor = {Jeffrey E. Arle and Jay L. Shils},
booktitle = {Essential Neuromodulation (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
pages = {21-48},
year = {2022},
isbn = {978-0-12-817000-7},
doi = {https://doi.org/10.1016/B978-0-12-817000-7.00002-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128170007000028},
author = {Sergio Canavero},
keywords = {Cortical Stimulation, Functional Neurosurgery, Pain, Psychiatry, TDCS, TMS},
abstract = {Cortical stimulation (CS) is a recent addition to electrical stimulation of the nervous system for therapeutic purposes. It can be administered both invasively (functional neurosurgery) or noninvasively mainly via transcranial magnetic stimulation or transcranial direct current stimulation. The primary indications for CS include neuropathic pain, and psychiatric disorders such as depression, but movement disorders, tinnitus, epilepsy, and coma rehabilitation are part of its purview. Neurosurgically implanted stimulators appear to achieve better results on a larger number of patients, but noninvasive stimulation has clear advantages in terms of safety and cost. Mixed results can be chalked up to individual anatomical and connectomic variations: the cortex is by far more complex than other neural, deeper targets, and current computational modeling efforts are underway to improve results. The next generation of CS will come with closed-loop capability and newer electrodes, in addition to refinements in personalized neuroimaging-guided targeting.}
}
@incollection{BOTTGE2010767,
title = {Math Instruction for Children with Special Needs},
editor = {Penelope Peterson and Eva Baker and Barry McGaw},
booktitle = {International Encyclopedia of Education (Third Edition)},
publisher = {Elsevier},
edition = {Third Edition},
address = {Oxford},
pages = {767-773},
year = {2010},
isbn = {978-0-08-044894-7},
doi = {https://doi.org/10.1016/B978-0-08-044894-7.01126-X},
url = {https://www.sciencedirect.com/science/article/pii/B978008044894701126X},
author = {B.A. Bottge},
keywords = {Cognitive strategy instruction, Concrete-to-representations-to-abstract (CRA) sequence, Curriculum-based assessment, Enhanced anchored instruction, K-12 math instruction, Learning disabilities, Meta-cognition, Schema-based instruction},
abstract = {Students with learning disabilities in math (denoted MD) display difficulties in developing conceptual understanding of number, in computation, and in formulating correct strategies for solving problems. Often, these students have concomitant reading difficulty, which severely limits their understanding of text-based problems. While explicit instruction of basic computation skills remains important, a greater emphasis is placed on the ability to solve problems, especially as a growing number of students with MD are included in general education classrooms. This article summarizes a small sample and brief descriptions of instructional interventions that hold promise for educating students with MD.}
}
@article{URKEN2012553,
title = {Designing evolvable systems in a framework of robust, resilient and sustainable engineering analysis},
journal = {Advanced Engineering Informatics},
volume = {26},
number = {3},
pages = {553-562},
year = {2012},
note = {Evolvability of Complex Systems},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2012.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S1474034612000535},
author = {Arnold B. Urken and Arthur {“Buck” Nimz} and Tod M. Schuck},
keywords = {Error Resilient Data Fusion (ERDF), Artificial systems, Cognitive radio, Electrical grids, Reflexive behaviors, System dynamics},
abstract = {“Evolvability” is a concept normally associated with biology or ecology, but recent work on control of interdependent critical infrastructures reveals that network informatics systems can be designed to enable artificial, human systems to “evolve”. To explicate this finding, we draw on an analogy between disruptive behavior and stable variation in the history of science and the adaptive patterns of robustness and resilience in engineered systems. We present a definition of an evolvable system in the context of a model of robust, resilient and sustainable systems. Our review of this context and standard definitions indicates that many analysts in engineering (as well as in biology and ecology) do not differentiate Resilience from Robustness. Neither do they differentiate overall dependable system adaptability from a multi-phase process that includes graceful degradation and time-constrained recovery, restabilization, and prevention of catastrophic failure. We analyze how systemic Robustness, Resilience, and Sustainability are related to Evolvability. Our analysis emphasizes the importance of Resilience as an adaptive capability that integrates Sustainability and Robustness to achieve Evolvability. This conceptual framework is used to discuss nine engineering principles that should frame systems thinking about developing evolvable systems. These principles are derived from Kevin Kelly’s book: Out of Control, which describes living and artificial self-sustaining systems. Kelly’s last chapter, “The Nine Laws of God,” distills nine principles that govern all life-like systems. We discuss how these principles could be applied to engineering evolvability in artificial systems. This discussion is motivated by a wide range of practical problems in engineered artificial systems. Our goal is to analyze a few examples of system designs across engineering disciplines to explicate a common framework for designing and testing artificial systems. This framework highlights managing increasing complexity, intentional evolution, and resistance to disruptive events. From this perspective, we envision a more imaginative and time-sensitive appreciation of the evolution and operation of “reliable” artificial systems. We conclude with a short discussion of two hypothetical examples of engineering evolvable systems in network-centric communications using Error Resilient Data Fusion (ERDF) and cognitive radio.}
}
@article{ZHEN2025103,
title = {A stochastic programming model for designing bus bridging services under metro disruptions},
journal = {Transportation Letters},
volume = {17},
number = {1},
pages = {103-118},
year = {2025},
issn = {1942-7867},
doi = {https://doi.org/10.1080/19427867.2024.2327811},
url = {https://www.sciencedirect.com/science/article/pii/S1942786724000146},
author = {Lu Zhen and Xueqin Du and Haolin Li and Zanyang Wu},
keywords = {Urban metro system, bus bridging, schedule, passenger assignment, stochastic programming, tabu search algorithm},
abstract = {ABSTRACT
With the growing reliance on urban metro networks, any accidental disruption can lead to rapid degradation and significant economic losses. Bus bridging services are common and efficient ways to minimize such adverse impacts. In this study, we investigate the problem of designing bus bridging services in response to unexpected metro disruptions, and propose a routing strategy with multiple bridging routes. In particular, to respond to uncertain factors such as passenger arrivals and bus travel times in the disruption environment, we develop a two-stage stochastic programming model for the collaborative optimization of bus bridging routes, schedules, and passenger assignments. To solve the computational challenges arising with the proposed model, a tailored tabu search algorithm is developed. Finally, several sets of numerical experiments are conducted and experimental results reveal that our proposed routing strategy can effectively improve the service level for the affected passengers during metro disruptions.}
}
@article{COMPANY2016108,
title = {A mixed derivative terms removing method in multi-asset option pricing problems},
journal = {Applied Mathematics Letters},
volume = {60},
pages = {108-114},
year = {2016},
issn = {0893-9659},
doi = {https://doi.org/10.1016/j.aml.2016.04.011},
url = {https://www.sciencedirect.com/science/article/pii/S0893965916301252},
author = {R. Company and V.N. Egorova and L. Jódar and F. Soleymani},
keywords = {Multiasset option pricing, Multidimensional partial differential equations, Mixed derivative terms,  factorization, Bunch–Kaufman factorization},
abstract = {The challenge of removing the mixed derivative terms of a second order multidimensional partial differential equation is addressed in this paper. The proposed method, which is based on proper algebraic factorization of the so-called diffusion matrix, depends on the semidefinite or indefinite character of this matrix. Computational cost of the transformed equation is considerably reduced and well-known numerical drawbacks are avoided.}
}
@article{ZHANG2023205,
title = {A survey for solving mixed integer programming via machine learning},
journal = {Neurocomputing},
volume = {519},
pages = {205-217},
year = {2023},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2022.11.024},
url = {https://www.sciencedirect.com/science/article/pii/S0925231222014035},
author = {Jiayi Zhang and Chang Liu and Xijun Li and Hui-Ling Zhen and Mingxuan Yuan and Yawen Li and Junchi Yan},
keywords = {Mixed integer programming, Machine learning, Combinatorial optimization},
abstract = {Machine learning (ML) has been recently introduced to solving optimization problems, especially for combinatorial optimization (CO) tasks. In this paper, we survey the trend of leveraging ML to solve the mixed-integer programming problem (MIP). Theoretically, MIP is an NP-hard problem, and most CO problems can be formulated as MIP. Like other CO problems, the human-designed heuristic algorithms for MIP rely on good initial solutions and cost a lot of computational resources. Therefore, researchers consider applying machine learning methods to solve MIP since ML-enhanced approaches can provide the solution based on the typical patterns from the training data. Specifically, we first introduce the formulation and preliminaries of MIP and representative traditional solvers. Then, we show the integration of machine learning and MIP with detailed discussions on related learning-based methods, which can be further classified into exact and heuristic algorithms. Finally, we propose the outlook for learning-based MIP solvers, the direction toward more combinatorial optimization problems beyond MIP, and the mutual embrace of traditional solvers and ML components. We maintain a list of papers that utilize machine learning technologies to solve combinatorial optimization problems, which is available at https://github.com/Thinklab-SJTU/awesome-ml4co.}
}
@article{SIMEONOV2017193,
title = {Some resonances between Eastern thought and Integral Biomathics in the framework of the WLIMES formalism for modeling living systems},
journal = {Progress in Biophysics and Molecular Biology},
volume = {131},
pages = {193-212},
year = {2017},
note = {Integral Biomathics 2017: The Necessary Conjunction of Western and Eastern Thought Traditions for Exploring the Nature of Mind and Life},
issn = {0079-6107},
doi = {https://doi.org/10.1016/j.pbiomolbio.2017.05.014},
url = {https://www.sciencedirect.com/science/article/pii/S0079610717301141},
author = {Plamen L. Simeonov and Andrée C. Ehresmann},
keywords = {Integral Biomathics, Artificial/synthetic and natural life, Phenomenology, Eastern philosophy, Higher-order logic, Wandering Logic Intelligence, Memory Evolutive Systems},
abstract = {Forty-two years ago, Capra published “The Tao of Physics” (Capra, 1975). In this book (page 17) he writes: “The exploration of the atomic and subatomic world in the twentieth century has …. necessitated a radical revision of many of our basic concepts” and that, unlike ‘classical’ physics, the sub-atomic and quantum “modern physics” shows resonances with Eastern thoughts and “leads us to a view of the world which is very similar to the views held by mystics of all ages and traditions.“ This article stresses an analogous situation in biology with respect to a new theoretical approach for studying living systems, Integral Biomathics (IB), which also exhibits some resonances with Eastern thought. Stepping on earlier research in cybernetics1 and theoretical biology,2 IB has been developed since 2011 by over 100 scientists from a number of disciplines who have been exploring a substantial set of theoretical frameworks. From that effort, the need for a robust core model utilizing advanced mathematics and computation adequate for understanding the behavior of organisms as dynamic wholes was identified. At this end, the authors of this article have proposed WLIMES (Ehresmann and Simeonov, 2012), a formal theory for modeling living systems integrating both the Memory Evolutive Systems (Ehresmann and Vanbremeersch, 2007) and the Wandering Logic Intelligence (Simeonov, 2002b). Its principles will be recalled here with respect to their resonances to Eastern thought.}
}
@incollection{RAHI2025597,
title = {Chapter 56 - Crowdsourcing and artificial intelligence based modeling framework for effective Public Healthcare Informatics and Smart eHealth System},
editor = {M.A. Ansari and R.S. Anand and Pragati Tripathi and Rajat Mehrotra and Md Belal Bin Heyat},
booktitle = {Artificial Intelligence in Biomedical and Modern Healthcare Informatics},
publisher = {Academic Press},
pages = {597-608},
year = {2025},
isbn = {978-0-443-21870-5},
doi = {https://doi.org/10.1016/B978-0-443-21870-5.00056-X},
url = {https://www.sciencedirect.com/science/article/pii/B978044321870500056X},
author = {Pankaj Rahi and Monika Dandotiya and Santi Basa and Souvik Sen and Mayur D. Jakhete and P. Vijayakumar},
keywords = {Artificial intelligence, Crowdsourcing, IoT, Machine learning, Public health informatics, Smart eHealth system},
abstract = {With the help of a multiagent interactive healthcare plan, healthy and independent aging is possible. Testing one's fitness and keeping tabs on one's health can both benefit from an awareness of one's regular routine. The Healthcare Strategies Partnership is introduced here. Geographic and economic factors, including the prevalence of infectious tropical diseases and an increasing number of chronic illnesses, have contributed to a shift in the region's medical requirements. This takes place in a world that is not only difficult but also intricate. This system employs a smartphone's sensor, a machine learning algorithm, multiple agents (including a doctor, fitness instructor, guardian, and intelligent ranker agent), and a smartphone itself to increase a user's sense of independence. A group of health professionals collaborate to assess the patient's day-to-day activities and offer suggestions for improvement. The algorithm figures out a typical day in the life of an adult. A smart autonomous agent using crowdsourced data recommends the best possible treatment plan. In contrast to crowdsourcing, which places value on the abilities of people to generate, aggregate, or filter original data, automatic tools make use of information retrieval techniques to analyze publicly available information. Crowdsourcing, which facilitates collaboration among numerous individuals, is increasingly being used in a variety of industries. Methodical crowdsourcing of useful data and human computation of interchangeable knowledge will aid future advancements in public health informatics. The disease burden on any country can and will be decreased by these efforts the share of healthcare costs borne by individuals and their families. The work also aids in achieving the most crucial objectives along the path to the Sustainable Development Goals. To improve public health surveillance for the prevention and control of communicable and noncommunicable diseases, this chapter presents the fundamental modeling for its design and development. So that a smart autonomous agent can recommend the best course of treatment, lowering the disease burden in any country. Crowdsourcing is defined, and how it can be used to better public health, in this chapter of a book. They are better able to achieve a healthy lifestyle exit, and the statistics and indicators that are tracked primarily under the Sustainable Development Goals Framework are improved as a result. The goal of this chapter is to provide a high-level overview of recent developments in applying artificial intelligence techniques to public health surveillance and response, which is the single most important step toward preventing the spread of disease and saving the lives of humans and other sentient beings.}
}
@article{VELICHKOVSKY2020547,
title = {New Insights into the Human Brain’s Cognitive Organization: Views from the Top, from the Bottom, from the Left and, particularly, from the Right},
journal = {Procedia Computer Science},
volume = {169},
pages = {547-557},
year = {2020},
note = {Postproceedings of the 10th Annual International Conference on Biologically Inspired Cognitive Architectures, BICA 2019 (Tenth Annual Meeting of the BICA Society), held August 15-19, 2019 in Seattle, Washington, USA},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.02.211},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920303343},
author = {Boris Velichkovsky and Artem Nedoluzhko and Elkhonon Goldberg and Olga Efimova and Fedor Sharko and Sergey Rastorguev and Anna Krasivskaya and Maxim Sharaev and Anastasia Korosteleva and Vadim Ushakov},
keywords = {fMRI, effective connectivity, levels of cognitive organization, dynamic causal modeling (DCM), protein-coding genes, microRNA, gene expression, modes of attention, hemispheric lateralization},
abstract = {The view that the left cerebral hemisphere in humans “dominates” over the “subdominant” right hemisphere has been so deeply entrenched in neuropsychology that no amount of evidence seems able to overcome it. In this article, we examine inhibitory cause-and-effect connectivity among human brain structures related to different parts of the triune evolutionary stratification —archicortex, paleocortex and neocortex— in relation to early and late phases of a prolonged resting-state functional magnetic resonance imaging (fMRI) experiment. With respect to the evolutionarily youngest parts of the human cortex, the left and right frontopolar regions, we also provide data on the asymmetries in underlying molecular mechanisms, namely on the differential expression of the protein-coding genes and regulatory microRNA sequences. In both domains of research, our results contradict the established view by demonstrating a pronounced right-to-left vector of causation in the hemispheric interaction at multiple levels of brain organization. There may be several not mutually exclusive explanations for the evolutionary significance of this pattern of lateralization. One of the explanations emphasizes the computational advantage of separating the neural substrates for processing novel information ("exploration") mediated predominantly by the right hemisphere, and processing with reliance on established cognitive routines and representations ("exploitation") mediated predominantly by the left hemisphere.}
}
@article{SUTTON2004503,
title = {Representation, levels, and context in integrational linguistics and distributed cognition},
journal = {Language Sciences},
volume = {26},
number = {6},
pages = {503-524},
year = {2004},
note = {Distributed cognition and integrational linguistics},
issn = {0388-0001},
doi = {https://doi.org/10.1016/j.langsci.2004.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S0388000104000452},
author = {John Sutton},
keywords = {Integrational linguistics, Distributed cognition, Mental representation, Reduction, Context, Memory},
abstract = {Distributed Cognition and Integrational Linguistics have much in common. Both approaches see communicative activity and intelligent behaviour in general as strongly context-dependent and action-oriented, and brains as permeated by history. But there is some tension between the two frameworks on three important issues. The majority of theorists of distributed cognition want to maintain some notions of mental representation and computation, and to seek generalizations and patterns in the various ways in which creatures like us couple with technologies, media, and other agents; many also want to offer explanations at subpersonal levels which may undercut the autonomy of personal-level accounts. In contrast, dominant views in integrational linguistics reject all invocation of representation, resist the explanatory search for similarity across contexts and moments, and see linguistics as a lay discipline which should not offer explanations in terms alien to ordinary agents. On each of these issues, I argue that integrationists could move closer to the distributed cognition framework without losing the most important aspects of their view: integrationist criticisms of mainstream or classical theories can be respected while alliances with revised cognitivist views about representation, context, and explanation are developed.}
}
@article{WANG2023e131,
title = {Interbody Fusion Cage Design Driven by Topology Optimization},
journal = {World Neurosurgery},
volume = {174},
pages = {e131-e143},
year = {2023},
issn = {1878-8750},
doi = {https://doi.org/10.1016/j.wneu.2023.03.010},
url = {https://www.sciencedirect.com/science/article/pii/S1878875023003042},
author = {Zuowei Wang and Jun Jiang and Fengzeng Jian and Zan Chen and Xingwen Wang and Wanru Duan and Weisheng Zhang},
keywords = {Fusion cage design, Interbody fusion, Moving morphable void approach, Topology optimization},
abstract = {Objective
We used topology optimization technology to explore the new theory and method of interbody fusion cage design and realized an innovative design of interbody cages.
Methods
The lumbar spine of a normal healthy volunteer was scanned to perform reverse modeling. Based on the scan data for the L1-L2 segments of the lumbar spine, a three dimensional model was reconstructed to obtain the complete simulation model of the L1-L2 segment. The boundary inversion method was used to obtain approximately isotropic material parameters that can effectively characterize the mechanical behavior of vertebrae, thereby reducing the computational complexity. The topology description function was used to model the clinically used traditional fusion cage to obtain Cage A. The moving morphable void-based topology optimization method was used for the integrated design of size, shape, and topology to obtain the optimized fusion cage, Cage B.
Results
The volume fraction of the bone graft window in Cage B was 74.02%, which was 60.67% higher than that (46.07%) in Cage A. Additionally, the structural strain energy in the design domain of Cage B was 1.48 mJ, which was lower than that of Cage A (satisfying the constraints). The maximum stress in the design domain of Cage B was 5.336 Mpa, which was 35.6% lower than that (8.286 Mpa) of Cage A. In addition, the surface stress distribution of Cage B was more uniform than that of Cage A.
Conclusions
This study proposed a new innovative design method for interbody fusion cages, which not only provides new insights into the innovative design of interbody fusion cages but may also guide the customized design of interbody fusion cages in different pathological environments.}
}
@article{PATTON20051082,
title = {The role of scanning in open intelligence systems},
journal = {Technological Forecasting and Social Change},
volume = {72},
number = {9},
pages = {1082-1093},
year = {2005},
note = {New Horizons and Challenges for Future-Oriented Technology Analysis: The 2004 EU-US Seminar},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2004.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S0040162504001441},
author = {Kermit M. Patton},
keywords = {Scanning process, Open intelligence systems, SRIC-BI},
abstract = {Every month, SRI Consulting Business Intelligence (SRIC-BI) professionals assemble more than 100 short abstracts of developments that they perceive to be signals of change, discontinuities, inflection points, outliers, or disruptive developments. The effort is part of a continuous scanning process and Scan program that allows SRIC-BI to gauge the ongoing turbulent confluence of culture, commerce, and technology that defines today's business environment. For more than 25 years, scanning has played an essential role in SRIC-BI's and SRI International's foresight capabilities by providing a systematic means for surveying the broad external environment for change vectors. Traditional monitoring processes in most organizations are largely arbitrary, depending on what concerned individuals or leaders in the organization are reading, thinking about, and sharing informally with each other. But in today's world, arbitrary is insufficient. No foresight function can operate with confidence without a disciplined process for spotting new patterns of change and bringing those issues into the organization for early consideration and action. This article describes the scanning process as SRIC-BI practices it, the importance of open intelligence systems, what benefits the scanning process can provide to organizations, and what problems organizations typically run into when setting up scanning systems.}
}
@article{DUMONTHEIL20101574,
title = {Taking perspective into account in a communicative task},
journal = {NeuroImage},
volume = {52},
number = {4},
pages = {1574-1583},
year = {2010},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2010.05.056},
url = {https://www.sciencedirect.com/science/article/pii/S1053811910007895},
author = {Iroise Dumontheil and Olivia Küster and Ian A. Apperly and Sarah-Jayne Blakemore},
keywords = {Theory of mind, Perspective taking, Social brain, Social cognition, Decision making, Inhibition},
abstract = {Previous neuroimaging studies of spatial perspective taking have tended not to activate the brain's mentalising network. We predicted that a task that requires the use of perspective taking in a communicative context would lead to the activation of mentalising regions. In the current task, participants followed auditory instructions to move objects in a set of shelves. A 2×2 factorial design was employed. In the Director factor, two directors (one female and one male) either stood behind or next to the shelves, or were replaced by symbolic cues. In the Object factor, participants needed to use the cues (position of the directors or symbolic cues) to select one of three possible objects, or only one object could be selected. Mere presence of the Directors was associated with activity in the superior dorsal medial prefrontal cortex (MPFC) and the superior/middle temporal sulci, extending into the extrastriate body area and the posterior superior temporal sulcus (pSTS), regions previously found to be responsive to human bodies and faces respectively. The interaction between the Director and Object factors, which requires participants to take into account the perspective of the director, led to additional recruitment of the superior dorsal MPFC, a region activated when thinking about dissimilar others' mental states, and the middle temporal gyri, extending into the left temporal pole. Our results show that using perspective taking in a communicative context, which requires participants to think not only about what the other person sees but also about his/her intentions, leads to the recruitment of superior dorsal MPFC and parts of the social brain network.}
}
@article{WANG2023101451,
title = {Unpacking the essential tension of knowledge recombination: Analyzing the impact of knowledge spanning on citation impact and disruptive innovation},
journal = {Journal of Informetrics},
volume = {17},
number = {4},
pages = {101451},
year = {2023},
issn = {1751-1577},
doi = {https://doi.org/10.1016/j.joi.2023.101451},
url = {https://www.sciencedirect.com/science/article/pii/S1751157723000767},
author = {Cheng-Jun Wang and Lihan Yan and Haochuan Cui},
keywords = {Knowledge Recombination, Category spanning, Disruption, Citation, Team size},
abstract = {Drawing on the theories of knowledge recombination, we aim to unpack the essential tension between tradition and innovation in scientific research. Using the American Physical Society data and computational methods, we analyze the relationship between knowledge spanning, citation impact, and disruptive innovation. The findings show that there exists a U-shaped relationship between knowledge spanning and disruptive innovation. In contrast, there is an inverted U-shaped relationship between knowledge spanning and citation impact, and the inverted U-shaped effect is moderated by team size. This study contributes to the theories of knowledge recombination by suggesting that either intellectual conformism or knowledge recombination can lead to disruptive innovation. That is, when evaluating the quality of scientific research with disruptive innovation, the essential tension seems to disappear.}
}
@incollection{CHRISTAKOS2004661,
title = {The cognitive basis of physical modelling},
editor = {Cass T. Miller and Matthew W. Farthing and William G. Gray and George F. Pinder},
series = {Developments in Water Science},
publisher = {Elsevier},
volume = {55},
pages = {661-669},
year = {2004},
booktitle = {Computational Methods in Water Resources: Volume 1},
issn = {0167-5648},
doi = {https://doi.org/10.1016/S0167-5648(04)80089-3},
url = {https://www.sciencedirect.com/science/article/pii/S0167564804800893},
author = {G. Christakos},
abstract = {We revisit the meaning of the term “solution” with regards to a physical model representing a natural system. We suggest that a (non-conventional) epistemic cognition solution (assuming that the model describes incomplete knowledge about nature, and focusing on conceptual mechanisms of scientific thinking) can lead to more realistic results than a (conventional) ontologic solution (assuming that the model describes nature as is, and focusing on form manipulations).}
}
@article{LIN202052,
title = {A novel deep neural network based approach for sparse code multiple access},
journal = {Neurocomputing},
volume = {382},
pages = {52-63},
year = {2020},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2019.11.066},
url = {https://www.sciencedirect.com/science/article/pii/S0925231219316686},
author = {Jinzhi Lin and Shengzhong Feng and Yun Zhang and Zhile Yang and Yong Zhang},
keywords = {Sparse code multiple access, Non-orthogonal multiple access, Machine learning, Dense code multiple access},
abstract = {Sparse code multiple access (SCMA) has been one of the non-orthogonal multiple access (NOMA) schemes aiming to support high spectral efficiency and ubiquitous access requirements for 5G communication networks. Conventional SCMA approaches are confronting challenges in designing low-complexity high-accuracy decoding algorithm and constructing optimum codebooks. Fortunately, the recent spotlighted deep learning technologies are of significant potentials in solving many communication engineering problems. Inspired by this, we propose and train a deep neural network (DNN) called DL-SCMA to learn to decode SCMA modulated signals corrupted by additive white Gaussian noise (AWGN). An autoencoder called AE-SCMA is established and trained to generate optimal SCMA codewords and reconstruct original bits. Furthermore, by manipulating the mapping vectors, an autoencoder is able to generalize SCMA, thus a dense code multiple access (DCMA) scheme is proposed. Simulations show that the DNN SCMA decoder significantly outperforms the conventional message passing algorithm (MPA) in terms of bit error rate (BER), symbol error rate (SER) and computational complexity, and AE-SCMA also demonstrates better performances via constructing better SCMA codebooks. The performance of deep learning aided DCMA is superior to the SCMA.}
}
@incollection{MOTTAMONTESERRAT202189,
title = {Chapter 5 - Computer language and linguistics},
editor = {Dioneia {Motta Monte-Serrat} and Carlo Cattani},
booktitle = {The Natural Language for Artificial Intelligence},
publisher = {Academic Press},
pages = {89-120},
year = {2021},
series = {Cognitive Data Science in Sustainable Computing},
isbn = {978-0-12-824118-9},
doi = {https://doi.org/10.1016/B978-0-12-824118-9.00005-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128241189000059},
author = {Dioneia {Motta Monte-Serrat} and Carlo Cattani},
keywords = {Computer language, Linguistics, Symbolic language, Generative model, Algorithmic core, Axiomatic-logical structure},
abstract = {Language is the ability to use complex communication systems and a set of signals used to encode and decode information. Mathematics and linguistics deal with the representation of thought through symbolic language under a guiding structure of language functioning. Computer language is formal, symbolic, and depends on linguistic principles of natural language. Computing regulates the behavior of the machine in the execution of specific tasks and, for that, generative models were developed. The underlying structure of the latter would be adequate to incorporate complex and high-dimensional data in a latent space with a supposed ability to replicate the original data. The distribution in probabilistic terms, however, does not reflect reality. We suggest the logical-axiomatic principle of natural language as an algorithmic core of computational methods capable of generalizing the execution of a set of factors.}
}
@article{MISCHKOWSKI201885,
title = {Think it through before making a choice? Processing mode does not influence social mindfulness},
journal = {Journal of Experimental Social Psychology},
volume = {74},
pages = {85-97},
year = {2018},
issn = {0022-1031},
doi = {https://doi.org/10.1016/j.jesp.2017.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S0022103117302858},
author = {Dorothee Mischkowski and Isabel Thielmann and Andreas Glöckner},
keywords = {Social mindfulness, Processing mode, Intuition versus deliberation, Spontaneous cooperation, Prosocial personality},
abstract = {Social mindfulness has recently been introduced as a type of prosocial behavior that emphasizes the importance of a skill to see other people's needs beyond the will to act accordingly. Correspondingly, social mindfulness has been proposed to involve processes of executive functioning and thus of deliberate thinking. In four studies, we tested the influence of processing mode on social mindfulness using different experimental manipulations (i.e., instructions to decide intuitively vs. deliberately, time pressure, and cognitive load). Contrary to the idea that social mindfulness requires conscious processing – and unlike recent findings suggesting intuitive cooperation – we consistently found negligible effect sizes for the influence of processing mode on social mindfulness. This was observable for both, prosocial and selfish individuals alike (i.e., those with high vs. low levels in Social Value Orientation or Honesty-Humility, respectively). Overall, the findings suggest that social mindfulness constitutes a general tendency to perceive and act prosocially in social situations that is unaffected by processing mode and, by implication, distinguishable from other types of prosocial behavior.}
}
@article{DANNENHAUER2014226,
title = {Toward Meta-level Control of Autonomous Agents},
journal = {Procedia Computer Science},
volume = {41},
pages = {226-232},
year = {2014},
note = {5th Annual International Conference on Biologically Inspired Cognitive Architectures, 2014 BICA},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2014.11.107},
url = {https://www.sciencedirect.com/science/article/pii/S187705091401552X},
author = {Dustin Dannenhauer and Michael T. Cox and Shubham Gupta and Matt Paisner and Don Perlis},
keywords = {Computational metacognition, cognitive architecture, metareasoning, long-duration autonomy},
abstract = {Metareasoning is an important capability for autonomous systems, particularly for those being deployed on long duration missions. An agent with increased self-observation and the ability to control itself in response to changing environments will be more capable in achieving its goals. This is essential for long-duration missions where system designers will not be able to, theoretically or practically, predict all possible problems that the agent may encounter. In this paper we describe preliminary work that integrates the metacognitive architecture MIDCA with an autonomous TREX agent, creating a more self-observable and adaptive agent.}
}
@article{OKAMURA2021,
title = {NMB4.0: development of integrated nuclear fuel cycle simulator from the front to back-end},
journal = {EPJ - Nuclear Sciences & Technologies},
volume = {7},
year = {2021},
issn = {2491-9292},
doi = {https://doi.org/10.1051/epjn/2021019},
url = {https://www.sciencedirect.com/science/article/pii/S2491929221000145},
author = {Tomohiro Okamura and Ryota Katano and Akito Oizumi and Kenji Nishihara and Masahiko Nakase and Hidekazu Asano and Kenji Takeshita},
abstract = {Nuclear Material Balance code version 4.0 (NMB4.0) has been developed through collaborative R&D between TokyoTech&JAEA. Conventional nuclear fuel cycle simulation codes mainly analyze actinides and are specialized for front-end mass balance analysis. However, quantitative back-end simulation has recently become necessary for considering R&D strategies and sustainable nuclear energy utilization. Therefore, NMB4.0 was developed to realize the integrated nuclear fuel cycle simulation from front- to back-end. There are three technical features in NMB4.0: 179 nuclides are tracked, more than any other code, throughout the nuclear fuel cycle; the Okamura explicit method is implemented, which contributes to reducing the numerical cost while maintaining the accuracy of depletion calculations on nuclides with a shorter half-life; and flexibility of back-end simulation is achieved. The main objective of this paper is to show the newly developed functions, made for integrated back-end simulation, and verify NMB4.0 through a benchmark study to show the computational performance.}
}
@article{COOPER20091351,
title = {Emergence as a computability-theoretic phenomenon},
journal = {Applied Mathematics and Computation},
volume = {215},
number = {4},
pages = {1351-1360},
year = {2009},
note = {Physics and Computation},
issn = {0096-3003},
doi = {https://doi.org/10.1016/j.amc.2009.04.050},
url = {https://www.sciencedirect.com/science/article/pii/S0096300309004159},
author = {S. Barry Cooper},
keywords = {Computability, Emergence, Definability, Turing invariance},
abstract = {In dealing with emergent phenomena, a common task is to identify useful descriptions of them in terms of the underlying atomic processes, and to extract enough computational content from these descriptions to enable predictions to be made. Generally, the underlying atomic processes are quite well understood, and (with important exceptions) captured by mathematics from which it is relatively easy to extract algorithmic content. A widespread view is that the difficulty in describing transitions from algorithmic activity to the emergence associated with chaotic situations is a simple case of complexity outstripping computational resources and human ingenuity. Or, on the other hand, that phenomena transcending the standard Turing model of computation, if they exist, must necessarily lie outside the domain of classical computability theory. In this talk we suggest that much of the current confusion arises from conceptual gaps and the lack of a suitably fundamental model within which to situate emergence. We examine the potential for placing emergent relations in a familiar context based on Turing’s 1939 model for interactive computation over structures described in terms of reals. The explanatory power of this model is explored, formalising informal descriptions in terms of mathematical definability and invariance, and relating a range of basic scientific puzzles to results and intractable problems in computability theory.}
}
@article{KWON2024488,
title = {On knowing a gene: A distributional hypothesis of gene function},
journal = {Cell Systems},
volume = {15},
number = {6},
pages = {488-496},
year = {2024},
issn = {2405-4712},
doi = {https://doi.org/10.1016/j.cels.2024.04.008},
url = {https://www.sciencedirect.com/science/article/pii/S2405471224001236},
author = {Jason J. Kwon and Joshua Pan and Guadalupe Gonzalez and William C. Hahn and Marinka Zitnik},
keywords = {lexical semantics, gene function, machine learning, artificial intelligence, distributed representations, word embeddings, large language models, transformers},
abstract = {Summary
As words can have multiple meanings that depend on sentence context, genes can have various functions that depend on the surrounding biological system. This pleiotropic nature of gene function is limited by ontologies, which annotate gene functions without considering biological contexts. We contend that the gene function problem in genetics may be informed by recent technological leaps in natural language processing, in which representations of word semantics can be automatically learned from diverse language contexts. In contrast to efforts to model semantics as “is-a” relationships in the 1990s, modern distributional semantics represents words as vectors in a learned semantic space and fuels current advances in transformer-based models such as large language models and generative pre-trained transformers. A similar shift in thinking of gene functions as distributions over cellular contexts may enable a similar breakthrough in data-driven learning from large biological datasets to inform gene function.}
}
@article{KUIPERS2008155,
title = {Drinking from the firehose of experience},
journal = {Artificial Intelligence in Medicine},
volume = {44},
number = {2},
pages = {155-170},
year = {2008},
note = {Artificial Consciousness},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2008.07.010},
url = {https://www.sciencedirect.com/science/article/pii/S0933365708000985},
author = {Benjamin Kuipers},
keywords = {Consciousness, Sensory trackers, Information content, Dynamical systems},
abstract = {Summary
Objective
Computational concepts from robotics and computer vision hold great promise to account for major aspects of the phenomenon of consciousness, including philosophically problematical aspects such as the vividness of qualia, the first-person character of conscious experience, and the property of intentionality.
Methods
We present a dynamical systems model describing human or robotic agents and their interaction with the environment. In order to cope with the enormous information content of the sensory stream, this model includes trackers for selected coherent spatio–temporal portions of the sensory input stream, and a self-constructed plausible coherent narrative describing the recent history of the agent’s sensorimotor interaction with the world.
Results
We describe how an agent can autonomously learn its own intentionality by constructing computational models of hypothetical entities in the external world. These models explain regularities in the sensorimotor interaction, and serve as referents for the agent’s symbolic knowledge representation. The high information content of the sensory stream allows the agent to continually evaluate these hypothesized models, refuting those that make poor predictions. The high information content of the sensory input stream also accounts for the vividness and uniqueness of subjective experience. We then evaluate our account against 11 features of consciousness “that any philosophical–scientific theory should hope to explain”, according to the philosopher and prominent AI critic John Searle.
Conclusion
The essential features of consciousness can, in principle, be implemented on a robot with sufficient computational power and a sufficiently rich sensorimotor system, embodied and embedded in its environment.}
}
@article{HE2023126590,
title = {Global priors guided modulation network for joint super-resolution and SDRTV-to-HDRTV},
journal = {Neurocomputing},
volume = {554},
pages = {126590},
year = {2023},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2023.126590},
url = {https://www.sciencedirect.com/science/article/pii/S0925231223007130},
author = {Gang He and Shaoyi Long and Li Xu and Chang Wu and Wenxin Yu and Jinjia Zhou},
keywords = {Convolutional neural network, Super-resolution, SDRTV-to-HDRTV, High dynamic range},
abstract = {Watching low resolution standard dynamic range (LR SDR) video on a 4K high dynamic range (HDR) TV is not the best viewing experience. Joint super-resolution (SR) and SDRTV-to-HDRTV aims to enhance the visual quality of LR SDR videos that have quality deficiencies in resolution and dynamic range. Previous methods that rely on learning local information typically cannot do well in preserving color conformity and long-range structural similarity, resulting in unnatural color transition and texture artifacts. In order to tackle these challenges, we propose a global priors guided modulation network (GPGMNet). In particular, we design a global priors extraction module (GPEM) to extract color conformity prior and structural similarity prior that are beneficial for SDRTV-to-HDRTV and SR tasks, respectively. To further exploit the global priors and preserve spatial information, we devise multiple global priors-guided spatial-wise modulation blocks (GSMBs) with a few parameters for intermediate feature modulation. In these GSMBs, the modulation parameters are generated by the shared global priors and the spatial features map from the spatial pyramid convolution block (SPCB). With these elaborate designs, the GPGMNet can achieve higher visual quality with lower computational complexity. Extensive experiments demonstrate that our proposed GPGMNet is superior to the state-of-the-art methods. Specifically, our proposed model exceeds the state-of-the-art by 0.64 dB in PSNR, with 69% fewer parameters and 3.1× speedup.}
}
@article{LI2024101038,
title = {One-stop multi-sensor fusion and multimodal precise quantified traditional Chinese medicine imaging health examination technology},
journal = {Journal of Radiation Research and Applied Sciences},
volume = {17},
number = {4},
pages = {101038},
year = {2024},
issn = {1687-8507},
doi = {https://doi.org/10.1016/j.jrras.2024.101038},
url = {https://www.sciencedirect.com/science/article/pii/S168785072400222X},
author = {Chuanxue Li and Ping Wang and Meifang Zheng and Wenxiang Li and Jun Zhou and Lin Fu},
keywords = {Traditional Chinese medicine imaging, Large language model, Knowledge graphs, Multimodal imaging, Health examination technology, Image fusion, Imaging agent, Deep learning},
abstract = {Except for single-mode traditional Chinese medicine imaging techniques such as infrared thermal imaging, the one-stop multimodal whole-body imaging health examination technology and device is still blank. We focus on infrared thermal imaging as the main modality, integrated various modalities of medical imaging intelligent sensing agents such as terahertz imaging. The upper and lower computer and virtual instrument architecture are used, and the imaging data are collected by the lower computers that each is an intelligent sensing agent. The upper computer is used for image reconstruction with intelligent algorithms. Based on the core theory of traditional Chinese medicine, intelligent fusion imaging is achieved through various modalities to achieve the ‘observation, hearing, questioning, and palpation’ four diagnostic integration. We use fractional Fourier transform to filter imaging data, Laplacian pyramid for image fusion. We have proposed an implementation method and process for combining traditional Chinese medicine imaging large language model with knowledge graph, and based on deep learning, we have studied the image and report generation algorithm that combines traditional Chinese medicine pathology and four diagnostic methods with knowledge graph fusion, as well as the traditional Chinese medicine human physiological and pathological interpretation and evaluation system. We have achieved some results, and through further research and development, we can achieve commercial applications.}
}
@article{PICCININI2008311,
title = {Some neural networks compute, others don’t},
journal = {Neural Networks},
volume = {21},
number = {2},
pages = {311-321},
year = {2008},
note = {Advances in Neural Networks Research: IJCNN ’07},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2007.12.010},
url = {https://www.sciencedirect.com/science/article/pii/S089360800700250X},
author = {Gualtiero Piccinini},
keywords = {Connectionism, Neural network, Computation, Mechanism, Cognition, Brain},
abstract = {I address whether neural networks perform computations in the sense of computability theory and computer science. I explicate and defend the following theses. (1) Many neural networks compute—they perform computations. (2) Some neural networks compute in a classical way. Ordinary digital computers, which are very large networks of logic gates, belong in this class of neural networks. (3) Other neural networks compute in a non-classical way. (4) Yet other neural networks do not perform computations. Brains may well fall into this last class.}
}
@article{BELL200163,
title = {Futures studies comes of age: twenty-five years after The limits to growth},
journal = {Futures},
volume = {33},
number = {1},
pages = {63-76},
year = {2001},
issn = {0016-3287},
doi = {https://doi.org/10.1016/S0016-3287(00)00054-9},
url = {https://www.sciencedirect.com/science/article/pii/S0016328700000549},
author = {Wendell Bell},
abstract = {Twenty-five years ago, the publication of The limits to growth marked a period of accomplishments in the futures field. Today, futures studies is experiencing another burst of development and is ready to move more fully into mainstream intellectual life and the standard educational curriculum. In addition to continued work on methods, theory, and empirical research, the resolution of three issues might help persuade established academic communities of the serious purposes and sound intellectual contributions of futurists. They are (1) the adoption of an adequate theory of knowledge (critical realism is proposed), (2) the recognition that prediction does play a role in futures studies (so we can deal explicitly with the philosophical challenges it poses), and (3) the formulation and justification of core values (so we have a valid basis by which to judge the desirability of alternative futures). I propose a critical discourse among futurists in order to resolve each issue. The desire to make futures thinking a part of everyone's education is not, of course, mere futurist chauvinism, but is based on the conviction that futures studies has important contributions to make to human well-being.}
}
@article{OUYANG2024e29176,
title = {Unmasking the challenges in ideological and political education in China: A thematic review},
journal = {Heliyon},
volume = {10},
number = {8},
pages = {e29176},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e29176},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024052071},
author = {Sha Ouyang and Wei Zhang and Jian Xu and Abdullah {Mat Rashid} and Shwu Pyng How and Aminuddin {Bin Hassan}},
keywords = {Ideological and political education, China, Thematic review},
abstract = {China's distinctive educational approach, particularly its emphasis on ideological and political education, has garnered considerable academic attention for its impact on shaping individual values, fostering citizenship, and maintaining social stability. Despite the Chinese government's prioritization of ideological and political education, academic research in this field appears constrained, with existing studies predominantly focusing on normative and descriptive aspects. Normative research delineates how ideological and political education should be executed, while descriptive research illustrates its practical implementation. The effectiveness of these approaches is significantly diminished if they are not adequately interconnected—when only the current reality is explained without providing tools for improvement or when prescribed steps for improvement lack a basis in specific contexts. This paper conducts a comprehensive review of research on ideological and political education using ATLAS. ti 9 for thematic analysis. The review aims to unveil the intricate landscape of current research in China and address key questions: What are the primary trends in the literature on ideological and political education between 2021 and July 2023? What challenges does ideological and political education face? Through a direct exploration of these issues, this paper seeks to optimize the ideological and political education system, elevate its adaptability and effectiveness, and open avenues for research, fostering a more dynamic, inclusive, and resilient development of ideological and political education.}
}
@article{LUCKRING2023100950,
title = {Model validation hierarchies for connecting system design to modeling and simulation capabilities},
journal = {Progress in Aerospace Sciences},
volume = {142},
pages = {100950},
year = {2023},
issn = {0376-0421},
doi = {https://doi.org/10.1016/j.paerosci.2023.100950},
url = {https://www.sciencedirect.com/science/article/pii/S0376042123000660},
author = {James M. Luckring and Scott Shaw and William L. Oberkampf and Rick E. Graves},
keywords = {Modeling and simulation, Validation hierarchy, Systems architecture, Physics taxonomy, Surface-to-air missile defense system},
abstract = {Hierarchical structures provide a means to systematically deconstruct an engineering system of arbitrary complexity into its subsystems, components, and physical processes. Model validation hierarchies can aid in understanding the coupling and interaction of subsystems and components, as well as improve the understanding of how simulation models are used to design and optimize the engineering system of interest. The upper tiers of the hierarchy address systems and subsystems architecture decompositions, while the lower tiers address physical processes that are both coupled and uncoupled. Recent work connects these two general sections of the hierarchy through a transition tier, which blends the focus of system functionality and physics modeling activities. This work also includes a general methodology for how a model validation hierarchy can be constructed for any type of engineering system in any operating environment, e.g., land, air, sea, or space. We review previous work on the construction and use of model validation hierarchies in not only the field of aerospace systems, but also from commercial nuclear power plant systems. Then an example of a detailed model validation hierarchy is constructed and discussed for a surface-to-air missile defense system with an emphasis on the missile subsystems.}
}
@article{HOPPENSTEADT201599,
title = {Spin torque oscillator neuroanalog of von Neumann's microwave computer},
journal = {Biosystems},
volume = {136},
pages = {99-104},
year = {2015},
note = {Selected papers presented at the Eleventh International Workshop on Neural Coding, Versailles, France, 2014},
issn = {0303-2647},
doi = {https://doi.org/10.1016/j.biosystems.2015.06.006},
url = {https://www.sciencedirect.com/science/article/pii/S0303264715000891},
author = {Frank Hoppensteadt},
keywords = {Neuromorphic computing, Spintronics, Spin torque oscillator, Spin waves, Logic machine, Nanomagnetics, Von Neumann microwave computer},
abstract = {Frequency and phase of neural activity play important roles in the behaving brain. The emerging understanding of these roles has been informed by the design of analog devices that have been important to neuroscience, among them the neuroanalog computer developed by O. Schmitt and A. Hodgkin in the 1930s. Later J. von Neumann, in a search for high performance computing using microwaves, invented a logic machine based on crystal diodes that can perform logic functions including binary arithmetic. Described here is an embodiment of his machine using nano-magnetics. Electrical currents through point contacts on a ferromagnetic thin film can create oscillations in the magnetization of the film. Under natural conditions these properties of a ferromagnetic thin film may be described by a nonlinear Schrödinger equation for the film's magnetization. Radiating solutions of this system are referred to as spin waves, and communication within the film may be by spin waves or by directed graphs of electrical connections. It is shown here how to formulate a STO logic machine, and by computer simulation how this machine can perform several computations simultaneously using multiplexing of inputs, that this system can evaluate iterated logic functions, and that spin waves may communicate frequency, phase and binary information. Neural tissue and the Schmitt-Hodgkin, von Neumann and STO devices share a common bifurcation structure, although these systems operate on vastly different space and time scales; namely, all may exhibit Andronov-Hopf bifurcations. This suggests that neural circuits may be capable of the computational functionality as described by von Neumann.}
}
@article{HUANG2025105310,
title = {Modeling student teachers’ self-regulated learning of complex professional knowledge: A sequential and clustering analysis with think-aloud protocols},
journal = {Computers & Education},
volume = {233},
pages = {105310},
year = {2025},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2025.105310},
url = {https://www.sciencedirect.com/science/article/pii/S0360131525000788},
author = {Lingyun Huang and Ying Zhan and Shen Ba},
keywords = {Self-regulated learning, Technological Pedagogical Content Knowledge, Sequential clustering analysis, Tthink-aloud protocols},
abstract = {There has been much discussion regarding the positive relationship between self-regulated learning (SRL) and technological pedagogical content knowledge (TPACK) development for student teachers. This study continued this claim and adopted advanced analytical methods to explain how SRL influences TPACK learning. Think-aloud protocols from 39 participants were collected and transcribed when they were learning TPACK by designing technology-infused lessons with nBrowser, a computer-based learning environment. Based on models, nine critical SRL events were retrieved from participants‘ think-aloud protocols and analyzed through sequential clustering analysis. The results show two SRL groups indicating distinct self-regulatory sequential patterns. One group had a shorter sequence length and dominantly enacted elaboration activities (Low-SRL group), while the other had longer sequence lengths and engaged in diverse SRL activities (High-regulation group). Relating to TPACK performance indicated by the quality of lesson plans, the results reveal that the participants in the High-SRL group outperformed their counterparts in the Low-SRL group. The findings are consistent with previous evidence and provide implications for practitioners about the importance of student teachers’ self-regulation trajectories.}
}
@article{HUANG202219,
title = {Numerical simulation and comparative study for the zinc smelting furnaces at the Tongmuling site in Qing Dynasty, Hunan Province, China},
journal = {Advances in Archaeomaterials},
volume = {3},
number = {1},
pages = {19-27},
year = {2022},
issn = {2667-1360},
doi = {https://doi.org/10.1016/j.aia.2022.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S2667136022000061},
author = {Xing Huang and Linheng Mo and Wenli Zhou and Shengqiang Luo and Ya Xiao and Jianli Chen},
keywords = {Zinc smelting furnace, Furnace profile, Retort, Numerical simulation, Tongmuling site},
abstract = {Brass, which appears golden in color, used to be a valuable alloy in ancient times. During the Ming and Qing Dynasties, the Chinese used special furnaces to smelt zinc for minting and exporting to overseas in large quantities. Archeological findings have revealed the overall structure of the zinc smelting furnaces at the Tongmuling site during the Qing Dynasty. In this study, computational fluid dynamics software was employed to simulate airflow fields within a furnace. Consequently, we observed that airflows were concentrated at the center of the lower chamber, after which they dispersed into the upper chamber through ceramic pads and finally were evenly distributed between the retorts. Increasing furnace height and improving thermal convection in the lower chamber helped increase the furnace temperature. The ceramic pads adjusted the airflow to ensure that temperature distribution in the upper chamber was uniform, and they supported burning in the upper chamber by preventing collapse. Compared with the heap smelting process recorded in Heavenly Creations and the large crucible furnaces used in modern times, zinc smelting furnaces at the Tongmuling site possess a unique structure. They serve as a link between preceding and subsequent technologies, offering important evidence for exploring the development of ancient Chinese zinc smelting technologies.}
}
@incollection{GHIORSO2015143,
title = {Chapter 6 - Chemical Thermodynamics and the Study of Magmas},
editor = {Haraldur Sigurdsson},
booktitle = {The Encyclopedia of Volcanoes (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {Amsterdam},
pages = {143-161},
year = {2015},
isbn = {978-0-12-385938-9},
doi = {https://doi.org/10.1016/B978-0-12-385938-9.00006-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780123859389000067},
author = {Mark S. Ghiorso and Guilherme A.R. Gualda},
keywords = {Chemical thermodynamics, Crystallization, Geobarometers, Geothermometers, Heat transfer, Magma chambers, Minimization, Open system, Phase equilibria, Thermodynamic potential},
abstract = {This chapter gives a brief overview of the application of chemical thermodynamics to magmatic systems. Topics covered include thermodynamic potentials and their application to various magma chamber evolution scenarios, including open systems and boundary conditions dictated by external controls on heat transfer and chamber volume. In addition, methods of finding the global minima of thermodynamic potentials are discussed along with algorithms for stable phase detection. The chapter ends with a summary of available tools of computational chemical thermodynamics that implement (1) geothermometers, geobarometers, and geohygrometers, (2) mineral and melt thermodynamic properties, (3) solubility calculators for volatiles in magmatic systems, and (4) computation of phase equilibria.}
}
@article{VODOVOTZ2017116,
title = {Solving Immunology?},
journal = {Trends in Immunology},
volume = {38},
number = {2},
pages = {116-127},
year = {2017},
issn = {1471-4906},
doi = {https://doi.org/10.1016/j.it.2016.11.006},
url = {https://www.sciencedirect.com/science/article/pii/S1471490616302022},
author = {Yoram Vodovotz and Ashley Xia and Elizabeth L. Read and Josep Bassaganya-Riera and David A. Hafler and Eduardo Sontag and Jin Wang and John S. Tsang and Judy D. Day and Steven H. Kleinstein and Atul J. Butte and Matthew C. Altman and Ross Hammond and Stuart C. Sealfon},
keywords = {mathematical modeling, conference, autoimmune disease, personalized medicine, translation},
abstract = {Emergent responses of the immune system result from the integration of molecular and cellular networks over time and across multiple organs. High-content and high-throughput analysis technologies, concomitantly with data-driven and mechanistic modeling, hold promise for the systematic interrogation of these complex pathways. However, connecting genetic variation and molecular mechanisms to individual phenotypes and health outcomes has proven elusive. Gaps remain in data, and disagreements persist about the value of mechanistic modeling for immunology. Here, we present the perspectives that emerged from the National Institute of Allergy and Infectious Disease (NIAID) workshop ‘Complex Systems Science, Modeling and Immunity’ and subsequent discussions regarding the potential synergy of high-throughput data acquisition, data-driven modeling, and mechanistic modeling to define new mechanisms of immunological disease and to accelerate the translation of these insights into therapies.}
}
@article{BEVEN2025106431,
title = {On the future of hydroecological models of everywhere},
journal = {Environmental Modelling & Software},
volume = {188},
pages = {106431},
year = {2025},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2025.106431},
url = {https://www.sciencedirect.com/science/article/pii/S136481522500115X},
author = {Keith Beven},
keywords = {Digital twins, Learning about places, Stakeholder communication},
abstract = {This paper addresses the potential for hydroecological models of everywhere to be used, in conjunction with interaction with local stakeholders, as a way of learning about places as well as being used as predictive tools. The importance of facilitating stakeholder involvement in defining assumptions and uncertainties, and in model evaluation is stressed. The potential for using data science and real-time updating in using the internet of things to contribute to a models of everywhere framework is also discussed.}
}
@incollection{MACLENNAN201584,
title = {Cognitive Modeling: Connectionist Approaches},
editor = {James D. Wright},
booktitle = {International Encyclopedia of the Social & Behavioral Sciences (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {84-89},
year = {2015},
isbn = {978-0-08-097087-5},
doi = {https://doi.org/10.1016/B978-0-08-097086-8.43021-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780080970868430217},
author = {Bruce MacLennan},
keywords = {Artificial intelligence, Backpropagation, Computability, Computational map, Connectionism, Correlational learning, Dynamic systems approach, Embodiment, Language of thought, Machine learning, Neural network, Perceptron, Representation, Situatedness, Subsymbolic},
abstract = {Connectionist approaches to cognitive modeling make use of large networks of simple computational units, which communicate by means of simple quantitative signals. Higher-level information processing emerges from the massively parallel interaction of these units by means of their connections, and a network may adapt its behavior by means of local changes in the strength of the connections. Connectionist approaches are related to neural networks and provide a distinct alternative to cognitive models inspired by the digital computer.}
}
@article{MICHIE19931,
title = {Turing's test and conscious thought},
journal = {Artificial Intelligence},
volume = {60},
number = {1},
pages = {1-22},
year = {1993},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(93)90032-7},
url = {https://www.sciencedirect.com/science/article/pii/0004370293900327},
author = {Donald Michie},
abstract = {Over forty years ago A.M. Turing proposed a test for intelligence in machines. Based as it is solely on an examinee's verbal responses, the Test misses some important components of human thinking. To bring these manifestations within its scope, the Turing Test would require substantial extension. Advances in the application of AI methods in the design of improved human-computer interfaces are now focussing attention on machine models of thought and knowledge from the altered standpoint of practical utility.}
}
@article{DING2025102351,
title = {Meta-analysis examining the relationship between framing effect and risky decisions},
journal = {Journal of Behavioral and Experimental Economics},
volume = {116},
pages = {102351},
year = {2025},
issn = {2214-8043},
doi = {https://doi.org/10.1016/j.socec.2025.102351},
url = {https://www.sciencedirect.com/science/article/pii/S2214804325000187},
author = {Xiaoqian Ding and Menghan Li and Junyi Qiao},
keywords = {Framing effect, Risky decision, Meta-analysis, Moderating analysis, Age, Problem domains, Culture},
abstract = {This study employed a meta-analysis to investigate the relationship between the framing effect and risky decisions. A systematic searched was conducted for relevant literature published in 12 electronic databases: Google Scholar, ProQuest Dissertations, Springer, Web of Science, PubMed, EBSCO, Elsevier SDOL, Chongqing VIP Information Co., WANFANG DATA, Chinese Selected Doctoral Dissertations and Master's Theses Full-Text Databases, and the China National Knowledge Infrastructure. A total of 40 relevant studies were identified, comprising a sample of 17,416 participants. The analysis employing the random-effects model revealed a statistically significant main effect of the framing effect on risky decisions (OR = 2.467). The moderator effect analysis revealed that problem domains and age served as moderating factors in the relationship between risky decisions and the framing effect, respectively. Culture, however, did not exert a moderating influence on the framing effect or risky decision-making. Specifically, individuals exhibited heightened susceptibility to the framing effect when making risky decisions in the problem domain of life-death, as compared to the problem domains of study and money. Adolescents, in contrast, were more vulnerable to the framing effect in making risky decisions than adulthood.}
}
@article{TOKAC2023101220,
title = {A programming grammar for robotic fabrication: Incorporating material agency into clay textures},
journal = {Design Studies},
volume = {88},
pages = {101220},
year = {2023},
issn = {0142-694X},
doi = {https://doi.org/10.1016/j.destud.2023.101220},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X23000613},
author = {Iremnur Tokac and Herman Bruyninckx and Andrew Vande Moere},
keywords = {human–computer interaction, computational model(s), design technology, reflective practice, making grammars},
abstract = {Material agency describes how material affordances and constraints have the inherent capacity to suggest formal transformations. Digital fabrication typically excludes material agency because it requires the final form is digitally modelled before it can be fabricated. To enrich the fabrication design space with material agency, we introduce 1) a programming grammar that relates the sensing of material states with the transformation of fabrication actions via explicit rule notations; 2) a grammatical compiler that translates these rule notations into a responsive robot executable program; 3) a set of critical reflections on how this grammar enhances the fabrication design space with material agency. Consequently, our contributions broaden digital fabrication to produce intricate material forms that cannot be simulated by geometrical definitions.}
}
@article{ACHARJYA2022100647,
title = {A rough set, formal concept analysis and SEM-PLS integrated approach towards sustainable wearable computing in the adoption of smartwatch},
journal = {Sustainable Computing: Informatics and Systems},
volume = {33},
pages = {100647},
year = {2022},
issn = {2210-5379},
doi = {https://doi.org/10.1016/j.suscom.2021.100647},
url = {https://www.sciencedirect.com/science/article/pii/S221053792100130X},
author = {D.P. Acharjya and Gladys Gnana {Kiruba B}},
keywords = {Rough set, Wearable computing, Path diagram, Composite reliability, Convergence, Discriminant validity},
abstract = {The rapid growth of sustainable computing towards the energy, power and environment seize an immense attention from bigger organizations to an individual life. Besides the world is advancing towards the digital mode and smartwatch is gaining its popularity because of additional importance to improve lifestyle. Moreover, it is not restricted to only time viewer rather paves a way in user's daily life. Therefore, it is highly cardinal in identifying the factors among consumers influencing the adoption of smartwatch in sustainable wearable computing. Traditional data modelling tools limited to technology acceptance model is used to this end. After all the study deals with user's behaviour that includes uncertainties and thus studying such problems using computational intelligence techniques is pivotal. In this research work we hybridize rough set, partial least square, and formal concept analysis to study smartwatch users adoption in wearable computing. Initially, the reliability and validity of the proposed model is analysed using structural equation modelling along with partial least square. Further, decision rules are generated using the rough set. Finally, important factors affecting the user's behavioural adoption towards sustainable wearable computing is discovered using formal concept analysis.}
}
@incollection{VALLERO2014929,
title = {Chapter 32 - Sustainable Approaches},
editor = {Daniel Vallero},
booktitle = {Fundamentals of Air Pollution (Fifth Edition)},
publisher = {Academic Press},
edition = {Fifth Edition},
address = {Boston},
pages = {929-952},
year = {2014},
isbn = {978-0-12-401733-7},
doi = {https://doi.org/10.1016/B978-0-12-401733-7.00032-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780124017337000323},
author = {Daniel Vallero},
keywords = {American society of mechanical engineers (ASME), Aquatic toxicity, Atmospheric oxidation, Bathtub curve, Benchmark, Benefit-cost ratio, Bioconcentration, Decision force field, Dematerialization, Design failure, Design for disassembly (DfD), Design for the environment (DfE), Design for the recyling (DfR), Energy return on the energy investment (EROEI), Ethanol, Exposure, Fuel change, Green algae, Green engineering, Green technology, Half-life, Hazard, Hazardous air pollutant (HAP), Henry's law, Holdup, Industrial ecology, Life cycle analysis (LCA), Life cycle assessment, Material, energy and waste (MEW) flow, Operation and maintenance (O&M), Oxides of nitrogen (NO), Pollution prevention, Pollution prevention act, Process change, Relative risk, Reliability, Remedial action, Resource conservation and recovery act (RCRA), Risk trade-off, Sludge sorption, Solubility, Sustainability, Thermodynamic efficiency, Utility, Vapor pressure, Waste audit, Waste minimization, WWT, WWTP},
abstract = {This chapter considers alternative means of achieving acceptable air quality in addition to or instead of the controls discussed in previous chapters. Air quality is considered from a systems thinking perspective. The concept of engineering reliability is applied to air pollution. The advantages and disadvantages of utility and benefit-cost analyses are discussed, with recommendations of other means of determining environmental value and evaluating risk trade-offs, especially life cycle analysis, green engineering and industrial ecology tools.}
}
@article{GOTHAM2016705,
title = {A Suitable Approach in Extracting Non Event Related Potential Sources from Brain of Disabled Patients},
journal = {Procedia Computer Science},
volume = {85},
pages = {705-712},
year = {2016},
note = {International Conference on Computational Modelling and Security (CMS 2016)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2016.05.257},
url = {https://www.sciencedirect.com/science/article/pii/S187705091630607X},
author = {Solomon Gotham and G. Sasibushana Rao},
keywords = {Electroencephalogram (EEG) ;Magneto encephalogram (EMG) ;Non Gaussianity, Evoked potentials (EVP) ;Non Event Related Potentials (NERP)},
abstract = {Brain is the most important, astonishing and complicated part of human body which is responsible for controlling and functioning of all other human organs. The physical movements and thinking capability (Cognition) of humans depend on the brain activity. Based on certain changes that occur within the brain, electric fields will be generated within the brain. Analyzing brain signals plays vital role in diagnosis and treatment of brain disorders. Brain signals are obtained from electrodes of Electroencephalogram (EEG) or Magneto encephalogram (EMG). These are linear mixture of evoked potentials (EVP) of large number of neurons due to variations in conductive and geometric properties in the layers of 3 layer head model or 4 layer head model. Earlier work1-5 considered processing these mixed signals for analyzing brain functioning of brain disabled patients. But working on the source signals gives an authoritative result. Hence there is a need to separate the source signals from the measured (electrode) signals. This work will suggest a suitable approach in extracting source signals of disabled patients while they were used as subjects under experiment of retrieving event related potentials (ERP). This work retrieved the signals of non target trails i.e., non event related potentials (NERP) and extracted original source signals by the best Gaussian estimate and the algorithm proposed.}
}
@article{NADERPOUR2014209,
title = {The explosion at institute: Modeling and analyzing the situation awareness factor},
journal = {Accident Analysis & Prevention},
volume = {73},
pages = {209-224},
year = {2014},
issn = {0001-4575},
doi = {https://doi.org/10.1016/j.aap.2014.09.008},
url = {https://www.sciencedirect.com/science/article/pii/S0001457514002644},
author = {Mohsen Naderpour and Jie Lu and Guangquan Zhang},
keywords = {Situation awareness, Situation assessment, Abnormal situations, Methomyl unit, Accident analysis},
abstract = {In 2008 a runaway chemical reaction caused an explosion at a methomyl unit in West Virginia, USA, killing two employees, injuring eight people, evacuating more than 40,000 residents adjacent to the facility, disrupting traffic on a nearby highway and causing significant business loss and interruption. Although the accident was formally investigated, the role of the situation awareness (SA) factor, i.e., a correct understanding of the situation, and appropriate models to maintain SA, remain unexplained. This paper extracts details of abnormal situations within the methomyl unit and models them into a situational network using dynamic Bayesian networks. A fuzzy logic system is used to resemble the operator’s thinking when confronted with these abnormal situations. The combined situational network and fuzzy logic system make it possible for the operator to assess such situations dynamically to achieve accurate SA. The findings show that the proposed structure provides a useful graphical model that facilitates the inclusion of prior background knowledge and the updating of this knowledge when new information is available from monitoring systems.}
}
@article{WEI202238,
title = {Promoter prediction in nannochloropsis based on densely connected convolutional neural networks},
journal = {Methods},
volume = {204},
pages = {38-46},
year = {2022},
issn = {1046-2023},
doi = {https://doi.org/10.1016/j.ymeth.2022.03.017},
url = {https://www.sciencedirect.com/science/article/pii/S1046202322000846},
author = {Pi-Jing Wei and Zhen-Zhen Pang and Lin-Jie Jiang and Da-Yu Tan and Yan-Sen Su and Chun-Hou Zheng},
keywords = {Nannochloropsis, Promoter, Deep learning, Densely connected convolutional neural networks, Within-group scrambling},
abstract = {Promoter is a key DNA element located near the transcription start site, which regulates gene transcription by binding RNA polymerase. Thus, the identification of promoters is an important research field in synthetic biology. Nannochloropsis is an important unicellular industrial oleaginous microalgae, and at present, some studies have identified some promoters with specific functions by biological methods in Nannochloropsis, whereas few studies used computational methods. Here, we propose a method called DNPPro (DenseNet-Predict-Promoter) based on densely connected convolutional neural networks to predict the promoter of Nannochloropsis. First, we collected promoter sequences from six Nannochloropsis strains and removed 80% similarity using CD-HIT for each strain to yield a reliable set of positive datasets. Then, in order to construct a robust classifier, within-group scrambling method was used to generate negative dataset which overcomes the limitation of randomly selecting a non-promoter region from the same genome as a negative sample. Finally, we constructed a densely connected convolutional neural network, with the sequence one-hot encoding as the input. Compared with commonly used sequence processing methods, DNPPro can extract long sequence features to a greater extent. The cross-strain experiment on independent dataset verifies the generalization of our method. At the same time, T-SNE visualization analysis shows that our method can effectively distinguish promoters from non-promoters.}
}
@article{KHAN2023110525,
title = {AAD-Net: Advanced end-to-end signal processing system for human emotion detection & recognition using attention-based deep echo state network},
journal = {Knowledge-Based Systems},
volume = {270},
pages = {110525},
year = {2023},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2023.110525},
url = {https://www.sciencedirect.com/science/article/pii/S0950705123002757},
author = {Mustaqeem Khan and Abdulmotaleb {El Saddik} and Fahd Saleh Alotaibi and Nhat Truong Pham},
keywords = {Affective computing, Attention mechanism, Convolution neural network, Echo state networks, Emotion recognition, Human–computer interaction, Audio speech signals},
abstract = {Speech signals are the most convenient way of communication between human beings and the eventual method of Human–Computer Interaction (HCI) to exchange emotions and information. Recognizing emotions from speech signals is a challenging task due to the sparse nature of emotional data and features. In this article, we proposed a Deep Echo-State-Network (DeepESN) system for emotion recognition with a dilated convolution neural network and multi-headed attention mechanism. To reduce the model complexity, we incorporate a DeepESN that combines reservoir computing for higher-dimensional mapping. We also used fine-tuned Sparse Random Projection (SRP) to reduce dimensionality and adopted an early fusion strategy to fuse the extracted cues and passed the joint feature vector via a classification layer to recognize emotions. Our proposed model is evaluated on two public speech corpora, EMO-DB and RAVDESS, and tested for subject/speaker-dependent/independent performance. The results show that our proposed system achieves a high recognition rate, 91.14, 85.57 for EMO-DB, and 82.01, 77.02 for RAVDESS, using speaker-dependent and independent experiments, respectively. Our proposed system outperforms the State-of-The-Art (SOTA) while requiring less computational time.}
}
@article{GALE2025101969,
title = {Are we sleepwalking into a fully automated medical imaging service?},
journal = {Journal of Medical Imaging and Radiation Sciences},
volume = {56},
number = {5},
pages = {101969},
year = {2025},
issn = {1939-8654},
doi = {https://doi.org/10.1016/j.jmir.2025.101969},
url = {https://www.sciencedirect.com/science/article/pii/S1939865425001195},
author = {Niamh Gale},
keywords = {Artificial intelligence, Medical imaging, Review, Impact, Higher education, Recommendations},
abstract = {Introduction
Artificial intelligence (AI) is already embedded in medical imaging services, but now that the National Institute for Health and Care Excellence (NICE) has released position statements looking favourably on AI use in healthcare, its use will embed even further.
Discussion
AI has brought many positives to medical imaging services and is far superior at making calculations using vast amounts of data. It can therefore help improve the speed and accuracy of diagnosis and treatment plans for many patients, but at what cost to the radiography profession? Surveys have shown that the majority of the workforce welcome AI, but admit that they don’t fully understand the principles behind it. AI developers are keen to improve patient output, and many are unconcerned about the possible negative effects on staff morale and expertise. As computers remove the autonomy and competency that radiographers have previously held with pride, staff may find that they become de-skilled and de-motivated, and it may eventually subsume the traditional role of the radiographer altogether. The profession needs to be aware of these potential impacts and prepare accordingly.
Conclusion
Higher education plays an important role in preparing radiographers of the future for the changing landscape of medical imaging and should include more engineering and data science modules in the curriculum to prevent radiographers from becoming irrelevant.
Résumé
Introduction
L'intelligence artificielle (IA) est déjà intégrée aux services d'imagerie médicale, mais son utilisation se généralisera encore davantage maintenant que le National Institute for Health and Care Excellence (NICE) a publié des prises de position favorables à l'utilisation de l'IA dans les soins de santé.
Discussion
L'IA a apporté de nombreux avantages aux services d'imagerie médicale et est bien plus performante pour effectuer des calculs à partir de grandes quantités de données. Elle peut donc contribuer à améliorer la rapidité et la précision des diagnostics et des plans de traitement pour de nombreux patients, mais à quel prix pour la profession de la radiographie? Des enquêtes ont montré que la majorité des travailleurs sont favorables à l'IA, mais admettent qu'ils n'en comprennent pas pleinement les principes. Les développeurs d'IA sont désireux d'améliorer le rendement au niveau des patients, et beaucoup ne se soucient pas des effets négatifs possibles sur le moral et l'expertise du personnel. Comme les ordinateurs suppriment l'autonomie et les compétences que les radiographes détenaient auparavant avec fierté, le personnel peut se retrouver déqualifié et démotivé, et cela peut finir par supplanter complètement le rôle traditionnel du radiographe. La profession doit être consciente de ces impacts potentiels et s'y préparer en conséquence.
Conclusion
L'enseignement supérieur joue un rôle important dans la préparation des futurs radiographes à l'évolution de l'imagerie médicale. Le programme d'études devrait donc inclure davantage de modules d'ingénierie et de science des données afin d'éviter que les radiographes ne deviennent obsolètes.}
}
@article{KAPITANYFOVENY202466,
title = {EEG based depression detection by machine learning: Does inner or overt speech condition provide better biomarkers when using emotion words as experimental cues?},
journal = {Journal of Psychiatric Research},
volume = {178},
pages = {66-76},
year = {2024},
issn = {0022-3956},
doi = {https://doi.org/10.1016/j.jpsychires.2024.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S0022395624004515},
author = {Máté Kapitány-Fövény and Mihály Vetró and Gábor Révy and Dániel Fabó and Danuta Szirmai and Gábor Hullám},
keywords = {EEG, Depression, Machine learning, Diagnostic approach},
abstract = {Background
Objective diagnostic approaches need to be tested to enhance the efficacy of depression detection. Non-invasive EEG-based identification represents a promising area.
Aims
The present EEG study addresses two central questions: 1) whether inner or overt speech condition result in higher diagnositc accuracy of depression detection; and 2) does the affective nature of the presented emotion words count in such diagnostic approach.
Methods
A matched case-control sample consisting of 10 depressed subjects and 10 healthy controls was assessed. An EEG headcap containing 64 electrodes measured neural responses to experimental cues presented in the form of 15 different words that belonged to three emotional categories: neutral, positive, and negative. 120 experimental cues was presented for every participant, each containing an “inner speech” and an “overt speech” segment. An EEGNet neural network was utilized.
Results
The highest diagnostic accuracy of the EEGNet model was observed in the case of the overt speech condition (i.e. 69.5%), while a an overall subject-wise accuracy of 80% was achieved by the model. Only a negligible difference in diagnostic accuracy could be found between aggregated emotion word categories, with the highest accuracy (i.e. 70.2%) associated with the presentation of positive emotion words. Model decision was primarily influenced by electrodes representing the regions of the left parietal, the left temporal lobe and the middle frontal areas.
Conclusions
While the generalizability of our results is limited by the small sample size and potentially uncontrolled confounders, depression was associated with sensitive and presumably network-like aspects of these brain areas, potentially implying a higher level of emotion regulation that increases primarily in open communication.}
}
@article{WANG202385,
title = {Deep generation network for multivariate spatio-temporal data based on separated attention},
journal = {Information Sciences},
volume = {633},
pages = {85-103},
year = {2023},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2023.03.062},
url = {https://www.sciencedirect.com/science/article/pii/S0020025523003602},
author = {Junkai Wang and Lianlei Lin and Sheng Gao and Zongwei Zhang},
keywords = {Metaverse, Spatio-temporal data, Separated attention, Homoscedasticity uncertainty},
abstract = {The multivariate spatio-temporal data contains complex spatio-temporal background and channel coupling information. Effective extraction of these features is crucial for data generation. In this paper, a separated multiple attention network is proposed, which can capture the correlation of multiple types of variables in the same space-time, different spaces at the same time, and different times in the same space. Meanwhile, a new multiscale loss processing method based on homoscedasticity uncertainty and the assumption of gaussian loss distribution is proposed to balance the numerical scale of each channel loss in training. The experiment shows that our method has better performance and robustness than the classical machine learning model and higher computational efficiency than the physical model. It can generate multivariate intermittent spatial-temporal fields with a maximum lead time of 3 days and multivariate continuous spatial-temporal fields with a maximum lead time of 7 days.}
}
@article{LANDETE2024103034,
title = {Uncapacitated single-allocation hub median location with edge upgrading: Models and exact solution algorithms},
journal = {Transportation Research Part B: Methodological},
volume = {187},
pages = {103034},
year = {2024},
issn = {0191-2615},
doi = {https://doi.org/10.1016/j.trb.2024.103034},
url = {https://www.sciencedirect.com/science/article/pii/S0191261524001589},
author = {Mercedes Landete and Juan M. Muñoz-Ocaña and Antonio M. Rodríguez-Chía and Francisco Saldanha-da-Gama},
keywords = {Hub location, Hub-network design, Single allocation, Edge upgrading, MILP models, Branch-and-cut},
abstract = {In this paper, a class of single-allocation hub location problems is investigated from the perspective of upgrading. The latter is understood as an improvement of a set of edges to increase their individual performance, e.g., a decreased unit transportation cost. The goal is to obtain an improved optimal solution to the problem compared to that obtained if upgrading was not done. A budget constraint is assumed to limit the upgrading operations. A flow-based formulation is initially proposed that extends a classical model for uncapacitated single-allocation hub location with complete hub networks. Nevertheless, the fact that the unit costs after upgrading may violate the triangle inequality needs to be accounted for. Since the proposed formulation has a high computing burden, different possibilities are discussed for enhancing it. This leads to devising an efficient branch-and-cut algorithm with different variants. Additionally, a formulation based on the discrete ordered median function is also introduced that is also enhanced and embedded into a branch-and-cut algorithm again with several variants. All models and algorithms are also adapted to problems embedding hub network design decisions. Extensive computational tests were conducted to assess the methodological contributions proposed.}
}
@article{ALBERT2008401,
title = {A formal framework for modelling the developmental course of competence and performance in the distance, speed, and time domain},
journal = {Developmental Review},
volume = {28},
number = {3},
pages = {401-420},
year = {2008},
issn = {0273-2297},
doi = {https://doi.org/10.1016/j.dr.2008.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S0273229708000257},
author = {Dietrich Albert and Michael D. Kickmeier-Rust and Fumiko Matsuda},
keywords = {Distance–speed–time system, Cognitive development, Overgeneralization, Competence-based Knowledge Space Theory},
abstract = {The developmental course in the distance–speed–time domain is still a matter of debate. Traditional stage models are contested by theories of continuous development and adaptive thinking. In the present work, we introduce a formal framework for modelling the developmental course in this domain, grounding on Competence-based Knowledge Space Theory. This framework, as a more general case, widely includes assumptions and facets of previous models and covers empirical findings collected based on different experimental paradigms. By a distinction of latent competences and observable performance, model validation is not bound to a certain experimental paradigm and no one-to-one correspondence between competences and tasks is required. Therefore, the framework has the potential to bridge the gap between stage models and models of continuous development. The approach also precisely defines misconceptions, for example overgeneralization, and empirically investigates their occurrence. In the present work, we established a prototypical model for the development of understanding the distance–speed–time system. We extended this model with definitions based on different perspectives of overgeneralization. The assumptions of the model and its extensions were examined on the basis of the results of two empirical investigations using six judgment task types. The results yielded a reasonably good fit of model and data. No evidence was found for the occurrence of overgeneralization in this domain. The theoretical model and empirical results are discussed with respect to their relationship to other developmental models and theories.}
}
@article{JURKOVA2023105046,
title = {Turing and von Neumann machines: Completing the new mechanism},
journal = {Biosystems},
volume = {234},
pages = {105046},
year = {2023},
issn = {0303-2647},
doi = {https://doi.org/10.1016/j.biosystems.2023.105046},
url = {https://www.sciencedirect.com/science/article/pii/S0303264723002216},
author = {Barbora Jurková and Lukáš Zámečník},
keywords = {Turing machine, von Neumann probe, New mechanism, Code biology, Extended mechanism},
abstract = {Turing (1937) introduces a model of code that is followed by other pioneers of computing machines (such as Flowers 1983, Eckert, Mauchly, Brainerd 1945 and others). One of them is John von Neumann, who defines the concept of optimal code in the context of the conception of EDVAC. He later uses it to build on in his theoretical considerations of the universal constructor (von Neumann 1966). Von Neumann (1963) further presents one of the first neural network models, in relation to the work of McCulloch and Pitts (1943), for both theoretical purposes (von Neumann probe) and practical applications (computer architecture of EDVAC). The aim of this paper is (1) to describe the differences between Turingʼs and von Neumannʼs conceptualizations of code and the mechanical computing model. Between von Neumann's abstract technical conception (von Neumann 1963 and 1966) and Turingʼs more concrete biochemical conception (Turing 1952). Furthermore, (2) we want to answer the question why these influential models of mechanisms (predominantly in computer science) have so far been ignored by philosophers of the new mechanism (Machamer, Darden, Craver 2000, Glennan 2017). We will show that these classical models of machines are not only compatible with the new mechanism, but moreover complement it, since they represent a completely separate type of model of mechanism, alongside producing, maintaining and underlying (Zámečník 2021). The final (3) and main goal of our paper will be an attempt to relate von Neumannʼs and Turingʼs notion of mechanism to Barbieriʼs notion of extended mechanism (Barbieri 2015).}
}
@article{ROYCHOWDHURY2004105,
title = {Diagnosis of the diseases––using a GA-fuzzy approach},
journal = {Information Sciences},
volume = {162},
number = {2},
pages = {105-120},
year = {2004},
note = {Medical Expert Systems},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2004.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S0020025504000660},
author = {Anish Roychowdhury and Dilip Kumar Pratihar and Nilav Bose and K.P Sankaranarayanan and N Sudhahar},
keywords = {Diagnosis, Jaundice, Pneumonia, GA-fuzzy approach},
abstract = {The objective of our study is to design an expert system by modelling the knowledge and thinking process of a doctor. A fuzzy logic controller (FLC) is used to model the process and a genetic algorithm (GA) helps to select a number of good rules from a manually constructed large rule base of an FLC, based on the opinion of 10 doctors. The GA-based tuning is done off-line. Once the optimized rule base of the FLC is obtained, it can diagnose the disease, on-line. The scope of the present work has been extended to two diseases, namely Pneumonia and Jaundice. The symptoms of each disease are fed as inputs to the FLC and the output, i.e., grade of a disease is determined.}
}
@article{GROSS20173,
title = {Prospects and problems for standardizing model validation in systems biology},
journal = {Progress in Biophysics and Molecular Biology},
volume = {129},
pages = {3-12},
year = {2017},
note = {Validation of Computer Modelling},
issn = {0079-6107},
doi = {https://doi.org/10.1016/j.pbiomolbio.2017.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S0079610716300177},
author = {Fridolin Gross and Miles MacLeod},
keywords = {Systems biology, Modeling, Standardization, Validation, Model selection},
abstract = {There are currently no widely shared criteria by which to assess the validity of computational models in systems biology. Here we discuss the feasibility and desirability of implementing validation standards for modeling. Having such a standard would facilitate journal review, interdisciplinary collaboration, model exchange, and be especially relevant for applications close to medical practice. However, even though the production of predictively valid models is considered a central goal, in practice modeling in systems biology employs a variety of model structures and model-building practices. These serve a variety of purposes, many of which are heuristic and do not seem to require strict validation criteria and may even be restricted by them. Moreover, given the current situation in systems biology, implementing a validation standard would face serious technical obstacles mostly due to the quality of available empirical data. We advocate a cautious approach to standardization. However even though rigorous standardization seems premature at this point, raising the issue helps us develop better insights into the practices of systems biology and the technical problems modelers face validating models. Further it allows us to identify certain technical validation issues which hold regardless of modeling context and purpose. Informal guidelines could in fact play a role in the field by helping modelers handle these.}
}
@article{VAKSER20141785,
title = {Protein-Protein Docking: From Interaction to Interactome},
journal = {Biophysical Journal},
volume = {107},
number = {8},
pages = {1785-1793},
year = {2014},
issn = {0006-3495},
doi = {https://doi.org/10.1016/j.bpj.2014.08.033},
url = {https://www.sciencedirect.com/science/article/pii/S0006349514009382},
author = {Ilya A. Vakser},
abstract = {The protein-protein docking problem is one of the focal points of activity in computational biophysics and structural biology. The three-dimensional structure of a protein-protein complex, generally, is more difficult to determine experimentally than the structure of an individual protein. Adequate computational techniques to model protein interactions are important because of the growing number of known protein structures, particularly in the context of structural genomics. Docking offers tools for fundamental studies of protein interactions and provides a structural basis for drug design. Protein-protein docking is the prediction of the structure of the complex, given the structures of the individual proteins. In the heart of the docking methodology is the notion of steric and physicochemical complementarity at the protein-protein interface. Originally, mostly high-resolution, experimentally determined (primarily by x-ray crystallography) protein structures were considered for docking. However, more recently, the focus has been shifting toward lower-resolution modeled structures. Docking approaches have to deal with the conformational changes between unbound and bound structures, as well as the inaccuracies of the interacting modeled structures, often in a high-throughput mode needed for modeling of large networks of protein interactions. The growing number of docking developers is engaged in the community-wide assessments of predictive methodologies. The development of more powerful and adequate docking approaches is facilitated by rapidly expanding information and data resources, growing computational capabilities, and a deeper understanding of the fundamental principles of protein interactions.}
}
@article{DAN2025100814,
title = {Social robot assisted music course based on speech sensing and deep learning algorithms},
journal = {Entertainment Computing},
volume = {52},
pages = {100814},
year = {2025},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2024.100814},
url = {https://www.sciencedirect.com/science/article/pii/S1875952124001824},
author = {Xiao Dan},
keywords = {Speech sensing, Deep learning algorithms, Social robots, Course in music},
abstract = {In the field of social robot teaching, research has focused on how to use technological means to provide better learning support and personalized interactive experiences. Social robots can interact with students and provide personalized learning support, thereby improving their learning effectiveness and engagement. The speech sensing model of social robots can perceive students’ emotions and feedback in real-time through technologies such as speech recognition and sentiment analysis, thereby providing intelligent responses and guidance. The deep learning recommendation model for music course resources extracts music features through deep learning techniques, and combines session interest extraction techniques to personalized recommend music resources suitable for students’ interests and abilities. By analyzing students’ interests and learning goals, robots can provide music learning resources that meet their needs based on recommendation algorithms, further stimulating their learning interest and enthusiasm. The experimental results show that the use of social robots in the learning environment significantly improves the learning effectiveness and participation of students. Through personalized interaction and intelligent response guidance, students are more likely to understand and master music knowledge, while experiencing joyful and positive learning emotions. The study validated the effectiveness of social robot assisted music courses based on speech sensing and deep learning algorithms, demonstrating its advantages in improving student learning effectiveness and engagement.}
}
@article{NARASIMHAN197879,
title = {Modelling behaviour: the need for a computational approach},
journal = {Journal of Social and Biological Structures},
volume = {1},
number = {1},
pages = {79-94},
year = {1978},
issn = {0140-1750},
doi = {https://doi.org/10.1016/0140-1750(78)90020-9},
url = {https://www.sciencedirect.com/science/article/pii/0140175078900209},
author = {R. Narasimhan},
abstract = {The principal objective of this paper is to argue the thesis that a science concerned with the study of behaviour requires the computational approach in a serious way for its theoretical advancement. It is pointed out that modelling behaviour requires the articulation of explanations at three levels. The methodology of computational simulations is indispensable to articulating explanations at the first level which underlie explanations at the other two levels. The paper contrasts the currently fashionable approaches in artificial intelligence studies to the kind of constraints viable behavioural models must satisfy. Teachability and open-endedness are two of the essential characteristics of organisms that any satisfactory model must have. It is argued that analogy-based computational techniques and paradigmatic learning/teaching techniques are two modelling aspects that require imaginative study.}
}
@article{CHIU2024100171,
title = {What are artificial intelligence literacy and competency? A comprehensive framework to support them},
journal = {Computers and Education Open},
volume = {6},
pages = {100171},
year = {2024},
issn = {2666-5573},
doi = {https://doi.org/10.1016/j.caeo.2024.100171},
url = {https://www.sciencedirect.com/science/article/pii/S2666557324000120},
author = {Thomas K.F. Chiu and Zubair Ahmad and Murod Ismailov and Ismaila Temitayo Sanusi},
keywords = {AI literacy, AI competency, K-12 education, Machine learning, Data literacy, Generative AI},
abstract = {Artificial intelligence (AI) education in K–12 schools is a global initiative, yet planning and executing AI education is challenging. The major frameworks are focused on identifying content and technical knowledge (AI literacy). Most of the current definitions of AI literacy for a non-technical audience are developed from an engineering perspective and may not be appropriate for K–12 education. Teacher perspectives are essential to making sense of this initiative. Literacy is about knowing (knowledge, what skills); competency is about applying the knowledge in a beneficial way (confidence, how well). They are strongly related. This study goes beyond knowledge (AI literacy), and its two main goals are to (i) define AI literacy and competency by adding the aspects of confidence and self-reflective mindsets, and (ii) propose a more comprehensive framework for K–12 AI education. These definitions are needed for this emerging and disruptive technology (e.g., ChatGPT and Sora, generative AI). We used the definitions and the basic curriculum design approaches as the analytical framework and teacher perspectives. Participants included 30 experienced AI teachers from 15 middle schools. We employed an iterative co-design cycle to discuss and revise the framework throughout four cycles. The definition of AI competency has five abilities that take confidence into account, and the proposed framework comprises five key components: technology, impact, ethics, collaboration, and self-reflection. We also identify five effective learning experiences to foster abilities and confidences, and suggest five future research directions: prompt engineering, data literacy, algorithmic literacy, self-reflective mindset, and empirical research.}
}
@article{MADUABUCHI2023125889,
title = {Deep neural networks for quick and precise geometry optimization of segmented thermoelectric generators},
journal = {Energy},
volume = {263},
pages = {125889},
year = {2023},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2022.125889},
url = {https://www.sciencedirect.com/science/article/pii/S036054422202775X},
author = {Chika Maduabuchi and Chibuoke Eneh and Abdulrahman Abdullah Alrobaian and Mohammad Alkhedher},
keywords = {Segmented thermoelectric generator, Solar energy, Deep neural networks, Geometry optimization, Thermo-mechanical analysis, Finite element method},
abstract = {To solve the problems of the current optimization methods for solar segmented thermoelectric generator performance based on numerical methods, this paper applied deep neural networks to optimize the device geometry for improved thermo-mechanical performance. The motivation for using the deep neural network is to overcome the lengthy computational time and very high computational energy required by the traditional numerical method in optimizing the segmented thermoelectric generator performance. The numerical model is built using ANSYS software and the effects of temperature dependency in the 4 thermoelectric materials are considered to ensure result accuracy. Furthermore, 16 possible geometry parameters which were previously not considered, encompassing the individual and combined segment's heights and cross-sectional areas are optimized to find which set of parameters are the best in maximizing the device performance. The deep neural network is a regressive multilayer perceptron with network hyperparameters comprising 2 hidden layers with 5 neurons per layer. The training process is governed by the Levenberg-Marquardt standard backpropagation algorithm to minimize the mean squared error and maximize the regression correlation between the neural network forecasted outputs and the numerical-generated dataset. The most significant contribution of the proposed deep neural network is that it was able to quickly and accurately forecast the device performance in just 10 s, which was 2880 times faster than the conventional numerical-based optimization approach. Additionally, the optimized device had a maximum efficiency of 18%, which was 78% higher than that of the unoptimized device. Also, the thermal stress of the optimized device was 73% less than that of the unoptimized device design, indicating an extension in the device mechanical reliability and service lifetime. The results reported in this paper will accelerate the ease at which efficient, long-lasting segmented thermoelectric generators are manufactured by harnessing the power of artificial intelligence.}
}
@article{ZHANG2025100137,
title = {AI Linguistics},
journal = {Natural Language Processing Journal},
volume = {10},
pages = {100137},
year = {2025},
issn = {2949-7191},
doi = {https://doi.org/10.1016/j.nlp.2025.100137},
url = {https://www.sciencedirect.com/science/article/pii/S2949719125000135},
author = {Guosheng Zhang},
keywords = {AI architecture, Embedding augmentation, Embedding dimensionality, Embedding domain, General embedding},
abstract = {This research investigates the development of a linguistics for artificial intelligence (AI) to demystify the ”black box” of AI. At its core, the language of AI is Embedding—a novel high-dimensional, intelligent language. Embedding exhibits dual characteristics: it operates both as a semantic domain and as a mathematical point. This duality enables Embedding to maintain the discrete, symbolic nature of human languages while facilitating continuous operations in high-dimensional spaces, unlocking significant potential for advanced intelligence. A series of specialized experiments were designed to explore Embedding’s intrinsic properties, including its behavior as a semantic cloud in high-dimensional space, its degrees of freedom, and spatial transformations. Key findings include the discovery of substantial redundant dimensions in embeddings, confirmation that embeddings lack critical dimensions, and the measurement of engineering dimensions in natural language. This research also establishes the linguistic foundations and application limits of techniques such as dropout strategies, AI model distillation, and scaling laws among others. Building on these insights, we propose innovative solutions across several fields, including AI architecture design, AI reasoning, domain-based embedding search, and the construction of a multi-intelligence spectrum for embeddings. Ultimately, we introduce a foundational methodology for embedding everything from real-world into the AI world, providing a comprehensive reference framework for the evolution of artificial general intelligence (AGI) and artificial superintelligence (ASI). Additionally, this research explores linguistic approaches to the co-evolution of human intelligence and artificial intelligence.}
}
@article{DAVID2022132522,
title = {Integrating fourth industrial revolution (4IR) technologies into the water, energy & food nexus for sustainable security: A bibliometric analysis},
journal = {Journal of Cleaner Production},
volume = {363},
pages = {132522},
year = {2022},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2022.132522},
url = {https://www.sciencedirect.com/science/article/pii/S0959652622021230},
author = {Love O. David and Nnamdi I. Nwulu and Clinton O. Aigbavboa and Omoseni O. Adepoju},
keywords = {WEF Nexus, Fourth industrial revolution, Industry 4.0, Cleaner production, Internet of things (IoT)},
abstract = {The technologies of the fourth Industrial Revolution (4IR/Industry 4.0) have been a technological catalyst for all fields of human endeavor, permeating the water, energy, and food (WEF) nexus. However, there is no empirical evidence of the extent of applications and the permeability level in ensuring the three resources’ security. This study explored the relationship of the fourth industrial revolution technologies and the water, energy, and food nexus by evaluating the applications of the various technologies of 4IR on WEF nexus and examined the effect of 4IR on WEF nexus. The objectives were achieved using the qualitative methodology and bibliometric analysis of content analysis. The result showed that most fourth industrial revolution technologies had not been integrated with the WEF nexus. The result showed that only the Internet of Things (IoT) and Big Data analytics had permeated the nexus, which shows that data of the resources will be the foundation of the nexus. The systematic collection, accuracy of data, and empirical analysis of data will determine the level of security of WEF nexus. The qualitative results show that there are applications of the fourth industrial revolution technologies to the individual sectors of the nexus, birthing Water 4.0, Energy 4.0, and Food 4.0. The Bibliometric analysis result shows that the integration of the fourth industrial revolution with the WEF nexus will lead to cleaner production practices relating to the technological processes of water, energy, and food resources. These practices will ensure the environment's safety from WEF wastes and the water, energy, and food security in production processes. The empirical research and bibliometric analysis result, rooted in the concept of cleaner production, shows that the fourth industrial revolution affected the WEF nexus. The effects are; the birth of clean technologies & industrial applications, the catalyst for sustainability security of WEF nexus leveraging on life cycle thinking, enablement of technological transfer, enhancement of economic growth, and urban planning. The study concludes that the fourth industrial revolution technologies affect WEF nexus, ensuring the popularization of cleaner production strategies and processes of the resources during trade-offs and synergies. The study recommends the integration of a cleaner production concept in WEF processing. It should follow the innovation diffusion theory (IDT) and Technology acceptance theory (TAM) when applying 4IR technologies to the nexus of water, energy, and food resources, for their sustainable security.}
}
@article{PAN2024102334,
title = {Novel blockchain deep learning framework to ensure video security and lightweight storage for construction safety management},
journal = {Advanced Engineering Informatics},
volume = {59},
pages = {102334},
year = {2024},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2023.102334},
url = {https://www.sciencedirect.com/science/article/pii/S1474034623004627},
author = {Xing Pan and Luoxin Shen and Botao Zhong and Da Sheng and Fang Huang and Luhan Yang},
keywords = {Construction safety management, Video security storage, Blockchain, Deep learning, Video summarization, Pre-defined rule},
abstract = {In construction management, video data tampering behavior like manual forging and deletion can negatively impact on-site safety and accident accountability. Blockchain technology holds the potential to address this issue by leveraging distributed ledger characteristics. However, blockchain's limited storage capacity and block size make it difficult to upload large-sized construction data such as daily monitoring video. Furthermore, it is unnecessary to store all construction data in any case. Therefore, this study proposes a blockchain deep learning framework that focuses on how to efficiently extract and securely store key information (i.e., video summarization that involves worker’s unsafe behavior) on-blockchain for data traceability. The framework involves a novel data-driven and rule-based keyframe extraction (DRKE) model to lightweight large-sized construction video in the nascent field of deep learning and blockchain combination. To define parameters for the DRKE model, specific construction rules (e.g., people’s unsafe behavior-based and people-based rules) have been pre-defined. This framework has been evaluated, and the results demonstrate its capability for effective video security storage, facilitating practical needs in construction management. The study extends existing research and provides a practical solution for large-sized construction video storage with security and lightweight considerations. The proposed video security storage and data lightweight process offers substantial benefits to construction management, such as streamlined accident investigation and accountability and improved on-site work efficiency, contributing to the smooth progress of construction projects.}
}
@article{BARKE2023618,
title = {Linking life cycle sustainability assessment and the sustainable development goals – Calculation of goal achievement},
journal = {Procedia CIRP},
volume = {116},
pages = {618-623},
year = {2023},
note = {30th CIRP Life Cycle Engineering Conference},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2023.02.104},
url = {https://www.sciencedirect.com/science/article/pii/S2212827123000999},
author = {Alexander Barke and Manbir S. Sodhi and Christian Thies and Thomas S. Spengler},
keywords = {Sustainable development goals (SDGs), SDG quantification, Sustainable development, Life cycle sustainability assessment},
abstract = {In 2015, the United Nations General Assembly proposed seventeen Sustainable Development Goals (SDGs) intended to ensure sustainable development worldwide at the economic, environmental, and social levels. SDGs are now being used by some corporations in formulating and expressing business strategies. However, assessing the effects of corporate activities and products regarding their contribution to SDGs is difficult. In this paper, we have developed a method for linking life cycle sustainability assessment (LCSA) with the SDGs and calculating the contribution to SDG achievement. An essential part of this approach is the weighting of LCSA impact categories, which is typically done using equal weighting. This weighting method enables compensation of negative contributions by positive contributions in different impact categories but results in ambiguity in the results. This article identifies alternative weighting methods, integrates them into a computational approach, and determines their influence on the SDG contribution scores. The analysis shows that the use of alternative weights changes SDG contribution scores. However, the same product always has the highest SDG contribution score, regardless of the weighting method used. Nonetheless, the recommendations for action with regard to the total product alternatives would change depending on the weighting method.}
}
@incollection{KOKINOV200699,
title = {Chapter 4 A Cognitive Approach to Context Effects on Individual Decision Making under Risk},
editor = {Richard Topol and Bernard Walliser},
series = {Contributions to Economic Analysis},
publisher = {Elsevier},
volume = {280},
pages = {99-116},
year = {2006},
booktitle = {Cognitive Economics},
issn = {0573-8555},
doi = {https://doi.org/10.1016/S0573-8555(06)80005-2},
url = {https://www.sciencedirect.com/science/article/pii/S0573855506800052},
author = {Boicho Kokinov and Daniela Raeva},
keywords = {choice under risk, computational models, context effects},
abstract = {This chapter compares and contrasts various approaches to understanding human decision making under risk, and is trying to formulate requirements for a cognitive economics theory of risky decision making. Then a first attempt is made to put forward such a theory by proposing a cognitive model JUDGEMAP based on the general cognitive architecture DUAL. This allows the model to be integrated with other cognitive processes such as perception, analogical reasoning, spreading activation memory retrieval, etc. The fact that all processes in DUAL are based on local computations and parallel processing allows for modelling the interplay between various cognitive processes during the decision-making process, in particular the model predicts that the unconscious and automatic process of spreading activation will influence the conscious process of argument building and comparison. This prediction is tested and confirmed by a psychological experiment that demonstrates that seemingly remote and irrelevant aspects of the environment can change the decision we make.}
}
@article{KLEINMUNTZ1994457,
title = {Toward intelligent computerized clinicians},
journal = {Computers in Human Behavior},
volume = {10},
number = {4},
pages = {457-466},
year = {1994},
issn = {0747-5632},
doi = {https://doi.org/10.1016/0747-5632(94)90040-X},
url = {https://www.sciencedirect.com/science/article/pii/074756329490040X},
author = {Benjamin Kleinmuntz},
abstract = {The current scientific interest in computer thinking is explored for constructing a simulated psychodiagnostician. Going beyond simply building an expert system, this paper proposes to use a self-learning and generalizing computer architecture. State Operator And Result, or the SOAR system, to model intelligent clinical behavior. Accordingly, it fosters the collection and analysis of process-tracing verbal protocols for building psychodiagnosticians. It also holds out the possibility of going beyond psychodiagnosis for functioning as a multitask clinical psychologist.}
}
@article{LUND2012192,
title = {The economic crisis and sustainable development: The design of job creation strategies by use of concrete institutional economics},
journal = {Energy},
volume = {43},
number = {1},
pages = {192-200},
year = {2012},
note = {2nd International Meeting on Cleaner Combustion (CM0901-Detailed Chemical Models for Cleaner Combustion)},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2012.02.075},
url = {https://www.sciencedirect.com/science/article/pii/S0360544212001892},
author = {Henrik Lund and Frede Hvelplund},
keywords = {Sustainable energy planning, Energy and Job creation, Renewable energy and economic growth},
abstract = {This paper presents Concrete Institutional Economics as an economic paradigm to understand how the wish for sustainable energy in times of economic crisis can be used to generate jobs as well as economic growth. In most countries, including European countries, the USA and China, the implementation of sustainable energy solutions involves the replacement of imported fossil fuels by substantial investments in energy conservation and renewable energy (RE). In such situation, it becomes increasingly essential to develop economic thinking and economic models that can analyse the concrete institutions in which the market is embedded. This paper presents such tools and methodologies and applies them to the case of the Danish heating sector. The case shows how investments in decreasing fossil fuels and CO2 emissions can be made in a way in which they have a positive influence on job creation and economic development as well as public expenditures.}
}
@article{QIAN2024117679,
title = {A novel dataset and feature selection for data-driven conceptual design of offshore jacket substructures},
journal = {Ocean Engineering},
volume = {303},
pages = {117679},
year = {2024},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2024.117679},
url = {https://www.sciencedirect.com/science/article/pii/S0029801824010163},
author = {Han Qian and Emmanouil Panagiotou and Mengyan Peng and Eirini Ntoutsi and Chongjie Kang and Steffen Marx},
keywords = {Offshore jacket substructure, Conceptual design, Data-driven method, Machine learning, Dataset, Feature selection},
abstract = {Conceptual design is crucial for designing offshore jacket substructures because it sets the direction for the entire design process. Nevertheless, conventional simulation-based optimization methods for jacket conceptual design face challenges, such as high computational costs and restricted optimization objectives. This paper proposes a data-driven method for offshore jacket conceptual design using machine learning (ML). First, a novel dataset of completed and under-construction jackets worldwide was established as the cornerstone of ML. The dataset comprised “in-action” data capturing key structural parameters of jackets and information on design boundary conditions. Subsequently, different features were comprehensively selected to identify and visualize their correlations for an interpretable data-driven design, ensuring the effectiveness of the dataset for training the ML models. Finally, random forest and eXtreme gradient boosting models were trained on the data from the selected feature subsets and then employed to predict individual jacket structural parameters. The predictive performance of the models indicates that the dataset and feature selection can capture the fundamental and shared characteristics of well-designed jackets, thereby improving the accuracy and efficiency of the conceptual design process. This study suggests the potential of a data-driven conceptual design for offshore jacket substructures.}
}
@article{UMNEY2018201,
title = {Designing frames: The use of precedents in parliamentary debate},
journal = {Design Studies},
volume = {54},
pages = {201-218},
year = {2018},
issn = {0142-694X},
doi = {https://doi.org/10.1016/j.destud.2017.10.008},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X17300789},
author = {Darren Umney and Peter Lloyd},
keywords = {design process, design precedents, framing, discourse analysis, legislation},
abstract = {Using the naturally-occurring data of official UK Parliamentary transcripts for the development of a new high speed rail project, this paper takes one characteristic of the design process, the use of precedent, to explore how problems and solutions are framed during discussion. In contrast to accounts of reframing that describe one big insight changing the design process we show how one particular precedent allows a series of attempts at reframing to take place in discussion. We conclude by arguing that precedents enable a diffusion of semi-objective meaning in discussion, similar to a prototype in a more conventional design process. This contrasts with other types of discourse elements, such as storytelling, that function through the subjective accumulation of meaning.}
}
@article{ZHANG202240,
title = {FPFS: Filter-level pruning via distance weight measuring filter similarity},
journal = {Neurocomputing},
volume = {512},
pages = {40-51},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2022.09.049},
url = {https://www.sciencedirect.com/science/article/pii/S092523122201164X},
author = {Wei Zhang and Zhiming Wang},
keywords = {Model compression, Neural network pruning, Distance and similarity, Deep convolutional neural network (DCNN)},
abstract = {Deep Neural Networks (DNNs) enjoy the welfare of convolution, while also bearing huge computational pressure. Therefore, model compression techniques are used to alleviate this problem, where filter-based neural network has received extensive attention as the research object of this paper. Common approaches treat filters as independent individuals and choose retrained filters by evaluating their performance, while more complex macro methods consider relationship between filters. Therefore, we propose a facile distance-based filter selection method, called FPFS, to visualize the similarity between filters from a global perspective. We calculate and sum the distance between filters to get filters’ “Distance Weight” which is applied as a metric to assess filters. We use four common and appropriate distances for filters evaluation. To verify the performance of our algorithm, we introduce FPFS to classical DCNNs and test it on general classification datasets CIFAR-10, CIFAR-100 and mageNet. For example, FPFS reduces Parameters and FLOPs of the lightweight model DenseNet-40 to about half of the original while maintain accuracy on CIFAR-10 by 94.40% (the original model is 94.80%). To ResNet-56 on CIFAR-100, FPFS compresses FLOPs to less than half of the original, while model accuracy reaches 71.46% (the original model is 71.44%). About ResNet-50 on ImageNet, FPFS achieves 60.3% FLOPs pruning rate accompanied by 0.96% top-1 accuracy loss. We also compare the experimental results with state-of-the-art filter pruning algorithms to highlight the effectiveness of FPFS.}
}
@article{HINZEN2024110952,
title = {The ‘L-factor’: Language as a transdiagnostic dimension in psychopathology},
journal = {Progress in Neuro-Psychopharmacology and Biological Psychiatry},
volume = {131},
pages = {110952},
year = {2024},
issn = {0278-5846},
doi = {https://doi.org/10.1016/j.pnpbp.2024.110952},
url = {https://www.sciencedirect.com/science/article/pii/S0278584624000204},
author = {Wolfram Hinzen and Lena Palaniyappan},
keywords = {Thought, Psychosis, Neurocognition, Psychopathology, Brain networks},
abstract = {Thoughts and moods constituting our mental life incessantly change. When the steady flow of this dynamics diverges in clinical directions, the possible pathways involved are captured through discrete diagnostic labels. Yet a single vulnerable neurocognitive system may be causally involved in psychopathological deviations transdiagnostically. We argue that language viewed as integrating cortical functions is the best current candidate, whose forms of breakdown along its different dimensions are then manifest as symptoms – from prosodic abnormalities and rumination in depression to distortions of speech perception in verbal hallucinations, distortions of meaning and content in delusions, or disorganized speech in formal thought disorder. Spontaneous connected speech provides continuous objective readouts generating a highly accessible bio-behavioral marker with the potential of revolutionizing neuropsychological measurement. This argument turns language into a transdiagnostic ‘L-factor’ providing an analytical and mechanistic substrate for previously proposed latent general factors of psychopathology (‘p-factor’) and cognitive functioning (‘c-factor’). Together with immense practical opportunities afforded by rapidly advancing natural language processing (NLP) technologies and abundantly available data, this suggests a new era of translational clinical psychiatry, in which both psychopathology and language may be rethought together.}
}
@article{BARRETO20114206,
title = {Lumping the States of a Finite Markov Chain Through Stochastic Factorization},
journal = {IFAC Proceedings Volumes},
volume = {44},
number = {1},
pages = {4206-4211},
year = {2011},
note = {18th IFAC World Congress},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20110828-6-IT-1002.00073},
url = {https://www.sciencedirect.com/science/article/pii/S1474667016442680},
author = {André M.S. Barreto and Marcelo D. Fragoso},
abstract = {Abstract
In this work we show how the lumping of states of a finite Markov chain can be regarded as a special decomposition of its transition matrix called stochastic factorization. The idea is simple: when a transition matrix is factored into the product of two stochastic matrices, one can swap the factors of the multiplication to obtain another model, potentially much smaller than the original one. We prove in the paper that the smaller Markov chain has the same reducibility and the same number of closed sets as the original model. Additionally, the stationary distributions of both chains are related through a linear transformation. By interpreting the lumping of states as a particular case of stochastic factorization, we discuss in which circumstances the lumped transition matrix can be used in place of the original one to compute its stationary distribution. To illustrate our ideas we use the computation of Google's PageRank as an example.}
}
@incollection{ZOHURI2022121,
title = {Chapter 5 - Mathematical modeling driven predication},
editor = {Bahman Zohuri and Farhang Mossavar-Rahmani and Farahnaz Behgounia},
booktitle = {Knowledge is Power in Four Dimensions: Models to Forecast Future Paradigm},
publisher = {Academic Press},
pages = {121-163},
year = {2022},
isbn = {978-0-323-95112-8},
doi = {https://doi.org/10.1016/B978-0-323-95112-8.00005-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780323951128000052},
author = {Bahman Zohuri and Farhang Mossavar-Rahmani and Farahnaz Behgounia},
keywords = {Data mining and data analytics, Forecasting and prediction, Modeling and mathematics},
abstract = {During the past decade, there has been a tremendous blast and progress in computation technology, and with it comes vast amounts of data in a variety of fields such as the economy, medicine, biology, banking services such as customer relation management and credit card fraud, finance, demographic population growth from a demographical point of view nationwide and worldwide, and the need for new lifestyles and growth in term of continuous renewable sources of energy and its production, as well as marketing are among the fields that can be mentioned. The challenge of understanding these data has led to the development of new tools such as predictive analytics in the field of statistics and spawned new areas such as data mining, machine learning, and bioinformatics to process these data and determine the integrity of their information for prediction analysis. Many of these tools have common underpinnings but are often expressed with different terminology. This chapter will summarize the important ideas in these areas in a common conceptual framework.}
}
@article{BAJAJ2021117750,
title = {Association between emotional intelligence and effective brain connectome: A large-scale spectral DCM study},
journal = {NeuroImage},
volume = {229},
pages = {117750},
year = {2021},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2021.117750},
url = {https://www.sciencedirect.com/science/article/pii/S1053811921000276},
author = {Sahil Bajaj and William D.S. Killgore},
abstract = {Introduction
Emotional Intelligence (EI) is a well-documented aspect of social and interpersonal functioning, but the underlying neural mechanisms for this capacity remain poorly understood. Here we used advanced brain connectivity techniques to explore the associations between EI and effective connectivity (EC) within four functional brain networks.
Methods
The Mayer-Salovey-Caruso Emotional Intelligence Test (MSCEIT) was used to collect EI data from 55 healthy individuals (mean age = 30.56±8.3 years, 26 males). The MSCEIT comprises two area cores – experiential EI (T1) and strategic EI (T2). The T1 core included two sub-scales – perception of emotions (S1) and using emotions to facilitate thinking (S2), and the T2 core included two sub-scales – understanding of emotions (S3) and management of emotions (S4). All participants underwent structural and resting-state functional magnetic resonance imaging (rsfMRI) scans. The spectral dynamic causal modeling approach was implemented to estimate EC within four networks of interest – the default-mode network (DMN), dorsal attention network (DAN), control-execution network (CEN) and salience network (SN). The strength of EC within each network was correlated with the measures of EI, with correlations at pFDR < 0.05 considered as significant.
Results
There was no significant association between any of the measures of EI and EC strength within the DMN and DAN. For CEN, however, we found that there were significant negative associations between EC strength from the right anterior prefrontal cortex (RAPFC) to the left anterior prefrontal cortex (LAPFC) and both S2 and T1, and significant positive associations between EC strength from LAPFC to RAPFC and S2. EC strength from the right superior parietal cortex (SPC) to RAPFC also showed significant negative association with S4 and T2. For the SN, S3 showed significant negative association with EC strength from the right insula to RAPFC and significant positive association with EC strength from the left insula to dorsal anterior cingulate cortex (DACC).
Conclusions
We provide evidence that the negative ECs within the right hemisphere, and from the right to left hemisphere, and positive ECs within the left hemisphere and from the left to right hemisphere of CEN (involving bilateral frontal and right parietal region) and SN (involving right frontal, anterior cingulate and bilateral insula) play a significant role in regulating and processing emotions. These findings also suggest that measures of EC can be utilized as important biomarkers to better understand the underlying neural mechanisms of EI.}
}
@article{CAHILL20172131,
title = {Building a Community of Practice to Prepare the HPC Workforce},
journal = {Procedia Computer Science},
volume = {108},
pages = {2131-2140},
year = {2017},
note = {International Conference on Computational Science, ICCS 2017, 12-14 June 2017, Zurich, Switzerland},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2017.05.059},
url = {https://www.sciencedirect.com/science/article/pii/S1877050917305902},
author = {Katharine J. Cahill and Scott Lathrop and Steven Gordon},
keywords = {HPC workforce, petascale computing, on-line education, graduate education, SPOC course},
abstract = {It has been well documented for more than 30 years, that significantly more effort is needed to prepare the HPC workforce needed today and well into the future. The Blue Waters Virtual School of Computational Science (VSCSE) provides an innovative model for addressing this critical need. The VSCSE uses a Small Private Online Course (SPOC) approach to providing graduate level credit courses to students at multiple institutions. In this paper, we describe the rationale for this approach, a description of the implementation, findings from external evaluations, and lessons learned. The paper concludes with recommendations for future strategies to build on this work to address the workforce needs of our global society.}
}
@article{KAUFFMAN2017115,
title = {Combining machine-based and econometrics methods for policy analytics insights},
journal = {Electronic Commerce Research and Applications},
volume = {25},
pages = {115-140},
year = {2017},
issn = {1567-4223},
doi = {https://doi.org/10.1016/j.elerap.2017.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S1567422317300145},
author = {Robert J. Kauffman and Kwansoo Kim and Sang-Yong Tom Lee and Ai-Phuong Hoang and Jing Ren},
keywords = {Causality, Computational Social Science, Data analytics, Econometrics, E-commerce, Empirical research, Fintech, Fusion analytics, Music popularity, Stock trading, Policy analytics, TV viewing, Video-on-demand (VoD)},
abstract = {Computational Social Science (CSS) has become a mainstream approach in the empirical study of policy analytics issues in various domains of e-commerce research. This article is intended to represent recent advances that have been made for the discovery of new policy-related insights in business, consumer and social settings. The approach discussed is fusion analytics, which combines machine-based methods from Computer Science (CS) and explanatory empiricism involving advanced Econometrics and Statistics. It explores several efforts to conduct research inquiry in different functional areas of Electronic Commerce and Information Systems (IS), with applications that represent different functional areas of business, as well as individual consumer, social and public issues. Recent developments and shifts in the scientific study of technology-related phenomena and Social Science issues in the presence of historically-large datasets prompt new forms of research inquiry. They include blended approaches to research methodology, and more interest in the production of research results that have direct application to industry contexts. This article showcases the methods shifts and several contemporary applications. They discuss: (1) feedback effects in mobile phone-based stock trading; (2) sustainability of top-rank chart popularity of music tracks; (3) household TV viewing patterns; and (4) household sampling and purchases of video-on-demand (VoD) services. The range of applicability of the ideas goes beyond the scope of these illustrations, to include issues in public services, healthcare, product and service deployment, public opinion and elections, electronic auctions, and travel and tourism services. In fact, the coverage is as broad as for-profit and for-non-profit, private and public, and governmental and non-governmental institutions.}
}
@article{MERRICK2019511,
title = {On choosing the resolution of normative models},
journal = {European Journal of Operational Research},
volume = {279},
number = {2},
pages = {511-523},
year = {2019},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2019.06.017},
url = {https://www.sciencedirect.com/science/article/pii/S0377221719304928},
author = {James H. Merrick and John P. Weyant},
keywords = {Problem structuring, Validation of OR computations, Information theory, Strategic planning, OR in environment and climate change},
abstract = {Long time horizon normative models are frequently used for policy analysis, strategic planning, and system analysis. Choosing the granularity of the temporal or spatial resolution of such models is an important modeling decision, often having a first order impact on model results. This type of decision is frequently made by modeler judgment, particularly when the predictive power of alternative choices cannot be tested. In this paper, we show how the implicit tradeoffs modelers make in these formulation decisions, in particular in the tradeoff between the accuracy of representation enabled by the available data and model parsimony, may be addressed with established information theoretic ideas. The paper provides guidance for modelers making these tradeoffs or, in certain cases, enables explicit tests for assessing appropriate levels of resolution. We will mainly focus on optimization based normative models in the discussion here, and draw our examples from the energy and climate domain.}
}
@article{ZAK2019139,
title = {Multiple Criteria Optimization of the Carpooling Problem},
journal = {Transportation Research Procedia},
volume = {37},
pages = {139-146},
year = {2019},
note = {21st EURO Working Group on Transportation Meeting, EWGT 2018, 17th – 19th September 2018, Braunschweig, Germany},
issn = {2352-1465},
doi = {https://doi.org/10.1016/j.trpro.2018.12.176},
url = {https://www.sciencedirect.com/science/article/pii/S235214651830591X},
author = {Jacek Żak and Maciej Hojda and Grzegorz Filcek},
keywords = {carpooling, multiple criteria optimisation, NSGA II, LBS},
abstract = {The paper presents a multiple criteria (MC) formulation of the carpooling optimization (CO) problem and a solution procedure that allows to solve it. The mathematical model of the MCCO problem includes two major sub-problems, such as planning of the routes and matching carpoolers (drivers and passengers). Different aspects, including: economic, social, technical and market-oriented are considered. The MCCO problem is solved with the application of an original computational procedure based on the multiple criteria genetic algorithm, called NSGA II and the solutions’ analysis and review technique, called Light Beam Search (LBS) method. The former method allows to generate a set of Pareto optimal solutions, while the latter assists the carpoolers in finding the most desired compromise solution (common route and match between carpoolers). The results of computational experiments are presented. We find that solving the formulating carpooling problem in a heuristic manner is possible in reasonable time}
}
@article{STRACHANREGAN2024e28340,
title = {The impact of room shape on affective states, heartrate, and creative output},
journal = {Heliyon},
volume = {10},
number = {6},
pages = {e28340},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e28340},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024043718},
author = {K. Strachan-Regan and O. Baumann},
keywords = {Built environment, Neuroarchitecture, Environmental psychology, Emotion, Creativity},
abstract = {The architectural design of space can deeply impact an individuals' mood, physiology, and mental health. While previous research has predominantly focused on elements like nature and lighting within architectural spaces, there is a growing literature base that also investigates the psychological and neurophysiological impacts of geometrical properties of architectural spaces. Employing virtual reality technology, the study sought to investigate the effects of curved and rectangular architectural spaces on affective states, heart rate, and creativity. A total of 35 participants were exposed to two distinct virtual environments: a curved room and a rectangular room. Participants' self-reported mood was assessed using the Positive and Negative Affect Schedule (PANAS-Long Form). Heart rate was monitored using a pulse oximeter, and creative output was evaluated using the Guilford Alternative Uses Task (GAUT). Statistical comparisons between the two room types indicated that participants experienced higher positive affect and lower negative affect in the curved room condition compared to the rectangular room condition. Furthermore, heart rate measurements revealed lower physiological arousal in the curved room. Additionally, participants exhibited higher creative output in the curved room as opposed to the rectangular room. These findings align with previous literature on the influence of geometric factors on affective responses. The implications of this study are significant as they pertain to individuals' daily environments and their impact on health and well-being. The positive influence of curved room geometry on mood, arousal, and creativity emphasises the importance of considering room layout and design in various settings, such as workplaces and educational environments. Architects and designers can utilise these findings to inform their decisions and promote neuroarchitecture that enhances positive emotional experiences and productivity.}
}
@article{WU20183,
title = {The five key questions of human performance modeling},
journal = {International Journal of Industrial Ergonomics},
volume = {63},
pages = {3-6},
year = {2018},
note = {Human Performance Modeling},
issn = {0169-8141},
doi = {https://doi.org/10.1016/j.ergon.2016.05.007},
url = {https://www.sciencedirect.com/science/article/pii/S0169814116300427},
author = {Changxu Wu},
keywords = {Human performance modeling},
abstract = {Via building computational (typically mathematical and computer simulation) models, human performance modeling (HPM) quantifies, predicts, and maximizes human performance, human-machine system productivity and safety. This paper describes and summarizes the five key questions of human performance modeling: 1) Why we build models of human performance; 2) What the expectations of a good human performance model are; 3) What the procedures and requirements in building and verifying a human performance model are; 4) How we integrate a human performance model with system design; and 5) What the possible future directions of human performance modeling research are. Recent and classic HPM findings are addressed in the five questions to provide new thinking in HPM's motivations, expectations, procedures, system integration and future directions.}
}
@article{WANG2025100738,
title = {Shaping the future of academic conferences},
journal = {The Innovation},
volume = {6},
number = {1},
pages = {100738},
year = {2025},
issn = {2666-6758},
doi = {https://doi.org/10.1016/j.xinn.2024.100738},
url = {https://www.sciencedirect.com/science/article/pii/S2666675824001760},
author = {Haijun Wang and Ji Dai and Yuanzheng Cui and Peijun Zhang and Fang Wang and Buxing Han and Erik Jeppesen}
}
@article{HACKENBERG2025101205,
title = {Decentering to support responsive teaching for middle school students},
journal = {The Journal of Mathematical Behavior},
volume = {77},
pages = {101205},
year = {2025},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2024.101205},
url = {https://www.sciencedirect.com/science/article/pii/S0732312324000828},
author = {Amy J. Hackenberg and Fetiye {Aydeniz Temizer} and Rebecca S. Borowski},
keywords = {Decentering, Units coordination, Classroom study, Proportional reasoning, Middle school students, Teaching practice},
abstract = {A classroom study was conducted to understand how to engage in responsive teaching with 18 seventh grade students at three stages of units coordination during a unit on proportional reasoning co-taught by the first author and classroom teacher. In the unit, students worked on making two cars travel the same speed. Students at all three stages of units coordination learned to do so, as reported elsewhere (Hackenberg et al., 2023). This paper focuses on the practice of inquiring responsively in small groups. We found that teacher-researcher decentering was a mechanism underlying this practice. Decentering involves adopting the perspective of another person by setting one’s own perspective to the side and using the other’s perspective as a basis for interaction. We found that two patterns of decentering actions and a type of question, leveraging questions, supported students across stages of units coordination to sustain challenges and learn.}
}
@article{BRADLEY2016400,
title = {Jet flame heights, lift-off distances, and mean flame surface density for extensive ranges of fuels and flow rates},
journal = {Combustion and Flame},
volume = {164},
pages = {400-409},
year = {2016},
issn = {0010-2180},
doi = {https://doi.org/10.1016/j.combustflame.2015.09.009},
url = {https://www.sciencedirect.com/science/article/pii/S0010218015003120},
author = {Derek Bradley and Philip H. Gaskell and Xiaojun Gu and Adriana Palacios},
keywords = {Jet flame height, Lift-off distance, Flamelet modelling, “Fracking”, Jet flame stability, Mean flame surface density},
abstract = {An extensive review and re-thinking of jet flame heights and structure, extending into the choked/supersonic regime is presented, with discussion of the limitations of previous flame height correlations. Completely new dimensionless correlations for the plume heights, lift-off distances, and mean flame surface densities of atmospheric jet flames, in the absence of a cross wind, are presented. It was found that the same flow rate parameter could be used to correlate both plume heights and flame lift-off distances. These are related to the flame structure, jet flame instability, and flame extinction stretch rates, as revealed by complementary experiments and computational studies. The correlations are based on a vast experimental data base, covering 880 flame heights. They encompass pool fires and flares, as well as choked and unchoked jet flames of CH4, C2H2, C2H4, C3H8, C4H10 and H2, over a wide range of conditions. Supply pressures range from 0.06 to 90 MPa, discharge diameters from 4 × 10−4 to 1.32 m, and flame heights from 0.08 to 110 m. The computational studies enabled reaction zone volumes to be estimated, as a proportion of the plume volumes, measured from flame photographs, and temperature contours. This enabled mean flame surface densities to be estimated, together with mean volumetric heat releases rates. There is evidence of a “saturation” mean surface density and increases in turbulent burn rates being accomplished by near pro rata increases in the overall volume of reacting mixture.}
}
@article{MAKRIDIS201328,
title = {Offshore wind power resource availability and prospects: A global approach},
journal = {Environmental Science & Policy},
volume = {33},
pages = {28-40},
year = {2013},
issn = {1462-9011},
doi = {https://doi.org/10.1016/j.envsci.2013.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S146290111300097X},
author = {Christos Makridis},
keywords = {Offshore wind energy, Renewable energy, Global perspective, Renewable energy investment},
abstract = {In the absence of structural incentives that price negative externalities, renewable energies rely primarily on investors’ expectations of future performance to succeed in the marketplace. While there have been many disparate regional analyses of the prospects for clean energy, in particular wind, there is yet a cohesive framework for thinking about global interactions. Using data from the National Renewable Energy Laboratory (NREL), the article addresses three shortcomings in empirical renewable policy literature. First, the article briefly synthesizes the current state of the offshore wind literature. Second, the article develops a linear programming model to assess the relative prospects of offshore wind energy throughout seven world regions: Organization for Economic Co-operation and Development (OECD) North America, OECD Europe, OECD Asia and Eurasia, Non-OECD Europe and Eurasia, Non-OECD Asia, Africa, and Central & South America. Third, the article applies the Interactive Agency Model (IAM) as a systems-level framework for thinking about offshore wind development in the presence of social, economic, and institutional attributes. Results suggest that OECD Asia and Eurasia, OECD Europe, and non-OECD Europe and Eurasia, respectively, have the highest potential for offshore wind sector performance. Despite simplifying assumptions, this article presents one of the first evaluations of global offshore wind energy potential for policymakers and industry to consider in crafting future renewable energy investment decisions.}
}
@article{GOEL2023,
title = {Users’ Concerns About Endometriosis on Social Media: Sentiment Analysis and Topic Modeling Study},
journal = {Journal of Medical Internet Research},
volume = {25},
year = {2023},
issn = {1438-8871},
doi = {https://doi.org/10.2196/45381},
url = {https://www.sciencedirect.com/science/article/pii/S1438887123006179},
author = {Rahul Goel and Vijayachitra Modhukur and Katrin Täär and Andres Salumets and Rajesh Sharma and Maire Peters},
keywords = {endometriosis, latent Dirichlet allocation, pain, Reddit, sentiment analysis, social media, surgery, topic modeling, user engagement},
abstract = {Background
Endometriosis is a debilitating and difficult-to-diagnose gynecological disease. Owing to limited information and awareness, women often rely on social media platforms as a support system to engage in discussions regarding their disease-related concerns.
Objective
This study aimed to apply computational techniques to social media posts to identify discussion topics about endometriosis and to identify themes that require more attention from health care professionals and researchers. We also aimed to explore whether, amid the challenging nature of the disease, there are themes within the endometriosis community that gather posts with positive sentiments.
Methods
We retrospectively extracted posts from the subreddits r/Endo and r/endometriosis from January 2011 to April 2022. We analyzed 45,693 Reddit posts using sentiment analysis and topic modeling–based methods in machine learning.
Results
Since 2011, the number of posts and comments has increased steadily. The posts were categorized into 11 categories, and the highest number of posts were related to either asking for information (Question); sharing the experiences (Rant/Vent); or diagnosing and treating endometriosis, especially surgery (Surgery related). Sentiment analysis revealed that 92.09% (42,077/45,693) of posts were associated with negative sentiments, only 2.3% (1053/45,693) expressed positive feelings, and there were no categories with more positive than negative posts. Topic modeling revealed 27 major topics, and the most popular topics were Surgery, Questions/Advice, Diagnosis, and Pain. The Survey/Research topic, which brought together most research-related posts, was the last in terms of posts.
Conclusions
Our study shows that posts on social media platforms can provide insights into the concerns of women with endometriosis symptoms. The analysis of the posts confirmed that women with endometriosis have to face negative emotions and pain daily. The large number of posts related to asking questions shows that women do not receive sufficient information from physicians and need community support to cope with the disease. Health care professionals should pay more attention to the symptoms and diagnosis of endometriosis, discuss these topics with patients to reduce their dissatisfaction with doctors, and contribute more to the overall well-being of women with endometriosis. Researchers should also become more involved in social media and share new science-based knowledge regarding endometriosis.}
}
@article{OXENFELDT197783,
title = {The computation of costs for price decisions},
journal = {Industrial Marketing Management},
volume = {6},
number = {2},
pages = {83-90},
year = {1977},
issn = {0019-8501},
doi = {https://doi.org/10.1016/0019-8501(77)90045-1},
url = {https://www.sciencedirect.com/science/article/pii/0019850177900451},
author = {A.R. Oxenfeldt},
abstract = {Business should compute costs in a particular way of pricing purposes. The correct cost computation varies with its purpose, though most executives still believe that an item has a true cost regardless of why it is computed. Even cost estimates made for price decisions will differ according to the type of price decision that is at issue. One finds important differences depending on whether one is estimating costs for a one-shot bid, for a promotional price offer that is to last for a short period, or for a decision concerning long-term price. Although the same basic principles would apply in all three cases, their application is different. The appropriate concept is that of “decision cost”, a very simple but powerful idea that leads to different cost conclusions than are reached by prevailing costing methods.}
}
@article{MEDFORD201536,
title = {From the Sabatier principle to a predictive theory of transition-metal heterogeneous catalysis},
journal = {Journal of Catalysis},
volume = {328},
pages = {36-42},
year = {2015},
note = {Special Issue: The Impact of Haldor Topsøe on Catalysis},
issn = {0021-9517},
doi = {https://doi.org/10.1016/j.jcat.2014.12.033},
url = {https://www.sciencedirect.com/science/article/pii/S0021951714003686},
author = {Andrew J. Medford and Aleksandra Vojvodic and Jens S. Hummelshøj and Johannes Voss and Frank Abild-Pedersen and Felix Studt and Thomas Bligaard and Anders Nilsson and Jens K. Nørskov},
keywords = {Heterogeneous catalysis, Transition metals, Theory, Computational catalysis, DFT, Sabatier principle, Scaling relation, Descriptor},
abstract = {We discuss three concepts that have made it possible to develop a quantitative understanding of trends in transition-metal catalysis: scaling relations, activity maps, and the d-band model. Scaling relations are correlations between surface bond energies of different adsorbed species including transition states; they open the possibility of mapping the many parameters determining the rate of a full catalytic reaction onto a few descriptors. The resulting activity map can be viewed as a quantitative implementation of the classical Sabatier principle, which states that there is an optimum “bond strength” defining the best catalyst for a given reaction. In the modern version, the scaling relations determine the relevant “bond strengths” and the fact that these descriptors can be measured or calculated makes it a quantitative theory of catalysis that can be tested experimentally by making specific predictions of new catalysts. The quantitative aspect of the model therefore provides new possibilities in catalyst design. Finally, the d-band model provides an understanding of the scaling relations and variations in catalytic activity in terms of the electronic structure of the transition-metal surface.}
}
@article{TAMSTORF2013362,
title = {Discrete bending forces and their Jacobians},
journal = {Graphical Models},
volume = {75},
number = {6},
pages = {362-370},
year = {2013},
issn = {1524-0703},
doi = {https://doi.org/10.1016/j.gmod.2013.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S1524070313000209},
author = {Rasmus Tamstorf and Eitan Grinspun},
keywords = {Discrete shells, Hinge angle Hessian, Bending force Jacobians, Dihedral angle},
abstract = {Computation of bending forces on triangle meshes is required for numerous simulation and geometry processing applications. In particular it is a key component in cloth simulation. A common quantity in many bending models is the hinge angle between two adjacent triangles. This angle is straightforward to compute, and its gradient with respect to vertex positions (required for the forces) is easily found in the literature. However, the Hessian of the bend angle, which is required to compute the associated force Jacobians is not documented in the literature. Force Jacobians are required for efficient numerics (e.g., implicit time stepping, Newton-based energy minimization) and are thus highly desirable. Readily available computations of the force Jacobian, such as those produced by symbolic algebra systems, or by autodifferentiation codes, are expensive to compute and therefore less useful. We present compact, easily reproducible, closed form expressions for the Hessian of the bend angle. Compared to automatic differentiation, we measure up to 7× speedup for the evaluation of the bending forces and their Jacobians.}
}