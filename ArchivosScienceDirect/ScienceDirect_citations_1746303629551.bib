@article{HUANG1993717,
title = {An investigation of gender differences in cognitive abilities among Chinese high school students},
journal = {Personality and Individual Differences},
volume = {15},
number = {6},
pages = {717-719},
year = {1993},
issn = {0191-8869},
doi = {https://doi.org/10.1016/0191-8869(93)90012-R},
url = {https://www.sciencedirect.com/science/article/pii/019188699390012R},
author = {Jiafen Huang},
abstract = {This study investigated gender differences in 11 cognitive tests from a sample of grade 11 students in Shanghai, China. Research found that the girls outperformed boys significantly on Word Knowledge and Word Span tasks, and also on a Computational Speed and Accuracy test. Boys outperformed girls only on the Paper Folding test. Factor based scores showed that girls were superior to boys on memory, and verbal composites, whereas boys were superior to girls on the spatial composites. No gender differences were found on the Mathematical Thinking test and other reasoning tests. The research findings seemed to suggest that where the social conditions were more uniform the gender differences on visual-spatial and mathematical reasoning skills would be smaller.}
}
@article{NAVLAKHA201864,
title = {Network Design and the Brain},
journal = {Trends in Cognitive Sciences},
volume = {22},
number = {1},
pages = {64-78},
year = {2018},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2017.09.012},
url = {https://www.sciencedirect.com/science/article/pii/S1364661317302000},
author = {Saket Navlakha and Ziv Bar-Joseph and Alison L. Barth},
abstract = {Neural circuits have evolved to accommodate similar information processing challenges as those faced by engineered systems. Here, we compare neural versus engineering strategies for constructing networks. During circuit development, synapses are overproduced and then pruned back over time, whereas in engineered networks, connections are initially sparse and are then added over time. We provide a computational perspective on these two different approaches, including discussion of how and why they are used, insights that one can provide the other, and areas for future joint investigation. By thinking algorithmically about the goals, constraints, and optimization principles used by neural circuits, we can develop brain-derived strategies for enhancing network design, while also stimulating experimental hypotheses about circuit development and function.}
}
@incollection{WANG2001297,
title = {Computational Intelligence in Agile Manufacturing Engineering},
editor = {A. Gunasekaran},
booktitle = {Agile Manufacturing: The 21st Century Competitive Strategy},
publisher = {Elsevier Science Ltd},
address = {Oxford},
pages = {297-315},
year = {2001},
isbn = {978-0-08-043567-1},
doi = {https://doi.org/10.1016/B978-008043567-1/50016-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780080435671500164},
author = {Kesheng Wang}
}
@article{GEMMELL201720,
title = {Establishing the structures within populations of models},
journal = {Progress in Biophysics and Molecular Biology},
volume = {129},
pages = {20-24},
year = {2017},
note = {Validation of Computer Modelling},
issn = {0079-6107},
doi = {https://doi.org/10.1016/j.pbiomolbio.2017.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S0079610716300128},
author = {Philip M. Gemmell},
abstract = {As computational biology matures as a field, increasing attention is being paid to the relation of computational models to their target. One aspect of this is addressing how computational models can appropriately reproduce the variation seen in experimental data, with one solution being to use populations of models united by a common set of equations (the framework), with each individual member of the population (each model) possessing its own unique set of equation parameters. These model populations are then calibrated and validated against experimental data, and as a whole reproduce the experimentally observed variation. The primary focus of validation thus becomes the population, with the individual models' validation seemingly deriving from their membership of this population. The role of individual models within the population is not clear, with uncertainty regarding the relationship between individual models and the population they make up. This work examines the role of models within the population, how they relate to the population they make up, and how both can be said to be validated in this context.}
}
@article{GERPOTT2024101783,
title = {New ways of seeing: Four ways you have not thought about Registered Reports yet},
journal = {The Leadership Quarterly},
volume = {35},
number = {2},
pages = {101783},
year = {2024},
issn = {1048-9843},
doi = {https://doi.org/10.1016/j.leaqua.2024.101783},
url = {https://www.sciencedirect.com/science/article/pii/S1048984324000122},
author = {Fabiola H. Gerpott and Roman Briker and George Banks},
keywords = {Registered Reports, Open Science, Transparency, Quantitative, Qualitative, Leadership},
abstract = {The Leadership Quarterly has helped as a pioneer in accepting Registered Reports (RRs), a submission format where authors provide the introduction, theory section, and methods of their paper for peer review before data collection. Proud but never satisfied, we aim to further boost the number of suitable RR submissions due to our firm belief in their potential for fostering transparent, high-impact research. To inspire authors to explore diverse data collection strategies and methods beyond experiments and survey-based (replication) studies, this work presents four distinct but equally suitable research formats for RRs: meta-analyses, qualitative research, computational approaches, and field intervention studies. Expanding prior research that has explored and promoted general practices and methodological standards for RRs, we offer unique recommendations for preparing an adequate RR proposal along each of these four RR avenues. Additionally, we provide a table of summary resources for authors, reviewers, and editors looking to engage more with RR. In conclusion, we envision a future where other top-tier journals and funding agencies follow The Leadership Quarterly by embracing the incorporation of RRs as a critical component of their strategic approach.}
}
@article{BUCHBERGER2006470,
title = {Theorema: Towards computer-aided mathematical theory exploration},
journal = {Journal of Applied Logic},
volume = {4},
number = {4},
pages = {470-504},
year = {2006},
note = {Towards Computer Aided Mathematics},
issn = {1570-8683},
doi = {https://doi.org/10.1016/j.jal.2005.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S1570868305000716},
author = {Bruno Buchberger and Adrian Crǎciun and Tudor Jebelean and Laura Kovács and Temur Kutsia and Koji Nakagawa and Florina Piroi and Nikolaj Popov and Judit Robu and Markus Rosenkranz and Wolfgang Windsteiger},
keywords = {Mathematical assistant, Automated reasoning, Theory exploration, “Lazy Thinking”, Theorema},
abstract = {Theorema is a project that aims at supporting the entire process of mathematical theory exploration within one coherent logic and software system. This survey paper illustrates the style of Theorema-supported mathematical theory exploration by a case study (the automated synthesis of an algorithm for the construction of Gröbner Bases) and gives an overview on some reasoners and organizational tools for theory exploration developed in the Theorema project.}
}
@article{PARKER20161,
title = {Coastal planning should be based on proven sea level data},
journal = {Ocean & Coastal Management},
volume = {124},
pages = {1-9},
year = {2016},
issn = {0964-5691},
doi = {https://doi.org/10.1016/j.ocecoaman.2016.02.005},
url = {https://www.sciencedirect.com/science/article/pii/S0964569116300205},
author = {A. Parker and C.D. Ollier},
keywords = {Sea level, Measurements, Computations, Tide gauges, Coastal management},
abstract = {There are two related measures of sea level, the absolute sea level, which is the increase in the sea level in an absolute reference frame, and relative sea level, which is the increase in sea level recorded by tide gauges. The first measure is a rather abstract computation, far from being reliable, and is preferred by activists and politicians for no scientific reason. For local and global problems it is better to use local tide gauge data. Proper coastal management should be based on proved measurements of sea level. Tide gauges provide the most reliable measurements, and best data to assess the rate of change. We show as the naïve averaging of all the tide gauges included in the PSMSL surveys show “relative” rates of rise about +1.04 mm/year (570 tide gauges of any length). If we consider only 100 tide gauges with more than 80 years of recording the rise is only +0.25 mm/year. This naïve averaging has been stable and shows that the sea levels are slowly rising but not accelerating. We also show as the additional information provided by GPS and satellite altimetry is of very little help. Computations of “absolute” sea levels suffer from inaccuracies with errors larger than the estimated trends. The GPS is more reliable than satellite altimetry, but the accuracy of the estimation of the vertical velocity at GPS domes is still well above ±1 mm/year and the relative motion of tide gauges vs. GPS domes is mostly unassessed. The satellite altimetry returns a noisy signal so that a +3.2 mm/year trend is only achieved by arbitrary “corrections”. We conclude that if the sea levels are only oscillating about constant trends everywhere as suggested by the tide gauges, then the effects of climate change are negligible, and the local patterns may be used for local coastal planning without any need of purely speculative global trends based on emission scenarios. Ocean and coastal management should acknowledge all these facts. As the relative rates of rises are stable worldwide, coastal protection should be introduced only where the rate of rise of sea levels as determined from historical data show a tangible short term threat. As the first signs the sea levels will rise catastrophically within few years are nowhere to be seen, people should start really thinking about the warnings not to demolish everything for a case nobody knows will indeed happen.}
}
@article{201119,
title = {Evolution of cognition might be down to brain chemistry},
journal = {New Scientist},
volume = {210},
number = {2806},
pages = {19},
year = {2011},
issn = {0262-4079},
doi = {https://doi.org/10.1016/S0262-4079(11)60726-4},
url = {https://www.sciencedirect.com/science/article/pii/S0262407911607264},
abstract = {The prefrontal cortex, the “thinking” part of our brain, has a radically different chemical balance to that of chimps and macaques}
}
@article{LIN2025122926,
title = {Assessment of power loss caused by soiling PV modules using a dual branch multi-modality deep learning network framework},
journal = {Renewable Energy},
volume = {248},
pages = {122926},
year = {2025},
issn = {0960-1481},
doi = {https://doi.org/10.1016/j.renene.2025.122926},
url = {https://www.sciencedirect.com/science/article/pii/S0960148125005889},
author = {Peijie Lin and Hang Chen and Shuying Cheng and Xiaoyang Lu and Yaohai Lin and Lei Sun},
keywords = {PV power generation, Soiling loss, Multi-modality feature fusion, CA-CMDAF, Deep learning},
abstract = {Soiling can reduce the output power and work efficiency of photovoltaic (PV) modules, causing serious economic losses to PV systems. The cleaning schedules can be optimized to save economic expenses through the methods capable of estimating the power loss of PV modules resulting from soiling. This paper proposes a deep learning framework that combines visible light and infrared image information with dual branch cross-modality feature fusion. Initially, the MobileNetV2 is applied as the backbone of the dual branch framework to enhance the training efficiency and reduce the computational complexity. Subsequently, a cross-modality differential aware fusion module based on the channel attention mechanism (CA-CMDAF) is introduced to improve the cross-modality feature fusion capability of the model. Moreover, a multi-cascade and cross-modality fusion network and a multi-scale fusion network are integrated to further facilitate the effectiveness of feature fusion and reduce the loss of visual details during the feature extraction. Lastly, extensive experiments are carried out on the multi-modality dataset. The comparison results demonstrate the superior performance of the proposed dual branch network framework with the average accuracy of 88.27 %, which is higher than that of the single-modality models trained on either visible light or infrared images alone.}
}
@article{GAGNE201889,
title = {When planning to survive goes wrong: predicting the future and replaying the past in anxiety and PTSD},
journal = {Current Opinion in Behavioral Sciences},
volume = {24},
pages = {89-95},
year = {2018},
note = {Survival circuits},
issn = {2352-1546},
doi = {https://doi.org/10.1016/j.cobeha.2018.03.013},
url = {https://www.sciencedirect.com/science/article/pii/S2352154618300305},
author = {Christopher Gagne and Peter Dayan and Sonia J Bishop},
abstract = {We increase our probability of survival and wellbeing by minimizing our exposure to rare, extremely negative events. In this article, we examine the computations used to predict and avoid such events and to update our models of the world and action policies after their occurrence. We also consider how these computations might go wrong in anxiety disorders and Post Traumatic Stress Disorder (PTSD). We review evidence that anxiety is linked to increased simulations of the future occurrence of high cost negative events and to elevated estimates of the probability of occurrence of such events. We also review psychological theories of PTSD in the light of newer, computational models of updating through replay and simulation. We consider whether pathological levels of re-experiencing symptomatology might reflect problems reconciling the traumatic outcome with overly optimistic priors or difficulties terminating off-line simulation focused on negative events and over-generalization to states sharing features with those antecedent to the trauma.}
}
@article{NKONGOLO2022182,
title = {Using Deep Packet Inspection Data to Examine Subscribers on the Network},
journal = {Procedia Computer Science},
volume = {215},
pages = {182-191},
year = {2022},
note = {4th International Conference on Innovative Data Communication Technology and Application},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.12.021},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922020920},
author = {Mike Nkongolo and Jacobus Phillipus {van Deventer} and Sydney Mambwe Kasongo},
keywords = {Deep packet inspection, machine learning, UGRansome, telecommunication, data science},
abstract = {This article proposes the creation of the deep packet inspection (DPI) dataset to study subscribers’ behavior on the network, applying ensemble learning to this dataset, and comparing it with the UGRansome dataset. The subscriber can be thought of as a person or a group of users using a network service or connectivity. The DPI features represent the subscriber network usage, and the ensemble learning approach is implemented on the DPI dataset to predict the subscriber's service category on the network. The classification and prediction problem addressed on the DPI dataset reached a precision of 100%. The paper predicts that the web and streaming categories with Netflix, Facebook, and YouTube services will be the most utilized in the next few years. This study will lead to a better understanding of the idiosyncratic behavior of active subscribers on the network, exposing novel network anomalies and facilitating the development of novel DPI systems.}
}
@article{BINKOWSKA201435,
title = {Computational and experimental study of charge distribution in the α-disulfonyl carbanions},
journal = {Journal of Molecular Structure},
volume = {1062},
pages = {35-43},
year = {2014},
issn = {0022-2860},
doi = {https://doi.org/10.1016/j.molstruc.2014.01.007},
url = {https://www.sciencedirect.com/science/article/pii/S0022286014000258},
author = {Iwona Binkowska and Jacek Koput and Arnold Jarczewski},
keywords = {Proton transfer, Carbon acids, Charge distribution,  computation},
abstract = {The electron densities of the disulfonyl carbanions were determined using experimental 13C chemical shifts. The 13C NMR spectra and electron densities for the disulfonyl, nitro, and cyano carbon acids were calculated at the MP2/cc-pVDZ level of theory. The calculated chemical shifts for disulfonyl carbanions show satisfying correlation with our own experimental data. The calculated π electron densities at the Cα atom correspond roughly to the “experimental” π electron densities estimated from the 13C chemical shifts. The natural charges at Cα in disulfonyl stabilized carbanions are significantly more negative than with other types of carbanions, partly because of the significant negative natural charge of the α carbon in parent carbon acids. The calculated increase of the negative charge caused by ionization is larger for sulfonyl carbon acids than for cyano- and nitroalkanes. The 13C chemical shifts δ of Cα in disulfonyl stabilized carbanions decrease with more negative calculated negative natural charge at Cα, with a slope of 220ppm/electron. The influence of phenyl ring para-substitution on the charge distribution in carbanions and relationship between the 13C chemical shifts and charge density have been discussed. It appears that the π electron density in these planar or nearly planar carbanions has a decisive impact on the chemical shifts.}
}
@article{SHIRALKAR2023100115,
title = {An intelligent method for supply chain finance selection using supplier segmentation: A payment risk portfolio approach},
journal = {Cleaner Logistics and Supply Chain},
volume = {8},
pages = {100115},
year = {2023},
issn = {2772-3909},
doi = {https://doi.org/10.1016/j.clscn.2023.100115},
url = {https://www.sciencedirect.com/science/article/pii/S2772390923000240},
author = {Kedar Shiralkar and Arunkumar Bongale and Satish Kumar and Anupkumar M. Bongale},
keywords = {Supply chain finance (SCF), Supplier segmentation, Supplier categorization, Risk portfolio model, Supply chain sustainability, Supplier relationship management, Modern portfolio theory, Trade credit, Factoring, Dynamic discounting},
abstract = {The COVID-19 pandemic-driven financial crisis grew significant interest among firms to adopt supply chain finance (SCF) to optimize working capital for the financial stability of the supply chain. However, it is impractical for firms with a diverse and extensive supplier base to strategize the SCF solutions for individual suppliers by assessing their financial risk. Hence, this study conceptualizes an intelligent method to demonstrate how supplier segmentation based on suppliers’ payment risk portfolios helps supply chain practitioners to assess suppliers’ financial risk and strategize manageable supply chain finance solutions for them. This method employs a stochastic optimization model to compute suppliers’ optimum payment risk portfolios and generate a supplier segmentation matrix to offer supply chain practitioners the cognitive ability to select appropriate SCF solutions for their suppliers. The proposed method can be implemented into an AI-driven explainable recommendation system to aid supply chain practitioners in applying smart strategic thinking in supply chain finance decision-making.}
}
@article{DEALMEIDA2022478,
title = {Assisting in the choice to fill a vacancy to compose the PROANTAR team: Applying VFT and the CRITIC-GRA-3N methodology},
journal = {Procedia Computer Science},
volume = {214},
pages = {478-486},
year = {2022},
note = {9th International Conference on Information Technology and Quantitative Management},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.11.202},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922019123},
author = {Isaque David Pereira {de Almeida} and Lucas Ramon dos Santos Hermogenes and Igor Pinheiro de Araújo Costa and Miguel Ângelo Lellis Moreira and Carlos Francisco Simões Gomes and Marcos {dos Santos} and David de Oliveira Costa and Ian José Agra Gomes},
keywords = {CRITIC-GRA-3N method, Brazilian Navy, COVID-19},
abstract = {Antarctica is the southernmost continent of our planet, and it has been verified as the coldest region on earth. The Brazilian Antarctic Program (PROANTAR) has as its main objective the promotion of high-quality scientific research in the Antarctic region, seeking to understand the events that occur there. PROANTAR, coordinated by the Navy Commander, has some sectors that are based in Brazil and others that are located in the Antarctic continent. The military that volunteers to occupy any vacancy that is allocated to that continent needs, besides passing through several pre-established criteria, to pass the selection process. The purpose of this article is to help the Naval Administration in the selection of volunteer officers to occupy a vacancy in the Antarctic continent. To obtain the alternatives, the officers that best fit the established vacancy, and the criteria to be evaluated, Value-Focused Thinking (VFT) was applied. Next, with all the necessary data, the CRITIC-GRA-3N method was used as a Multicriteria Decision Support (MDS) technique, the CRITIC-GRA-3N method, the CRITIC Importance Through Intercriteria Correlation (CRITIC) method to obtain the criteria weights and the Grey Relational Analysis (GRA) method, with three normalizations, to order the alternatives. At the end of the application of the methods, the article can generate five ordinations of the volunteer officers to occupy the vacancy offered in PROANTAR.}
}
@article{HABTEMARIAM1990653,
title = {Research in computational epidemiology},
journal = {Mathematical and Computer Modelling},
volume = {14},
pages = {653-658},
year = {1990},
issn = {0895-7177},
doi = {https://doi.org/10.1016/0895-7177(90)90263-M},
url = {https://www.sciencedirect.com/science/article/pii/089571779090263M},
author = {T. Habtemariam and D. Oryang and F. Gabreab and V. Robnett and G. Trammell},
abstract = {The emerging new area referred to as computational science or science done on a computer adds a third dimension to the traditional methods of theoretical and experimental approaches. Counterparts to computational science such as computational linguistice, computational engineering and others arc beginning to take roots. Naturally, new research paths and opportunities in computational epidemiology must also be explored. One of the major challenges in epidemiologic research is the issue of how to realistically and effectively handle complex bioepidemiologic dynamics involving interactions between humans or animals, etiological agents and the multiple array of environmental and socioeconomic determinants which affect these populations. To understand the behavior of such complex biological systems, it is useful to devise computer based simulation models. Computational epidemiologic approaches now provide alternative avenues to classical laboratory and/or field experimental methods. Systems which may be impractical because they are too large, or, not feasible because the cost is too prohibitive can now be simulated realistically. In the past obtaining solutions to biomathematical equations with any degree of complexity was impossible. However, the availability of powerful computers now makes the quantitative analysis of such systems feasible and indeed practical. With this in mind our research at Tuskegee University has focused on: a) Epidemiologic modelling and expert systems, and, b) Hypertext/hypermedia based epidemiologic knowledge management. The case studies for our research involve the bioepidemiologic dynamics of two complex host-parasite systems of trypanosoma and schistosoma. The ultimate goal is to develop resources and methodologies based on computational technology to advance epidemiologic research. The paper will address the methodological issues and findings as well as questions related to configuring an appropriate research workstation for computational epidemiology.}
}
@article{EDLA2015254,
title = {Is heart rate variability better than routine vital signs for prehospital identification of major hemorrhage?},
journal = {The American Journal of Emergency Medicine},
volume = {33},
number = {2},
pages = {254-261},
year = {2015},
issn = {0735-6757},
doi = {https://doi.org/10.1016/j.ajem.2014.11.046},
url = {https://www.sciencedirect.com/science/article/pii/S073567571400881X},
author = {Shwetha Edla and Andrew T. Reisner and Jianbo Liu and Victor A. Convertino and Robert Carter and Jaques Reifman},
abstract = {Objective
During initial assessment of trauma patients, metrics of heart rate variability (HRV) have been associated with high-risk clinical conditions. Yet, despite numerous studies, the potential of HRV to improve clinical outcomes remains unclear. Our objective was to evaluate whether HRV metrics provide additional diagnostic information, beyond routine vital signs, for making a specific clinical assessment: identification of hemorrhaging patients who receive packed red blood cell (PRBC) transfusion.
Methods
Adult prehospital trauma patients were analyzed retrospectively, excluding those who lacked a complete set of reliable vital signs and a clean electrocardiogram for computation of HRV metrics. We also excluded patients who did not survive to admission. The primary outcome was hemorrhagic injury plus different PRBC transfusion volumes. We performed multivariate regression analysis using HRV metrics and routine vital signs to test the hypothesis that HRV metrics could improve the diagnosis of hemorrhagic injury plus PRBC transfusion vs routine vital signs alone.
Results
As univariate predictors, HRV metrics in a data set of 402 subjects had comparable areas under receiver operating characteristic curves compared with routine vital signs. In multivariate regression models containing routine vital signs, HRV parameters were significant (P < .05) but yielded areas under receiver operating characteristic curves with minimal, nonsignificant improvements (+0.00 to +0.05).
Conclusions
A novel diagnostic test should improve diagnostic thinking and allow for better decision making in a significant fraction of cases. Our findings do not support that HRV metrics add value over routine vital signs in terms of prehospital identification of hemorrhaging patients who receive PRBC transfusion.}
}
@article{KNIGHT20061084,
title = {‘When I first came here, I thought medicine was black and white’: Making sense of medical students’ ways of knowing},
journal = {Social Science & Medicine},
volume = {63},
number = {4},
pages = {1084-1096},
year = {2006},
issn = {0277-9536},
doi = {https://doi.org/10.1016/j.socscimed.2006.01.017},
url = {https://www.sciencedirect.com/science/article/pii/S0277953606000451},
author = {Lynn Valerie Knight and Karen Mattick},
keywords = {Medical training, Professional knowledge, Epistemology, Evidence-based medicine, United Kingdom},
abstract = {Personal beliefs about what knowledge is and how we understand, integrate and apply knowledge (known as personal epistemologies) are entrenched in the process of decision-making. Evidence-based medicine in all its forms brings with it the need for an ever more sophisticated appreciation of individual patients’ perspectives and ‘scientific’ perspectives within the clinical encounter. However, current theoretical perspectives on personal epistemology focus more on scientific ways of knowing where knowledge is abstracted and logical. We conducted semi-structured interviews to investigate medical students’ personal epistemological thinking towards the end of their second year of training at a new medical school in the South West of England. Whilst responses were varied, students appeared to express predominantly simplistic levels of epistemological thinking according to current developmental models of personal epistemology. However, the process of professional identity formation together with epistemological thinking brought together both scientific and experiential ways of knowing in a way that has largely been ignored by current theorists in the domain of personal epistemology.}
}
@article{FU2022107,
title = {Everyday Creativity is Associated with Increased Frontal Electroencephalography Alpha Activity During Creative Ideation},
journal = {Neuroscience},
volume = {503},
pages = {107-117},
year = {2022},
issn = {0306-4522},
doi = {https://doi.org/10.1016/j.neuroscience.2022.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S0306452222004596},
author = {Lei Fu and Jia Zhao and Jiangzhou Sun and Yuchi Yan and Mujie Ma and Qunlin Chen and Jiang Qiu and Wenjing Yang},
keywords = {Everyday creativity, Alpha power, Alpha coherence, Creative ideation, Frontal cortex},
abstract = {Everyday creativity is the basic ability of human survival and penetrates every aspect of life. Nevertheless, the neural mechanisms underlying everyday creativity was largely unexplored. In this study, seventy-five participants completed the creative behaviour inventory, a tool for assessing creative behaviour in daily life. The participants also completed the alternate uses task (AUT) during an electroencephalography (EEG) assessment to evaluate creative thinking. Alpha power was used to quantify neural oscillations during the creative process, while alpha coherence was used to quantify information communication between frontal regions and other sites during creative ideation. Moreover, these two task-related quantitative measures were combined to investigate the relationship between individual differences in everyday creativity and EEG alpha activity during creative idea generation. Compared with the reference period, increased alpha power was observed in the frontal cortex of the right hemisphere and increased functional coupling was observed between frontal and parietal/temporal regions during the activation period. Interestingly, individual differences in everyday creativity were associated with distinct patterns of EEG alpha activity. Specifically, individuals with higher everyday creativity had increased alpha power in the frontal cortex, and increased changes in coherence in frontal-temporal regions of the right hemisphere while performing the AUT. It might indicate that individuals with higher everyday creativity had an enhanced ability to focus on internal information processing and control bottom-up stimuli, as well as better selection of novel semantic information when performing creative ideation tasks.}
}
@article{KIREEV1994143,
title = {Approximate molecular electrostatic potential computations: applications to quantitative structure-activity relationships},
journal = {Journal of Molecular Structure: THEOCHEM},
volume = {304},
number = {2},
pages = {143-150},
year = {1994},
issn = {0166-1280},
doi = {https://doi.org/10.1016/S0166-1280(96)80006-6},
url = {https://www.sciencedirect.com/science/article/pii/S0166128096800066},
author = {Dmitry B. Kireev and Valery I. Fetisov and Nikolai S. Zefirov},
abstract = {Two new methods for calculating molecular electrostatic potentials are considered, taking into account QSAR requirements. The first of these is based on quantum chemical approximations; the other uses the topology of molecules. A program for displaying potential contour maps generated by various methods is presented. Examples of the successful use of these methods are given.}
}
@article{OZENCIRA2023101273,
title = {Mapping research on musical creativity: A bibliometric review of the literature from 1990 to 2022},
journal = {Thinking Skills and Creativity},
volume = {48},
pages = {101273},
year = {2023},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2023.101273},
url = {https://www.sciencedirect.com/science/article/pii/S1871187123000433},
author = {Gözde Ozenc-Ira},
keywords = {Musical creativity, Creativity, Bibliometric review, Science mapping, VOSviewer},
abstract = {This study aims to map the research literature on musical creativity that was published from 1990 to 2022 by using metadata extracted from 1,177 Web of Science-indexed publications in terms of trends in publications and citations data, leading journals, authors, institutions/organizations, and countries, collaborative networks between authors, institutions, and countries, and trends in keyword frequencies and co-occurrences. The main findings of this study are that (1) research on musical creativity has undergone an incipient phase and has had a growing scientific interest since the mid-2000s, (2) musical creativity is a relatively more specific research field compared to general creativity research that has been represented by more specific sub-fields, e.g., music psychology and ethnomusicology, (3) a small number of scholars – especially from the USA, England, Russia, Spain, Australia, and some countries from South Europe – have made the more impactful contribution as regards musical creativity, (4) there is a small number of research collaborations among scholars, yet the collaborative networks among countries and institutions occur intercontinentally, (5) musical creativity research is growing with cross-disciplinary links with several branches of psychology, neurosciences, cognitive sciences, education, sociology, arts and humanities, and computer sciences, and (6) eight main topical foci have been founded in the literature from 1990 to date – i.e., computational creativity, processes of improvisation, improvisation teaching and learning, interactions/collaboration during improvisation, effects of improvisation practice, innovative music technology, esthetic aspect of everyday creativity, and music therapy. Further research on musical creativity could map the literature by focusing on contextual themes.}
}
@article{GARLING1994355,
title = {Computational-process modelling of household activity scheduling},
journal = {Transportation Research Part B: Methodological},
volume = {28},
number = {5},
pages = {355-364},
year = {1994},
issn = {0191-2615},
doi = {https://doi.org/10.1016/0191-2615(94)90034-5},
url = {https://www.sciencedirect.com/science/article/pii/0191261594900345},
author = {Tommy Gärling and Mei-Po Kwan and Reginald G. Golledge},
abstract = {Models of households' travel choices are an important focus of research. For some time, it has been known that such models need to incorporate how travel depends on activity choices. It is argued that production system models constitute an alternative or necessary complementary approach if the goal is to develop models of interdependent activity and travel choices, or activity scheduling, which are based on behavioral science theories of higher cognitive processes. Several computational-process models (CPMs) which implement production systems as computer programs are reviewed. Currently, no encompassing CPM exists but some may be possible to integrate in a descriptive model of activity scheduling.}
}
@article{LIU2024111728,
title = {DuaPIN: Auxiliary task enhanced dual path interaction network for civil court view generation},
journal = {Knowledge-Based Systems},
volume = {295},
pages = {111728},
year = {2024},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.111728},
url = {https://www.sciencedirect.com/science/article/pii/S0950705124003630},
author = {Nayu Liu and Luyao Ma and Yiquan Wu and Kaiwen Wei and Cunhang Fan and Yating Zhang},
keywords = {Dual path interaction network, Auxiliary task, Civil court view generation, Natural language processing},
abstract = {Civil court view generation (CCVG) is a novel but important task for legal intelligence that aims to automatically generate a judge’s opinion based on the plaintiff’s claims and fact descriptions to interpret the judgment result. The task is more challenging than criminal court view generation as the latter generates views based only on criminal facts as input, whereas the CCVG must consider both the plaintiff’s claims and civil facts under the principle of “no claim, no trial.” However, current approaches still follow criminal domain practices to solve problems in civil cases. Moreover, the explicit modeling of the potential correspondence between claims and facts has often been neglected, as court views are required to respond to each corresponding claim based on factual evidence. To address the issues, we propose a dual path interaction network augmented by two self-supervised auxiliary tasks (named DuaPIN), which follows a bionic design by simulating the thinking logic of judges when writing opinions. Specifically, we construct a structurally symmetric Transformer-based dual path multi-encoder–decoder model such that the two inputs, claim and fact, contribute equally to the generation of civil court views. Moreover, an auxiliary task enhanced (ATE) training paradigm using multiple DuaPIN decoders is proposed to explicitly model the potential correspondence between claims and facts. Extensive experiments on public legal document dataset demonstrated that DuaPIN achieves competitive performance compared with previous methods and offers certain performance improvements to popular pre-trained language models via the ATE training method.}
}
@article{BLISS19921,
title = {Reasoning supported by computational tools},
journal = {Computers & Education},
volume = {18},
number = {1},
pages = {1-9},
year = {1992},
issn = {0360-1315},
doi = {https://doi.org/10.1016/0360-1315(92)90030-9},
url = {https://www.sciencedirect.com/science/article/pii/0360131592900309},
author = {Joan Bliss and Jon Ogborn and Richard Boohan and Jonathan Briggs and Tim Brosnan and Derek Brough and Harvey Mellar and Rob Miller and Caroline Nash and Cathy Rodgers and Babis Sakonidis},
abstract = {This paper sets out the work of the Tools for Exploratory Learning Programme within the ESRC Initiative Information Technology in Education. The research examines young secondary children's reasoning with computational tools. We distinguish between exploratory and expressive modes of learning, that is, interaction with another's model and creation of one's own model, respectively. The research focuses on reasoning, rather than learning, along three dimensions: quantitative, qualitative, and semi-quantitative. It provides a 3 × 2 classification of tasks according to modes of learning and types of reasoning. Modelling tools were developed for the study and descriptions of these are given. The research examined children's reasoning with tools in all three dimensions looking more exhaustively at the semi-quantitative. Pupils worked either in an exploratory mode or an expressive mode on one of the following topics: Traffic, Health and Diet, and Shops and Profits. They spent 3–4 h individually with a researcher over 2 weeks, carrying out four different activities: reasoning without the computer; learning to manipulate first the computer then later the tool and finally carrying out a task with the modelling tool. Pupils were between 12 and 14 yr. Research questions both about children's reasoning when working with or creating models and about the nature of the tools used are discussed. Finally an analytic scheme is set out which describes the nature of the causal and non-causal reasoning observed together with some tentative results.}
}
@article{MANDAVE2023100276,
title = {Bio-inspired computing algorithms in dementia diagnosis – a application-oriented review},
journal = {Results in Control and Optimization},
volume = {12},
pages = {100276},
year = {2023},
issn = {2666-7207},
doi = {https://doi.org/10.1016/j.rico.2023.100276},
url = {https://www.sciencedirect.com/science/article/pii/S2666720723000784},
author = {Deepa D. Mandave and Lalit V. Patil},
keywords = {Dementia, Biomotivated algorithms, Image segmentation, Meta-heuristic, Alzheimer, Optimization, Feature selection},
abstract = {Dementia is a major neurocognitive disease which affects memory, thinking skills, attitudes, and social behavior, extremely causing disturbances in daily routine activities and social activities. Alzheimer is the most general form of dementia in the elderly. Recently, biomotivated techniques have become famous in the domain of healthcare and have obtained appreciable success. This review shows that these techniques are mostly utilized to resolve various problems such as image segmentation, feature selection, classification, and optimization in the detection of various disorders like cancer, anemia, Alzheimer, kidney and skin diseases. It is observed that the dementia diagnosis was performed using classical approaches which led to reduced performance (accuracy, precision). This performance parameter can be enhanced by using biomotivated techniques. This paper presents a comprehensive analysis of the different role of biomotivated metaheuristics in the domain of dementia diagnosis with a detailed analysis of published work. The results showed that a biomotivated technique plays an important role in dementia diagnosis.}
}
@article{ERKELENS19982999,
title = {A computational model of depth perception based on headcentric disparity},
journal = {Vision Research},
volume = {38},
number = {19},
pages = {2999-3018},
year = {1998},
issn = {0042-6989},
doi = {https://doi.org/10.1016/S0042-6989(98)00084-4},
url = {https://www.sciencedirect.com/science/article/pii/S0042698998000844},
author = {Casper J. Erkelens and Raymond {van Ee}},
keywords = {Binocular vision, Stereopsis, Disparity, Binocular saccades},
abstract = {It is now well established that depth is coded by local horizontal disparity and global vertical disparity. We present a computational model which explains how depth is extracted from these two types of disparities. The model uses the two (one for each eye) headcentric directions of binocular targets, derived from retinal signals and oculomotor signals. Headcentric disparity is defined as the difference between headcentric directions of corresponding features in the left and right eye’s images. Using Helmholtz’s coordinate systems we decompose headcentric disparity into azimuthal and elevational disparity. Elevational disparities of real objects are zero if the signals which contribute to headcentric disparity do not contain any errors. Azimuthal headcentric disparity is a 1D quantity from which an exact equation relating distance and disparity can be derived. The equation is valid for all headcentric directions and for all binocular fixation positions. Such an equation does not exist if disparity is expressed in retinal coordinates. Possible types of errors in oculomotor signals (six) produce global elevational disparity fields which are characterised by different gradients in the azimuthal and elevational directions. Computations show that the elevational disparity fields uniquely characterise both the type and size of the errors in oculomotor signals. Our model uses a measure of the global elevational disparity field together with local azimuthal disparity to accurately derive headcentric distance throughout the visual field. The model explains existing data on whole-field disparity transformations as well as hitherto unexplained aspects of stereoscopic depth perception.}
}
@article{OREILLY2016547,
title = {Creative Engineers: Is Abductive Reasoning Encouraged enough in Degree Project Work?},
journal = {Procedia CIRP},
volume = {50},
pages = {547-552},
year = {2016},
note = {26th CIRP Design Conference},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2016.04.155},
url = {https://www.sciencedirect.com/science/article/pii/S221282711630395X},
author = {Ciarán J. O’Reilly},
keywords = {Creative design, abductive reasoning, education, degree project},
abstract = {Creativity is considered to be an important ability for an engineer to have, and it is therefore important that the development of this ability is structured into the education of engineering students, along with the ability to apply, analyse and evaluate based on existent knowledge. In this paper, the importance of abduction in creative engineering processes is briefly reviewed. It has been shown that abductive reasoning plays a key role in design as it is the only logical operation that introduces new ideas. Its encouragement within the KTH Royal Institute of Technology's degree projects at the Department of Aeronautical and Vehicle Engineering is analysed by examining the stated intended learning outcomes, and through interviewing students. It is found that abductive reasoning is not explicitly encouraged within the intended learning outcomes of these degree project courses, despite its importance in creative thinking. Although, it is very likely that at least some abduction takes place in the project work, its absence from the intended learning outcomes means that students may not have a felt need to demonstrate their abductive reasoning, and supervisors may encourage only non-creative deductive or inductive reasoning. A more explicit inclusion of abductive reasoning in the intended learning outcomes would help both students and supervisors to include creative thinking in the degree project courses.}
}
@article{DECARVALHO2025111158,
title = {A game-inspired algorithm for marginal and global clustering},
journal = {Pattern Recognition},
volume = {160},
pages = {111158},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111158},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324009099},
author = {Miguel {de Carvalho} and Gabriel Martos and Andrej Svetlošák},
keywords = {Cluster analysis, Mixture models, Model-based clustering, Similarity-based clustering, Unsupervised learning},
abstract = {An often overlooked pitfall of model-based clustering is that it typically results in the same number of clusters per margin, an assumption that may not be natural in practice. We develop a clustering method that takes advantage of the sturdiness of model-based clustering, while attempting to mitigate this issue. The proposed approach allows each margin to have a varying number of clusters and employs a strategy game-inspired algorithm, named ‘Reign-and-Conquer’, to cluster the data. Since the proposed clustering approach only specifies a model for the margins, but leaves the joint unspecified, it has the advantage of being partially parallelizable; hence, the proposed approach is computationally appealing as well as more tractable for moderate to high dimensions than a ‘full’ (joint) model-based clustering approach. A battery of numerical experiments on simulated data indicates an overall good performance of the proposed methods in a variety of scenarios, and real datasets are used to showcase their usefulness in practice.}
}
@article{FURLAN2022163,
title = {The earth vibrates with analogies: The Dirac sea and the geology of the vacuum},
journal = {Studies in History and Philosophy of Science},
volume = {93},
pages = {163-174},
year = {2022},
issn = {0039-3681},
doi = {https://doi.org/10.1016/j.shpsa.2022.03.008},
url = {https://www.sciencedirect.com/science/article/pii/S0039368122000590},
author = {Stefano Furlan and Rocco Gaudenzi},
keywords = {Analogies, Analogical thinking, Heuristics, History of quantum physics, Vacuum, Spontaneous symmetry breaking},
abstract = {The debate around analogy in modern physics that focuses on its role as a logical inference often correspondingly overlooks its historical dimension and the other equally important functions and aspects that are intertwined with this dimension. Inspired by a close investigation of the primary sources and archival material of a few historical actors, this paper lays out a framework on analogy-making which preserves as much as possible its historical complexity. While not losing sight of the logical role, our framework puts a special emphasis on the heuristic process, and aims at offering to the historian and philosopher of science as well as the physicist some tools to capture the subtle functions of analogical reasoning involved in such a process. After having traced it out theoretically, we make use of this framework to interpret the growth of the ideas of two remarkable physicists dealing with the multifaceted notion of vacuum in 20th century physics. We first consider the trajectory followed by John A. Wheeler, between the 1960s and 1970s, towards (in his own words) a “geology of the vacuum”; and then examine, starting from the hitherto neglected Japanese reception of the idea of Dirac sea in the early 1930s, the pathway that led Yoichiro Nambu to the discovery of spontaneous symmetry breaking.}
}
@article{BEECH2023105401,
title = {Consequences of phonological variation for algorithmic word segmentation},
journal = {Cognition},
volume = {235},
pages = {105401},
year = {2023},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2023.105401},
url = {https://www.sciencedirect.com/science/article/pii/S0010027723000355},
author = {Caroline Beech and Daniel Swingley},
keywords = {Language acquisition, Computational modeling, Word segmentation, Phonological variation},
abstract = {Over the first year, infants begin to learn the words of their language. Previous work suggests that certain statistical regularities in speech could help infants segment the speech stream into words, thereby forming a proto-lexicon that could support learning of the eventual vocabulary. However, computational models of word segmentation have typically been tested using language input that is much less variable than actual speech is. We show that using actual, transcribed pronunciations rather than dictionary pronunciations of the same speech leads to worse segmentation performance across models. We also find that phonologically variable input poses serious problems for lexicon building, because even correctly segmented word forms exhibit a complex, many-to-many relationship with speakers' intended words. Many phonologically distinct word forms were actually the same intended word, and many identical transcriptions came from different intended words. The fact that previous models appear to have substantially overestimated the utility of simple statistical heuristics suggests a need to consider the formation of the lexicon in infancy differently.}
}
@article{YANG2023414,
title = {A review of sequential three-way decision and multi-granularity learning},
journal = {International Journal of Approximate Reasoning},
volume = {152},
pages = {414-433},
year = {2023},
issn = {0888-613X},
doi = {https://doi.org/10.1016/j.ijar.2022.11.007},
url = {https://www.sciencedirect.com/science/article/pii/S0888613X2200192X},
author = {Xin Yang and Yanhua Li and Tianrui Li},
keywords = {Three-way decision, Granular computing, Sequential three-way decision, Three-way multi-granularity learning},
abstract = {The concept of three-way decision, interpreted and described as thinking, problem solving, and information processing in “threes”, has been widely studied and applied in machine learning and data engineering in recent years. In open-world environment, the connection and interaction of dynamic and uncertainty by multi-granularity learning gives more vitality to three-way decision. In this paper, we investigate and summarize the initial and development models of three-way decision. Then we revisit the historical line of sequential three-way decision from rough set to granular computing. Besides, we focus on exploring a unified framework of three-way multi-granularity learning with four crucial problems on mining uncertain region continually. Finally, we give some proposals on three-way decision associated with open-continual learning.}
}
@article{BINA2020102475,
title = {Beyond techno-utopia and its discontents: On the role of utopianism and speculative fiction in shaping alternatives to the smart city imaginary},
journal = {Futures},
volume = {115},
pages = {102475},
year = {2020},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2019.102475},
url = {https://www.sciencedirect.com/science/article/pii/S0016328719303374},
author = {Olivia Bina and Andy Inch and Lavínia Pereira},
keywords = {Smart cities, Ways of knowing, Urban imaginaries, Utopianism, Fiction},
abstract = {In recent years, the ösmart city’ has become established in policy and planning discourse, embedding visions of an urban future where ubiquitous technology offers efficient solutions to the pathologies of the contemporary city. In response, a rapidly growing social-scientific literature is critically exploring how the smart city imaginary (SCI) promotes ötechno-utopian’ fantasies, ignoring the risks of a technologically determined future. In this paper we begin by considering SCI as emblematic of the colonization of contemporary (urban) futures by vested interests, arguing for the need for diverse and plural imaginaries and thus for a re-engagement of the social sciences. We explore how critical social scientific contributions to shaping futures might be deepened through further engagement with utopian theory and speculative fiction, two traditions of future-orientated thinking that seek to combine critique with constructive thinking about alternatives. We therefore contribute to ö50 + 50 Theme 2: Framing Futures in 2068-the limits of and opportunities for futures research’ by 1) extending critique of contemporary claims about (smart urban) futures, and; 2) exploring how utopianism and fiction can expand ways of thinking, imagining and knowing futures.}
}
@article{HILLERT2021103158,
title = {How did language evolve in the lineage of higher primates?},
journal = {Lingua},
volume = {264},
pages = {103158},
year = {2021},
issn = {0024-3841},
doi = {https://doi.org/10.1016/j.lingua.2021.103158},
url = {https://www.sciencedirect.com/science/article/pii/S0024384121001303},
author = {Dieter Hillert},
keywords = {Broca’s area, Comparative studies, Homo erectus, Language capacity, Neural circuits, Prehistoric artefacts},
abstract = {Speech components emerged in the hominin lineage before the rise of modern human behavior and were already in place in monkey species. Evidence from genetics to archaeological records points to an accumulative increase of those computational properties required for modern language. At about 2.4 mya, the polytypical species Homo erectus sensu lato (s.l.) appeared with significant cortical growth indicated by neural migration factors and fossil skulls. The evidence suggests that early Homo erectus s.l. was equipped with a computational capacity for premodern language. The same species developed Acheulean toolmaking and showed signs of a symbolic and aesthetic mind at about half a mya. We conclude that the modern language capacity evolved at around 1 mya in the merging species late Homo erectus s.l. and pre-archaic Homo sapiens.}
}
@article{SCHACTER1999403,
title = {Computer-based performance assessments: a solution to the narrow measurement and reporting of problem-solving☆☆The findings and opinions expressed in this report do not reflect the position or policies of ISX, Advanced Research Projects Agency, the Department of the Navy, or the Department of Defense; nor do they reflect the positions or policies of the National Institute on Student Achievement, Curriculum, and Assessment, the Office of Educational Research and Improvement, or the US Department of Education.},
journal = {Computers in Human Behavior},
volume = {15},
number = {3},
pages = {403-418},
year = {1999},
issn = {0747-5632},
doi = {https://doi.org/10.1016/S0747-5632(99)00029-1},
url = {https://www.sciencedirect.com/science/article/pii/S0747563299000291},
author = {J. Schacter and H.E. Herl and G.K.W.K. Chung and R.A. Dennis and H.F. O'Neil},
keywords = {Assessment, Problem solving, Computers, Internet, Technology, Education},
abstract = {Although performance assessments test for higher order thinking and problem solving, they rarely report students' thinking process data back to teachers, students, or the public. Web-based database-backed performance assessments provide a viable means for concurrently reporting both performance and thinking process data. In the research conducted here, we report our findings from a study that assessed student problem solving using networked computers. Both performance and process data could be reported back to teachers and students such that they could diagnose and understand how they performed and what problem-solving processes contributed to or detracted from their performance.}
}
@article{DALLAT2019266,
title = {Risky systems versus risky people: To what extent do risk assessment methods consider the systems approach to accident causation? A review of the literature},
journal = {Safety Science},
volume = {119},
pages = {266-279},
year = {2019},
issn = {0925-7535},
doi = {https://doi.org/10.1016/j.ssci.2017.03.012},
url = {https://www.sciencedirect.com/science/article/pii/S0925753517305295},
author = {Clare Dallat and Paul M. Salmon and Natassia Goode},
keywords = {Risk, Risk assessment, Risk assessment methods, Systems thinking},
abstract = {Accidents are now widely acknowledged to be a systems phenomenon. As part of a proactive approach to safety management, organisations use risk assessment methods to identify the hazards and associated risks that may lead to accidents. Although there is an extensive body of literature on the need for a systems thinking approach in accident analysis, little has been said regarding the theoretical underpinnings of risk assessment methods. The aim of this paper was to systematically review the risk assessment methods presented in the literature and evaluate the extent to which they are underpinned by a systems thinking approach. A total of 342 methods spanning a range of safety-critical domains were evaluated using Rasmussen’s tenets of accident causation. A key finding is that the majority of existing risk assessment methods are not consistent with Rasmussen’s model of accident causation (arguably the most popular model in safety science circles). Instead, the majority of risk assessment methods focus on risks at the so called sharp-end and largely view accidents as emerging from a linear, or chain-of-events process. This overlooks emergent risks at other levels of the system, including supervisory, managerial, regulatory and government levels. The findings therefore suggest that the majority of existing risk assessment methods may be inadequate for identifying hazards and analysing risks within complex sociotechnical systems. The implications for risk assessment practice are discussed.}
}
@article{TOOBY2025106687,
title = {The evolution of war and its cognitive foundations},
journal = {Evolution and Human Behavior},
volume = {46},
number = {3},
pages = {106687},
year = {2025},
issn = {1090-5138},
doi = {https://doi.org/10.1016/j.evolhumbehav.2025.106687},
url = {https://www.sciencedirect.com/science/article/pii/S1090513825000364},
author = {John Tooby and Leda Cosmides},
keywords = {War, Coalitional aggression, Cooperation, Cognition, Conflict},
abstract = {Coalitional aggression evolved because it allowed the participants to promote their fitness by gaining access to disputed, reproduction-enhancing resources that would otherwise be denied to them. Few species engage in coalitional aggression, even though the social conditions that would favor its evolution seem to be widespread. Why? Forming coalitions to exploit these opportunities requires individuals to solve highly complex and specialized information processing problems involving cooperation, coordination, and social exchange. The difficulty of evolving cognitive mechanisms capable of solving these problems—especially when the individuals involved are not kin—may explain why multi-individual coalitions are phylogenetically rare. We propose that humans and a few other cognitively pre-adapted species have evolved specialized cognitive programs that govern coalitional behavior, which constitute a distinctive coalitional psychology. To derive a preliminary map of this psychology, we started with a task analysis of the adaptive information-processing problems that arise during coalitional aggression. This exercise can shine light on our evolved psychology because algorithms that motivate and organize coalitional aggression would need design features that solve these problems well to be favored by selection. These problems include decisions about when to form a coalition or join one, when to initiate an attack, and how to allocate the costs and benefits that result from coalitional action. The risk contract of war identifies circumstances under which natural selection would favor decisions to initiate an attack. When the conditions of this model are met, mortality rates will not negatively impact the fitness of males in the winning coalition. This outcome has implications for the design of computational systems that motivate coalitional attacks; it may explain why warfare is so favored an activity among men, despite its risks to the participating individuals' welfare.}
}
@article{SIMPSON2017166,
title = {Preparing industry for additive manufacturing and its applications: Summary & recommendations from a National Science Foundation workshop},
journal = {Additive Manufacturing},
volume = {13},
pages = {166-178},
year = {2017},
issn = {2214-8604},
doi = {https://doi.org/10.1016/j.addma.2016.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S2214860416302019},
author = {Timothy W. Simpson and Christopher B. Williams and Michael Hripko},
keywords = {Additive manufacturing, Design for additive manufacturing, STEM education, 3D printing, Workforce development},
abstract = {Accompanying the increasing advances and interest in Additive Manufacturing (AM) technologies is an increasing demand for an industrial workforce that is knowledgeable about the technologies and how to apply them to solve real-world problems. As a step towards addressing this knowledge gap, a workshop was held at the National Science Foundation (NSF) to discuss the educational needs to prepare industry for AM and its use in different fields. The workshop participants – 66 representatives from academia, industry, and government – identified several key educational themes: (1) AM processes and process/material relationships, (2) engineering fundamentals with an emphasis on materials science and manufacturing, (3) professional skills for problem solving and critical thinking, (4) design practices and tools that leverage the design freedom enabled by AM, and (5) cross-functional teaming and ideation techniques to nurture creativity. This paper summarizes the industry speakers and presentations from the workshop, along with several new educational partnerships identified by small working groups. Based on the presentations and partnerships, the following recommendations are offered to advance the AM workforce. First, ensure that all AM curricula provide students with an understanding of (i) AM and traditional manufacturing processes to enable them to effectively select the appropriate process for product realization; (ii) the relationships between AM processes and material properties; and (iii) “Design for AM”, including computational tools for AM design as well as frameworks for process selection, costing, and solution generation that take advantage of AM capabilities. Second, establish a national network for AM education that, by leveraging existing “distributed” educational models and NSF’s Advanced Technology Education (ATE) Programs, provides open source resources as well as packaged activities, courses, and curricula for all educational levels (K-Gray). Third, support K-12 educational programs in STEAM (STEM plus the arts) and across all formal and informal learning environments in order to learn the unique capabilities of AM while engaging students in hands-on, tactile, and visual learning activities to prepare them for jobs in industry while learning how to think differently when designing for AM. Fourth, provide support for collaborative and community-oriented maker spaces that promote awareness of AM among the public and provide AM training programs for incumbent workers in industry and students seeking alternative pathways to gain AM knowledge and experience. Recommendations for scaling and coordination across local, regional, and national levels are also discussed to create synergies among the proposed activities and existing efforts.}
}
@article{GREENSPAN1990490,
title = {A counterexample of the use of energy as a measure of computational accuracy},
journal = {Journal of Computational Physics},
volume = {91},
number = {2},
pages = {490-494},
year = {1990},
issn = {0021-9991},
doi = {https://doi.org/10.1016/0021-9991(90)90051-2},
url = {https://www.sciencedirect.com/science/article/pii/0021999190900512},
author = {Donald Greenspan}
}
@article{BOVE20031040,
title = {Computational fluid dynamics in the evaluation of hemodynamic performance of cavopulmonary connections after the norwood procedure for hypoplastic left heart syndrome},
journal = {The Journal of Thoracic and Cardiovascular Surgery},
volume = {126},
number = {4},
pages = {1040-1047},
year = {2003},
issn = {0022-5223},
doi = {https://doi.org/10.1016/S0022-5223(03)00698-6},
url = {https://www.sciencedirect.com/science/article/pii/S0022522303006986},
author = {Edward L. Bove and Marc R. {de Leval} and Francesco Migliavacca and Gualtiero Guadagni and Gabriele Dubini},
keywords = {17, 21},
abstract = {Objective
Computational fluid dynamics have been used to study the hemodynamic performance of surgical operations, resulting in improved design. Efficient designs with minimal energy losses are especially important for cavopulmonary connections. The purpose of this study was to compare hydraulic performance between the hemi-Fontan and bidirectional Glenn procedures, as well as the various types of completion Fontan operations.
Methods
Three-dimensional models were constructed of typical hemi-Fontan and bidirectional Glenn operations according to anatomic data derived from magnetic resonance scans, angiocardiograms, and echocardiograms. Boundary conditions were imposed, and fluid dynamics were calculated from a mathematic code. Power losses, flow distribution to each lung, and pressures were measured at three predetermined levels of pulmonary arteriolar resistance. Models of the lateral tunnel, total cavopulmonary connection, and extracardiac conduit completion Fontan operations were constructed, and power losses, total flow distribution, vena caval and pulmonary arterial pressures, and flow distribution of inferior vena caval return were calculated.
Results
The hemi-Fontan and bidirectional Glenn procedures performed nearly identically, with similar power losses and nearly equal flow distributions to each lung at all levels of pulmonary arteriolar resistance. However, the lateral tunnel Fontan procedure as performed after the hemi-Fontan operation had lower power losses (6.9 mW, pulmonary arteriolar resistance 3 units) than the total cavopulmonary connection (40.5 mW) or the extracardiac conduit (42.9 mW), although the inclusion of an enlargement patch toward the right in the total cavopulmonary connection was effective in reducing the difference (10.0 mW). Inferior vena caval flow to the right lung was 52% for the lateral tunnel, compared with 19%, 30%, 19%, and 15% for the total cavopulmonary connection, total cavopulmonary connection with right-sided enlargement patch, extracardiac conduit, and extracardiac conduit with a bevel to the left lung, respectively.
Conclusions
According to these methods, the hemi-Fontan and bidirectional Glenn procedures performed equally well, but important differences in energy losses and flow distribution were found after the completion Fontan procedures. The superior hydraulic performance of the lateral tunnel Fontan operation after the hemi-Fontan procedure relative to any other method may be due to closer to optimal caval offset achieved in the surgical reconstruction.}
}
@article{HESAMI20251,
title = {Trends in production, consumption, trade, and research of dry beans across the globe and Canada},
journal = {Canadian Journal of Plant Science},
volume = {105},
pages = {1-14},
year = {2025},
issn = {0008-4220},
doi = {https://doi.org/10.1139/cjps-2024-0185},
url = {https://www.sciencedirect.com/science/article/pii/S0008422025000223},
author = {Mohsen Hesami and Mohsen Yoosefzadeh-Najafabadi},
keywords = {agri-food system, computational biology, food security, , plant breeding, yield},
abstract = {Dry beans (Phaseolus vulgaris L.) are known as a significant component of global agri-food systems, in regions such as Southern Asia, Eastern Africa, and South America, where they also serve as a valuable source of feed. Over the past few decades, global production has grown significantly, driven by rising demand, technological advancements, improved yields, and expanded cultivation areas. Canada, in particular, has become a significant player in the dry bean industry, leveraging its rich agricultural landscape and advanced agricultural technologies. Canadian research initiatives, financially supported by both governmental and private funding, have concentrated on developing new bean varieties with higher yields, resistance to pests and diseases, better adaptation to local growing conditions, and improved nutritional profiles. This study reviews trends in dry bean production, consumption, and international trade over the past decades, emphasizing the implications for research on both global and Canadian scales. Collaborative efforts between Canadian institutions and international research organizations have facilitated the exchange of genetic resources and agronomic techniques, thereby enhancing productivity and sustainability. By investing in these innovative endeavors, Canada not only bolsters its strengthened agricultural sector but also contributes significantly to global food security and the achievement of sustainable development goals.}
}
@article{DAS2024100104,
title = {AI and data-driven urbanism: The Singapore experience},
journal = {Digital Geography and Society},
volume = {7},
pages = {100104},
year = {2024},
issn = {2666-3783},
doi = {https://doi.org/10.1016/j.diggeo.2024.100104},
url = {https://www.sciencedirect.com/science/article/pii/S2666378324000266},
author = {Diganta Das and Berwyn Kwek},
keywords = {Singapore, Smart cities, Smart nation, Artificial intelligence (AI), Digital urbanism},
abstract = {This paper presents a deep and critical analysis of Singapore's new wave of state-built digital tools and services and how it connects to its larger smart urbanism project, also known as Smart Nation. The COVID-19 pandemic, and particularly Singapore's response, served as a real-world testing ground for smart urbanist strategies. In particular, we analysed the logic that emanates from these novel digital interventions, how they operate on the complex urban built environment and the population, and their effects on urban and citizenry morphologies. Next, we examined a series of state-led technological implementations that have emerged since the Covid-19 pandemic, providing digital solutions that assist citizens with the changing rhythms of everyday living, data-capturing sensors and gantries to aid authorities in contract tracing efforts and enforce vaccination differentiation measures, geospatial digital mapping of demographic data, in withal robotics for automated policing and cleaning activities; and the use of AI and automated data-driven tools in public health to improve service delivery and care to patients. While we are unable to exhaust every piece of technology for the purpose of this paper, these developments, along with their design thinking and operations, we argue, are helpful in revealing the contemporary conjectures of Singaporean digital urban idealism and the governing strategies of the state. By examining Singapore's response, this study aims to contribute to the ongoing discourse on smart urbanism, offering insights into how cities can leverage technology effectively while balancing technological innovation with privacy and public trust.}
}
@article{KAVLOCK2005265,
title = {Computational Toxicology: Framework, Partnerships, and Program Development: September 29–30, 2003, Research Triangle Park, North Carolina},
journal = {Reproductive Toxicology},
volume = {19},
number = {3},
pages = {265-280},
year = {2005},
note = {Systems Biology/Computational Toxicology},
issn = {0890-6238},
doi = {https://doi.org/10.1016/j.reprotox.2004.04.013},
url = {https://www.sciencedirect.com/science/article/pii/S0890623804000747},
author = {Robert Kavlock and Gerald T. Ankley and Tim Collette and Elaine Francis and Karen Hammerstrom and Jack Fowle and Hugh Tilson and Greg Toth and Patricia Schmieder and Gilman D. Veith and Eric Weber and Douglas C. Wolf and Doug Young}
}
@article{SENVAR20161140,
title = {Hospital Site Selection via Hesitant Fuzzy TOPSIS},
journal = {IFAC-PapersOnLine},
volume = {49},
number = {12},
pages = {1140-1145},
year = {2016},
note = {8th IFAC Conference on Manufacturing Modelling, Management and Control MIM 2016},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2016.07.656},
url = {https://www.sciencedirect.com/science/article/pii/S2405896316309296},
author = {Ozlem Senvar and Irem Otay and Eda Bolturk},
keywords = {Facility Layout, Location Selection, Multi criteria decision making (MCDM), TOPSIS, Hesitant fuzzy set (HFS)},
abstract = {This study handles the problem of establishing a well-organized and distributed network of a hospital that delivers its services to the target population. We propose a new multi criteria decision making (MCDM) process that integrates hesitant fuzzy sets (HFSs) to Technique for Order Preference by Similarity to Ideal Solution (TOPSIS). The MCDM process defined under uncertainties are perfectly defined by HFSs reflecting comprehensively hesitant thinking of decision makers. Our proposed methodology is implemented to select the optimum site for a new hospital in Istanbul.}
}
@article{BAUSO201776,
title = {Consensus via multi-population robust mean-field games},
journal = {Systems & Control Letters},
volume = {107},
pages = {76-83},
year = {2017},
issn = {0167-6911},
doi = {https://doi.org/10.1016/j.sysconle.2017.07.010},
url = {https://www.sciencedirect.com/science/article/pii/S0167691117301287},
author = {D. Bauso},
keywords = {Synchronization, Consensus, Mean-field games},
abstract = {In less prescriptive environments where individuals are told ‘what to do’ but not ‘how to do’, synchronization can be a byproduct of strategic thinking, prediction, and local interactions. We prove this in the context of multi-population robust mean-field games. The model sheds light on a multi-scale phenomenon involving fast synchronization within the same population and slow inter-cluster oscillation between different populations.}
}
@article{BERNUS201583,
title = {Enterprise architecture: Twenty years of the GERAM framework},
journal = {Annual Reviews in Control},
volume = {39},
pages = {83-93},
year = {2015},
issn = {1367-5788},
doi = {https://doi.org/10.1016/j.arcontrol.2015.03.008},
url = {https://www.sciencedirect.com/science/article/pii/S1367578815000097},
author = {Peter Bernus and Ovidiu Noran and Arturo Molina},
abstract = {Apart from the 20-year anniversary in 2014 of the first publication of the GERAM (‘Generalised Enterprise Reference Architecture and Methodology’) Enterprise Architecture Framework, the timeliness of this paper lies in the new interest in the use of systems theory in enterprise architecture (EA), and consequently, ‘light-weight’ architecture frameworks (AFs). Thus, this paper is about the use of systems thinking and systems theory in EA and about how it is possible to reconcile and understand, based on a single overarching framework, the interplay of two major enterprise change endeavours: on one hand enterprise engineering (i.e. deliberate change) and on the other hand evolutionary, organic change. The paper also demonstrates how such change processes can be illustrated by employing systems thinking to construct dynamic business models; the evolution of these concepts is exemplified using past applications in networked enterprise building, and more recent proposals in environmental-, disaster- and healthcare management. Finally, the paper attempts to plot the way GERAM, as a framework to think about the creation and evolution of complex socio-technical systems, will continue to contribute to the society in the context of future challenges and emerging opportunities.}
}
@article{ZHANG2022104545,
title = {Watching a hands-on activity improves students’ understanding of randomness},
journal = {Computers & Education},
volume = {186},
pages = {104545},
year = {2022},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2022.104545},
url = {https://www.sciencedirect.com/science/article/pii/S0360131522001166},
author = {Icy (Yunyi) Zhang and Mary C. Tucker and James W. Stigler},
keywords = {Hands-on demonstration, Computer simulation, Statistics education, Multimedia learning, Online instruction, Instructional sequence, Embodied cognition},
abstract = {Introductory statistics students struggle to understand randomness as a data generating process, and especially its application to the practice of data analysis. Although modern computational techniques for data analysis such as simulation, randomization, and bootstrapping have the potential to make the idea of randomness more concrete, representing such random processes with R code is not as easy for students to understand as is something like a coin-flip, which is both concrete and embodied. In this study, in the context of multimedia learning, we designed and tested the efficacy of an instructional sequence that preceded computational simulations with embodied demonstrations. We investigated the role that embodied hands-on movement might play in facilitating students’ understanding of the shuffle function in R. Our findings showed that students who watched a video of hands shuffling data written on pieces of paper learned more from a subsequent live-coding demonstration of randomization using R than did students only introduced to the concept using R. Although others have found an advantage of students themselves engaging in hands-on activities, this study showed that merely watching someone else engage can benefit learning. Implications for online and remote instruction are discussed.}
}
@incollection{BARDINI202537,
title = {Chapter 4 - From sketch to landscape: Transforming neuronal concepts across technological change},
editor = {Babak Sokouti},
booktitle = {Systems Biology and In-Depth Applications for Unlocking Diseases},
publisher = {Academic Press},
pages = {37-52},
year = {2025},
isbn = {978-0-443-22326-6},
doi = {https://doi.org/10.1016/B978-0-443-22326-6.00004-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780443223266000043},
author = {Roberta Bardini},
keywords = {Artificial intelligence, Brain complexity, Breakthroughs in neuroscience, Cell theory, Computational biology, Electrophysiology, Emerging technologies in neuroscience, History of neuroscience, History of technology, Interdisciplinary approaches in neuroscience, Machine learning, Multiomics, Nervous system, Neural networks (biological sciences), Neurology, Neuron, Neuron doctrine, Neuroscience, Neuroscience research advances, Patch-clamp, Patch-seq, Systems biology, Techniques in neuroscience},
abstract = {This chapter delves into the evolution of neuronal concepts, tracing their journey through technological change, from the early days of hand-drawn sketches to the advanced techniques of computational modeling in systems neuroscience. Covering a timeline from the 17th century's scientific revolution to the 21st century's technological boom, this analysis highlights key milestones in the history of neurology and the intricate understanding of neuronal functions. It presents a narrative that emphasizes the significant role technological innovations have played in shaping our perception of neuronal cells. With each scientific breakthrough, we gain deeper insights into the neuron's critical role within the brain's complex network. This chapter not only provides a historical perspective but also sets the stage for future discoveries that will continue to revolutionize our understanding of the neuroscience landscape.}
}
@article{ITO2024326,
title = {A Discrete Predator-Prey Brain Storm Optimization Technique for Optimal Allocation of Micro-PMUs in Distribution System State Estimation},
journal = {IFAC-PapersOnLine},
volume = {58},
number = {13},
pages = {326-331},
year = {2024},
note = {12th IFAC Symposium on Control of Power and Energy Systems - CPES 2024},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2024.07.503},
url = {https://www.sciencedirect.com/science/article/pii/S2405896324006025},
author = {Akio Ito and Hiroyuki Mori and Hsiao-Dong Chiang},
keywords = {distribution network state estimation, distribution automation, evolutionary computation, micro-PMU, smart grids, intelligent control of power systems},
abstract = {This paper proposes a practical method for determining the optimal allocation of micro-Phasor Measurement Units (μPMUs) for distribution system state estimation (DSSE). The proposed method formulates the state estimation based on the nest structure of DistFlow, which is used for power flow calculation in distribution systems due to the redundancy of less than 1. The use of μPMUs can significantly improve the accuracy of estimates. However, Distribution System Operators (DSOs) need to consider optimization constraints before deciding on the optimal location of μPMUs. To tackle this issue, this paper proposes the use of DPPBSO (Discrete Predator-Prey Brain Storm Optimization) of Evolutionary Computation (EC) to optimize the location of μPMUs. PPBSO is an extension method that applies the Predator-Prey strategy to Brain Storm Optimization (BSO), and DPPSO is the discrete version of PPBSO used in solving combinatorial optimization problems. The Predator-Prey strategy is critical in improving the solution candidates by intensifying and diversifying solution searches in EC. Simulation results demonstrate the effectiveness of the proposed method in the IEEE 69-node distribution system.}
}
@article{SLANKSTERSCHMIERER202538,
title = {Modernizing toxicology: The importance of accessible NAM training},
journal = {Toxicology Letters},
volume = {406},
pages = {38-39},
year = {2025},
issn = {0378-4274},
doi = {https://doi.org/10.1016/j.toxlet.2025.02.011},
url = {https://www.sciencedirect.com/science/article/pii/S0378427425000402},
author = {Eryn Slankster-Schmierer},
keywords = {Training, Professional development, Education and outreach, New approach methodologies, In vitro toxicology, Computational toxicology, Regulatory toxicology, Alternative approaches, Nonanimal, NAM},
abstract = {Current toxicology curricula and certifications are heavily reliant on animal-based research and lack mandatory education and training in New Approach Methodologies (NAMs). Traditional animal-based toxicological methods come with many concerns, including translatability and reproducibility, which NAMs are aptly positioned to address. The NAM Use for Regulatory Application (NURA) program aims to bridge this educational gap by providing training to toxicologists, method developers, regulators, and legislators on the use of NAMs, helping to build confidence in NAM use and facilitate the shift to more human-based methods.}
}
@article{BELLA2023100509,
title = {Circular dichroism simulations of chiral buckybowls by means curvature analyses},
journal = {FlatChem},
volume = {40},
pages = {100509},
year = {2023},
issn = {2452-2627},
doi = {https://doi.org/10.1016/j.flatc.2023.100509},
url = {https://www.sciencedirect.com/science/article/pii/S2452262723000417},
author = {Giovanni Bella and Giuseppe Bruno and Antonio Santoro},
keywords = {Buckybowl, Chirality, Curvature, TD-DFT, Circular dichroism},
abstract = {A detailed understanding and interpretation of chiral properties of molecular systems, especially in condensed phase, often requires computational models that allow their structural and electronic features to be connected to the observed experimental spectra. The present paper is focused on modelling the circular dichroism spectra of chiral buckybowls, combining topological aspects and the density functional theory. For the first time Ball Pivoting Algorithm was proposed to hook up the chemical topology to the DFT through the surface reconstruction. Particularly, the gaussian curvature of a constructed probe set of corannulene and sumanene derivatives was used as discriminant parameter to benchmark a list of 10 functionals (B3LYP, B97D, M06-2X, HSEH1PBE, wB97XD, CAM-B3LYP, LC-wPBE, TPSSTPSS, mPW1PW91 and APFD). The latter provide to be noticeably accurate to reproduce the curvature effect of the considered molecules. A TD-DFT/BOMD mixed approach provided a comprehensive overview of the spectral chiral pattern prediction trends when multiple DFT functionals are scanned. The preliminary topological analysis efforts were then recompensed with the very precise computed CD spectra, again APFD confirmed as the leader functional, this time for TD-DFT vertical transition calculations. Therefore, we strongly recommend the use of the of dispersion embedded APFD functional coupled with the 6–311++G(2d,2p) basis set for the computation of the functionalized chiral buckybowls ECD spectra. © 2017 Elsevier Inc. All rights reserved.}
}
@article{VIGNAPIANO201999,
title = {Disorganization and cognitive impairment in schizophrenia: New insights from electrophysiological findings},
journal = {International Journal of Psychophysiology},
volume = {145},
pages = {99-108},
year = {2019},
note = {The Neurophysiology of Schizophrenia: A Critical Update},
issn = {0167-8760},
doi = {https://doi.org/10.1016/j.ijpsycho.2019.03.008},
url = {https://www.sciencedirect.com/science/article/pii/S016787601830984X},
author = {Annarita Vignapiano and Thomas Koenig and Armida Mucci and Giulia M. Giordano and Antonella Amodio and Mario Altamura and Antonello Bellomo and Roberto Brugnoli and Giulio Corrivetti and Giorgio {Di Lorenzo} and Paolo Girardi and Palmiero Monteleone and Cinzia Niolu and Silvana Galderisi and Mario Maj},
keywords = {Disorganization dimension, Difficulty in abstract thinking, Neurocognitive domains, Alpha rhythm, Spectral power, Topographic analysis},
abstract = {In subjects with schizophrenia (SCZ), the disorganization dimension is a strong predictor of real-life functioning. “Conceptual disorganization” (P2), “Difficulty in abstract thinking” (N5) and “Poor attention” (G11) are core features of the disorganization factor, evaluated using the Positive and Negative Syndrome Scale. The heterogeneity of this dimension and its overlap with neurocognitive deficits are still debated. Within the multicenter study of the Italian Network for Research on Psychoses, we investigated electrophysiological and neurocognitive correlates of disorganization and its component items to assess the heterogeneity of this dimension and its possible overlap with neurocognitive deficits. Resting state EEG was recorded in 145 stabilized SCZ and 69 matched healthy controls (HC). Spectral amplitude was averaged in ten frequency bands. Neurocognitive domains were assessed by MATRICS Consensus Cognitive Battery (MCCB). RAndomization Graphical User software explored band spectral amplitude differences between groups and correlations with disorganization and MCCB scores in SCZ. Correlations between disorganization and MCCB scores were also investigated. Compared to HC, SCZ showed increased delta, theta, and beta 1 and decreased alpha 2 activity. A negative correlation between alpha 1 and disorganization was observed in SCZ. At the item level, only “N5” showed the same correlation. MCCB neurocognitive composite score was associated with disorganization, “P2” and “N5”. Our findings suggest only a partial overlap between disorganization and neurocognitive impairment. The association of alpha 1 with the “N5” item suggests that some aspects of disorganization could be underpinned by the impairment of basic neurobiological functions that are only partially evaluated using MCCB.}
}
@article{SIMON1993431,
title = {Experience in using SIMD and MIMD parallelism for computational fluid dynamics},
journal = {Applied Numerical Mathematics},
volume = {12},
number = {5},
pages = {431-442},
year = {1993},
issn = {0168-9274},
doi = {https://doi.org/10.1016/0168-9274(93)90103-X},
url = {https://www.sciencedirect.com/science/article/pii/016892749390103X},
author = {Horst D. Simon and Leonardo Dagum},
keywords = {Parallel architectures, MIMD, SIMD, computational fluid dynamics.},
abstract = {One of the key objectives of the Applied Research Branch in the Numerical Aerodynamic Simulation (NAS) Systems Division at NASA Ames Research Center is the accelerated introduction of highly parallel machines into a fully operational environment. In this report we summarize some of the experiences with the parallel testbed machines at the NAS Applied Research Branch. We discuss the performance results obtained from the implementation of two computational fluid dynamics (CFD) applications, an unstructured grid solver and a particle simulation, on the Connection Machine CM-2 and the Intel iPSC/860.}
}
@article{GOSAK2024103888,
title = {The ChatGPT effect and transforming nursing education with generative AI: Discussion paper},
journal = {Nurse Education in Practice},
volume = {75},
pages = {103888},
year = {2024},
issn = {1471-5953},
doi = {https://doi.org/10.1016/j.nepr.2024.103888},
url = {https://www.sciencedirect.com/science/article/pii/S1471595324000179},
author = {Lucija Gosak and Lisiane Pruinelli and Maxim Topaz and Gregor Štiglic},
keywords = {Artificial Intelligence, ChatGPT, Documentation, Education, Nursing, Nursing Diagnosis},
abstract = {Aim
The aim of this study is to present the possibilities of nurse education in the use of the Chat Generative Pre-training Transformer (ChatGPT) tool to support the documentation process.
Background
The success of the nursing process is based on the accuracy of nursing diagnoses, which also determine nursing interventions and nursing outcomes. Educating nurses in the use of artificial intelligence in the nursing process can significantly reduce the time nurses spend on documentation.
Design
Discussion paper.
Methods
We used a case study from Train4Health in the field of preventive care to demonstrate the potential of using Generative Pre-training Transformer (ChatGPT) to educate nurses in documenting the nursing process using generative artificial intelligence. Based on the case study, we entered a description of the patient's condition into Generative Pre-training Transformer (ChatGPT) and asked questions about nursing diagnoses, nursing interventions and nursing outcomes. We further synthesized these results.
Results
In the process of educating nurses about the nursing process and nursing diagnosis, Generative Pre-training Transformer (ChatGPT) can present potential patient problems to nurses and guide them through the process from taking a medical history, setting nursing diagnoses and planning goals and interventions. Generative Pre-training Transformer (ChatGPT) returned appropriate nursing diagnoses, but these were not in line with the North American Nursing Diagnosis Association – International (NANDA-I) classification as requested. Of all the nursing diagnoses provided, only one was consistent with the most recent version of the North American Nursing Diagnosis Association – International (NANDA-I). Generative Pre-training Transformer (ChatGPT) is still not specific enough for nursing diagnoses, resulting in incorrect answers in several cases.
Conclusions
Using Generative Pre-training Transformer (ChatGPT) to educate nurses and support the documentation process is time-efficient, but it still requires a certain level of human critical-thinking and fact-checking.}
}
@article{CHING201765,
title = {Children's understanding of the commutativity and complement principles: A latent profile analysis},
journal = {Learning and Instruction},
volume = {47},
pages = {65-79},
year = {2017},
issn = {0959-4752},
doi = {https://doi.org/10.1016/j.learninstruc.2016.10.008},
url = {https://www.sciencedirect.com/science/article/pii/S0959475216301906},
author = {Boby Ho-Hong Ching and Terezinha Nunes},
keywords = {Additive reasoning, Commutativity principle, Complement principle, Latent profile analysis},
abstract = {This study examined patterns of individual differences in the acquisition of the knowledge of the commutativity and complement principles in 115 five-to six-year-old children and explored the role of concrete materials in helping children understand the prinicples. On the basis of latent profile analysis, four groups of children were identified: The first group succeeded in commutativity tasks with concrete materials but in no other tasks; the second succeeded in commutativity tasks in both concrete and abstract conditions, but not in complement tasks; the third group succeeded in all commutativity tasks and in complement tasks with concrete materials, and the final group succeeded in all the tasks. The four groups of children suggest a developmental trend – (1) Knowledge of the commutativity and of the complement principles seems to develop from thinking in the context of specific quantities to thinking about more abstract symbols; (2) There may be an order of understanding of the principles – from the commutativity to the complement principle; (3) Children may acquire the knowledge of the commutativity principle in the more abstract tasks before they start to acquire the knowledge of the complement principle. This study contributes to the literature by showing that assessing additive reasoning in different ways and identifying profiles with classification analyses may be useful for educators to understand more about the developmental stage where each child is placed. It appears that a more fine-grained assessment of additive reasoning can be achieved by incorporating both concrete materials and relatively abstract symbols in the assessment.}
}
@article{LANGE2024101191,
title = {What are explanatory proofs in mathematics and how can they contribute to teaching and learning?},
journal = {The Journal of Mathematical Behavior},
volume = {76},
pages = {101191},
year = {2024},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2024.101191},
url = {https://www.sciencedirect.com/science/article/pii/S0732312324000683},
author = {Marc Lange},
keywords = {Explanation, Proof, Generalization, Pedagogy, Unification, Coincidence},
abstract = {This paper will examine several simple examples (drawn from the mathematics literature) where there are multiple proofs of the same theorem, but only some of these proofs are widely regarded by mathematicians as explanatory. These examples will motivate an account of explanatory proofs in mathematics. Along the way, the paper will discuss why deus ex machina proofs are not explanatory, what a mathematical coincidence is, and how a theorem's proper setting reflects the naturalness of various mathematical kinds. The paper will also investigate how context influences which features of a theorem are salient and consequently which proofs are explanatory. The paper will discuss several ways in which explanatory proofs can contribute to teaching and learning, including how shifts in context (and hence in a proof’s explanatory power) can be exploited in a classroom setting, leading students to dig more deeply into why some theorem holds. More generally, the paper will examine how “Why?” questions operate in mathematical thinking, teaching, and learning.}
}
@incollection{PESCE2024123,
title = {Chapter Seven - Creativity and consciousness in motion: The roundtrip of “mindful” and “mindless” processes in embodied creativity},
editor = {Tal Dotan Ben-Soussan and Joseph Glicksohn and Narayanan Srinivasan},
series = {Progress in Brain Research},
publisher = {Elsevier},
volume = {287},
pages = {123-151},
year = {2024},
booktitle = {The Neurophysiology of Silence (C): Creativity, Aesthetic Experience and Time},
issn = {0079-6123},
doi = {https://doi.org/10.1016/bs.pbr.2024.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S0079612324000785},
author = {Caterina Pesce and Nicoletta Tocci},
keywords = {Creative thinking, Motor creativity, Embodiment, Hypofrontality, Flow, Incubation, Mind wandering, Nature, Green exercise, Mindful movements},
abstract = {In this opinion paper, we make a journey across different accounts of creativity that emphasize either the mindful, conscious and cognitive expression of creativity, or its mindless, unconscious and sensorimotor expression. We try to go beyond dichotomy, putting creativity in motion and outlining its embodied and enactive features. Based on the assumption that no creative act is purely conscious or purely unconscious, our discussion on creativity relies on the distinction of three types of creativity that complementarily contribute to the creative process through shifts in the activation of their substrates in the brain: the deliberate, spontaneous and flow types of creativity. The latter is a hybrid and embodied type, in which movement and physical activity meet creativity. We then focus on the most fascinating contribution of unconscious processes and mind wandering to spontaneous and flow modes of creativity, exploring what happens when the individual apparently takes a break from a deliberate and effortful search for solutions and the creative process progresses through an incubation phase. This phase and the overall creative process can be facilitated by physical activity which, depending on its features and context, can disengage the cognitive control network and free the mind from filters that constrain cognitive processes or, conversely, can engage attentional control on sensorimotor and cognitive task components in a mindful way. Lastly, we focus on the unique features of the outer natural environment of physical activity and of the inner environment during mindful movements that can restore capacities and boost creativity.}
}
@article{ZHENG20034147,
title = {A novel approach of three-dimensional hybrid grid methodology: Part 1. Grid generation},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {192},
number = {37},
pages = {4147-4171},
year = {2003},
issn = {0045-7825},
doi = {https://doi.org/10.1016/S0045-7825(03)00385-2},
url = {https://www.sciencedirect.com/science/article/pii/S0045782503003852},
author = {Yao Zheng and Meng-Sing Liou},
keywords = {Computational fluid dynamics, Grid generation, Hybrid grid},
abstract = {We propose a novel approach of three-dimensional hybrid grid methodology, the DRAGON grid method in the three-dimensional space. The DRAGON grid is created by means of a Direct Replacement of Arbitrary Grid Overlapping by Nonstructured grid, and is structured-grid dominated with unstructured grids in small regions. The DRAGON grid scheme is an adaptation to the Chimera thinking. It is capable of preserving the advantageous features of both the structured and unstructured grids, and eliminates/minimizes their shortcomings. In the present paper, we describe essential and programming aspects, and challenges of the three-dimensional DRAGON grid method, with respect to grid generation. We demonstrate the capability of generating computational grids for multi-components complex configurations.}
}
@article{HALLOWELL2023100240,
title = {Democratising or disrupting diagnosis? Ethical issues raised by the use of AI tools for rare disease diagnosis},
journal = {SSM - Qualitative Research in Health},
volume = {3},
pages = {100240},
year = {2023},
issn = {2667-3215},
doi = {https://doi.org/10.1016/j.ssmqr.2023.100240},
url = {https://www.sciencedirect.com/science/article/pii/S2667321523000240},
author = {Nina Hallowell and Shirlene Badger and Francis McKay and Angeliki Kerasidou and Christoffer Nellåker},
keywords = {Computational phenotyping, Rare disease, Diagnosis, AI, Qualitative interviews},
abstract = {Computational phenotyping (CP) technology uses facial recognition algorithms to classify and potentially diagnose rare genetic disorders on the basis of digitised facial images. This AI technology has a number of research as well as clinical applications, such as supporting diagnostic decision-making. Using the example of CP, we examine stakeholders’ views of the benefits and costs of using AI as a diagnostic tool within the clinic. Through a series of in-depth interviews (n ​= ​20) with: clinicians, clinical researchers, data scientists, industry and support group representatives, we report stakeholder views regarding the adoption of this technology in a clinical setting. While most interviewees were supportive of employing CP as a diagnostic tool in some capacity we observed ambivalence around the potential for artificial intelligence to overcome diagnostic uncertainty in a clinical context. Thus, while there was widespread agreement amongst interviewees concerning the public benefits of AI assisted diagnosis, namely, its potential to increase diagnostic yield and enable faster more objective and accurate diagnoses by up skilling non specialists and thereby enabling access to diagnosis that is potentially lacking, interviewees also raised concerns about ensuring algorithmic reliability, expunging algorithmic bias and that the use of AI could result in deskilling the specialist clinical workforce. We conclude that, prior to widespread clinical implementation, on-going reflection is needed regarding the trade-offs required to determine acceptable levels of bias and conclude that diagnostic AI tools should only be employed as an assistive technology within the dysmorphology clinic.}
}
@article{HEGG201856,
title = {Preservice teacher proficiency with transformations-based congruence proofs after a college proof-based geometry class},
journal = {The Journal of Mathematical Behavior},
volume = {51},
pages = {56-70},
year = {2018},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2018.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S0732312317301153},
author = {Meredith Hegg and Dimitri Papadopoulos and Brian Katz and Timothy Fukawa-Connelly},
keywords = {Mathematics teacher education, Transformational geometry, Proof},
abstract = {This report explores pre-service teachers’ proficiency with concepts of transformational geometry at the end of a semester-long advanced geometry course. In the course, the instructor incorporated transformational geometry content, including congruence proofs, in an attempt to prepare the pre-service teachers to teach high school geometry in alignment with the Common Core State Standards for Mathematics. At the conclusion of the course, students expressed a preference for using traditional triangle congruence criteria (SAS, ASA, SSS, and AAS) over using transformations to complete proofs, but were nevertheless generally successful in completing proofs using transformations. Similarly, while the students often described thinking of transformations in terms of analytic forms, they were successfully able to prove triangle congruences in synthetic contexts. Finally, some evidence indicates that students may have motion or process conceptions of transformations, but not map or object conceptions, but this evidence is not conclusive.}
}
@article{DUKHANOV2016449,
title = {Big Data and Artificial Intelligence for Digital Humanities: An International Master Program via Trans-Eurasian Universities Network},
journal = {Procedia Computer Science},
volume = {101},
pages = {449-451},
year = {2016},
note = {5th International Young Scientist Conference on Computational Science, YSC 2016, 26-28 October 2016, Krakow, Poland},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2016.11.052},
url = {https://www.sciencedirect.com/science/article/pii/S1877050916327211},
author = {Alexey Dukhanov and Alexander Boukhanovsky and Tatyana Sidorova and Natalya Spitsyna},
keywords = {trans-Eurasian universities’ network, international Master's program, digital humanities, skills of contemporary professional},
abstract = {This paper presents an intention of two Russian universities located at opposite sides of Russia to build with partners – leading world educational centers (in the Top-100 Universities of Times Higher Education) – a trans-Eurasian international network with Master's program “Big Data and Artificial Intelligence for Digital Humanities.” This program significantly extends the area of fostering students’ talent. In addition, it allows students to develop valuable global skills of a contemporary professional: domain expertise, soft skills including creative and system thinking, self-development, working in an international and intercultural team on a research project, etc. After graduation, the alumni will have a wide choice of opportunities to continue their academic career or to get a well-paid job in developing and developed countries around the World.}
}
@article{IONESCU2014275,
title = {Embodied Cognition: Challenges for Psychology and Education},
journal = {Procedia - Social and Behavioral Sciences},
volume = {128},
pages = {275-280},
year = {2014},
note = {International Conference: EDUCATION AND PSYCHOLOGY CHALLENGES - TEACHERS FOR THE KNOWLEDGE SOCIETY – 2nd EDITION EPC – TKS 2013},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2014.03.156},
url = {https://www.sciencedirect.com/science/article/pii/S1877042814022472},
author = {Thea Ionescu and Dermina Vasc},
keywords = {embodiment, cognition, sensory-motor processes, action, education.},
abstract = {Embodied cognition considers that human cognition is fundamentally grounded in sensory-motor processes and in our body's morphology and internal states. In this paper, we discuss some of the features of this post-cognitivist approach and the challenges that follow for psychology and education. These challenges point to the need to reconsider cognition and the way we pursue education today. If we want to have an efficient educational system we have to look at fundamental research in cognitive science to have an accurate description of what cognition is. Only then can we design optimal educational settings for the development of thinking.}
}
@article{JIANG2025100010,
title = {AI4Materials: Transforming the landscape of materials science and enigneering},
journal = {Review of Materials Research},
volume = {1},
number = {1},
pages = {100010},
year = {2025},
issn = {3050-9130},
doi = {https://doi.org/10.1016/j.revmat.2025.100010},
url = {https://www.sciencedirect.com/science/article/pii/S3050913025000105},
author = {Xue Jiang and Dezhen Xue and Yang Bai and William Yi Wang and Jianjun Liu and Mingli Yang and Yanjing Su},
keywords = {Intelligent computation, Machine learning, Materials data infrastructure, Autonomous experiment, Intelligent manufacture},
abstract = {New materials, crucial for economic and technological progress, are prioritized globally with strategies to accelerate their advancement through big data and AI. AI for Materials (AI4Mater) serves as an overall framework for integrating AI into Materials Science and Engineering, which is structured around three main elements: materials data infrastructure, AI4Mater techniques, and applications. This article reviews the development procedure and recent innovations in materials data infrastructure, machine learning in materials, autonomous experiment, intelligent computation, and intelligent manufacture. These efforts aim to foster open access to AI resources and enhance the collective advancement of materials science, ultimately accelerating breakthroughs and elevating the engineering application of new materials in a sustainable manner.}
}
@article{MASROURI2025102428,
title = {Animal-skin-pattern-inspired multifunctional composites by generative AI},
journal = {Cell Reports Physical Science},
volume = {6},
number = {2},
pages = {102428},
year = {2025},
issn = {2666-3864},
doi = {https://doi.org/10.1016/j.xcrp.2025.102428},
url = {https://www.sciencedirect.com/science/article/pii/S266638642500027X},
author = {Milad Masrouri and Akshay Vilas Jadhav and Zhao Qin},
keywords = {composite design, bioinspiration, generative AI, molecular dynamics, elastic network, animal pattern, biomimicry, 3D printing, toughness modulus, multifunctional materials},
abstract = {Summary
Bioinspired composite materials offer several advantages by mimicking the structure of natural counterparts. However, their complex hierarchical structure, compared to the limited number of observations, makes it difficult to extract all the structural features and vary the structure to optimize the materials’ functions without losing their natural features. We applied generative artificial intelligence (GenAI) to design composites inspired by animal skin patterns, leveraging a small dataset to generate diverse configurations that closely emulate natural designs. Our computational simulations investigated the structure-mechanics relationship in these materials, revealing significant variations in mechanical functions and identifying patterns that exhibited superior mechanical properties. We validated these outstanding configurations’ performance through tensile tests on specimens produced by a multimaterial printer. We showcase GenAI’s role in structural augmentation that can yield rational bioinspired designs, complemented by an educational web page with interactive games for public access.}
}
@incollection{BATCHELDER2015808,
title = {Mathematical Psychology: History},
editor = {James D. Wright},
booktitle = {International Encyclopedia of the Social & Behavioral Sciences (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {808-815},
year = {2015},
isbn = {978-0-08-097087-5},
doi = {https://doi.org/10.1016/B978-0-08-097086-8.43059-X},
url = {https://www.sciencedirect.com/science/article/pii/B978008097086843059X},
author = {William H. Batchelder},
keywords = {Bayesian theory, Choice, Correlation coefficient, Decision making, Differential psychology, Experimental psychology, Factor analysis, Game theory, Information science, Learning theory, Marginal utility, Measurement theory, Memory, Paired-comparison scaling, Psychometrics, Psychophysics, Response time, Scaling, Signal detection theory, Stochastic processes, Theory of grammar, Utility theory},
abstract = {The application of mathematics to certain problems within the field of psychology dates back to at least the seventeenth century. This article reviews some of these early applications, most of which either involve theories for experimental phenomena or statistical methods for measuring individual differences. These later applications led to the field of psychometrics in the 1930s; and the former led to the field of mathematical psychology in the 1950s, and both fields are active today. Early mathematical psychology was characterized by testable, formal theories in the areas of learning and memory, perception and psychophysics, choice and decision making, language and thinking, and measurement and scaling; and these areas still characterize the field today.}
}
@article{RUTTEN20211,
title = {50 Years of Russian Literature: Mapping, Mixing, and Queering Slavic Literary Studies},
journal = {Russian Literature},
volume = {125-126},
pages = {1-8},
year = {2021},
note = {50 Years of Russian Literature & Teffi’s Theatrical & Cinematic Work},
issn = {0304-3479},
doi = {https://doi.org/10.1016/j.ruslit.2021.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S0304347921000673},
author = {Ellen Rutten},
keywords = {Russian Literature, Editorial, Transdisciplinarity, Slavic Literary Studies, Transnational Academic Communication},
abstract = {Russian Literature turned fifty this year. In this editorial contribution, editor-in-chief Ellen Rutten reflects on the journal’s past, its current profile, and future editorial plans. As Rutten argues, Russian Literature has three distinguishing features. First, the journal has always generously invited other disciplines on board – and its transdisciplinary inclusivity has increased in recent years – while maintaining a steady gaze on Slavic literary studies. Second, the journal acts as a transnational and transcontinental scholarly contact zone – a status that cannot be isolated from our choice to publish both Anglophone and Russophone analyses. And third, Russian Literature brings together a range of scholarly voices and genres that is unusually broad for a scholarly periodical, through a strategy of active editorial outreach to young talents and leading experts in the field. Rutten concludes with a few words on upcoming volumes and plans, including new archival publications and volumes-in-the-making inspired by recent shifts in thinking about geopolitics, gender, and health and environment.}
}
@article{KOSCHINSKY2013172,
title = {The case for spatial analysis in evaluation to reduce health inequities},
journal = {Evaluation and Program Planning},
volume = {36},
number = {1},
pages = {172-176},
year = {2013},
note = {Special Section: Rethinking Evaluation of Health Equity Initiatives},
issn = {0149-7189},
doi = {https://doi.org/10.1016/j.evalprogplan.2012.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S0149718912000237},
author = {Julia Koschinsky},
keywords = {Spatial analysis, Spatial perspective, Program evaluation, Evaluation, Health inequities, Realist evaluation, Randomized control trials (RCTs)},
abstract = {The article begins by giving an overview of spatial thinking concepts that are relevant to evaluation. The article relates the spatial perspective to both a realist evaluation and a randomized control trial perspective in evaluation to demonstrate the benefits of a spatialized program and evaluation perspective. The article mainly suggests that the adoption of a spatial perspective can add new insights to the theory and practice of evaluation in ways that helps evaluation move closer to reducing health inequities.}
}
@article{GOULETCOULOMBE2025,
title = {Time-varying parameters as ridge regressions},
journal = {International Journal of Forecasting},
year = {2025},
issn = {0169-2070},
doi = {https://doi.org/10.1016/j.ijforecast.2024.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S0169207024000931},
author = {Philippe {Goulet Coulombe}},
abstract = {Time-varying parameter (TVP) models are frequently used in economics to capture structural change. I highlight a rather underutilized fact—that these are actually ridge regressions. Instantly, this makes computations, tuning, and implementation much easier than in the state-space paradigm. Among other things, solving the equivalent dual ridge problem is computationally very fast even in high dimensions, and the crucial ‘amount of time variation’ is tuned by cross-validation. Evolving volatility is dealt with using a two-step ridge regression. I consider extensions that incorporate sparsity (the algorithm selects which parameters vary and which do not) and reduced-rank restrictions (variation is tied to a factor model). To demonstrate the usefulness of the approach, I use it to study the evolution of monetary policy in Canada using large time-varying local projections and TVP-VARs with demanding lag lengths. The applications require the estimation of up to 4600 TVPs, a task within the reach of the new method.}
}
@article{XIA2023100730,
title = {Understanding common human driving semantics for autonomous vehicles},
journal = {Patterns},
volume = {4},
number = {7},
pages = {100730},
year = {2023},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2023.100730},
url = {https://www.sciencedirect.com/science/article/pii/S2666389923000703},
author = {Yingji Xia and Maosi Geng and Yong Chen and Sudan Sun and Chenlei Liao and Zheng Zhu and Zhihui Li and Washington Yotto Ochieng and Panagiotis Angeloudis and Mireille Elhajj and Lei Zhang and Zhenyu Zeng and Bing Zhang and Ziyou Gao and Xiqun (Michael) Chen},
keywords = {human-machine interaction, neuroscience, hierarchical understanding abstraction, electroencephalography, neural-informed model, driving behavior perception, driving semantics, autonomous vehicle},
abstract = {Summary
Autonomous vehicles will share roads with human-driven vehicles until the transition to fully autonomous transport systems is complete. The critical challenge of improving mutual understanding between both vehicle types cannot be addressed only by feeding extensive driving data into data-driven models but by enabling autonomous vehicles to understand and apply common driving behaviors analogous to human drivers. Therefore, we designed and conducted two electroencephalography experiments for comparing the cerebral activities of human linguistics and driving understanding. The results showed that driving activates hierarchical neural functions in the auditory cortex, which is analogous to abstraction in linguistic understanding. Subsequently, we proposed a neural-informed, semantics-driven framework to understand common human driving behavior in a brain-inspired manner. This study highlights the pathway of fusing neuroscience into complex human behavior understanding tasks and provides a computational neural model to understand human driving behaviors, which will enable autonomous vehicles to perceive and think like human drivers.}
}
@incollection{FROEHLICH2023685,
title = {Mixed methods and social network analysis},
editor = {Robert J Tierney and Fazal Rizvi and Kadriye Ercikan},
booktitle = {International Encyclopedia of Education (Fourth Edition)},
publisher = {Elsevier},
edition = {Fourth Edition},
address = {Oxford},
pages = {685-692},
year = {2023},
isbn = {978-0-12-818629-9},
doi = {https://doi.org/10.1016/B978-0-12-818630-5.11059-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128186305110590},
author = {Dominik E. Froehlich},
keywords = {Data collection, Education research, Ethics, Mixed methods social network analysis, Mixed methods, Relational methods, Research design, Social network analysis, Structure},
abstract = {In this chapter, we discuss the application of mixed methods thinking to social network analysis, a methodological approach that focuses on social relationships and structures. For that purpose, we first define mixed methods and social network analysis and their intersection, which we call Mixed Methods Social Network Analysis (MMSNA). We then summarize the historical developments in social network analysis, which also explain the reason for the increasing application of MMSNA in educational research. The majority of the chapter then focuses on how MMSNA is applied in educational research and what the main topics of the current academic debates are.}
}
@article{REN20231643,
title = {An Edge Computing Algorithm Based on Multi-Level Star Sensor Cloud},
journal = {CMES - Computer Modeling in Engineering and Sciences},
volume = {136},
number = {2},
pages = {1643-1659},
year = {2023},
issn = {1526-1492},
doi = {https://doi.org/10.32604/cmes.2023.025248},
url = {https://www.sciencedirect.com/science/article/pii/S1526149223002813},
author = {Siyu Ren and Shi Qiu and Keyang Cheng},
keywords = {Star-sensing, sensor cloud, fuzzy set, edge computing, mapping},
abstract = {Star sensors are an important means of autonomous navigation and access to space information for satellites. They have been widely deployed in the aerospace field. To satisfy the requirements for high resolution, timeliness, and confidentiality of star images, we propose an edge computing algorithm based on the star sensor cloud. Multiple sensors cooperate with each other to form a sensor cloud, which in turn extends the performance of a single sensor. The research on the data obtained by the star sensor has very important research and application values. First, a star point extraction model is proposed based on the fuzzy set model by analyzing the star image composition, which can reduce the amount of data computation. Then, a mapping model between content and space is constructed to achieve low-rank image representation and efficient computation. Finally, the data collected by the wireless sensor is delivered to the edge server, and a different method is used to achieve privacy protection. Only a small amount of core data is stored in edge servers and local servers, and other data is transmitted to the cloud. Experiments show that the proposed algorithm can effectively reduce the cost of communication and storage, and has strong privacy.}
}
@article{ASIF202532,
title = {Machine learning-driven catalyst design, synthesis and performance prediction for CO2 hydrogenation},
journal = {Journal of Industrial and Engineering Chemistry},
volume = {144},
pages = {32-47},
year = {2025},
issn = {1226-086X},
doi = {https://doi.org/10.1016/j.jiec.2024.09.035},
url = {https://www.sciencedirect.com/science/article/pii/S1226086X24006269},
author = {Muhammad Asif and Chengxi Yao and Zitu Zuo and Muhammad Bilal and Hassan Zeb and Seungjae Lee and Ziyang Wang and Taesung Kim},
keywords = {Heterogeneous catalysis, DFT calculation, Machine learning, 3D printing, CO hydrogenation},
abstract = {Atmospheric concentrations of CO2 must be lowered to mitigate climate change and rising global temperatures. CO2 utilization is the most promising approach for the sustainable reduction of CO2 emissions. Interdisciplinary research is gaining increasing attention due to its broader application potential and the promising results of combining various fields. Computational approaches in catalytic research could be cost-effective and environmentally friendly. Machine Learning (ML) and 3D printing technologies may soon be able to produce nanoscale raw materials to synthesize the catalyst for commercial-scale applications. In this review article, recent advances in catalyst synthesis using 3D printing technologies and ML-based catalytic reactions, particularly those in CO2 hydrogenation, are critically analyzed, with a focus on the function of ML model prediction. ML approaches with high prediction accuracies are discussed comprehensively. Based on the literature Gray-box models can provide useful insights by revealing the essential catalytic traits, factors, and circumstances that affect the results. They can also provide a practical solution by fusing the benefits of black-box algorithms, such as ensemble models and NNs, with feature importance analysis. Finally, suggestions and recommendations for the potential applications of ML in chemical science, especially in heterogeneous catalysis, are provided along with future research directions.}
}
@article{WANG2024109589,
title = {Cellular gradient algorithm for solving complex mechanical optimization design problems},
journal = {International Journal of Mechanical Sciences},
volume = {282},
pages = {109589},
year = {2024},
issn = {0020-7403},
doi = {https://doi.org/10.1016/j.ijmecsci.2024.109589},
url = {https://www.sciencedirect.com/science/article/pii/S0020740324006301},
author = {Rugui Wang and Xinpeng Li and Haibo Huang and Zhipeng Fan and Fuqiang Huang and Ningjuan Zhao},
keywords = {Mechanical optimization, Optimization algorithm, Discrete integrable problem, Cellular automaton, Gradient descent},
abstract = {In mechanical optimization design problems, there are often some non-continuous or non-differentiable objective functions. For these non-continuous and non-differentiable optimization objectives, it is often difficult for existing optimal design algorithms to find the desired optimal solutions. In this paper, we incorporate the idea of gradient descent into cellular automata and propose a Cellular Gradient (CG) method. First, we have given the basic rules and algorithmic framework of CG and designed three kinds of growth and extinction rules respectively. Then, the three evolutionary rules for cellular within a single cycle are analyzed separately for form and ordering. The best expressions for the cellular jealous neighbor rule and the solitary regeneration rule are given, and the most appropriate order in which the rules are run is selected. Finally, the solution results of the cellular gradient algorithm and other classical optimization design algorithms are compared with a multi-objective multi-parameter mechanical optimization design problem as an example. The computational results show that the cellular gradient algorithm has an advantage over other algorithms in solving global and dynamic mechanical optimal design problems. The novelty of CG is to provide a new way of thinking for solving optimization problems with global discontinuities.}
}
@article{NAGLE2019100684,
title = {Using APOS theory as a framework for considering slope understanding},
journal = {The Journal of Mathematical Behavior},
volume = {54},
pages = {100684},
year = {2019},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2018.12.003},
url = {https://www.sciencedirect.com/science/article/pii/S0732312318301469},
author = {Courtney Nagle and Rafael Martínez-Planell and Deborah Moore-Russo},
keywords = {APOS, Slope, APOS levels between stages, Precalculus, Rate of change, Totality},
abstract = {In this paper a framework for slope is proposed using APOS (Action-Process-Object-Schema) Theory and conceptualizations of slope previously identified in research. The proposed APOS-slope framework allows for discussion of students’ cognitive development in relation to different conceptualizations of slope. As such, it may be adopted as a means to advance future research or as a way to plan instruction. In particular, the framework uses specific examples to consider interrelations between the ways of thinking about slope that have been reported to provide additional insight on how individuals understand this concept. The proposed framework contributes to the field by bringing together a number of past studies related to slope and providing a common ground under which these works might be interpreted.}
}
@article{VASSILIADIS2024380,
title = {Reloading Process Systems Engineering within Chemical Engineering},
journal = {Chemical Engineering Research and Design},
volume = {209},
pages = {380-398},
year = {2024},
issn = {0263-8762},
doi = {https://doi.org/10.1016/j.cherd.2024.07.066},
url = {https://www.sciencedirect.com/science/article/pii/S0263876224004568},
author = {Vassilios S. Vassiliadis and Vasileios Mappas and Thomas A. Espaas and Bogdan Dorneanu and Adeniyi Isafiade and Klaus Möller and Harvey Arellano-Garcia},
keywords = {Chemical Engineering, Process Systems Engineering, Process model construction and deployment, Digital Twinning, Machine Learning},
abstract = {Established as a sub-discipline of Chemical Engineering in the 1960s by the late Professor R.W.H. Sargent at Imperial College London, Process Systems Engineering (PSE) has played a significant role in advancing the field, positioning it as a leading engineering discipline in the contemporary technological landscape. Rooted in Applied Mathematics and Computing, PSE aligns with the key components driving advancements in our modern, information-centric era. Sargent’s visionary foresight anticipated the evolution of early computational tools into fundamental elements for future technological and scientific breakthroughs, all while maintaining a central focus on Chemical Engineering. This paper aims to present concise and concrete ideas for propelling PSE into a new era of progress. The objective is twofold: to preserve PSE’s extensive and diverse knowledge base and to reposition it more prominently within modern Chemical Engineering, while also establishing robust connections with other data-driven engineering and applied science domains that play important roles in industrial and technological advancements. Rather than merely reacting to contemporary challenges, this article seeks to proactively create opportunities to lead the future of Chemical Engineering across its vital contributions in education, research, technology transfer, and business creation, fully leveraging its inherent multidisciplinarity and versatile character.}
}
@article{LU2022100056,
title = {Nonlinear EEG signatures of mind wandering during breath focus meditation},
journal = {Current Research in Neurobiology},
volume = {3},
pages = {100056},
year = {2022},
issn = {2665-945X},
doi = {https://doi.org/10.1016/j.crneur.2022.100056},
url = {https://www.sciencedirect.com/science/article/pii/S2665945X22000298},
author = {Yiqing Lu and Julio Rodriguez-Larios},
keywords = {EEG, Mind wandering, Meditation, Complexity, Nonlinear analysis},
abstract = {In meditation practices that involve focused attention to a specific object, novice practitioners often experience moments of distraction (i.e., mind wandering). Previous studies have investigated the neural correlates of mind wandering during meditation practice through Electroencephalography (EEG) using linear metrics (e.g., oscillatory power). However, their results are not fully consistent. Since the brain is known to be a chaotic/nonlinear system, it is possible that linear metrics cannot fully capture complex dynamics present in the EEG signal. In this study, we assess whether nonlinear EEG signatures can be used to characterize mind wandering during breath focus meditation in novice practitioners. For that purpose, we adopted an experience sampling paradigm in which 25 participants were iteratively interrupted during meditation practice to report whether they were focusing on the breath or thinking about something else. We compared the complexity of EEG signals during mind wandering and breath focus states using three different algorithms: Higuchi's fractal dimension (HFD), Lempel-Ziv complexity (LZC), and Sample entropy (SampEn). Our results showed that EEG complexity was generally reduced during mind wandering relative to breath focus states. We conclude that EEG complexity metrics are appropriate to disentangle mind wandering from breath focus states in novice meditation practitioners, and therefore, they could be used in future EEG neurofeedback protocols to facilitate meditation practice.}
}
@article{KORN2023102578,
title = {Navigating large chemical spaces in early-phase drug discovery},
journal = {Current Opinion in Structural Biology},
volume = {80},
pages = {102578},
year = {2023},
issn = {0959-440X},
doi = {https://doi.org/10.1016/j.sbi.2023.102578},
url = {https://www.sciencedirect.com/science/article/pii/S0959440X23000520},
author = {Malte Korn and Christiane Ehrt and Fiorella Ruggiu and Marcus Gastreich and Matthias Rarey},
abstract = {The size of actionable chemical spaces is surging, owing to a variety of novel techniques, both computational and experimental. As a consequence, novel molecular matter is now at our fingertips that cannot and should not be neglected in early-phase drug discovery. Huge, combinatorial, make-on-demand chemical spaces with high probability of synthetic success rise exponentially in content, generative machine learning models go hand in hand with synthesis prediction, and DNA-encoded libraries offer new ways of hit structure discovery. These technologies enable to search for new chemical matter in a much broader and deeper manner with less effort and fewer financial resources. These transformational developments require new cheminformatics approaches to make huge chemical spaces searchable and analyzable with low resources, and with as little energy consumption as possible. Substantial progress has been made in the past years with respect to computation as well as organic synthesis. First examples of bioactive compounds resulting from the successful use of these novel technologies demonstrate their power to contribute to tomorrow's drug discovery programs. This article gives a compact overview of the state-of-the-art.}
}
@article{BOSL2025101480,
title = {Dynamical measures of developing neuroelectric fields in emerging consciousness},
journal = {Current Opinion in Behavioral Sciences},
volume = {61},
pages = {101480},
year = {2025},
issn = {2352-1546},
doi = {https://doi.org/10.1016/j.cobeha.2024.101480},
url = {https://www.sciencedirect.com/science/article/pii/S2352154624001311},
author = {William J Bosl and Jenny R {Capua Shenkar}},
abstract = {Human consciousness emerges over time. From the moment of conception, a process of neurodevelopment and complexification begins, generating and supporting a neuroelectric field that can be quantified by computational methods from dynamical systems theory. In the early embryo, genetically driven cellular processes are mediated by endogenous electromagnetic fields and intrinsic electrical fields produced by migrating neurons. In the ambient cellular environment, these interactions influence each other, impacting neural migration. The emergence of Theory of Mind, often considered a hallmark of conscious awareness, is accompanied by increasing neural connectivity, neuroelectric field complexity, and more integrated information processing. Neurodegeneration in old age and the often-associated decline in conscious awareness correlate closely with changes in the dynamical complexity of the neuroelectric field. Monitoring trajectories of the neuroelectric field and its complexity changes through the lifespan presents a developmental perspective and empirical correlation for studying the emergence and decline of human consciousness.}
}
@article{QIAN2024120487,
title = {E3WD: A three-way decision model based on ensemble learning},
journal = {Information Sciences},
volume = {667},
pages = {120487},
year = {2024},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2024.120487},
url = {https://www.sciencedirect.com/science/article/pii/S0020025524004006},
author = {Jin Qian and Di Wang and Ying Yu and XiBei Yang and Shang Gao},
keywords = {Three-way decision, Ensemble strategy, Cluster ensemble, Membership degree, Critical threshold},
abstract = {Three-way decision model is an effective way to deal with complex decision problems. However, since the three-way decision models now proposed are all based on a single decision criterion, the decision results typically reflect only one preference of decision-makers. Thus, these models may also not effectively deal with complex decision-making problems. To solve the above problems, this paper proposes a new three-way decision model based on ensemble learning. Specifically, we first obtain different three-way decision results by employing different decision criteria. Then, we can acquire the core and candidate sets of the positive and negative regions through set operations. Next, we use the K-means algorithm to divide the candidate sets into three disjoint subsets based on similarities. After that, we adopt a hierarchical filtering method to select suitable objects from the candidate sets and add them to the core sets. Finally, we employ four three-way decision models with different decision criteria as examples to conduct experiments on eight datasets. Experimental results show that our proposed model can obtain higher classification accuracy and lower deferment rate than other traditional three-way decision models under most experimental conditions.}
}
@article{ALDAYA2024116708,
title = {Tachyons in “momentum-space” representation},
journal = {Nuclear Physics B},
volume = {1008},
pages = {116708},
year = {2024},
issn = {0550-3213},
doi = {https://doi.org/10.1016/j.nuclphysb.2024.116708},
url = {https://www.sciencedirect.com/science/article/pii/S0550321324002748},
author = {V. Aldaya and J. Guerrero and F.F. López-Ruiz},
abstract = {Obtaining the momentum space associated with tachyonic “particles” from the Poincaré group manifold proves to be rather intricate, departing very much from the ordinary dual to Minkowski space directly parametrized by space-time translations of the Poincaré group. In fact, although described by the constants of motion (Noether invariants) associated with space-time translations, they depend non-trivially on the parameters of the rotation subgroup. However, once the momentum space is parametrized by the Noether invariants, it behaves as that of ordinary particles. On the other hand, the evolution parameter is no longer the one associated with time translation, whose Noether invariant, Po, is now a basic one. Evolution takes place in a spatial direction. These facts not only make difficult the computation of the corresponding representation, but also force us to a sound revision of several traditional ingredients related to Cauchy hypersurface, scalar product and, of course, causality. After that, the theory becomes consistent and could shed new light on some special physical situations like inflation or traveling inside a black hole.}
}
@article{SAQIB2024105516,
title = {Novel Recurrent neural networks for efficient heat transfer analysis in radiative moving porous triangular fin with heat generation},
journal = {Case Studies in Thermal Engineering},
volume = {64},
pages = {105516},
year = {2024},
issn = {2214-157X},
doi = {https://doi.org/10.1016/j.csite.2024.105516},
url = {https://www.sciencedirect.com/science/article/pii/S2214157X24015478},
author = {Sana Ullah Saqib and Umar Farooq and Nahid Fatima and Yin-Tzer Shih and Ahmed Mir and Lioua Kolsi},
keywords = {Permeable fin in a triangle form, Convection radiation fin effectiveness, Recurrent neural networks (RNNs), Lobatto III-A technique, AI-Based intelligent computing},
abstract = {This paper investigates the use of Artificial Intelligence (AI), notably Recurrent Neural Networks (RNNs), to analyze heat transfer in moving radiative porous triangular systems with heat generation (HTMPTHG). AI-based RNN models are employed to simulate and forecast the complex heat transfer behavior in these environments, offering a more precise and efficient analysis as compared to traditional numerical methods. The findings of the study highlights the intricate interactions among thermal radiation, porous media, and internal heat generation which plays an integral role in a number of industrial and engineering applications. Recurrent neural network (RNN) is validated to examine the temperature distribution efficiency in a new configuration of triangular, porous, moving fins. Various dimensionless parameters are analyzed for their impact on the effectiveness of portable, transparent, triangular fins. These parameters include permeability, radiation-conduction, Peclet number, thermo-geometric factors, convection-conduction, and surface temperature. The Lobatto III-A numerical technique for HTMPTHG is simulated computationally to provide the synthetic datasets. Then, the RNN supervised computational technique is applied to the generated datasets and the RNN outputs show negligible errors and closely align with numerical observations for all model variant. The effectiveness of Recurrent Neural Networks (RNNs) is rigorously proved through extensive experiments, demonstrating iterative convergence curves for mean squared error, control metrics of optimization and error distribution via histograms.The mean absolute percent error (MAPE), mean absolute error (MAE), and Nash-Sutcliffe efficiency (NSE) are all nearly zero, while the coefficient of determination (R2) is close to 1.Furthermore, there is strong evidence of the prediction accuracy and dependability of the RNN in the regression results for the HTMPTHG model.}
}
@article{LIMONGELLI199289,
title = {Abstract specification of structures and methods in symbolic mathematical computation},
journal = {Theoretical Computer Science},
volume = {104},
number = {1},
pages = {89-107},
year = {1992},
issn = {0304-3975},
doi = {https://doi.org/10.1016/0304-3975(92)90167-E},
url = {https://www.sciencedirect.com/science/article/pii/030439759290167E},
author = {C. Limongelli and M. Temperini},
abstract = {This paper describes a methodology based on the object-oriented programming paradigm, to support the design and implementation of a symbolic computation system. The requirements of the system are related to the specification and treatment of mathematical structures. This treatment is considered from both the numerical and the symbolic points of view. The resulting programming system should be able to support the formal definition of mathematical data structures and methods at their highest level of abstraction, to perform computations on instances created from such definitions, and to handle abstract data structures through the manipulation of their logical properties. Particular consideration is given to the correctness aspects. Some examples of convenient application of the proposed design methodology are presented.}
}
@article{FARHAT199361,
title = {Two-dimensional viscous flow computations on the Connecti on Machine: Unstructured meshes, upwind schemes and massively parallel computations},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {102},
number = {1},
pages = {61-88},
year = {1993},
issn = {0045-7825},
doi = {https://doi.org/10.1016/0045-7825(93)90141-J},
url = {https://www.sciencedirect.com/science/article/pii/004578259390141J},
author = {Charbel Farhat and Loula Fezoui and Stéphane Lanteri},
abstract = {Here we report on our effort in simulating two-dimensional viscous flows on the Connection Machine, using a second-order accurate monotomic upwind scheme for conservation laws (MUSCL) on fully unstructured grids. The spatial approximation combines an upwind finite volume method for the discretization of the convective fluxes with a classical Galerkin finite element method for the discretization of the diffusive fluxes. The resulting semi-discrete equations are time integrated with a second-order low-storage explicit Runge-Kutta method. A communication efficient strategy for mapping thousands of processors onto an arbitrary mesh is presented and proposed as an alternative to the fast north-east-west-south (NEWS) communication mechanism, which is restricted to structured grids. Measured performance results for the simulation of low Reynolds number chaotic flows indicate that an 8K CM-2 (8192 processors) with single precision floating point arithmetic is at least as fast as one CRAY-2 processor.}
}
@article{NISSEL2024105856,
title = {Why wearing a yellow hat is impossible: Chinese and U.S. children's possibility judgments},
journal = {Cognition},
volume = {251},
pages = {105856},
year = {2024},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2024.105856},
url = {https://www.sciencedirect.com/science/article/pii/S0010027724001422},
author = {Jenny Nissel and Jiaying Xu and Lihanjing Wu and Zachary Bricken and Jennifer M. Clegg and Hui Li and Jacqueline D. Woolley},
keywords = {Cognitive development, Social development, Possibility, Intuitive theories, Cross-cultural, LIWC},
abstract = {When thinking about possibility, one can consider both epistemic and deontic principles (i.e., physical possibility and permissibility). Cultural influences may lead individuals to weigh epistemic and deontic obligations differently; developing possibility conceptions are therefore positioned to be affected by cultural surroundings. Across two studies, 251 U.S. and Chinese 4-, 6-, and 8-year-olds sampled from major metropolitan areas in Texas and the Hubei, Sichuan, Gansu, and Guangdong Provinces judged the possibility of impossible, improbable, and ordinary events. Across cultures and ages, children judged ordinary events as possible and impossible events as impossible; cultural differences emerged in developing conceptions of improbable events. Whereas U.S. children became more likely to judge these events possible with age, Chinese children's judgments remained consistent with age: Chinese 4- to 8-year-olds judged these events to be possible ∼25% of the time. In Study 2, to test whether this difference was attributable to differential prioritization of epistemic versus deontic constraints, children also judged whether each event was an epistemic violation (i.e., required magic to happen) and a deontic violation (i.e., would result in someone getting in trouble). With age, epistemic judgments were increasingly predictive of possibility judgments for improbable events for U.S. children, and decreasingly so for Chinese children. Contrary to our predictions, deontic judgments were not predictive. We propose that cultural valuation of norms might shape children's developing intuitions about possibility. We discuss our findings in light of three accounts of possibility conceptions, suggesting ways to integrate cultural context into each.}
}
@article{SOUZA2025112323,
title = {Techniques for eliciting IoT requirements: Sensorina Map and Mind IoT},
journal = {Journal of Systems and Software},
volume = {222},
pages = {112323},
year = {2025},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2024.112323},
url = {https://www.sciencedirect.com/science/article/pii/S0164121224003674},
author = {Sabrina Souza and Eriky Rodrigues and Maria Meireles and Tanara Lauschner and Leandro Carvalho and José Carlos Maldonado and Tayana Conte},
keywords = {Requirements engineering, Design Thinking, Requirements’ elicitation techniques, Internet of Things},
abstract = {Context:
The Internet of Things (IoT) involves heterogeneous devices that interact and process data via the Internet. In the development of IoT systems, requirement elicitation is crucial. However, challenges such as heterogeneity, interoperability, scalability, and requirements volatility necessitate new approaches or adapting traditional techniques.
Objective:
In this context, this work proposes the Sensorina Map and IoT Mind as techniques adapted from the Empathy Map and Mind Map, respectively, to support requirement elicitation in IoT systems.
Method:
Two empirical studies were conducted in an academic environment to assess the feasibility of the techniques, then, a case study in industry environment.
Results:
The first study analyzed the ease of use and evaluated if it assisted software engineers in remembering the system requirements. The participants’ perceptions were collected through a Focus Group, refining the techniques. Subsequently, an observational study evaluated the techniques’ usefulness and ease of use. The results of the study demonstrated that the participants considered the methods feasible. The case study results revealed that the Sensorina Map is more suitable for advanced stages. At the same time, the Mind IoT suits better the initial phases, emphasizing the need for practical examples and adaptations to suit diverse user profiles.
Conclusion:
This work is expected to advance research in IoT systems and benefit professionals and researchers in this area.}
}
@article{SINGH2024101269,
title = {An empirical approach to understand the role of emotions in code comprehension},
journal = {Journal of Computer Languages},
volume = {79},
pages = {101269},
year = {2024},
issn = {2590-1184},
doi = {https://doi.org/10.1016/j.cola.2024.101269},
url = {https://www.sciencedirect.com/science/article/pii/S2590118424000121},
author = {Divjot Singh and Ashutosh Mishra and Ashutosh Aggarwal},
keywords = {Code comprehension, Systematic literature review, Emotions, Cognitive skills},
abstract = {Programming and cognitive skills are two pivotal abilities of programmers to maintain software products. First, this study included a systematic literature review on code comprehension, emotions, cognitive psychology, and belief-desire-intention domains to analyse various code comprehension monitoring techniques, performance metrics, and computational methodologies. Second, a case study is conducted to examine the influence of various emotional stages on programmers’ programming and cognitive skills while comprehending the software code. The categorization of the participants is done empirically based on their expertism level, and the same results are verified using various machine learning models and performance metrics.}
}
@article{MATHIS20245814,
title = {Decoding the brain: From neural representations to mechanistic models},
journal = {Cell},
volume = {187},
number = {21},
pages = {5814-5832},
year = {2024},
issn = {0092-8674},
doi = {https://doi.org/10.1016/j.cell.2024.08.051},
url = {https://www.sciencedirect.com/science/article/pii/S0092867424009802},
author = {Mackenzie Weygandt Mathis and Adriana {Perez Rotondo} and Edward F. Chang and Andreas S. Tolias and Alexander Mathis},
keywords = {decoding, encoding, deep learning, data-driven, normative models, BCIs, language},
abstract = {Summary
A central principle in neuroscience is that neurons within the brain act in concert to produce perception, cognition, and adaptive behavior. Neurons are organized into specialized brain areas, dedicated to different functions to varying extents, and their function relies on distributed circuits to continuously encode relevant environmental and body-state features, enabling other areas to decode (interpret) these representations for computing meaningful decisions and executing precise movements. Thus, the distributed brain can be thought of as a series of computations that act to encode and decode information. In this perspective, we detail important concepts of neural encoding and decoding and highlight the mathematical tools used to measure them, including deep learning methods. We provide case studies where decoding concepts enable foundational and translational science in motor, visual, and language processing.}
}
@article{ENGLAND2008163,
title = {Rattling the cage: computational models of chaperonin-mediated protein folding},
journal = {Current Opinion in Structural Biology},
volume = {18},
number = {2},
pages = {163-169},
year = {2008},
note = {Theory and simulation / Macromolecular assemblages},
issn = {0959-440X},
doi = {https://doi.org/10.1016/j.sbi.2007.12.013},
url = {https://www.sciencedirect.com/science/article/pii/S0959440X08000067},
author = {Jeremy England and Del Lucent and Vijay Pande},
abstract = {Chaperonins are known to maintain the stability of the proteome by facilitating the productive folding of numerous misfolded or aggregation-prone proteins and are thus essential for cell viability. Despite their established importance, the mechanism by which chaperonins facilitate protein folding remains unknown. Computer simulation techniques are now being employed to complement experimental ones in order to shed light on this mystery. Here we review previous computational models of chaperonin-mediated protein folding in the context of the two main hypotheses for chaperonin function: iterative annealing and landscape modulation. We then discuss new results pointing to the importance of solvent (a previously neglected factor) in chaperonin activity. We conclude with our views on the future role of simulation in studying chaperonin activity as well as protein folding in other biologically relevant confined contexts.}
}
@article{STEPHEN2021103085,
title = {Automated essay scoring (AES) of constructed responses in nursing examinations: An evaluation},
journal = {Nurse Education in Practice},
volume = {54},
pages = {103085},
year = {2021},
issn = {1471-5953},
doi = {https://doi.org/10.1016/j.nepr.2021.103085},
url = {https://www.sciencedirect.com/science/article/pii/S1471595321001219},
author = {Tracey C. Stephen and Mark C. Gierl and Sharla King},
keywords = {Automated essay scoring, Constructed-response examinations, Nursing education assessment, Reliability measures},
abstract = {Nursing students’ higher-level thinking skills are ideally assessed through constructed-response items. At the baccalaureate level in North America, however, this exam format has largely fallen into disuse owing to the labor-intensive process of scoring written exam papers. The authors sought to determine if automated essay scoring (AES) would be an efficient and reliable alternative to human scoring. Four constructed-response exam items were administered to an initial cohort of 359 undergraduate nursing students in 2016 and to a second cohort of 40 students in 2018. The items were graded by two human raters (HR1 & HR2) and an AES software platform. AES approximated or surpassed agreement and reliability measures achieved by the HR1 and HR2 with each other, and AES surpassed both human raters in efficiency. A list of answer keywords was created to increase the efficiency and reliability of AES. Low agreement between human raters may be explained by rater drift and fatigue, and shortcomings in the development of Item 1 may have reduced its overall agreement and reliability measures. It can be concluded that AES is a reliable and cost-effective means of scoring constructed-response nursing examinations, but further studies employing greater sample sizes are needed to establish this definitively.}
}
@article{LEUNG2020345,
title = {Limited cognitive ability and selective information processing},
journal = {Games and Economic Behavior},
volume = {120},
pages = {345-369},
year = {2020},
issn = {0899-8256},
doi = {https://doi.org/10.1016/j.geb.2020.01.005},
url = {https://www.sciencedirect.com/science/article/pii/S0899825620300063},
author = {Benson Tsz Kin Leung},
keywords = {Limited ability, Information overload, Information avoidance, Confirmation bias, Wishful thinking, Polarization},
abstract = {This paper studies the information processing behavior of a decision maker (DM) who can only process a subset of all information he receives: before taking an action, the DM receives sequentially a number of signals and decides whether to process or ignore each of them as it is received. The model generates an information processing behavior consistent with that documented in the psychological literature: first, the DM chooses to process signals that are strong; second, his processing strategy exhibits confirmation bias if he has a strong prior belief; third, he tends to process signals that suggest favorable outcomes (wishful thinking). As an application I analyze how the Internet and the induced change in information availability affects the processing behavior of the DM. I show that providing more/better information to the DM could strengthen his confirming bias.}
}
@article{ORJI2022100626,
title = {Assessing the pre-conditions for the pedagogical use of digital tools in the Nigerian higher education sector},
journal = {The International Journal of Management Education},
volume = {20},
number = {2},
pages = {100626},
year = {2022},
issn = {1472-8117},
doi = {https://doi.org/10.1016/j.ijme.2022.100626},
url = {https://www.sciencedirect.com/science/article/pii/S1472811722000283},
author = {Ifeyinwa Juliet Orji and Frank Ojadi and Ukoha Kalu Okwara},
keywords = {Digitalization, Higher education, TOE theory, Social media, Learning outcomes, Nigeria},
abstract = {Currently, there is a burgeoning interest in digitalization as evidenced in extant literature. Nevertheless, the effect, based on teachers’ own perspectives, of the pedagogical use of digital technologies on learning outcomes in the higher education sector has been under-investigated. Thus, this paper aims to investigate the pre-conditions for the effective adoption of social media tools in the Nigerian higher education sector and to assess the impact of the adoption on specific learning outcomes. A multi-criteria decision-making (MCDM) methodology was proposed for study analysis, aided by views of experts with sufficient teaching experience in Nigerian business school programs. The results indicate that adequate budgetary allocations, technical competence, a sufficient level of privacy, and an effective government regulatory framework are the most important of the investigated pre-conditions. Additionally, the pedagogical use of social media in business school programs is more strongly associated with learning outcomes such as professionalism and strategic thinking, emotional intelligence, and social maturity. Hence, the article offers guidance to decision-makers in the higher education sector on how to actualize the successful adoption of social media for pedagogical use and build effective business strategies at various levels of the digitalization process.}
}
@article{WANG2024111842,
title = {Three-way decision based island harmony search algorithm for robust flow-shop scheduling with uncertain processing times depicted by big data},
journal = {Applied Soft Computing},
volume = {162},
pages = {111842},
year = {2024},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2024.111842},
url = {https://www.sciencedirect.com/science/article/pii/S1568494624006161},
author = {Bing Wang and Pengfei Zhang and Xiaozhi Wang and Quanke Pan},
keywords = {Robust flow-shop scheduling, Island harmony search, Big data, Three-way decision, Surrogate worst-case scenario},
abstract = {This paper discusses an uncertain two-machine permutation flow-shop scheduling problem (2PFSP) with total weighted tardiness and common due date. Uncertain processing times are described by a large set of discrete scenarios, which is a type of big data. The objective is to minimize the schedule performance under the worst-case scenario. Identifying the worst-case scenario for each evaluated schedule is quite time-consuming in the situation that the scenario set size is large so that the objective evaluation might be computationally expensive. To handle this difficulty, three-way decision is used to preprocess the large-size scenario set to get a reduced scenario set so that the concept of surrogate worst-case scenario is adopted. A hybrid harmony search algorithm of combining three-island framework and the scenario-based local search is developed to solve the discussed problem. Based on the single-scenario knowledge of 2PFSP, a problem-specific scenario-dependent neighborhood structure is constructed under the surrogate worst-case scenario. An extensive experiment was carried out. The computational results show that the application of surrogate worst-case scenario based on three-way decision is effective in reducing the time consuming while keeping schedule performance evaluation. Being compared to the worst-case scenario objective evaluation, for an example in the case of the middle bad-scenario ratio, the surrogate worst-case scenario objective evaluation made the solution algorithm save 12.95 % in average CPU time for all instances while the relative performance difference is only 1.809 % in average. Being compared to possible alternative algorithms derived from the state-of-the-art algorithms, the developed algorithm is advantageous for the addressed problems.}
}
@article{XU2023108916,
title = {Joint optimization task offloading and trajectory control for unmanned-aerial-vehicle-assisted mobile edge computing},
journal = {Computers and Electrical Engineering},
volume = {111},
pages = {108916},
year = {2023},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2023.108916},
url = {https://www.sciencedirect.com/science/article/pii/S0045790623003403},
author = {Fei Xu and Sen Wang and Weiya Su and Lin Zhang},
keywords = {Edge computing, Computation offloading, Deep reinforcement learning, Unmanned Aerial Vehicle, Trajectory control},
abstract = {The appearance of Mobile Edge Computing (MEC) and Unmanned Aerial Vehicle (UAV) is significant for the future progress of the Internet of Things (IoT). Since the system model with a continuous action space and high-dimensional state space, the joint optimization of UAV trajectory and the computational offloading problem is non-convex, and traditional algorithms for instance ant colony algorithm, genetic algorithm, Actor Critic (AC) algorithm, and Deep Deterministic Policy Gradient (DDPG) algorithm are difficult to cope with. Reasonably formulating the computational task offloading strategy and the trajectory control of the UAV is crucial for the high-efficiency completion of the task. In this paper, a computational offloading and trajectory control system model for UAV-assisted MEC is proposed. We seek to maximize the user ratio of coverage by jointly optimizing computing offload scheduling and UAV trajectories. We propose an improved DDPG algorithm to optimize the objective function and achieve the optimal solution. Meanwhile, our algorithm can achieve an improvement in the user rate of coverage while avoiding obstacles as compared with baseline algorithms, AC, and DDPG.}
}
@article{KINNEAR2024101190,
title = {Lecturers' use of questions in undergraduate mathematics lectures},
journal = {The Journal of Mathematical Behavior},
volume = {76},
pages = {101190},
year = {2024},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2024.101190},
url = {https://www.sciencedirect.com/science/article/pii/S0732312324000671},
author = {George Kinnear and Gemma Hood and Eloise Lardet and Colette Sheard and Colin Foster},
keywords = {Funneling, Lecturing styles, Questioning, Student participation, University mathematics},
abstract = {Mathematics lecturers frequently ask questions in their lectures, and these questions presumably play an important role in students’ thinking about and learning of the lecture content. We replicated and developed a coding scheme used in previous research in the US to categorise lecturers’ questions in a sample of 136 lectures given by 24 lecturers at a research-intensive UK university. We found that the coding scheme could be applied reliably, and that factual questions were predominant (as in previous research). We explore differences in the lecturers’ use of questions – both between our UK sample and the previous US work, and between individual lecturers in our sample. We note the presence of strings of related successive questions from the lecturer, which we term ‘question chains’. We explore the nature of these, examine their prevalence, and seek to account for them in terms of the lecturers’ possible intentions.}
}
@article{ZHOU2023126996,
title = {Coal consumption prediction in thermal power units: A feature construction and selection method},
journal = {Energy},
volume = {273},
pages = {126996},
year = {2023},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2023.126996},
url = {https://www.sciencedirect.com/science/article/pii/S0360544223003900},
author = {Jian Zhou and Wei Zhang},
keywords = {Thermal power units, Coal consumption prediction, Regression analysis, K-means algorithm, Genetic algorithm},
abstract = {Digitization and related facilities have enabled the thermal power generation enterprises to record real-time data of thermal power units. There are many data-driven applications based on real-time monitoring and operational data in power units, while limited studies lay on the operational improvements, especially on coal consumption prediction under all working conditions. We build an intelligent prediction model of coal consumption based on key features selection, working condition clustering, and regression analysis. We combine feature construction and feature selection methods to cope with the problem caused by directly specifying feature subset for model building of traditional prediction method, which may fall into the thinking pattern and miss potentially better feature subset. Besides, to cope with the different coal consumption under different working conditions, we apply cluster analysis to construct a sub-coal consumption prediction model for each cluster category. Numerical results show that compared with other methods, it has the advantages of lower regression error and moderate model complexity, which can provide efficient decision support for operational improvement in thermal power generation.}
}
@incollection{TONDEUR2024184,
title = {Chapter 6 - Batch control spike},
editor = {Yves Tondeur},
booktitle = {Sustainable Quality Improvements for Isotope Dilution in Molecular Ultratrace Analyses},
publisher = {Elsevier},
pages = {184-239},
year = {2024},
isbn = {978-0-443-29034-3},
doi = {https://doi.org/10.1016/B978-0-443-29034-3.00031-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780443290343000314},
author = {Yves Tondeur},
keywords = {Compliance with PBMS/methods innovation rule & ISO 17025, Critical step, Erroneous beliefs & mental models, Function of the labeled standards, Out-of-control analytical system, Performance improvement, Sample fortification integrity, Systematic errors, Technology-in-use definition, Thinking method, Traditional QC samples effectiveness, Working relative response factors},
abstract = {When we are unaware of the causes for the observed deviations, we are more likely to react. When we react or are unaware of problems, we fail. Increasing the effectiveness of quality control samples starts with addressing what we do not know about them. So, this chapter first clarifies what we want, then describes what we have, and lastly what can be done to fill the gaps. The development, validation, and application of the batch control spike is a great illustration highlighting the importance the quality of the performance feedback and learning capacity the control samples are supposed to provide. When done correctly—while contextually questioning the relevance and appropriateness of current operating criteria, imposed limits and standards—the introduction of the batch control spike allows a process of critical errors identification, compensation, and progressive elimination to take place so that at the end, no significant systematic errors remain. This fact is demonstrated using z-scores from multiple international round-robin studies. In the context of achieving accuracy, the batch control spike examines the relationships between the standards used, when they are spiked, how they are spiked, and their purpose, that is, it renders the technology-in-use (isotope dilution) transparent and keeps it honest. It is also an excellent teaching tool. It is a quality learning sample helping the transformation of our methods into “thinking tools.”}
}
@article{RIEBEL2024105084,
title = {Transient modeling of stratified thermal storage tanks: Comparison of 1D models and the Advanced Flowrate Distribution method},
journal = {Case Studies in Thermal Engineering},
volume = {61},
pages = {105084},
year = {2024},
issn = {2214-157X},
doi = {https://doi.org/10.1016/j.csite.2024.105084},
url = {https://www.sciencedirect.com/science/article/pii/S2214157X24011158},
author = {Adrian Riebel and Ian Wolde and Rodrigo Escobar and Rodrigo Barraza and José M. Cardemil},
keywords = {Sensible heat storage, TES, Thermal modeling, Transient simulation, Experimental validation},
abstract = {Thermal energy storage (TES) is one of the key technologies for enabling a higher deployment of renewable energy. In this context, the present study analyzes the modeling strategies of one of the most common TES systems: stratified thermal storage tanks. These systems are essential to many solar thermal installations and heat pumps, among other clean energy technologies. Three different one-dimensional tank models are compared by their computing speed and resilience to long time steps. Two of the models analyzed are numerical, one being explicit and the other one implicit, and the other is analytical. The models are validated against data from experiments carried out considering small-scale stratified tanks, showing that their performance can be improved by using the Advanced Flowrate Distribution (AFD) method. The results show that the analytical model maintains its accuracy with longer time steps and is robust against divergence. Conversely, the numerical models show equivalent performance for short time steps, while the computation time is reduced. Although the AFD method shows promising results by achieving an improvement of 43% in terms of Dynamic Time Warping, its parameter optimization must be generalized for different tank designs, flow rates, and temperatures.}
}
@article{LEONIDOV2022112279,
title = {Strategic stiffening/cooling in the Ising game},
journal = {Chaos, Solitons & Fractals},
volume = {160},
pages = {112279},
year = {2022},
issn = {0960-0779},
doi = {https://doi.org/10.1016/j.chaos.2022.112279},
url = {https://www.sciencedirect.com/science/article/pii/S0960077922004891},
author = {Andrey Leonidov and Ekaterina Vasilyeva},
keywords = {Binary choice game, Ising game, Graph, Forward-looking, Myopic, Noise},
abstract = {The dynamic noisy binary choice (Ising) game of forward-looking agents on a complete graph is analysed. It is shown that strategic considerations lead to effective interaction strengthening (noise reduction) as compared to the myopic game. We show that strategic agents are able to come to consensus in the wider range of noise values than myopic ones. Effective population dynamics with time-dependent probabilities reflecting this strategic stiffening/cooling effect is described.}
}
@article{HERTENSTEIN20191213,
title = {Modulation of creativity by transcranial direct current stimulation},
journal = {Brain Stimulation},
volume = {12},
number = {5},
pages = {1213-1221},
year = {2019},
issn = {1935-861X},
doi = {https://doi.org/10.1016/j.brs.2019.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S1935861X19302293},
author = {Elisabeth Hertenstein and Elena Waibel and Lukas Frase and Dieter Riemann and Bernd Feige and Michael A. Nitsche and Christoph P. Kaller and Christoph Nissen},
keywords = {Creativity, Flexibility, Transcranial direct current stimulation, Frontal cortex, Electroencephalography},
abstract = {Background
Creativity is the use of original ideas to accomplish something innovative. Previous research supports the notion that creativity is facilitated by an activation of the right and/or a deactivation of the left prefrontal cortex. In contrast, recent brain imaging studies suggest that creativity improves with left frontal activation.
Objective
The present study was designed to further elucidate the neural basis of and ways to modulate creativity, based on the modulation of prefrontal cortical activity through the non-invasive brain stimulation technique transcranial direct current stimulation (tDCS).
Methods
Ninety healthy University students performed three tasks on major aspects of creativity: conceptual expansion (Alternate Uses Task, AUT), associative thinking (Compound Remote Associate Task, CRA), and set shifting ability (Wisconsin Card Sorting Task, WCST). Simultaneously, they received cathodal stimulation of the left and anodal stimulation of the right inferior frontal gyrus (IFG), the reverse protocol, or sham stimulation.
Results
The main pattern of results was a superior performance with bilateral left cathodal/right anodal stimulation, and an inferior performance in the reversed protocol compared to sham stimulation. As a potential underlying physiological mechanism, resting state EEG beta power, indicative of enhanced cortical activity, in the right frontal area increased with anodal stimulation and was associated with better performance.
Conclusion
The findings provide new insights into ways of modulating creativity, whereby a deactivation of the left and an activation of the right prefrontal cortex with tDCS is associated with increased creativity. Potential future applications might include tDCS for patients with mental disorders and for healthy individuals in creative professions.}
}
@article{LU2024103920,
title = {The integrated multi-performance fast optimization strategy for battery thermal management system},
journal = {Case Studies in Thermal Engineering},
volume = {54},
pages = {103920},
year = {2024},
issn = {2214-157X},
doi = {https://doi.org/10.1016/j.csite.2023.103920},
url = {https://www.sciencedirect.com/science/article/pii/S2214157X23012261},
author = {Hao Lu and Xiaole Tang and Hongchang Li and Wenjun Zhao and Xiqiang Chang and Weifang Lin},
keywords = {Short-cut computation, Computational fluid dynamics, Weighted average, Optimization algorithm},
abstract = {Increased battery energy density is required to boost electric vehicle endurance; however, this also raises the possibility of thermal runaway and power battery explosion. Improving the cooling system performance requires optimization and enhancement of classical systems. Traditional design approaches struggle to simultaneously enhance multiple aspects of performance, while an optimization based on Computational Fluid Dynamics (CFD) methods is often inefficient. Therefore, by integrating a flow resistance network model (FRNM) with a weighted average optimization algorithm (INFO), an efficient optimization for the comprehensive performance of the system can be achieved. Five optimized systems under different airflow rates were obtained through optimization. A comparison with two existing systems validated the effectiveness of the optimized system. The results demonstrate that, compared to the two reference systems, the optimized system decreases the maximum temperature difference by 65.51 % and 39.07 %, respectively. Furthermore, the improvement in temperature uniformity is more significant, increasing by 63.76 % and 34.40 %, respectively.}
}
@article{RAMAMURTHY20124247,
title = {Design for sustainability: The role of CAD},
journal = {Renewable and Sustainable Energy Reviews},
volume = {16},
number = {6},
pages = {4247-4256},
year = {2012},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2012.03.009},
url = {https://www.sciencedirect.com/science/article/pii/S1364032112001918},
author = {Sudhir {Rama Murthy} and Monto Mani},
keywords = {Design technology, Sustainability, Design process, Design tools, Creativity, Computer Aided Design},
abstract = {The term design in this paper particularly refers to the process (verb) and less to the outcome or product. Design comprises a complex set of activities today involving both man and machine. Sustainability is a fundamental paradigm and carries significance in any process, natural or manmade, and its outcome. In simple terms, sustainability implies a state of sustainable living, viz. health and continuity, nurtured by diversity and evolution (innovations) in an ever-changing world. Design, in a similar line, has been comprehensively investigated and its current manifestations including design-aids (Computer Aided Design) have been evaluated in terms of sustainability. The paper investigates the rationale of sustainability to design as a whole – its purpose, its adoption in the natural world, its relevance to humankind and the technologies involved. Throughout its history, technology has been used to aid design. But in the current context of advanced algorithms and computational capacity, design no longer remains an exclusively animate faculty. Given this scenario, investigating sustainability in the light of advanced design aids such as CAD becomes pertinent. Considering that technology plays a part in design activities, the paper explores where technology must play a part and to what degree amongst the various activities that comprise design. The study includes an examination of the morphology of design and the development of a systems-thinking integrated forecasting model to evaluate the implications of CAD tools in design and sustainability. The results of the study along with a broad range of recommendations have been presented.}
}
@article{LIANG2019341,
title = {Is ecoregional scale precise enough for lake nutrient criteria? Insights from a novel relationship-based clustering approach},
journal = {Ecological Indicators},
volume = {97},
pages = {341-349},
year = {2019},
issn = {1470-160X},
doi = {https://doi.org/10.1016/j.ecolind.2018.10.034},
url = {https://www.sciencedirect.com/science/article/pii/S1470160X18308094},
author = {Zhongyao Liang and Yong Liu and Huili Chen and Yao Ji},
keywords = {Spatial scale, Nutrient criteria, Relationship-based clustering approach, Relationship mapping, Hierarchical clustering, Leave-one-out cross-validation},
abstract = {While the ecoregional lake nutrient criteria have been widely used in the past two decades, the overconfidence on their applicability may mislead the pollution management decisions, considering the spatial heterogeneity within the ecoregion. The exploration of applicability is thereby important, but is hindered by the difficulty in recognizing reliable relationship patterns between the nutrient and management endpoint. We propose a novel relationship-based clustering approach (RCA) to explore whether the ecoregional scale is precise enough for nutrient criteria. The approach (a) simulates relationships using Bayesian Linear Models, (b) clusters lakes according to relationship similarities via relationship mapping and hierarchical clustering, and (c) identifies reliable relationship patterns based on the leave-one-out cross-validation. The RCA is then employed to explore Chlorophyll a-total phosphorus relationships of 34 lakes in four Ecological Drainage Units (EDUs) in the U.S. Long-term water quality data is from a newly established database (LAGOS-NE). The results show that multiple relationship patterns exist in all the EDUs. The ecoregional relationships misestimate the nutrient effect in over a half of lakes. Therefore, we determine that the ecoregional scale is not precise enough for nutrient criteria and the sub-ecoregional scale is then recommended. Besides, the RCA provides a backward thinking for determining the spatial scale and can be used in some other fields where relationship-based clustering is needed.}
}