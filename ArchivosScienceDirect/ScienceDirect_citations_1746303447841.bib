@article{LEESON2008630,
title = {Cognitive ability, personality, and academic performance in adolescence},
journal = {Personality and Individual Differences},
volume = {45},
number = {7},
pages = {630-635},
year = {2008},
issn = {0191-8869},
doi = {https://doi.org/10.1016/j.paid.2008.07.006},
url = {https://www.sciencedirect.com/science/article/pii/S0191886908002444},
author = {Peter Leeson and Joseph Ciarrochi and Patrick C.L. Heaven},
keywords = {Cognitive ability, Personality, Academic performance, Adolescents, Hope, Self-esteem, Attributional style, Psychometric },
abstract = {Does positive thinking predict variance in school grades over and above that predicted by cognitive ability? Six hundred and thirty nine high school students participated in a three-year longitudinal study that predicted grades using cognitive ability and three positive thinking variables – self-esteem, hope, and attributional style. Hope, positive attributional style and cognitive ability predicted higher grades, whilst self-esteem was a less consistent predictor of academic performance. Structural equation modelling revealed significant paths from cognitive ability, gender, and a second order positive thinking factor to grades. The results suggest that intelligence, gender, and positive thinking each play a unique role in predicting academic performance in youth. Some suggestions for further research are made.}
}
@article{BATT2002185,
title = {Lateral thinking: 2-D interpretation of thermochronology in convergent orogenic settings},
journal = {Tectonophysics},
volume = {349},
number = {1},
pages = {185-201},
year = {2002},
note = {Low Temperature Thermochronology: From Tectonics to Landscape Evolution},
issn = {0040-1951},
doi = {https://doi.org/10.1016/S0040-1951(02)00053-7},
url = {https://www.sciencedirect.com/science/article/pii/S0040195102000537},
author = {Geoffrey E. Batt and Mark T. Brandon},
keywords = {Lateral motion, Thermochronology, Orogenic regions},
abstract = {Lateral motion of material relative to the regional thermal and kinematic frameworks is important in the interpretation of thermochronology in convergent orogens. Although cooling ages in denuded settings are commonly linked to exhumation, such data are not related to instantaneous behavior but rather to an integration of the exhumation rates experienced between the thermochronological ‘closure’ at depth and subsequent exposure at the surface. The short spatial wavelength variation of thermal structure and denudation rate typical of orogenic regions thus renders thermochronometers sensitive to lateral motion during exhumation. The significance of this lateral motion varies in proportion with closure temperature, which controls the depth at which isotopic closure occurs, and hence, the range of time and length scales over which such data integrate sample histories. Different chronometers thus vary in the fundamental aspects of the orogenic character to which they are sensitive. Isotopic systems with high closure temperature are more sensitive to exhumation paths and the variation in denudation and thermal structure across a region, while those of lower closure temperature constrain shorter-term behaviour and more local conditions. Discounting lateral motion through an orogenic region and interpreting cooling ages purely in terms of vertical exhumation can produce ambiguous results because variation in the cooling rate can result from either change in kinematics over time or the translation of samples through spatially varying conditions. Resolving this ambiguity requires explicit consideration of the physical and thermal framework experienced by samples during their exhumation. This can be best achieved through numerical simulations coupling kinematic deformation to thermal evolution. Such an approach allows the thermochronological implications of different kinematic scenarios to be tested, and thus provides an important means of assessing the contribution of lateral motion to orogenic evolution.}
}
@article{JACKSON201386,
title = {Airflow reversal and alternating corkscrew vortices in foredune wake zones during perpendicular and oblique offshore winds},
journal = {Geomorphology},
volume = {187},
pages = {86-93},
year = {2013},
issn = {0169-555X},
doi = {https://doi.org/10.1016/j.geomorph.2012.12.037},
url = {https://www.sciencedirect.com/science/article/pii/S0169555X13000081},
author = {Derek W.T. Jackson and Meiring Beyers and Irene Delgado-Fernandez and Andreas C.W. Baas and Andrew J. Cooper and Kevin Lynch},
keywords = {Computational fluid dynamics, Aeolian, Foredunes, Transport, Airflow modelling, Lee side eddies},
abstract = {On all sandy coastlines fringed by dunes, understanding localised air flow allows us to examine the potential sand transfer between the beach and dunes by wind-blown (Aeolian) action. Traditional thinking into this phenomenon had previously included only onshore winds as effective drivers of this transfer. Recent research by the authors, however, has shown that offshore air-flow too can contribute significantly, through lee-side back eddies, to the overall windblown sediment budget to coastal dunes. Under rising sea levels and increased erosion scenarios, this is an important process in any post-storm recovery of sandy beaches. Until now though, full visualisation in 3D of this newly recognised mechanism in offshore flows has not been achieved. Here, we show for the first time, this return flow eddy system using 3D computational fluid dynamics modelling, and reveal the presence of complex corkscrew vortices and other phenomena. The work highlights the importance of relatively small surface undulations in the dune crest which act to induce the spatial patterns of airflow (and transport) found on the adjacent beach.}
}
@article{RIVAS20011369,
title = {Computational identification of noncoding RNAs in E. coli by comparative genomics},
journal = {Current Biology},
volume = {11},
number = {17},
pages = {1369-1373},
year = {2001},
issn = {0960-9822},
doi = {https://doi.org/10.1016/S0960-9822(01)00401-8},
url = {https://www.sciencedirect.com/science/article/pii/S0960982201004018},
author = {Elena Rivas and Robert J. Klein and Thomas A. Jones and Sean R. Eddy},
abstract = {Some genes produce noncoding transcripts that function directly as structural, regulatory, or even catalytic RNAs 1, 2. Unlike protein-coding genes, which can be detected as open reading frames with distinctive statistical biases, noncoding RNA (ncRNA) gene sequences have no obvious inherent statistical biases [3]. Thus, genome sequence analyses reveal novel protein-coding genes, but any novel ncRNA genes remain invisible. Here, we describe a computational comparative genomic screen for ncRNA genes. The key idea is to distinguish conserved RNA secondary structures from a background of other conserved sequences using probabilistic models of expected mutational patterns in pairwise sequence alignments. We report the first whole-genome screen for ncRNA genes done with this method, in which we applied it to the “intergenic” spacers of Escherichia coli using comparative sequence data from four related bacteria. Starting from >23,000 conserved interspecies pairwise alignments, the screen predicted 275 candidate structural RNA loci. A sample of 49 candidate loci was assayed experimentally. At least 11 loci expressed small, apparently noncoding RNA transcripts of unknown function. Our computational approach may be used to discover structural ncRNA genes in any genome for which appropriate comparative genome sequence data are available.}
}
@article{ZENASNI2009353,
title = {Perception of emotion, alexithymia and creative potential},
journal = {Personality and Individual Differences},
volume = {46},
number = {3},
pages = {353-358},
year = {2009},
issn = {0191-8869},
doi = {https://doi.org/10.1016/j.paid.2008.10.030},
url = {https://www.sciencedirect.com/science/article/pii/S0191886908004108},
author = {F. Zenasni and T.I. Lubart},
keywords = {Creativity, Ability EI, Alexythimia, Emotional creativity, Divergent thinking},
abstract = {Theoretical proposals suggest that emotional intelligence (EI) may favor creativity. In the present paper, two studies are reported with French adults to examine the degree to which the ability to identify emotion is related to creative performance. This component of ability EI was hypothesized to be positively associated with a divergent thinking task involving emotional information. Contrary to our expectations, the first study (n=95) indicated that ability to identify emotions in faces and images was negatively related to idea generation ability. The second study (n=100) including a measure of alexithymia confirmed this relation. Moreover, evaluating emotional creativity, we observed a significant negative link between the ability to identify emotions and the tendency to experience emotions differently from those of others. We discuss these results suggesting an opposition between consensual/convergent thinking concerning emotions (ability EI) and divergent thinking.}
}
@article{LI2024141569,
title = {Programming experiment course for innovative and sustainable education: A case study of Java for Millikan Oil-Drop experiment},
journal = {Journal of Cleaner Production},
volume = {447},
pages = {141569},
year = {2024},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2024.141569},
url = {https://www.sciencedirect.com/science/article/pii/S0959652624010175},
author = {Yizheng Li and Guandong Su and Haocheng Pan and Chengwei Tan and Gongsheng Li},
keywords = {Laboratory instruction, Java object-oriented programming, Innovative and sustainable education, Student-centered learning},
abstract = {A teaching approach of programming experiment courses with a new education process is proposed in this study to improve the sustainability of engineering education and extend the accessibility of lab experiments across experiment courses, with the goal of encouraging innovative and scientific thinking among students. The Millikan Oil-Drop experiment combined with Java object-oriented programming is demonstrated as a case study to validate the feasibility and advantages of this teaching approach. The new education process of the experiment course is designed based on Jean Piaget's cognitive development theory, which has high practical potential for popularization among tertiary institutions without additional cost. Additionally, this work discussed the relevance of the general criterion of the Accreditation Board for Engineering and Technology on student outcomes to educational accreditation, further indicating that introducing programming into experiment education can improve students' all-round ability and strengthen the triangular relationship among the three main subjects in tertiary education, namely, students, faculty, and higher educational institutions. This study may serve as an educational guide for teachers and tertiary institutions to pursue innovative and sustainable education.}
}
@article{GADALLA2023200201,
title = {Concepts and experiments on psychoanalysis driven computing},
journal = {Intelligent Systems with Applications},
volume = {18},
pages = {200201},
year = {2023},
issn = {2667-3053},
doi = {https://doi.org/10.1016/j.iswa.2023.200201},
url = {https://www.sciencedirect.com/science/article/pii/S2667305323000261},
author = {Minas Gadalla and Sotiris Nikoletseas and José Roberto {de A. Amazonas} and José D.P. Rolim},
keywords = {Lacanian discourses, Psychoanalysis computing, GPT-3},
abstract = {This research investigates the effective incorporation of the human factor and user perception in text-based interactive media. In such contexts, the reliability of user texts is often compromised by behavioural and emotional dimensions. To this end, several attempts have been made in the state of the art, to introduce psychological approaches in such systems, including computational psycholinguistics, personality traits and cognitive psychology methods. In contrast, our method is fundamentally different since we employ a psychoanalysis-based approach; in particular, we use the notion of Lacanian discourse types, to capture and deeply understand real (possibly elusive) characteristics, qualities and contents of texts, and evaluate their reliability. As far as we know, this is the first time computational methods are systematically combined with psychoanalysis. We believe such psychoanalytic framework is fundamentally more effective than standard methods, since it addresses deeper, quite primitive elements of human personality, behaviour and expression which usually escape methods functioning at “higher”, conscious layers. In fact, this research is a first attempt to form a new paradigm of psychoanalysis-driven interactive technologies, with broader impact and diverse applications. To exemplify this generic approach, we apply it to the case-study of fake news detection; we first demonstrate certain limitations of the well-known Myers–Briggs Type Indicator (MBTI) personality type method, and then propose and evaluate our new method of analysing user texts and detecting fake news based on the Lacanian discourses psychoanalytic approach.}
}
@article{ARUN2009S1115,
title = {P03-116 Damage to object oriented programming in the brain explains many of the psychopathological features of schizophrenia},
journal = {European Psychiatry},
volume = {24},
pages = {S1115},
year = {2009},
note = {17th EPA Congress - Lisbon, Portugal, January 2009, Abstract book},
issn = {0924-9338},
doi = {https://doi.org/10.1016/S0924-9338(09)71348-3},
url = {https://www.sciencedirect.com/science/article/pii/S0924933809713483},
author = {C.P. Arun},
abstract = {Introduction
Modern computers often use programs that incorporate a programming technique called Object Oriented Programming (OOP), allowing users to manipulate complex ‘computational objects’ such as menus, screen windows, etc with very little effort, say the click of a mouse. OOP deals with structures called objects and allows time and computational effort saving devices such as inheritance, polymorphism and encapsulation. We examine whether the brain itself may use OOP and if representation of objects suffers a breakdown in schizophrenia.
Review of literature
Previous models fail to provide a unifying explanation with a computational basis that could explain the psychopathology in schizophrenia. METHODS Using the object oriented programming language JavaTM we designed a system of self-objects named ‘hand’, ‘action monitor’ etc interacting with non-self objects ‘scissors’, ‘hammer’, ‘wall’, etc. In computational experiments, we allow the ‘action monitor’ to fail; the features of disparate objects are allowed to merge, some features of an object are allowed to be shared with other objects, etc.
Results
By transposing only a few lines of code, it is possible to duplicate various features of the psychopathology of schizophrenia.
Discussion
Our model can demonstrate overinclusion (overabstraction), concrete thinking (underabstraction), loss of ego boundaries (conjoining of disparate objects), delusions (misattribution of object function), lack of insight (poor monitoring of object activity) and passivity (loss of monitoring and misattribution of object activity).
Conclusion
The brain must use the OOP model in its computations. Failure of object representation and manipulation must lie at the core of the psychopathology of schizophrenia.}
}
@article{MARENDA20232152,
title = {Sliding pendulum isolators without secretes},
journal = {Procedia Structural Integrity},
volume = {44},
pages = {2152-2157},
year = {2023},
note = {XIX ANIDIS Conference, Seismic Engineering in Italy},
issn = {2452-3216},
doi = {https://doi.org/10.1016/j.prostr.2023.01.275},
url = {https://www.sciencedirect.com/science/article/pii/S2452321623002846},
author = {Ivan Marenda and Agostino Marioni and Marco Banfi and Roberto Dalpedri},
keywords = {friction coefficient, pendulum device, contact, isolation system},
abstract = {Over the last decades, anti-seismic devices have gained increasing interest in the civil engineering field. The introduction of the base isolation system has led to a new concept in the construction panorama in terms of human life safety, a new way of thinking on new constructions, improvement and retrofitting on existent structures. Therefore, rubber and friction isolators have been deeply investigated to hence performances and predict dynamic behaviour during an earthquake. While the response of the former is characterised by the composition of the elastomeric compound, the latter features special materials able to dissipate energy by moving on smooth surfaces. This paper focuses on friction pendulum devices and addresses its attention on the behaviour of sliding materials. It is well-known that stick-slip phenomenon occurs when friction excitation is present and, in the anti-seismic field is important to reduce it and have a well-representative mathematical law able to describe it. Therefore, Hirun International after performing several treatments of the sliding materials has set up a special processing to guarantee a stable response of the HI-M material used on pendulum devices. The paper, after a brief presentation of the special sliding material, shows a comparison between the material with and without the treatment in terms of the force-displacement law. The paper also analyses in detail the cinematic behaviour of the sliding pendulum with one or two main sliding surfaces, with and without central articulation and determines the stress distribution in the sliding surfaces for the different cases.}
}
@incollection{OBRIEN2014141,
title = {7 - Reasoning with graphs},
editor = {Jamie O’Brien},
booktitle = {Shaping Knowledge},
publisher = {Chandos Publishing},
pages = {141-174},
year = {2014},
series = {Chandos Information Professional Series},
isbn = {978-1-84334-751-4},
doi = {https://doi.org/10.1533/9781780634326.141},
url = {https://www.sciencedirect.com/science/article/pii/B9781843347514500078},
author = {Jamie O’Brien},
keywords = {graph databases, logic and computing, reasoning, spatial data structures, visualization},
abstract = {Abstract:
Knowledge complexity poses a problem to the modeller of representation and, in turn, of reasoning. We seek to overcome this problem by using our ‘privileged’ sense of vision. This means that we render dynamic, multi-modal phenomena as graphic depictions, be they technical visualizations, thought experiments, logic constructions or network graphs. The ‘graphic act’ has been described a being a fundamental activity in human perception, and both scientists and artists have undertaken advanced analyses of the human perception of nature based on visual experiments. Analysis based on reasoning is similarly a graphic act, in the sense that it seeks out patterns of connectivity among socio-spatial agents and entities. Reasoning also depends upon symbolism, which serves to overcome the problem of infinity in nature (a matter that lies at the heart of machine computation). Hence, this chapter introduces some elements in logical reasoning, set theory and computation, and outlines the particular importance of working with symbols. It also provides an introduction to data modelling with graphs, including current advances in graph databases. It provides some ‘tools for thinking’ about knowledge complexity and suggests the potential power in adapting these technologies to organize knowledge of dynamic, complex domains. It also introduces some standard methods for spatial data modelling, including powerful surface network models, which borrow from physical landscape analysis, to support reasoning about knowledge-driven domains.}
}
@article{CHANG2025101578,
title = {Effects of light variations on drone’s visual positioning},
journal = {Internet of Things},
volume = {31},
pages = {101578},
year = {2025},
issn = {2542-6605},
doi = {https://doi.org/10.1016/j.iot.2025.101578},
url = {https://www.sciencedirect.com/science/article/pii/S2542660525000915},
author = {Che-Cheng Chang and Po-Ting Wu and Bo-Yu Liu and Bo-Ren Chen},
keywords = {Drone visual positioning, Convolutional neural network (CNN), Light variations},
abstract = {Positioning systems and algorithms play a crucial role in drone applications. Although Global Positioning Systems (GPS) are the most widely used method for drone localization, they are not always reliable and accurate in some scenarios. A recent study explores the visual-based positioning method, using Convolutional Neural Networks (CNNs) to match geometric features for drone positioning. The authors use an orthophotomap obtained from an actual drone to evaluate their algorithm. This can reduce the gap between research and practical operation. However, the approach overlooks the impact of lighting variations on positioning performance, i.e., brightness and color temperature. To address this limitation, we propose a novel CNN architecture to handle lighting variations. Our method improves reliability, accuracy, and computational complexity under varying lighting conditions by incorporating several critical components into the network. Remarkably, our architecture has only 51.35% trainable parameters and 83.97% floating point operations (FLOPs) of the existing one. Still, we can exceed it by 3.73% while not considering light variations and average 2.36% while considering light variations. The experimental results, also derived from an orthophotomap obtained via an actual drone, demonstrate that our approach effectively mitigates the challenges induced by lighting changes, ensuring reliable and accurate drone localization.}
}
@article{CARBONARO20101098,
title = {Computer-game construction: A gender-neutral attractor to Computing Science},
journal = {Computers & Education},
volume = {55},
number = {3},
pages = {1098-1111},
year = {2010},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2010.05.007},
url = {https://www.sciencedirect.com/science/article/pii/S0360131510001399},
author = {Mike Carbonaro and Duane Szafron and Maria Cutumisu and Jonathan Schaeffer},
keywords = {Computing Science, Females in Science, Computer game construction},
abstract = {Enrollment in Computing Science university programs is at a dangerously low level. A major reason for this is the general lack of interest in Computing Science by females. In this paper, we discuss our experience with using a computer game construction environment as a vehicle to encourage female participation in Computing Science. Experiments with game construction in grade 10 English classes showed that females enjoyed this activity as much as males and were just as successful. In this paper, we argue that: a) computer game construction is a viable activity for teaching higher-order thinking skills that are essential for Science; b) computer game construction that involves scripting teaches valuable Computing Science abstraction skills; c) this activity is an enjoyable introduction to Computing Science; and d) outcome measures for this activity are not male-dominated in any of the three aspects (higher-order thinking, Computing Science abstraction skills, activity enjoyment). Therefore, we claim that this approach is a viable gender-neutral approach to teaching Computing Science in particular and Science in general that may increase female participation in the discipline.}
}
@incollection{WARE2021425,
title = {Chapter Twelve - Designing Cognitively Efficient Visualizations},
editor = {Colin Ware},
booktitle = {Information Visualization (Fourth Edition)},
publisher = {Morgan Kaufmann},
edition = {Fourth Edition},
pages = {425-456},
year = {2021},
series = {Interactive Technologies},
isbn = {978-0-12-812875-6},
doi = {https://doi.org/10.1016/B978-0-12-812875-6.00012-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128128756000128},
author = {Colin Ware},
keywords = {Interactive visualization design, Visual thinking, Visual thinking design patterns, Visual working memory, Visualization design, Visualization types},
abstract = {A design methodology for producing cognitively efficient visualizations is introduced. The method involves seven steps (1) a high-level cognitive task description; (2) a data inventory; (3) cognitive task refinement; (4) identification of appropriate visualization types; (5) applying visual thinking design patterns (VTDPs); (6) prototype development; and (7) evaluation and design refinement. Most of the chapter is devoted to a set of VTDPs. These are descriptions of interactive visualization methods that have demonstrated value, together with a description of perceptual and cognitive issues relating to their use and guidelines for applicability. VDTPs provide a method for taking into account perceptual and cognitive issues in designing interaction especially with respect to key bottlenecks in the visual thinking process, such as limited visual working memory capacity. They also provide a way of reasoning about semiotic issues in perceptual terms via the concept of the visual query.}
}
@article{ANDROUTSOPOULOS2024100817,
title = {Scaling as method: A three-stage, mixed-methods approach to digital discourse analysis},
journal = {Discourse, Context & Media},
volume = {62},
pages = {100817},
year = {2024},
issn = {2211-6958},
doi = {https://doi.org/10.1016/j.dcm.2024.100817},
url = {https://www.sciencedirect.com/science/article/pii/S2211695824000631},
author = {Jannis Androutsopoulos},
keywords = {Scaling, Mixed-methods, Digital discourse, Abduction, Indignation mark, Reddit},
abstract = {Drawing on research on graphic contextualization cues in punctuation and typography, this paper describes a three-stage, mixed-methods approach to digital discourse analysis. It introduces the terms ‘scale’ and ‘scaling’ as methodological metaphors for a researcher’s planned, yet contingent movement through formations of digital textual data that differ in terms of volume, method of collection, processing, and analysis. ‘Scaling-as-method’ aims to replace static binaries (such as ‘micro’ and ‘macro’, ‘small’ and ‘big’ data, ‘manual’ and ‘automated’ processing) by the vision of a researcher who shifts their degree of abstraction, or ‘distance’, towards digital data, while moving from close to distant reading and back again. The paper exemplifies this three-stage process on the example of the indignation mark, aka <!!1>, a twist on the iterated exclamation mark that is attested in digital discourse in various languages as a cue of double-voicing. The explorative examination of a small dataset (Stage 1) leads to the computational collection and distributional analysis of a much larger dataset (‘scaling up’, Stage 2), followed by the manual annotation of a selected subset of this data (‘scaling down’, Stage 3). Each stage draws on a different amount of data, which enables different techniques of processing and analysis, and relies on a specific combination of abductive, deductive, and inductive reasoning. Yet all three stages complement one another in a kaleidoscopic way towards understanding connections between punctuation practices and participatory political discourse online. Scaling as method is not a closed recipe, but an adaptable procedure that can be applied to a variety of discrete digital features. It does not aim to replace established methods of computational social media analysis, but to boost research that is predominantly based on the manual collection and annotation of social media data, and to enables a dialogue between multiple understandings of context.}
}
@incollection{MAYER2010273,
title = {Problem Solving and Reasoning},
editor = {Penelope Peterson and Eva Baker and Barry McGaw},
booktitle = {International Encyclopedia of Education (Third Edition)},
publisher = {Elsevier},
edition = {Third Edition},
address = {Oxford},
pages = {273-278},
year = {2010},
isbn = {978-0-08-044894-7},
doi = {https://doi.org/10.1016/B978-0-08-044894-7.00487-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780080448947004875},
author = {R.E. Mayer},
keywords = {Convergent thinking, Creativity, Deductive reasoning, Directed thinking, Divergent thinking, Einstellung, Everyday thinking, Expert problem solving, Functional fixedness, Ill-defined problem, Inductive reasoning, Insight, Means-ends analysis, Nonroutine problem, Problem solving, Problem space, Productive thinking, Reasoning, Reproductive thinking, Routine problem, Thinking, Transfer, Well-defined problem},
abstract = {A major goal of education is to help students become effective problem solvers, that is, people who can generate useful and original solutions when they are confronted with problems they have never seen before. This article covers definitions of problem solving and reasoning, types of problems, cognitive processes and types of knowledge in problem solving, rigidity in thinking, problem-solving transfer, the distinction between productive and reproductive thinking, the nature of insight, problem space and search processes, and problem solving in realistic situations.}
}
@article{LI201985,
title = {Government accounting optimization based on computational linguistics},
journal = {Cognitive Systems Research},
volume = {57},
pages = {85-91},
year = {2019},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2018.10.017},
url = {https://www.sciencedirect.com/science/article/pii/S1389041718304650},
author = {Jiyou Li},
keywords = {Computational linguistics, Accounting optimization, Research},
abstract = {The level of moral development and moral intensity in cognitive psychology will not only affect the ethical behavior of accountants, but also have a direct impact on the quality and level of accounting work. Therefore, in this paper, the ethical behavior of accountants was analyzed from the perspective of cognitive psychology. Computer-aided data mining techniques were introduced, and government accounting risk assessment management of financial accountants was studied. In this paper, the principle of cognitive psychology to measure the ethical level of accountants was first described. The predicament of moral judgments was analyzed and an optimization plan to improve the ethical intention of accountants was proposed. Support Vector Machine classification technology in data mining was studied to explore how to conduct effective and reliable evaluation, so as to provide a scientific basis for decision-making in improving accounting management. After the simulation experiment, it is proved that continuously improving the ethical standards of accountants and strengthening the forecast of accounting risks can continue to optimize the accounting office management.}
}
@article{ROSSITER20083713,
title = {Compromises between feasibility and performance within linear MPC},
journal = {IFAC Proceedings Volumes},
volume = {41},
number = {2},
pages = {3713-3718},
year = {2008},
note = {17th IFAC World Congress},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20080706-5-KR-1001.00627},
url = {https://www.sciencedirect.com/science/article/pii/S147466701639526X},
author = {J.A. Rossiter and Yihang Ding},
keywords = {Constraints, Feasibility, Performance, Computational Efficiency, Contours},
abstract = {This paper explores the issues of feasibility and performance within predictive control. Conventional thinking is that there is typically a trade off between performance and the volume of the feasible region. However, this paper seeks to show that the trade off is often not as stark as might be expected and in fact one can sometimes gain huge amounts in feasibility with an almost negligible loss in performance while using a simple and conventional MPC algorithm.}
}
@article{YANG202075,
title = {Local temporal-spatial multi-granularity learning for sequential three-way granular computing},
journal = {Information Sciences},
volume = {541},
pages = {75-97},
year = {2020},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2020.06.020},
url = {https://www.sciencedirect.com/science/article/pii/S0020025520306009},
author = {Xin Yang and Yingying Zhang and Hamido Fujita and Dun Liu and Tianrui Li},
keywords = {Three-way granular computing, Sequential three-way decision, Local neighborhood, Temporal-spatial, Multi-granularity},
abstract = {Based on multiple levels of granularity, the notion of sequential three-way granular computing focuses on a multiple stages of thinking, problem-solving, and information processing in threes. This paper interprets, represents, and implements sequential three-way granular computing by a framework of temporal-spatial multi-granularity learning, which is described with the temporality of data and the spatiality of parameters. In real-world decision-making, such a sequential approach is useful to make faster decisions for some objects with the lower cost of decision process and the acceptable accuracy when information is insufficient or unavailable. However, the cost of time-consuming computation for hierarchical multilevel granularity is our concern. To address this issue, we utilize a local strategy to accelerate a sequence of neighborhood-based granulation induced by Gaussian kernel function. Subsequently, local three-way decision rules are investigated based on the Bayesian minimum risk criterion. Moreover, by the construction of a novel local trisection model, we propose a local sequential approach of three-way granular computing under a temporal-spatial multilevel granular structure. Finally, a series of comparative experiments between global and local perspectives is carried out to verify the effectiveness of our proposed models.}
}
@article{RONAYNE2021318,
title = {Evaluating the sunk cost effect},
journal = {Journal of Economic Behavior & Organization},
volume = {186},
pages = {318-327},
year = {2021},
issn = {0167-2681},
doi = {https://doi.org/10.1016/j.jebo.2021.03.029},
url = {https://www.sciencedirect.com/science/article/pii/S0167268121001293},
author = {David Ronayne and Daniel Sgroi and Anthony Tuckwell},
keywords = {Sunk cost effect, Sunk cost fallacy, Endowment effect, Cognitive ability, Psychological scales, Scale validation},
abstract = {We provide experimental evidence of behavior consistent with the sunk cost effect. Subjects who earned a lottery via a real-effort task were given an opportunity to switch to a dominant lottery; 23% chose to stick with their dominated lottery. The endowment effect accounts for roughly only one third of the effect. Subjects’ capacity for cognitive reflection is a significant determinant of sunk cost behavior. We also find stocks of knowledge or experience (crystallized intelligence) predict sunk cost behavior, rather than algorithmic thinking (fluid intelligence) or the personality trait of openness. We construct and validate a scale, the “SCE-8”, which encompasses many resources individuals can spend, and offers researchers an efficient way to measure susceptibility to the sunk cost effect.}
}
@incollection{HERLIHY201469,
title = {Chapter 4 - Colorless Wait-Free Computation},
editor = {Maurice Herlihy and Dmitry Kozlov and Sergio Rajsbaum},
booktitle = {Distributed Computing Through Combinatorial Topology},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {69-95},
year = {2014},
isbn = {978-0-12-404578-1},
doi = {https://doi.org/10.1016/B978-0-12-404578-1.00004-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780124045781000048},
author = {Maurice Herlihy and Dmitry Kozlov and Sergio Rajsbaum},
keywords = {Configurations, Executions, Layered executions, Layered protocols, Processes, Protocols, Schedules, Tasks},
abstract = {We outline the basic connection between distributed computing and combinatorial topology in terms of two formal models: a conventional operational model, in which systems consist of communicating state machines whose behaviors unfold over time, and the combinatorial model, in which all possible behaviors are captured statically using topological notions. We start with one particular system model (shared memory) and focus on a restricted (but important) class of problems (so-called “colorless” tasks).}
}
@article{LOWE20219898316,
title = {In-Depth Computational Analysis of Natural and Artificial Carbon Fixation Pathways},
journal = {BioDesign Research},
volume = {2021},
pages = {9898316},
year = {2021},
issn = {2693-1257},
doi = {https://doi.org/10.34133/2021/9898316},
url = {https://www.sciencedirect.com/science/article/pii/S2693125724000566},
author = {Hannes Löwe and Andreas Kremling},
abstract = {In the recent years, engineering new-to-nature CO2- and C1-fixing metabolic pathways made a leap forward. New, artificial pathways promise higher yields and activity than natural ones like the Calvin-Benson-Bassham (CBB) cycle. The question remains how to best predict their in vivo performance and what actually makes one pathway “better” than another. In this context, we explore aerobic carbon fixation pathways by a computational approach and compare them based on their specific activity and yield on methanol, formate, and CO2/H2 considering the kinetics and thermodynamics of the reactions. Besides pathways found in nature or implemented in the laboratory, this included two completely new cycles with favorable features: the reductive citramalyl-CoA cycle and the 2-hydroxyglutarate-reverse tricarboxylic acid cycle. A comprehensive kinetic data set was collected for all enzymes of all pathways, and missing kinetic data were sampled with the Parameter Balancing algorithm. Kinetic and thermodynamic data were fed to the Enzyme Cost Minimization algorithm to check for respective inconsistencies and calculate pathway-specific activities. The specific activities of the reductive glycine pathway, the CETCH cycle, and the new reductive citramalyl-CoA cycle were predicted to match the best natural cycles with superior product-substrate yield. However, the CBB cycle performed better in terms of activity compared to the alternative pathways than previously thought. We make an argument that stoichiometric yield is likely not the most important design criterion of the CBB cycle. Still, alternative carbon fixation pathways were paretooptimal for specific activity and product-substrate yield in simulations with C1 substrates and CO2/H2 and therefore hold great potential for future applications in Industrial Biotechnology and Synthetic Biology.}
}
@article{SHEFFIELD20221149,
title = {Belief Updating and Paranoia in Individuals With Schizophrenia},
journal = {Biological Psychiatry: Cognitive Neuroscience and Neuroimaging},
volume = {7},
number = {11},
pages = {1149-1157},
year = {2022},
issn = {2451-9022},
doi = {https://doi.org/10.1016/j.bpsc.2022.03.013},
url = {https://www.sciencedirect.com/science/article/pii/S2451902222000799},
author = {Julia M. Sheffield and Praveen Suthaharan and Pantelis Leptourgos and Philip R. Corlett},
keywords = {Belief updating, Computational psychiatry, Delusions, Paranoia, Volatility, Worry},
abstract = {Background
Persecutory delusions are among the most common delusions in schizophrenia and represent the extreme end of the paranoia continuum. Paranoia is accompanied by significant worry and distress. Identifying cognitive mechanisms underlying paranoia is critical for advancing treatment. We hypothesized that aberrant belief updating, which is related to paranoia in human and animal models, would also contribute to persecutory beliefs in individuals with schizophrenia.
Methods
Belief updating was assessed in 42 participants with schizophrenia and 44 healthy control participants using a 3-option probabilistic reversal learning task. Hierarchical Gaussian Filter was used to estimate computational parameters of belief updating. Paranoia was measured using the Positive and Negative Syndrome Scale and the revised Green et al. Paranoid Thoughts Scale. Unusual thought content was measured with the Psychosis Symptom Rating Scale and the Peters et al. Delusions Inventory. Worry was measured using the Dunn Worry Questionnaire.
Results
Paranoia was significantly associated with elevated win-switch rate and prior beliefs about volatility both in schizophrenia and across the whole sample. These relationships were specific to paranoia and did not extend to unusual thought content or measures of anxiety. We observed a significant indirect effect of paranoia on the relationship between prior beliefs about volatility and worry.
Conclusions
This work provides evidence that relationships between belief updating parameters and paranoia extend to schizophrenia, may be specific to persecutory beliefs, and contribute to theoretical models implicating worry in the maintenance of persecutory delusions.}
}
@article{LIU2006207,
title = {Evolutionary design in a multi-agent design environment},
journal = {Applied Soft Computing},
volume = {6},
number = {2},
pages = {207-220},
year = {2006},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2005.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S156849460500013X},
author = {Hong Liu and Mingxi Tang},
keywords = {Multi-agent system, Evolutionary computing, Generic algorithm, Computer-aided design, Creative design},
abstract = {This paper presents a novel evolutionary design approach in a multi-agent design environment. Multi-agent system architecture offers a promising framework for dynamically managing cooperative agents in a distributed environment while the tree structure based generic algorithm provides a foundation for supporting evolutionary and innovative design abilities. Design is a complex knowledge discovery process. Creative design is a human trait that is not easily converted into a computational tool. Rather than to implement the innovative design by computers, this environment is used to stimulate the imagination of designers and extend their thinking space. It wants to explore a feasible and useful evolutionary approach in a distributed environment that will give the designers concrete help for the creative designs. This approach is illustrated by a mobile phone design example, which used binary algebraic expression tree to form sketch shapes and a feature based product tree to produce component combination choices. Because evolution is guided by human selectors, the evolutionary algorithm is not complex. It shows that approach is able to generate some creative solutions, demonstrating the power of explorative evolution.}
}
@article{RUNNELS201563,
title = {Capturing plasticity effects in overdriven shocks on the finite scale},
journal = {Mathematics and Computers in Simulation},
volume = {111},
pages = {63-79},
year = {2015},
issn = {0378-4754},
doi = {https://doi.org/10.1016/j.matcom.2014.12.006},
url = {https://www.sciencedirect.com/science/article/pii/S0378475414003334},
author = {Scott R. Runnels},
keywords = {Shocks, Plasticity, Hardening, Hydrodynamics, Radial return},
abstract = {An ordinary differential equation (ODE) form of the radial return algorithm, which is essentially a Prandtl-Reuss material model, is combined with a strain-rate hardening model to produce an ODE that describes deviatoric stress through a prescribed density rise. An analytical solution is found to the resulting ODE for a specific choice of one of the hardening model’s parameters. That solution is used to prove that if the prescribed density rise is allowed to be infinitely thin, i.e., like a shock in the mathematical sense, the resulting deviatoric stress is still bounded. In other words, the singularity is integrable; integration of the radial return ODE regularizes the infinite strain rate and resulting yield stress in the presence of an ideal shock singularity. The analytical tools developed for this line of thinking are applied to study the variation of deviatoric stress through a nearly shock-like density rise using different density rise profiles, revealing the impact of the shape choice. The tools are also used to compute what rise times are needed to converge upon the correct value of deviatoric stress through a shock; the results indicate that most contemporary hydrocodes cannot be expected to achieve those rise times. A demonstration of connecting the analytical tools to a hydrocode, using surrogate numerical shock shapes, is provided thereby opening the door for using such surrogates to perform sub-grid computations of converged shock behavior for strain-rate hardening materials.}
}
@article{TAY20211,
title = {Modelability across time as a signature of identity construction on YouTube},
journal = {Journal of Pragmatics},
volume = {182},
pages = {1-15},
year = {2021},
issn = {0378-2166},
doi = {https://doi.org/10.1016/j.pragma.2021.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S0378216621002307},
author = {Dennis Tay},
keywords = {Identity construction, Social media, LIWC, Modelability, ARIMA, Time series analysis},
abstract = {Linguistic self-representation and identity construction on social media have attracted much scholarly attention. However, relevant studies tend to overlook the temporal dimension of social media, potentially systematic patterning of linguistic behavior across time, and the attendant implications of such a temporal perspective on identity. Combining an automated lexical tool (LIWC) and the Box–Jenkins method of statistical time series analysis, this paper shows how the ‘modelability’ of linguistic choices across time can be interpreted as signatures of identity construction and complement existing frameworks for identity analysis. Two levels of modelability are discussed—the availability of a well-fitting time series model as evidence of temporal patterning, and specific parameters of that model interpreted in context. These are demonstrated with a case study of the construction of ‘amateur expertise’ over 109 consecutive makeup tutorial videos on the popular YouTube channel ‘Nikkiestutorials’. Results show that the linguistic display of ‘analytical thinking’ reflects a strategy of ‘short term momentum’, the display of ‘clout’ and ‘authenticity’ a strategy of ‘short term restoration’, while the display of ‘emotional tone’ fluctuates randomly across time. The approach is further discussed in terms of its general principles and potential applications in other contexts of identity and related research.}
}
@incollection{KENDRICK2008685,
title = {Chapter 17 The Supporting Role of Molecular Modelling and Computational Chemistry in Polymer Analysis},
editor = {John M. Chalmers and Robert J. Meier},
series = {Comprehensive Analytical Chemistry},
publisher = {Elsevier},
volume = {53},
pages = {685-734},
year = {2008},
booktitle = {Molecular Characterization and Analysis of Polymers},
issn = {0166-526X},
doi = {https://doi.org/10.1016/S0166-526X(08)00417-0},
url = {https://www.sciencedirect.com/science/article/pii/S0166526X08004170},
author = {John Kendrick},
abstract = {Publisher Summary
Molecular modeling covers a wide range of techniques and the calculation of an even wider range of properties. Although for polymers, the possibility of treating a polymer chain quantum mechanically is formidable, it is clear that the modeling approach allows calculations on monomers, dimmers, and oligomers to guide the interpretation of many spectroscopic observations with great success. For those systems, where longer times scales and larger size scales are important, molecular mechanics and molecular dynamics methods are available, but the issue of the force field and the approximations that it introduces remain significant. The key to the change in attitude to modeling and its role have to lie in the availability of mature algorithms with well-known and well-understood properties. The density functional theory method in quantum mechanics has introduced a new era in applications of quantum mechanical methods.}
}
@article{JIANG201814,
title = {Computational intelligence techniques for maximum power point tracking in PV systems: A review},
journal = {Renewable and Sustainable Energy Reviews},
volume = {85},
pages = {14-45},
year = {2018},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2018.01.006},
url = {https://www.sciencedirect.com/science/article/pii/S1364032118300054},
author = {Lian L. Jiang and R. Srivatsan and Douglas L. Maskell},
keywords = {Maximum power point tracking, PV system, Computational intelligence algorithm, Heuristic algorithm, Global tracking, Partial shading},
abstract = {Maximum power point (MPP) tracking (MPPT) is an important technique for maximizing the power extraction from photovoltaic (PV) systems under varying climatic conditions. In an array of PV modules it is possible to observe multiple peaks in the power versus voltage (P-V) curve due to the current versus voltage (I–V) PV cell mismatch caused by differences in the received irradiance, such as occurs during partial shading. In these circumstances, the ability of the MPPT devices to track the global MPP of the PV array directly influences the system efficiency. In the literature, various MPPT techniques have been proposed. Among them, computational intelligence (CI) algorithm based MPPT methods have demonstrated the ability to find the global MPP. This paper presents a detailed and specific review of CI- based MPPT techniques. Each method type is classified into one of several subcategories according to its application strategy. The various ways of applying CI into MPPTs are analyzed in detail. The advantages and disadvantages of each method are discussed and compared. The purpose of this study is to provide a compendium on CI-based MPPT techniques for users to understand and select an appropriate method based on application requirements and system constraints.}
}
@article{THAKIRABED20232293,
title = {The computation intelligent system of role of parental leadership in organizational familiarity in Iraqi Airways employees},
journal = {Materials Today: Proceedings},
volume = {80},
pages = {2293-2301},
year = {2023},
note = {SI:5 NANO 2021},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2021.06.318},
url = {https://www.sciencedirect.com/science/article/pii/S221478532104709X},
author = {Marwan {Thakir Abed}},
keywords = {Parental leadership, Organizational familiarity, Empowerment, Iraqi Airways},
abstract = {The research aimed to know the effect of parental leadership represented by (benevolent leadership, moral leadership, and authoritarian leadership) found in the research sample, in the organizational familiarity (employee morale, empowerment, and objective merit), the research relied on the questionnaire as a key instrument to collect the necessary data to meet its goal. As (60) forms were distributed to find the level of availability of parental leadership and organizational harmony, while (56) forms were retrieved. A set of statistical methods were used, represented by normal distribution, stability factor (Alpha Kronbach), reliability, arithmetic mean, standard deviation, and coefficient Simple correlation Pearson, multiple regression coefficient. The results showed that there is a positive correlation and effect relationship with statistically significant between parental leadership with its dimensions (benevolent leadership, moral leadership, authoritarian leadership) and organizational affiliation with its dimensions (employee morale, empowerment, and merit's Objectivity), and the research showed a direct impact relationship between parental leadership and the organizational affiliation of the studied sample. Accordingly, the research concluded that the study sample should pay attention to the nature and type of empowering workers in order to give them freedom and independence in making decisions regarding the tasks assigned to them.}
}
@article{HOYTE2006S348,
title = {Computational model of levator ani muscle stretch during vaginal delivery},
journal = {Journal of Biomechanics},
volume = {39},
pages = {S348},
year = {2006},
note = {Abstracts of the 5th World Congress of Biomechanics},
issn = {0021-9290},
doi = {https://doi.org/10.1016/S0021-9290(06)84382-4},
url = {https://www.sciencedirect.com/science/article/pii/S0021929006843824},
author = {L. Hoyte and P. Krysl and G. Chukkapalli and A. Majumdar and D.J. Choi and A. Trivedi and S.K. Warfield and M.S. Damaser}
}
@article{JURISICA2024102006,
title = {Explainable biology for improved therapies in precision medicine: AI is not enough},
journal = {Best Practice & Research Clinical Rheumatology},
volume = {38},
number = {4},
pages = {102006},
year = {2024},
note = {Genetics of Rheumatic Diseases},
issn = {1521-6942},
doi = {https://doi.org/10.1016/j.berh.2024.102006},
url = {https://www.sciencedirect.com/science/article/pii/S1521694224000779},
author = {I Jurisica},
keywords = {Precision medicine, Rheumatoid arthritis, Artificial intelligence, Integrative computational biology},
abstract = {Technological advances and high-throughput bio-chemical assays are rapidly changing ways how we formulate and test biological hypotheses, and how we treat patients. Most complex diseases arise on a background of genetics, lifestyle and environment factors, and manifest themselves as a spectrum of symptoms. To fathom intricate biological processes and their changes from healthy to disease states, we need to systematically integrate and analyze multi-omics datasets, ontologies, and diverse annotations. Without proper management of such complex biological and clinical data, artificial intelligence (AI) algorithms alone cannot be effectively trained, validated, and successfully applied to provide trustworthy and patient-centric diagnosis, prognosis and treatment. Precision medicine requires to use multi-omics approaches effectively, and offers many opportunities for using AI, “big data” analytics, and integrative computational biology workflows. Advances in optical and biochemical assay technologies including sequencing, mass spectrometry and imaging modalities have transformed research by empowering us to simultaneously view all genes expressed, identify proteome-wide changes, and assess interacting partners of each individual protein within a dynamically changing biological system, at an individual cell level. While such views are already having an impact on our understanding of healthy and disease conditions, it remains challenging to extract useful information comprehensively and systematically from individual studies, ensure that signal is separated from noise, develop models, and provide hypotheses for further research. Data remain incomplete and are often poorly connected using fragmented biological networks. In addition, statistical and machine learning models are developed at a cohort level and often not validated at the individual patient level. Combining integrative computational biology and AI has the potential to improve understanding and treatment of diseases by identifying biomarkers and building explainable models characterizing individual patients. From systematic data analysis to more specific diagnostic, prognostic and predictive biomarkers, drug mechanism of action, and patient selection, such analyses influence multiple steps from prevention to disease characterization, and from prognosis to drug discovery. Data mining, machine learning, graph theory and advanced visualization may help identify diagnostic, prognostic and predictive biomarkers, and create causal models of disease. Intertwining computational prediction and modeling with biological experiments leads to faster, more biologically and clinically relevant discoveries. However, computational analysis results and models are going to be only as accurate and useful as correct and comprehensive are the networks, ontologies and datasets used to build them. High quality, curated data portals provide the necessary foundation for translational research. They help to identify better biomarkers, new drugs, precision treatments, and should lead to improved patient outcomes and their quality of life. Intertwining computational prediction and modeling with biological experiments, efficiently and effectively leads to more useful findings faster.}
}
@incollection{BUNGE1980155,
title = {CHAPTER 7 - Thinking and Knowing},
editor = {MARIO BUNGE},
booktitle = {The Mind–Body Problem},
publisher = {Pergamon},
pages = {155-173},
year = {1980},
isbn = {978-0-08-024720-5},
doi = {https://doi.org/10.1016/B978-0-08-024720-5.50012-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780080247205500128},
author = {MARIO BUNGE},
abstract = {Publisher Summary
This chapter discusses the process of thinking and knowing. Forming concepts, propositions, problems, and directions are examples of thinking. Thinking can be visual, verbal, or abstract. It can be chaotic or orderly, creative, or routine. Thinking of any kind is an activity of some plastic neural systems. Of all mental activities, thinking is probably the one most affected by chemical changes and changes in basic properties of neurons. For example, humans unable to oxidize the amino acid phenylalanine cannot think, thyroid hypofunction produces cretinism, and normal subjects cannot think straight when in states of extreme stress, which are often states of hormonal imbalance, or when under the action of psychotropic drugs. The chapter also discusses the concept of cognition. All cognition is learned but not every learned item is of a cognitive nature. All cognition is cognition of some object, concrete or conceptual, and it consists in some information about its object—complete or partial, true or false. Cognition can be behavioral, perceptual, or conceptual.}
}
@article{GALTIER201683,
title = {Radiative transfer and spectroscopic databases: A line-sampling Monte Carlo approach},
journal = {Journal of Quantitative Spectroscopy and Radiative Transfer},
volume = {172},
pages = {83-97},
year = {2016},
note = {Eurotherm Conference No. 105: Computational Thermal Radiation in Participating Media V},
issn = {0022-4073},
doi = {https://doi.org/10.1016/j.jqsrt.2015.10.016},
url = {https://www.sciencedirect.com/science/article/pii/S0022407315003192},
author = {Mathieu Galtier and Stéphane Blanco and Jérémi Dauchet and Mouna {El Hafi} and Vincent Eymet and Richard Fournier and Maxime Roger and Christophe Spiesser and Guillaume Terrée},
keywords = {Radiative transfer, Monte Carlo method, Null-collision, Line sampling, Statistical approach, Spectroscopic databases},
abstract = {Dealing with molecular-state transitions for radiative transfer purposes involves two successive steps that both reach the complexity level at which physicists start thinking about statistical approaches: (1) constructing line-shaped absorption spectra as the result of very numerous state-transitions, (2) integrating over optical-path domains. For the first time, we show here how these steps can be addressed simultaneously using the null-collision concept. This opens the door to the design of Monte Carlo codes directly estimating radiative transfer observables from spectroscopic databases. The intermediate step of producing accurate high-resolution absorption spectra is no longer required. A Monte Carlo algorithm is proposed and applied to six one-dimensional test cases. It allows the computation of spectrally integrated intensities (over 25cm−1 bands or the full IR range) in a few seconds, regardless of the retained database and line model. But free parameters need to be selected and they impact the convergence. A first possible selection is provided in full detail. We observe that this selection is highly satisfactory for quite distinct atmospheric and combustion configurations, but a more systematic exploration is still in progress.}
}
@article{WANG20231225,
title = {Parameterization Design of 3D Fractal Images in Packaging Design Based on Genetic Algorithm},
journal = {Procedia Computer Science},
volume = {228},
pages = {1225-1232},
year = {2023},
note = {3rd International Conference on Machine Learning and Big Data Analytics for IoT Security and Privacy},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.11.104},
url = {https://www.sciencedirect.com/science/article/pii/S187705092301935X},
author = {Jinxia Wang},
keywords = {Genetic Algorithm, Packaging Design, 3D Fractal Image, NSGA - II},
abstract = {Packaging design, as an important element in product appearance, can directly affect customers' sensory perception of the product. Many universities even offer packaging design majors, which mainly use natural science and aesthetic knowledge to promote product sales. However, many old brands remain complacent and their packaging design still adopts traditional thinking, which to some extent affects their sales. Therefore, this article decided to use genetic algorithms as a tool to parameterize the 3D fractal images in packaging design, aiming to create more creative and eye-catching packaging designs. At the end of this article, an experiment was conducted on two branches of a certain brand. Branch 1 tried out the new design provided in this article, while Branch 2 continued to use the original design. After Branch 1 fully adopted the design, sales skyrocketed, from the original daily sales of 50-60 units to 70-85 units. Branch 2 remained unchanged, with a sharp contrast.}
}
@article{DENEF20071096,
title = {Computational complexity of the landscape: Part I},
journal = {Annals of Physics},
volume = {322},
number = {5},
pages = {1096-1142},
year = {2007},
issn = {0003-4916},
doi = {https://doi.org/10.1016/j.aop.2006.07.013},
url = {https://www.sciencedirect.com/science/article/pii/S0003491606001382},
author = {Frederik Denef and Michael R. Douglas},
abstract = {We study the computational complexity of the physical problem of finding vacua of string theory which agree with data, such as the cosmological constant, and show that such problems are typically NP hard. In particular, we prove that in the Bousso–Polchinski model, the problem is NP complete. We discuss the issues this raises and the possibility that, even if we were to find compelling evidence that some vacuum of string theory describes our universe, we might never be able to find that vacuum explicitly. In a companion paper, we apply this point of view to the question of how early cosmology might select a vacuum.}
}
@article{AKCAOGLU201472,
title = {Cognitive outcomes from the Game-Design and Learning (GDL) after-school program},
journal = {Computers & Education},
volume = {75},
pages = {72-81},
year = {2014},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2014.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S0360131514000372},
author = {Mete Akcaoglu and Matthew J. Koehler},
keywords = {Game-design, Problem-solving, Quasi-experimental, Constructionism},
abstract = {The Game-Design and Learning (GDL) initiative engages middle school students in the process of game-design in a variety of in-school, after-school, and summer camp settings. The goal of the GDL initiative is to leverage students' interests in games and design to foster their problem-solving and critical reasoning skills. The present study examines the effectiveness of an after-school version of the GDL program using a quasi-experimental design. Students enrolled in the GDL program were guided in the process of designing games aimed at solving problems. Compared to students in a control group who did not attend the program (n = 24), the children who attended the GDL program (n = 20) showed a significant increase in their problem-solving skills. The results provide empirical support for the hypothesis that participation in the GDL program leads to measurable cognitive changes in children's problem-solving skills. This study bears important implications for educators and theory.}
}
@article{ALTARABICHI2023118528,
title = {Fast Genetic Algorithm for feature selection — A qualitative approximation approach},
journal = {Expert Systems with Applications},
volume = {211},
pages = {118528},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.118528},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422016049},
author = {Mohammed Ghaith Altarabichi and Sławomir Nowaczyk and Sepideh Pashami and Peyman Sheikholharam Mashhadi},
keywords = {Feature selection, Evolutionary computation, Genetic Algorithm, Particle Swarm Intelligence, Fitness approximation, Meta-model, Optimization},
abstract = {Evolutionary Algorithms (EAs) are often challenging to apply in real-world settings since evolutionary computations involve a large number of evaluations of a typically expensive fitness function. For example, an evaluation could involve training a new machine learning model. An approximation (also known as meta-model or a surrogate) of the true function can be used in such applications to alleviate the computation cost. In this paper, we propose a two-stage surrogate-assisted evolutionary approach to address the computational issues arising from using Genetic Algorithm (GA) for feature selection in a wrapper setting for large datasets. We define “Approximation Usefulness” to capture the necessary conditions to ensure correctness of the EA computations when an approximation is used. Based on this definition, we propose a procedure to construct a lightweight qualitative meta-model by the active selection of data instances. We then use a meta-model to carry out the feature selection task. We apply this procedure to the GA-based algorithm CHC (Cross generational elitist selection, Heterogeneous recombination and Cataclysmic mutation) to create a Qualitative approXimations variant, CHCQX. We show that CHCQX converges faster to feature subset solutions of significantly higher accuracy (as compared to CHC), particularly for large datasets with over 100K instances. We also demonstrate the applicability of the thinking behind our approach more broadly to Swarm Intelligence (SI), another branch of the Evolutionary Computation (EC) paradigm with results of PSOQX, a qualitative approximation adaptation of the Particle Swarm Optimization (PSO) method. A GitHub repository with the complete implementation is available.22https://github.com/Ghaith81/Fast-Genetic-Algorithm-For-Feature-Selection.}
}
@article{ALAUX2025,
title = {Parameters influencing the prospective life cycle emissions of the Austrian building stock: a global sensitivity analysis},
journal = {Sustainable Production and Consumption},
year = {2025},
issn = {2352-5509},
doi = {https://doi.org/10.1016/j.spc.2025.04.024},
url = {https://www.sciencedirect.com/science/article/pii/S2352550925000983},
author = {Nicolas Alaux and Benedict Schwark and Marco Scherz and Marcella Ruschi Mendes Saade and Alexander Passer},
keywords = {Prospective life cycle assessment, Carbon reduction strategy, Sufficiency measure, Behavioural change, Embodied greenhouse gas emissions, Building stock model},
abstract = {Environmental building stock models have increasingly been developed to inform local policies. However, identifying and quantifying all relevant parameters for such large-scale modelling remains a challenge. This study identifies key dynamic parameters used to project life cycle greenhouse gas (GHG) emissions of the Austrian building stock through 2050, while also analysing their systemic interactions. Both operational GHG emissions from energy consumption in buildings and embodied GHG emissions from new constructions, renovations and demolitions are included. The Morris screening analysis and the Sobol global sensitivity analysis are applied to a building stock model that combines dynamic material flow analysis with prospective life cycle assessment. The two sensitivity analysis methods produce different rankings of the most influential parameters, underscoring the need to combine them for a more comprehensive analysis. In addition to energy-related parameters, the average living area per person also emerges as one of the most influential parameters, emphasizing the importance of incorporating efficient use of space into current policies. The study also evaluates the impact of the rate at which each parameter will evolve by 2050. For instance, if GHG reduction measures are implemented slowly, reducing personal heating consumption or changing construction material production will have a greater impact than changing heating systems. However, if heating system upgrades can be quickly implemented, they would have a greater effect than the other two measures. This suggests that prioritising GHG reduction strategies should also take into account the speed at which these measures can be deployed. Additional research is necessary to enhance the application of systems thinking in building stock models.}
}
@article{CREELY2025101727,
title = {Creative partnerships with generative AI. Possibilities for education and beyond},
journal = {Thinking Skills and Creativity},
volume = {56},
pages = {101727},
year = {2025},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2024.101727},
url = {https://www.sciencedirect.com/science/article/pii/S1871187124002682},
author = {Edwin Creely and Jo Blannin},
keywords = {Generative AI, Creative production, Posthumanism, Education, Autoethnography, Alterity relations},
abstract = {The impact of generative artificial intelligence (AI) on creative production in industry and education is just beginning to be experienced and understood. This impact is likely to accelerate and become even more significant as the computational potential of generative AI grows through training on more diverse and more extensive language models and data sets. Emerging research in this new field suggests that previous models of understanding the interactions between machine and human may no longer be sufficient in a world of generative AI. The significant question is how emerging generative AI technologies will relate to and be a part of human creativity and creative outputs. In this article, we adopt a posthuman stance and conceive of creative output involving generative AI and humans in terms of a yet-to-be-fully-realised and emergent relationship that will likely become more integrated and complex. To investigate and experiment with this relational notion, each of us (as part of an autoethnographic approach) developed a creative output using ChatGPT: a poem and a multimodal narrative. We then employed the idea of alterity relations from the American philosopher of technology, Don Ihde, to conceive of the possibilities and limitations in working relationally and productively with generative AI. As two academics working in teacher education, we applied our learning from this exploration to possibilities in educational contexts. In this article, we offer several important implications and provocations for practitioners, researchers, educators and policymakers, not only in terms of practical concerns but also for rethinking the nature of the creative output.}
}
@incollection{GALLICCHIO201127,
title = {Recent theoretical and computational advances for modeling protein–ligand binding affinities},
editor = {Christo Christov},
series = {Advances in Protein Chemistry and Structural Biology},
publisher = {Academic Press},
volume = {85},
pages = {27-80},
year = {2011},
booktitle = {Computational chemistry methods in structural biology},
issn = {1876-1623},
doi = {https://doi.org/10.1016/B978-0-12-386485-7.00002-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780123864857000028},
author = {Emilio Gallicchio and Ronald M. Levy},
keywords = {Quasi-chemical description, Statistical mechanics, Potential of mean force, PDT, MM/PBSA, free energy perturbation, BEDAM, double decoupling},
abstract = {We review recent theoretical and algorithmic advances for the modeling of protein ligand binding free energies. We first describe a statistical mechanics theory of noncovalent association, with particular focus on deriving the fundamental formulas on which computational methods are based. The second part reviews the main computational models and algorithms in current use or development, pointing out the relations with each other and with the theory developed in the first part. Particular emphasis is given to the modeling of conformational reorganization and entropic effect. The methods reviewed are free energy perturbation, double decoupling, the Binding Energy Distribution Analysis Method, the potential of mean force method, mining minima and MM/PBSA. These models have different features and limitations, and their ranges of applicability vary correspondingly. Yet their origins can all be traced back to a single fundamental theory.}
}
@article{BENSASSI20233123,
title = {Fuzzy knowledge based assessment system for K-12 Scientific Reasoning Competencies},
journal = {Procedia Computer Science},
volume = {225},
pages = {3123-3132},
year = {2023},
note = {27th International Conference on Knowledge Based and Intelligent Information and Engineering Sytems (KES 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.10.306},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923014643},
author = {Manel BenSassi and Henda Ben Ghezala},
keywords = {Learning analytics, educational recommendations, fuzzy competencies assessment, knowledge representation, fuzzy ontology, Scientific Reasoning competencies},
abstract = {Developing Scientific Reasoning (SR) competencies at an early age, are challenging to meet expectation of the 4th sustainable development goal. Hence, educators and educational decision-makers try to embed these competencies into such subjects as the arts, language, technology, economics, mathematics and science, using an inter-disciplinary approach. In this context, this paper proposes a fuzzy knowledge-based solution to build practical pupils, educators, and decision-makers recommender system to support the development of SR competencies in a data driver manner. Our system consists of:(1) inferring and computational module that calculates in a fuzzy manner the global appreciation to each SR-competencies. (2) recommendation module that aims to help learners, educators and decision makers to assess the degree of development of SR competencies and to get alternative suggestion of remediation. The proposed solution has been tested on the last two levels of science education in four Tunisian elementary schools in different regions. A preliminary analysis showed that the learning process should be more focused on Tunisian pupil's profile, and that investigation and collaborative based learning should be applied further in Tunisian classroom.}
}
@article{DHINAKARAN2025127584,
title = {Safeguarding confidentiality and privacy in cloud-enabled healthcare systems with spectrasafe encryption and dynamic k-anonymity algorithm},
journal = {Expert Systems with Applications},
volume = {279},
pages = {127584},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.127584},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425012060},
author = {D. Dhinakaran and N. {Jagadish Kumar} and N.P. Ponnuviji and B. {Praveen kumar}},
keywords = {Data security, Privacy, Homomorphic Encryption, Multi-Party Computation, Cryptography, Hash Algorithm},
abstract = {Healthcare is no exception to the requirement of cutting-edge solutions for healthcare data security and privacy: novel strategies need be formulated on a regular basis given contemporary threat in order to reliable shield over sensitive information. We therefore introduce in this paper a novel technique to increase the data security and privacy of cloud-based healthcare systems. We employ state-of-the-art encryption mechanisms such as Spectrasafe Algorithm, Facile Hash Algorithm and Dynamic k-anonymity (DKA Algorithm) to protect healthcare data at rest and during transit. Conducting a series of performance comparisons, using this high-quality data meticulously prepared from Kaggle to compare it with existing methods SMPC (Secure Multi-Party Computation), MSHC (Multi Server Homomorphic Encryption), AHC-ASS (Attribute-Hiding Cryptography with Attribute-Based Secret Sharing), MP-FHE（Multiparty Fully Homomorphic Encryption). Our encryption and decryption runtime are intensive computationally, with an average runtime of 99 ms and 108 ms, respectively. In terms of memory size our approach is 25 % smaller than existing state-of-the-art methods which indicates that it has a higher efficiency. Our method attains a detection accuracy of 95 %, which is also significant than results of previous methods. Moreover, the proposed method is shown to scale across a wide variety of healthcare datasets and updated requirements seen in modern healthcare systems. We intend to make a significant contribution towards secure and efficient healthcare data management in cloud environments, by extending the proposed privacy preservation mechanisms.}
}
@article{BAKEEVA2022676,
title = {Increasing the student talking time parameter under the digitalization in transport engineering learning},
journal = {Transportation Research Procedia},
volume = {63},
pages = {676-685},
year = {2022},
note = {X International Scientific Siberian Transport Forum — TransSiberia 2022},
issn = {2352-1465},
doi = {https://doi.org/10.1016/j.trpro.2022.06.062},
url = {https://www.sciencedirect.com/science/article/pii/S2352146522003179},
author = {L Bakeeva and L Brylevskaya and L Gonchar and E Pastukhova and Y Romanova and O Skepko},
keywords = {Digitalization of education, transport engineering, student talking time, study-train-explain methodology},
abstract = {The most recent information technologies have become an integral part of modern life. As the study shows, along with the obvious benefits, their use can lead to negative consequences, namely the loss of communication and soft skills, changes in the ability to absorb information, decreased motivation to acquire new knowledge among the younger age group. The authors propose a new methodology for organizing the educational process Study-Train-Explain. The aim of the method is to increase the Student Talking Time parameter to develop the skills of mathematical data analysis, systematic and analytical thinking to master the methods of description and construction of mathematical model of the phenomenon or process. These competencies are extremely in demand in the professional field related to the organization of transportation and operation of transport-technological machines and complexes under the conditions of digitalization of global processes. The article presents an algorithm and the results of the experimental training process carried out by the authors according to the specified methodology.}
}
@incollection{BACSKAY2025,
title = {Fifty-five years of quantum chemistry},
series = {Advances in Quantum Chemistry},
publisher = {Academic Press},
year = {2025},
issn = {0065-3276},
doi = {https://doi.org/10.1016/bs.aiq.2025.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S0065327625000036},
author = {György B. Bácskay},
keywords = {Configuration interaction (CI), Direct CI, Coupled cluster theory, Quadratically convergent- self-consistent field (QC-SCF) method, Molecular properties, Thermal decomposition, Chemical bonding, Thermochemistry},
abstract = {Started doing Quantum Chemistry as a PhD student with Linnett in 1968 at Cambridge: configuration interaction (CI) calculations on small hydrogenic molecules with a mixed basis of Slater and Gaussian functions. Joined Hush at the University of Sydney in 1972 and worked with a number of graduate students on Coupled Cluster Theory, direct CI, the calculation of ionization energies, the computation of molecular properties and vibrational energies, studies of hydrogen bonded clusters and quadratically convergent Hartree-Fock theory. With Nordholm, worked on the implementation of Absorbing Boundary and Generalized Finite Element methods. As computational chemist, during the nineties and beyond, studied dihydrogen complexes of osmium and the catalytic properties of cis-platins, the potential energy surfaces relevant to thermal decomposition of small molecules, properties of halocarbenes, spectroscopy of C2, thermochemistry and covalent bonding. Retired in 2005. Research-active as Honorary Reader.}
}
@article{EBERBACH2007200,
title = {The $-calculus process algebra for problem solving: A paradigmatic shift in handling hard computational problems},
journal = {Theoretical Computer Science},
volume = {383},
number = {2},
pages = {200-243},
year = {2007},
note = {Complexity of Algorithms and Computations},
issn = {0304-3975},
doi = {https://doi.org/10.1016/j.tcs.2007.04.012},
url = {https://www.sciencedirect.com/science/article/pii/S0304397507003192},
author = {Eugene Eberbach},
keywords = {Problem solving, Process algebras, Anytime algorithms, SuperTuring models of computation, Bounded rational agents, $-calculus, Intractability, Undecidability, Completeness, Optimality, Search optimality, Total optimality},
abstract = {The $-calculus is the extension of the π-calculus, built around the central notion of cost and allowing infinity in its operators. We propose the $-calculus as a more complete model for problem solving to provide a support to handle intractability and undecidability. It goes beyond the Turing Machine model. We define the semantics of the $-calculus using a novel optimization method (the kΩ-optimization), which approximates a nonexisting universal search algorithm and allows the simulation of many other search methods. In particular, the notion of total optimality has been utilized to provide an automatic way to deal with intractability of problem solving by optimizing together the quality of solutions and search costs. The sufficient conditions needed for completeness, optimality and total optimality of problem solving search are defined. A very flexible classification scheme of problem solving methods into easy, hard and solvable in the limit classes has been proposed. In particular, the third class deals with non-recursive solutions of undecidable problems. The approach is illustrated by solutions of some intractable and undecidable problems. We also briefly overview two possible implementations of the $-calculus.}
}
@article{ONGUR2025220,
title = {Embracing complexity in psychiatry—from reductionistic to systems approaches},
journal = {The Lancet Psychiatry},
volume = {12},
number = {3},
pages = {220-227},
year = {2025},
issn = {2215-0366},
doi = {https://doi.org/10.1016/S2215-0366(24)00334-1},
url = {https://www.sciencedirect.com/science/article/pii/S2215036624003341},
author = {Dost Öngür and Martin P Paulus},
abstract = {Summary
The understanding and treatment of psychiatric disorders present unique challenges due to these conditions' multifaceted nature, comprising dynamic interactions between biological, psychological, social, and environmental factors. Traditional reductionistic approaches often simplify these conditions into linear cause-and-effect relationships, overlooking the complexity and interconnectedness inherent in psychiatric disorders. Advances in complex systems approaches provide a comprehensive framework to capture and quantify the non-linear and emergent properties of psychiatric disorders. This Personal View emphasises the importance of identifying rules for generative models that govern brain and behaviour over time, which might contribute to personalised assessments and interventions for psychiatric disorders. For instance, mood fluctuations in bipolar disorder can be understood through dynamical systems modelling, which identifies modifiable parameters, such as circadian disruption, that can be addressed through targeted therapies such as light therapy. Similarly, recognition of depression as an emergent property arising from complex interactions highlights the need for integrated treatment strategies that enhance adaptive reactions in the individual. A framework for quantifying multilevel interactions and network dynamics can help researchers and clinicians to understand the interplay between neural circuits, behaviours, and social contexts. Probabilistic models and self-organisation concepts contribute to building concrete dynamical systems models of mental disorders, facilitating early identification of risk states and promoting resilience through adaptive interventions delivered with optimal timing. Embracing these complex systems approaches in psychiatry could capture the true nature of psychiatric disorders as properties of a dynamic complex system and not the manifestation of any lesion or insult. This line of thinking might improve diagnosis and treatment, offering new hope for individuals affected by psychiatric conditions and paving the way for more effective, personalised mental health care.}
}
@article{ZORARPACI2024108162,
title = {A fast intrusion detection system based on swift wrapper feature selection and speedy ensemble classifier},
journal = {Engineering Applications of Artificial Intelligence},
volume = {133},
pages = {108162},
year = {2024},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2024.108162},
url = {https://www.sciencedirect.com/science/article/pii/S0952197624003208},
author = {Ezgi Zorarpaci},
keywords = {Data mining, Ensemble classifier, Feature selection, Intrusion detection system},
abstract = {Due to the widespread use of the internet, computer network systems may be exposed to different types of attacks. For this reason, the intrusion detection systems (IDSs) are often used to protect the network systems. Network traffic data (i.e., network packets) includes many features. However, most of them are irrelevant and can lead to a decrease in the runtime and/or the detection performance of the IDS. Although various data mining methods have been applied to improve the effectiveness of IDS, research regarding IDSs having high detection rates and better runtime performance (i.e., lower computational cost) is ongoing. On the other hand, the dimensionality reduction techniques help to eliminate unnecessary features and reduce the computation time of a classification algorithm. In the literature, the feature selection methods (i.e., filter and wrapper) have been widely used for the dimensionality reduction in IDSs. Although the wrapper feature selection techniques outperform the filters, they are time-consuming. Again, the ensemble classifiers can achieve higher detection rates for IDSs compared to the stand-alone classifiers, but they require more computation time to build the model. In order to improve the runtime performance and the detection rate of IDS, a swift wrapper feature selection and a speedy ensemble classifier are proposed in this study. For the dimensionality reduction, the swift wrapper feature selection (i.e., DBDE-QDA) is used, which consists of dichotomous binary differential evolution (DBDE) and quadratic discriminant analysis (QDA). For attack detection, the speedy ensemble classifier is used, which combines Holte's 1R, random tree, and reduced error pruning tree. In the experiments, the NSL-KDD, UNSW-NB15, and CICDDoS2019 datasets are used. According to the experimental results, the proposed IDS reaches 95%–97.4%, 82.7%, and 99.5%–99.9% detection rates for the NSL-KDD, UNSW-NB15, and CICDDoS2019 datasets. In this way, the proposed IDS competes with the state-of-the-art methods in terms of detection rate and false alarm rate. In addition, the proposed IDS has a lower computational cost than the state-of-the-art methods. Moreover, DBDE-QDA reduces the dimension by 60.97%–82.92%, 73.46%, and 96.55%–98.85% for the NSL-KDD, UNSW-NB15, and CICDoS2019 datasets.}
}
@incollection{BALAKRISHNAN202131,
title = {Chapter 2 - Computational intelligence in healthcare and biosignal processing},
editor = {Janmenjoy Nayak and Bighnaraj Naik and Danilo Pelusi and Asit Kumar Das},
booktitle = {Handbook of Computational Intelligence in Biomedical Engineering and Healthcare},
publisher = {Academic Press},
pages = {31-64},
year = {2021},
isbn = {978-0-12-822260-7},
doi = {https://doi.org/10.1016/B978-0-12-822260-7.00015-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128222607000157},
author = {Nagaraj Balakrishnan and Valentina E. Balas and Arunkumar Rajendran},
keywords = {ANN, Clustering, Data classification, Data mining, Deep clustering networks, Deep learning},
abstract = {In this new era, technological advancement toward the mission of a better tomorrow is reaching its limit because the exploration of the advanced possibilities of Artificial Intelligence is bounded with certain limitations. The application of analyzing various features of biosignal processing is key in the fields of medicine and healthcare. Biosignals such as Electroencephalogram (EEG), Electrocardiogram (ECG), Electromyography (EMG), Electrooculography (EOG), Galvanic Skin Response(GSR), and Magnetoencephalography (MEG) is already giving deep insight into the human body toward the identification of diverse nature and disorders. In recent years, the research toward analyzing biosignal gained interest among many researchers. The primary limitation for the algorithms to analyze these signals for more possibilities of insight is its uncertainty. Even though the algorithms of Artificial Intelligence have the capabilities to unravel the mysteries, it is bounded with specific difficulties. The machine learning algorithms designed to manage uncertain data but lacks accuracy due to many factors. Also, complete supervision is needed in a training process that involves the extraction and selection of adequate features for the training. The deep learning method (a subset of machine learning) comes into the picture due to one of these facts. This, indeed, as a supervised learning method, needs a massive volume of data to train to reach the accuracy goal. The deep learning algorithm plays a significant role in today's Artificial Intelligence–based applications. However, this platform needs many requirements, such as (a) high computational power like graphical processing units (GPU); (b) similar to machine learning methods, a massive labeled dataset for supervised learning; (c) adequate parameter selection to avoid overfitting or underfitting. To overcome the problems highlighted, the strategy of adopting the behaviors of unsupervised learning (performed by the clustering algorithm) in the deep learning methodology is needed. To achieve the goal, two-phase operations were processed, such as (1) transformation of the data elements into a latent feature space (Z) is processed through a nonlinear mapping of deep learning networks; (2) clustering the latent feature space to k-clusters, and simultaneously, the clustering loss is fed to the deep learning network for the next iteration of operation concerning the objective function convergence analyzed by the Kullback–Leibler divergence. Various strategies of enhancing the nature of deep learning methods and clustering methodologies for an unsupervised learning process are addressed in this chapter.}
}
@article{INAL2024101338,
title = {Current clinical status of IC/BPS and what the future holds in basic & translational science},
journal = {Continence},
volume = {11},
pages = {101338},
year = {2024},
issn = {2772-9737},
doi = {https://doi.org/10.1016/j.cont.2024.101338},
url = {https://www.sciencedirect.com/science/article/pii/S2772973724002716},
author = {Guldal Inal and Dick Janssen and Naside Mangir and Francisco Cruz and Ana Charrua},
keywords = {IC/BPS, Biomarkers, Bioinformatics, Artificial intelligence, Animal models},
abstract = {The present review summarizes the scientific content in the workshop “Current clinical status of IC/BPS and what the future holds in basic & translational science” at the International Continence Society (ICS) 2023, Toronto. In the workshop, clinicians and scientists from different disciplines and nationalities discussed the current clinical status of IC/BPS diagnostic and treatment. They defined the available trends in biomarker search and translational medicine. The recent contribution of computational science and bioinformatics, together with artificial intelligence and the recent improvements in the use of animal models were also explored. The search for diagnostic and predictive biomarkers for IC/BPS patients is important. The use of well-refined and characterized animal models and the use of bioinformatics and artificial intelligence will be useful in this search.}
}
@article{LIM2025103550,
title = {Exploring trends and topics in hybrid intelligence using keyword co-occurrence networks and topic modelling},
journal = {Futures},
volume = {167},
pages = {103550},
year = {2025},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2025.103550},
url = {https://www.sciencedirect.com/science/article/pii/S0016328725000138},
author = {Jihye Lim and Junseok Hwang},
keywords = {Hybrid Intelligence, Artificial Intelligence, Human Intelligence, Keywords Co-occurrence Network, Topic Modeling, Knowledge classification},
abstract = {Hybrid Intelligence (HI) represents the ability to solve problems by using human and artificial intelligence (AI) together, pursuing the strengths of both the former (e.g., ethical, creative, and common-sense thinking) and the latter (e.g., the fast and efficient ability). This study conducted a keyword network analysis and topic modelling to extract topics, examine research trends, and explore future development directions. HI-related research has increased rapidly since 2017, and the applied research fields have increased in diversity. The most active research fields are computer science, engineering, and mathematics. Based on the topic and keyword analysis, we found four trending topics (decision making, medical science, social factors, and automation) and three emerging topics (crowdsourcing, data science, and teamwork). Among the four types of knowledge (factual, conceptual, expectational, and methodological), previous papers have been lacking focus on methodological research in terms of basic research. Since HI can include both human and artificial intelligences, it offers endless possibilities for methodological knowledge in terms of the research process. We therefore propose the development of research methodologies leveraging HI as a promising future research topic.}
}
@incollection{GARDNER202477,
title = {Chapter 4 - Smart design for urban activation and placemaking},
editor = {Nicole Gardner},
booktitle = {Scaling the Smart City},
publisher = {Elsevier},
pages = {77-102},
year = {2024},
series = {Smart Cities},
isbn = {978-0-443-18452-9},
doi = {https://doi.org/10.1016/B978-0-443-18452-9.00008-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780443184529000082},
author = {Nicole Gardner},
keywords = {Cyber-physical systems, Design, Digital placemaking, Interaction, Placemaking, Smart design, Urban activation, Urban amenitization},
abstract = {This chapter charts out the distinction between the concepts of digital placemaking and smart placemaking. It examines existing and speculative urban technology projects that combine spatial design thinking and physical computing to address placemaking design goals such as urban activation, amenitization, and safety and security. In ways different from conventional smart city initiatives that surveil urban dynamics en masse to imperceptibly calibrate large-scale urban services and infrastructural systems, the projects discussed in this chapter engage sensor-based technologies to create localized, responsive, and interactive urban environments to address the social, cultural, and aesthetic dimensions of urban livability.}
}
@article{FOLTZ2023127,
title = {Reflections on the nature of measurement in language-based automated assessments of patients' mental state and cognitive function},
journal = {Schizophrenia Research},
volume = {259},
pages = {127-139},
year = {2023},
note = {Language and Speech Analysis in Schizophrenia and Related Psychoses},
issn = {0920-9964},
doi = {https://doi.org/10.1016/j.schres.2022.07.011},
url = {https://www.sciencedirect.com/science/article/pii/S0920996422002833},
author = {Peter W. Foltz and Chelsea Chandler and Catherine Diaz-Asper and Alex S. Cohen and Zachary Rodriguez and Terje B. Holmlund and Brita Elvevåg},
keywords = {Natural language processing, Speech technologies, Artificial intelligence},
abstract = {Modern advances in computational language processing methods have enabled new approaches to the measurement of mental processes. However, the field has primarily focused on model accuracy in predicting performance on a task or a diagnostic category. Instead the field should be more focused on determining which computational analyses align best with the targeted neurocognitive/psychological functions that we want to assess. In this paper we reflect on two decades of experience with the application of language-based assessment to patients' mental state and cognitive function by addressing the questions of what we are measuring, how it should be measured and why we are measuring the phenomena. We address the questions by advocating for a principled framework for aligning computational models to the constructs being assessed and the tasks being used, as well as defining how those constructs relate to patient clinical states. We further examine the assumptions that go into the computational models and the effects that model design decisions may have on the accuracy, bias and generalizability of models for assessing clinical states. Finally, we describe how this principled approach can further the goal of transitioning language-based computational assessments to part of clinical practice while gaining the trust of critical stakeholders.}
}
@article{VEITAS201716,
title = {Living Cognitive Society: A ‘digital’ World of Views},
journal = {Technological Forecasting and Social Change},
volume = {114},
pages = {16-26},
year = {2017},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2016.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S0040162516300610},
author = {Viktoras Veitas and David Weinbaum},
keywords = {Cognitive system, Living society, Information and communication technologies, Future social governance, Individuation, Cognitive development},
abstract = {The current social reality is characterized by all-encompassing change, which disrupts existing social structures at all levels. Yet the approach based on the ontological primacy of stable and often hierarchical structures is still prevalent in theoretical and, most importantly, practical thinking about social systems. We propose a conceptual framework for thinking about a dynamically changing social system: the Living Cognitive Society. Importantly, we show how it follows from a much broader philosophical framework, guided by the theory of individuation, which emphasizes the importance of relationships and interactive processes in the evolution of a system. The framework addresses society as a living cognitive system – an ecology of interacting social subsystems – each of which is also a living cognitive system. We argue that this approach can help us to conceive sustainable social systems that will thrive in the circumstances of accelerating change. The Living Cognitive Society is explained in terms of its fluid structure, dynamics and the mechanisms at work. We then discuss the disruptive effects of Information and Communication Technologies on the mechanisms at work. We conclude by delineating a major topic for future research – distributed social governance – which focuses on processes of coordination rather than on stable structures within global society.}
}
@article{ZHU2024115675,
title = {Identifying influential nodes in social networks via improved Laplacian centrality},
journal = {Chaos, Solitons & Fractals},
volume = {189},
pages = {115675},
year = {2024},
issn = {0960-0779},
doi = {https://doi.org/10.1016/j.chaos.2024.115675},
url = {https://www.sciencedirect.com/science/article/pii/S096007792401227X},
author = {Xiaoyu Zhu and Rongxia Hao},
keywords = {Social network, Influential nodes, Centrality measure, Improved Laplacian centrality},
abstract = {Identifying influential nodes in social networks has significant applications in terms of social analysis and information dissemination. How to capture the crucial features of influential nodes without increasing the computational complexity is an urgent issue to be solved in the context of big data. Laplacian centrality (LC) measures nodal influence by computing nodes' degree, making it extremely low complexity. However, there is still significant room for improvement. Consequently, we propose the improved Laplacian centrality (ILC) to identify influential nodes based on the concept of self-consistent. Identifying results on 9 real networks prove that ILC is superior to LC and other 6 classical measures in terms of ranking accuracy, top-k nodes identification and discrimination capability. Moreover, the computational complexity of ILC has not significantly increased compared to LC, and remains the linear order of magnitude O(m). Additionally, ILC has excellent robustness and universality such that there is no need to adjust parameters according to different network structures.}
}
@article{WANG2016357,
title = {Research on Application of Abduction to Fire Investigation},
journal = {Procedia Engineering},
volume = {135},
pages = {357-362},
year = {2016},
note = {2015 International Conference on Performance-based Fire and Fire Protection Engineering (ICPFFPE 2015)},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2016.01.142},
url = {https://www.sciencedirect.com/science/article/pii/S1877705816001466},
author = {Shi Wang and Zhong-jun Shu},
keywords = {Fire investigation, Abduction, Logical thinking},
abstract = {To solve the problem of fire investigation caused by lack of exacting logical reasoning, it is of significance in helping that abduction, an important logical thinking should be introduced to the field of fire investigation. This paper first analyzes the fundamental reasoning forms of abduction as well as its general situation of application. Combined with practical work experience, the mode of application of abduction to fire investigation is put forward. The author shows it in detail by analyzing a real fire case. It is north noting that some matters needing attention in application are presented in the end. This paper will be conductive to constructing the right logical reasoning model in fire investigation.}
}
@article{DINU20242902,
title = {An integrated benchmark for verbal creativity testing of LLMs and humans},
journal = {Procedia Computer Science},
volume = {246},
pages = {2902-2911},
year = {2024},
note = {28th International Conference on Knowledge Based and Intelligent information and Engineering Systems (KES 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.09.380},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924024141},
author = {Anca Dinu and Andra Maria Florescu},
keywords = {LLM creativity, AI creativity, verbal creativity tests, human-machine comparison},
abstract = {Until fairly recently, creativity was a human-specific characteristic. Computational creativity or artificial creativity was established as a domain in the late 90’s, with different fields such as verbal, musical, or graphical creativity. With the latest technological advances and the appearance of Large Language Models (LLMs), creativity as a feature of machines gained more and more interest in the scientific community. The scope of this study is twofold: to design a comprehensive benchmark for verbal creativity assessment of LLMs and then to run the same creativity tests on different LLMs as well as on humans, for a direct comparison. We aimed to raise the replicability and extensibility of the creativity assessment of LLMs. Hence, we adapted different types of creativity tests and different criteria from psychology to fit the LLMs profile. We also employed computer-assisted evaluation methods, by using the Open Creativity Scoring with Artificial Intelligence (OCSAI), as we wanted to focus exclusively on automated approaches to assessing creativity. We quantitatively and qualitatively analyzed the data set of both human and machine-generated answers and interpreted the results. Finally, we provide both the original verbal creativity test that we have designed, and the curated data comprising all the collected answers, from the LLMs and from the humans that participated in this research.}
}
@article{MARCHETTI20121517,
title = {Mindwandering heightens the accessibility of negative relative to positive thought},
journal = {Consciousness and Cognition},
volume = {21},
number = {3},
pages = {1517-1525},
year = {2012},
issn = {1053-8100},
doi = {https://doi.org/10.1016/j.concog.2012.05.013},
url = {https://www.sciencedirect.com/science/article/pii/S1053810012001420},
author = {Igor Marchetti and Ernst H.W. Koster and Rudi {De Raedt}},
keywords = {Mindwandering, Negative cognitions, Mood, Depression, Individual differences},
abstract = {Mindwandering (MW) is associated with both positive and negative outcomes. Among the latter, negative mood and negative cognitions have been reported. However, the underlying mechanisms linking mindwandering to negative mood and cognition are still unclear. We hypothesized that MW could either directly enhance negative thinking or indirectly heighten the accessibility of negative thoughts. In an undergraduate sample (n=79) we measured emotional thoughts during the Sustained Attention on Response Task (SART) which induces MW, and accessibility of negative cognitions by means of the Scrambled Sentences Task (SST) after the task. We also measured depressive symptoms and rumination. Results show that in individuals with elevated levels of depressive symptoms MW during SART predicts higher accessibility of negative thoughts after the task, rather than negative thinking during the task. These findings contribute to our understanding of the underlying mechanisms of MW and provide insight into the relationship between task-involvement and affect.}
}
@article{HU2022116276,
title = {Multi granularity based label propagation with active learning for semi-supervised classification},
journal = {Expert Systems with Applications},
volume = {192},
pages = {116276},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2021.116276},
url = {https://www.sciencedirect.com/science/article/pii/S0957417421015840},
author = {Shengdan Hu and Duoqian Miao and Witold Pedrycz},
keywords = {Semi-supervised learning, Granular computing, Multi granularity, Label propagation, Active learning, Three-way decision},
abstract = {Semi-supervised learning (SSL) methods, which exploit both the labeled and unlabeled data, have attracted a lot of attention. One of the major categories of SSL methods, graph-based semi-supervised learning (GBSSL) learns labels of unlabeled data on an adjacency graph, where neighborhood sparse graph is often used to reduce computational complexity. However, the neighborhood size is difficult to set. Instead of assigning a concrete value of neighborhood size, we propose a new label propagation algorithm called multi granularity based label propagation (MGLP) and developed from the view of granular computing. In MGLP, labels of unlabeled data are learned by two classic label propagation processes with diverse neighborhood size k, where granular computing delivers a guiding strategy to leverage multiple level neighborhood information granules, and three-way decision acts as an active learning strategy to select the unlabeled data for further annotating. Through the iterative procedures of label propagating, data annotating and data subset updating, the ultimate pseudo label accuracy of unlabeled data may be higher. Theoretically, the accuracy of pseudo labels is enhanced in some scenarios. Experimentally, the results of simulation studies on ten benchmark datasets, show that the proposed method MGLP can rise pseudo labels accuracy by 8.6% than LP (label propagation), 6.5% than LNP (linear neighborhood propagation), 6.4% than LPSN (label propagation through sparse neighborhood), 4.5% than Adaptive-NP (adaptive neighborhood propagation) and 4.6% than CRLP (consensus rate-based label propagation). It also provides a novel way to annotate data.}
}
@article{BEHR199399,
title = {Computation of incompressible flows with implicit finite element implementations on the Connection Machine},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {108},
number = {1},
pages = {99-118},
year = {1993},
issn = {0045-7825},
doi = {https://doi.org/10.1016/0045-7825(93)90155-Q},
url = {https://www.sciencedirect.com/science/article/pii/004578259390155Q},
author = {M. Behr and A. Johnson and J. Kennedy and S. Mittal and T. Tezduyar},
abstract = {Two implicit finite element formulations for incompressible flows have been implemented on the Connection Machine supercomputers and successfully applied to a set of time-dependent problems. The stabilized space-time formulation for moving boundaries and interfaces, and a new stabilized velocity-pressure-stress formulation are both described, and significant aspects of the implementation of these methods on massively parallel architectures are discussed. Several numerical results for flow problems involving moving as well as fixed cylinders and airfoils are reported. The parallel implementation, taking full advantage of the computational speed of the new generation of supercomputers, is found to be a significant asset in fluid dynamics research. Its current capability to solve large-scale problems, especially when coupled with the potential for growth enjoyed by massively parallel computers, make the implementation a worthwhile enterprise.}
}
@article{GLASSMAN20101412,
title = {Pragmatism, connectionism and the internet: A mind’s perfect storm},
journal = {Computers in Human Behavior},
volume = {26},
number = {6},
pages = {1412-1418},
year = {2010},
note = {Online Interactivity: Role of Technology in Behavior Change},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2010.04.019},
url = {https://www.sciencedirect.com/science/article/pii/S0747563210000956},
author = {Michael Glassman and Min Ju Kang},
keywords = {Internet, Dewey, Connectionism, Democracy},
abstract = {This paper explores that natural relationships between Pragmatic theory of knowing, the dynamic structuring of the mind and thinking suggested by connectionist theory, and the way information is distributed and organized through the world wide web (www). We suggest that these three “innovations” can be brought together to offer a better understanding of the way the human mind works. The internet and the information revolution may finally offer the opportunity to use and develop inductive learning practices and information based social inquiry in ways Pragmatic philosophers envisioned a hundred years ago, while the recent rise of connectionist and cognitive architecture works provides a concrete context for such developments. This confluence of process represents the type of synergy that only history can offer. The information revolution – exemplified by both the rise of connectionism and the internet – is the apotheosis of the Pragmatic revolution – bringing together radical empiricism and democratization of information in community practice. We offer three important realizations in our understanding of how information is organized and thinking progresses made possible by burgeoning virtual communities on the internet – open source thinking, scale-free networks, and interrelationships in the development of blogs to illustrate our thesis.}
}
@article{DEKKER2017554,
title = {Rasmussen's legacy and the long arm of rational choice},
journal = {Applied Ergonomics},
volume = {59},
pages = {554-557},
year = {2017},
note = {The Legacy of Jens Rasmussen},
issn = {0003-6870},
doi = {https://doi.org/10.1016/j.apergo.2016.02.007},
url = {https://www.sciencedirect.com/science/article/pii/S0003687016300254},
author = {Sidney W.A. Dekker},
keywords = {Rasmussen, Rational choice, Human error, Second victim, Incidents},
abstract = {Rational choice theory says that operators and others make decisions by systematically and consciously weighing all possible outcomes along all relevant criteria. This paper first traces the long historical arm of rational choice thinking in the West to Judeo-Christian thinking, Calvin and Weber. It then presents a case study that illustrates the consequences of the ethic of rational choice and individual responsibility. It subsequently examines and contextualizes Rasmussen's legacy of pushing back against the long historical arm of rational choice, showing that bad outcomes are not the result of human immoral choice, but the product of normal interactions between people and systems. If we don't understand why people did what they did, Rasmussen suggested, it is not because people behaved inexplicably, but because we took the wrong perspective.}
}
@article{20213190,
title = {Eunji Cheong},
journal = {Neuron},
volume = {109},
number = {20},
pages = {3190-3192},
year = {2021},
issn = {0896-6273},
doi = {https://doi.org/10.1016/j.neuron.2021.09.027},
url = {https://www.sciencedirect.com/science/article/pii/S0896627321007005},
abstract = {In Korea, the pandemic has elevated scientists as trusted sources for both policy decisions and dinner table conversation. In an interview with Neuron, Eunji Cheong discusses how we need to support future generations by fostering scientific thinking, patience, and flexibility.}
}
@incollection{KAMAREDDINE2012801,
title = {Russell's Orders in Kripke's Theory of Truth and Computational Type Theory},
editor = {Dov M. Gabbay and Akihiro Kanamori and John Woods},
series = {Handbook of the History of Logic},
publisher = {North-Holland},
volume = {6},
pages = {801-845},
year = {2012},
booktitle = {Sets and Extensions in the Twentieth Century},
issn = {1874-5857},
doi = {https://doi.org/10.1016/B978-0-444-51621-3.50011-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780444516213500116},
author = {Fairouz Kamareddine and Twan Laan and Robert Constable}
}
@article{HONDA201718,
title = {The difference in foresight using the scanning method between experts and non-experts},
journal = {Technological Forecasting and Social Change},
volume = {119},
pages = {18-26},
year = {2017},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2017.03.005},
url = {https://www.sciencedirect.com/science/article/pii/S004016251730313X},
author = {Hidehito Honda and Yuichi Washida and Akihito Sudo and Yuichiro Wajima and Keigo Awata and Kazuhiro Ueda},
keywords = {Foresight, Scanning method, Divergent thinking, Difference between experts and non-experts, Creativity},
abstract = {We examined the factors that produce differences in generating scenarios on the near future using the scanning method. Participants were asked to briefly read (scan) 151 articles about new technology, the latest customs, fashion, social change, value system transition, or emerging social problems, and then to generate three scenarios about the near future based on the articles. We compared the generated scenarios between scanning method experts and non-experts with no prior experience with the scanning method. We found that experts generated more unique scenarios than non-experts did, and that experts and non-experts differed in the diversity of articles referenced when generating scenarios. We discuss the relationship between the present findings and previous findings on divergent thinking.}
}
@incollection{CORMACK2005325,
title = {4.1 - Computational Models of Early Human Vision},
editor = {AL BOVIK},
booktitle = {Handbook of Image and Video Processing (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {Burlington},
pages = {325-IX},
year = {2005},
series = {Communications, Networking and Multimedia},
isbn = {978-0-12-119792-6},
doi = {https://doi.org/10.1016/B978-012119792-6/50083-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780121197926500838},
author = {Lawrence K. Cormack}
}
@article{OBERKAMPF2002209,
title = {Verification and validation in computational fluid dynamics},
journal = {Progress in Aerospace Sciences},
volume = {38},
number = {3},
pages = {209-272},
year = {2002},
issn = {0376-0421},
doi = {https://doi.org/10.1016/S0376-0421(02)00005-2},
url = {https://www.sciencedirect.com/science/article/pii/S0376042102000052},
author = {William L. Oberkampf and Timothy G. Trucano},
abstract = {Verification and validation (V&V) are the primary means to assess accuracy and reliability in computational simulations. This paper presents an extensive review of the literature in V&V in computational fluid dynamics (CFD), discusses methods and procedures for assessing V&V, and develops a number of extensions to existing ideas. The review of the development of V&V terminology and methodology points out the contributions from members of the operations research, statistics, and CFD communities. Fundamental issues in V&V are addressed, such as code verification versus solution verification, model validation versus solution validation, the distinction between error and uncertainty, conceptual sources of error and uncertainty, and the relationship between validation and prediction. The fundamental strategy of verification is the identification and quantification of errors in the computational model and its solution. In verification activities, the accuracy of a computational solution is primarily measured relative to two types of highly accurate solutions: analytical solutions and highly accurate numerical solutions. Methods for determining the accuracy of numerical solutions are presented and the importance of software testing during verification activities is emphasized. The fundamental strategy of validation is to assess how accurately the computational results compare with the experimental data, with quantified error and uncertainty estimates for both. This strategy employs a hierarchical methodology that segregates and simplifies the physical and coupling phenomena involved in the complex engineering system of interest. A hypersonic cruise missile is used as an example of how this hierarchical structure is formulated. The discussion of validation assessment also encompasses a number of other important topics. A set of guidelines is proposed for designing and conducting validation experiments, supported by an explanation of how validation experiments are different from traditional experiments and testing. A description is given of a relatively new procedure for estimating experimental uncertainty that has proven more effective at estimating random and correlated bias errors in wind-tunnel experiments than traditional methods. Consistent with the authors’ contention that nondeterministic simulations are needed in many validation comparisons, a three-step statistical approach is offered for incorporating experimental uncertainties into the computational analysis. The discussion of validation assessment ends with the topic of validation metrics, where two sample problems are used to demonstrate how such metrics should be constructed. In the spirit of advancing the state of the art in V&V, the paper concludes with recommendations of topics for future research and with suggestions for needed changes in the implementation of V&V in production and commercial software.}
}
@article{ZHU2025106252,
title = {Executive functions and mathematical ability in early elementary school children: The moderating role of family socioeconomic status},
journal = {Journal of Experimental Child Psychology},
volume = {256},
pages = {106252},
year = {2025},
issn = {0022-0965},
doi = {https://doi.org/10.1016/j.jecp.2025.106252},
url = {https://www.sciencedirect.com/science/article/pii/S002209652500058X},
author = {Xiaoliang Zhu and Yixin Tang and Zhuoyue Pang and Xin Zhao},
keywords = {Executive functions, Mathematical ability, Family socioeconomic status, Early elementary school children, Longitudinal study},
abstract = {Children’s executive functions (EFs) and family socioeconomic status (SES) play critical roles in the development of mathematical ability in early elementary education. However, the potential interplay between EFs and SES remains underexplored. This study addressed this gap by comprehensively investigating the moderating role of SES in the relationship between EF subcomponents (i.e., interference inhibition, response inhibition, and working memory) and children’s concurrent and future mathematical abilities (i.e., arithmetic operations and logical–visuospatial skills). A total of 172 participants (Mage = 6.78 years; 107 boys) took part in the study at the beginning of first grade in elementary school (T1) and 20 months later (T2). We measured EFs, SES, and mathematical ability at T1 and mathematical ability at T2. Results from hierarchical linear regression models indicated that working memory was positively associated with T1 arithmetic operations and logical–visuospatial skills as well as with T2 arithmetic operations. Furthermore, family SES was positively associated with arithmetic operations at both T1 and T2. Notably, we found a significant interaction effect between interference inhibition and SES on T1 arithmetic operations and logical–visuospatial skills. Specifically, interference inhibition was positively related to T1 arithmetic operations and logical–visuospatial skills for children from low- and middle-SES families, but not for children from high-SES families. Our findings contribute to a nuanced understanding of how cognitive and environmental factors jointly influence mathematical development, underscoring the need for targeted interventions for children from different SES backgrounds to support their mathematical ability development.}
}
@article{WANG201854,
title = {Studying cognitive development in cultural context: A multi-level analysis approach},
journal = {Developmental Review},
volume = {50},
pages = {54-64},
year = {2018},
note = {Towards a Cultural Developmental Science},
issn = {0273-2297},
doi = {https://doi.org/10.1016/j.dr.2018.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S0273229717301041},
author = {Qi Wang},
keywords = {Culture, Cognition, Episodic thinking, Memory, Multiple levels of analysis},
abstract = {I discuss a multi-level analysis approach in the study of cognitive development in cultural context. In this approach, culture is conceived of as a system and a process of symbolic mediation, where values, norms, and beliefs manifest in and through customs, rituals, and practices in directing and regulating both intrapersonal and interpersonal psychological functions. To capture the dynamic process in which cognitive development unfolds in cultural context, this approach examines the influence of culture on the developing cognitive skills between groups – group level analysis, between the child and socialization agents – dyadic level analysis, at the level of the child – individual level analysis, and within the child – situation level analysis. The temporal level analysis further situates cognitive development in historical time. By utilizing different analytical and methodological strategies, the multi-level analysis approach provides converging evidence for the development of cognitive skills in cultural context. Important challenges in this approach include the development of a “big picture” to guide the investigation, methodological training for research assistants, and difficulties in collecting, managing, and analyzing diverse datasets. I discuss the conceptual and methodological issues by using our work on the development of episodic thinking as an example.}
}
@incollection{TOPLAK202253,
title = {3 - Development of the ability to detect and override miserly information processing},
editor = {Maggie E. Toplak},
booktitle = {Cognitive Sophistication and the Development of Judgment and Decision-Making},
publisher = {Academic Press},
pages = {53-87},
year = {2022},
isbn = {978-0-12-816636-9},
doi = {https://doi.org/10.1016/B978-0-12-816636-9.00011-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128166369000116},
author = {Maggie E. Toplak},
keywords = {Miserly information processing, Dual process models, Children and youth, Development, Ratio bias, Belief bias syllogisms, Cognitive reflection},
abstract = {Several judgment and decision-making tasks require overriding an incorrect response that is signaled by miserly information processes. The successful detection and override of conflict between heuristic and analytic processes has been a focus of dual processes models, especially in adult samples. These miserly processing tendencies have also been described in developmental samples. The measurement of resistance to miserly information processing has been assessed using several tasks, including ratio bias, belief bias syllogisms, cognitive reflection, and disjunctive thinking tasks. Several of these tasks have been studied in developmental samples, including in the longitudinal study described in this volume. There is evidence to suggest that resistance to miserly information processing is measurable in children and youth. While judgment and decision-making tasks vary in the degree to which override of miserly processing is required, individuals also vary in their ability to resist miserly processing tendencies. Individual differences in resistance to miserly information processing serve as an additional foundation to support rational thinking performance.}
}
@article{MASOUD201293,
title = {Synthesis, computational, spectroscopic, thermal and antimicrobial activity studies on some metal–urate complexes},
journal = {Spectrochimica Acta Part A: Molecular and Biomolecular Spectroscopy},
volume = {90},
pages = {93-108},
year = {2012},
issn = {1386-1425},
doi = {https://doi.org/10.1016/j.saa.2012.01.028},
url = {https://www.sciencedirect.com/science/article/pii/S1386142512000418},
author = {Mamdouh S. Masoud and Alaa E. Ali and Medhat A. Shaker and Gehan S. Elasala},
keywords = {Uric, Complexes, Synthesis, Spectroscopy, Thermal analysis, Computational},
abstract = {New sixteen uric acid metal complexes of different stoichiometry, stereo-chemistries and modes of interactions were synthesized using different metals Cr, Mn, Fe, Co, Ni, Cu, Cd, UO2, Na and K. The synthesized complexes were characterized by elemental analysis, spectral (IR, UV–Vis and ESR) methods, thermal analysis (TG, DTA and DSC) and magnetic susceptibility studies. Molecular modeling calculations were used to characterize the ligation sites of the free ligand. Furthermore, quantum chemical parameters of uric acid such as the energies of highest occupied molecular orbital (EHOMO), energies of lowest unoccupied molecular orbital (ELUMO), the separation energy (ΔE=ELUMO−EHOMO), the absolute electronegativity, χ, the chemical potential, Pi, the absolute hardness, η and the softness (σ) were obtained for uric acid. Eight different microbial categories were used to study the antimicrobial activity of the free ligand and ten of its complexes. The results indicate that the ligand and its metal complexes possess antimicrobial properties. The stoichiometry of iron–uric acid complex was studied by using different spectrophotometric methods.}
}
@article{ROBERTS2024100502,
title = {New approach methodologies (NAMs) in drug safety assessment: A vision of the future},
journal = {Current Opinion in Toxicology},
volume = {40},
pages = {100502},
year = {2024},
issn = {2468-2020},
doi = {https://doi.org/10.1016/j.cotox.2024.100502},
url = {https://www.sciencedirect.com/science/article/pii/S2468202024000445},
author = {Ruth A. Roberts},
keywords = {3Rs, NAMs, FTIH, Drug development, ICH, Nanobots},
abstract = {Much progress has been made in reducing and refining animal use in toxicology testing, but progress in the use of new approach methodologies (NAMs) to replace animals is disappointing. There are many highly sophisticated NAMs available, but societal, regulatory and political barriers to their implementation remain. Change requires vision, starting with imagining a future where we are successful. Specifically, this would comprise the registration of safe and effective medicines without animal tests. How do we achieve this vision? Thinking differently, in silico methods could be used to provide a detailed assessment of target- and modality-related toxicological risks, coupled with modelling of exposure. In vitro NAMs such as microphysiological systems, microelectrode array and ion channel panels could then be employed to address hypothetical risks. Finally, the safety of first time in human trials could be assessed and assured using circulating nanobots that measure conventional clinical pathology parameters alongside new biomarkers such as circulating tissue DNA. This may seem the stuff of fantasy, but imagination is key to shaping a better future and all change starts with a vision, however far-fetched it may seem today.}
}
@article{BAYNE201332,
title = {Thought},
journal = {New Scientist},
volume = {219},
number = {2935},
pages = {32-39},
year = {2013},
issn = {0262-4079},
doi = {https://doi.org/10.1016/S0262-4079(13)62293-9},
url = {https://www.sciencedirect.com/science/article/pii/S0262407913622939},
author = {Tim Bayne},
abstract = {Conscious or unbidden, thoughts fill our heads from morning to night. But what are they, and what exactly is thinking? Join philosopher Tim Bayne on a journey into the fantastic, elusive and ceaseless world our minds create}
}
@article{FREYBERG20231,
title = {The morphological paradigm in robotics},
journal = {Studies in History and Philosophy of Science},
volume = {100},
pages = {1-11},
year = {2023},
issn = {0039-3681},
doi = {https://doi.org/10.1016/j.shpsa.2023.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S0039368123000742},
author = {Sascha Freyberg and Helmut Hauser},
keywords = {Bionics, Embodied cognition, Morphology, Morphological computation, Soft robotics, Principles of orientation and control},
abstract = {In the paper, we are going to show how robotics is undergoing a shift in a bionic direction after a period of emphasis on artificial intelligence and increasing computational efficiency, which included isolation and extreme specialization. We assemble these new developments under the label of the morphological paradigm. The change in its paradigms and the development of alternatives to the principles that dominated robotics for a long time contains a more general epistemological significance. The role of body, material, environment, interaction and the paradigmatic status of biological and evolutionary systems for the principles of control are crucial here. Our focus will be on the introduction of the morphological paradigm in a new type of robotics and to contrast the interests behind this development with the interests shaping former models. The article aims to give a clear account of the changes in principles of orientation and control as well as concluding general observation in terms of historical epistemology, suggesting further political-epistemological analysis.}
}
@article{LI2023104935,
title = {Development of a distributed MR-IoT method for operations and maintenance of underground pipeline network},
journal = {Tunnelling and Underground Space Technology},
volume = {133},
pages = {104935},
year = {2023},
issn = {0886-7798},
doi = {https://doi.org/10.1016/j.tust.2022.104935},
url = {https://www.sciencedirect.com/science/article/pii/S0886779822005764},
author = {Wei Li and Zhoujing Ye and Yajian Wang and Hailu Yang and Songli Yang and Zhenlong Gong and Linbing Wang},
keywords = {Underground Pipeline Network, Mixed Reality, IoT Cloud Platform, Data Communication, Operation and Maintenance},
abstract = {The underground pipeline network (UPN) is an essential infrastructure and plays an irreplaceable role in national defense and urban activities. The complexity of structural environment and management makes its operation and maintenance difficult. To solve this problem, a distributed mixed reality (MR) and internet of things (IoT) system is developed through game thinking. Firstly, digital models are created based on design drawings and real-world environments, and then an MR system for the UPN is built by the game engine and the OpenXR platform. Secondly, an IoT cloud platform is built to connect with the MR system based on the API sets and cloud services; the data communication between sensors and MR devices is linked with the Socket method, and the data filtering model is constructed by the Kalman algorithm to realize the information exchange between the field workers and the backend managers. Finally, the National Center for Materials Service Safety at the University of Science and Technology Beijing (NCMS_USTB) is used as the experimental site to test this system, and its underground sewage and rainwater pipeline network are used to simulate the key problems in the operation and maintenance. The effect of the application shows that there is potential technical complementarity between the MR and IoT, and the distributed MR-IoT approach can be used as a new technical reference for the operation and maintenance of the UPN.}
}
@article{DENNER2012240,
title = {Computer games created by middle school girls: Can they be used to measure understanding of computer science concepts?},
journal = {Computers & Education},
volume = {58},
number = {1},
pages = {240-249},
year = {2012},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2011.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S0360131511001849},
author = {Jill Denner and Linda Werner and Eloy Ortiz},
keywords = {Construction of computer games, Secondary education, Programming, After-school},
abstract = {Computer game programming has been touted as a promising strategy for engaging children in the kinds of thinking that will prepare them to be producers, not just users of technology. But little is known about what they learn when programming a game. In this article, we present a strategy for coding student games, and summarize the results of an analysis of 108 games created by middle school girls using Stagecast Creator in an after school class. The findings show that students engaged in moderate levels of complex programming activity, created games with moderate levels of usability, and that the games were characterized by low levels of code organization and documentation. These results provide evidence that game construction involving both design and programming activities can support the learning of computer science concepts.}
}
@article{FRANTZ2003265,
title = {Herbert Simon. Artificial intelligence as a framework for understanding intuition},
journal = {Journal of Economic Psychology},
volume = {24},
number = {2},
pages = {265-277},
year = {2003},
note = {The Economic Psychology of Herbert A. Simon},
issn = {0167-4870},
doi = {https://doi.org/10.1016/S0167-4870(02)00207-6},
url = {https://www.sciencedirect.com/science/article/pii/S0167487002002076},
author = {Roger Frantz},
keywords = {Herbert Simon, Intuition, Artificial intelligence, Bounded rationality, Economics and psychology},
abstract = {Herbert Simon made overlapping substantive contributions to the fields of economics, psychology, cognitive science, artificial intelligence, decision theory, and organization theory. Simon’s work was motivated by the belief that neither the human mind, human thinking and decision making, nor human creativity need be mysterious. It was after he helped create “thinking” machines that Simon came to understand human intuition as subconscious pattern recognition. In doing so he showed that intuition need not be associated with magic and mysticism, and that it is complementary with analytical thinking. This paper will show how the overlaps in his work and especially his work on AI affected his view towards intuition.}
}
@article{FU20012567,
title = {Analytical and computational description of effect of grain size on yield stress of metals},
journal = {Acta Materialia},
volume = {49},
number = {13},
pages = {2567-2582},
year = {2001},
issn = {1359-6454},
doi = {https://doi.org/10.1016/S1359-6454(01)00062-3},
url = {https://www.sciencedirect.com/science/article/pii/S1359645401000623},
author = {H.-H. Fu and D.J. Benson and M.A. Meyers},
keywords = {Nanocrystalline materials, Grain size, Hall–Petch},
abstract = {Four principal factors contribute to grain-boundary strengthening: (a) the grain boundaries act as barriers to plastic flow; (b) the grain boundaries act as dislocation sources; (c) elastic anisotropy causes additional stresses in grain-boundary surroundings; (d) multislip is activated in the grain-boundary regions, whereas grain interiors are initially dominated by single slip, if properly oriented. As a result, the regions adjoining grain boundaries harden at a rate much higher than grain interiors. A phenomenological constitutive equation predicting the effect of grain size on the yield stress of metals is discussed and extended to the nanocrystalline regime. At large grain sizes, it has the Hall–Petch form, and in the nanocrystalline domain the slope gradually decreases until it asymptotically approaches the flow stress of the grain boundaries. The material is envisaged as a composite, comprised of the grain interior, with flow stress σfG, and grain boundary work-hardened layer, with flow stress σfGB. The predictions of this model are compared with experimental measurements over the mono, micro, and nanocrystalline domains. Computational predictions are made of plastic flow as a function of grain size incorporating differences of dislocation accumulation rate in grain-boundary regions and grain interiors. The material is modeled as a monocrystalline core surrounded by a mantle (grain-boundary region) with a high work hardening rate response. This is the first computational plasticity calculation that accounts for grain size effects in a physically-based manner. A discussion of statistically stored and geometrically necessary dislocations in the framework of strain-gradient plasticity is introduced to describe these effects. Grain-boundary sliding in the nanocrystalline regime is predicted from calculations using the Raj–Ashby model and incorporated into the computations; it is shown to predispose the material to shear localization.}
}
@article{YANG2001167,
title = {Computational verb systems: computing with perceptions of dynamics},
journal = {Information Sciences},
volume = {134},
number = {1},
pages = {167-248},
year = {2001},
note = {Computing with Words},
issn = {0020-0255},
doi = {https://doi.org/10.1016/S0020-0255(01)00096-2},
url = {https://www.sciencedirect.com/science/article/pii/S0020025501000962},
author = {Tao Yang},
abstract = {The concepts and principles of (computational) verb logic, (computational) verb set, (computational) verb number, (computational) verb prediction and (computational) verb control are presented.}
}
@article{DEBRIGARD2021104574,
title = {Perceived similarity of imagined possible worlds affects judgments of counterfactual plausibility},
journal = {Cognition},
volume = {209},
pages = {104574},
year = {2021},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2020.104574},
url = {https://www.sciencedirect.com/science/article/pii/S0010027720303930},
author = {Felipe {De Brigard} and Paul Henne and Matthew L. Stanley},
keywords = {Imagination, Counterfactual thinking, Plausibility, Similarity, Possible worlds},
abstract = {People frequently entertain counterfactual thoughts, or mental simulations about alternative ways the world could have been. But the perceived plausibility of those counterfactual thoughts varies widely. The current article interfaces research in the philosophy and semantics of counterfactual statements with the psychology of mental simulations, and it explores the role of perceived similarity in judgments of counterfactual plausibility. We report results from seven studies (N = 6405) jointly supporting three interconnected claims. First, the perceived plausibility of a counterfactual event is predicted by the perceived similarity between the possible world in which the imagined situation is thought to occur and the actual world. Second, when people attend to differences between imagined possible worlds and the actual world, they think of the imagined possible worlds as less similar to the actual world and tend to judge counterfactuals in such worlds as less plausible. Lastly, when people attend to what is identical between imagined possible worlds and the actual world, they think of the imagined possible worlds as more similar to the actual world and tend to judge counterfactuals in such worlds as more plausible. We discuss these results in light of philosophical, semantic, and psychological theories of counterfactual thinking.}
}
@article{ZHOU2025104052,
title = {Adaptive-solver framework for dynamic strategy selection in large language model reasoning},
journal = {Information Processing & Management},
volume = {62},
number = {3},
pages = {104052},
year = {2025},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2024.104052},
url = {https://www.sciencedirect.com/science/article/pii/S0306457324004114},
author = {Jianpeng Zhou and Wanjun Zhong and Yanlin Wang and Jiahai Wang},
keywords = {Large language models, Reasoning, Math word problems, Test-time computation allocation, Dynamic strategy selection},
abstract = {Large Language Models (LLMs) demonstrate impressive ability in handling reasoning tasks. However, unlike humans who can instinctively adapt their problem-solving strategies to the complexity of task, most LLM-based methods adopt a one-size-fits-all approach. These methods employ consistent models, sample sizes, prompting methods and levels of problem decomposition, regardless of the problem complexity. The inflexibility of these methods can bring unnecessary computational overhead or sub-optimal performance. To address this limitation, we introduce an Adaptive-Solver (AS) framework that dynamically adapts solving strategies to suit various problems, enabling the flexible allocation of test-time computational resources. The framework functions with two primary modules. The initial evaluation module assesses the reliability of the current solution using answer consistency. If the solution is deemed unreliable, the subsequent adaptation module comes into play. Within this module, various types of adaptation strategies are employed collaboratively. Through such dynamic and multi-faceted adaptations, our framework can help reduce computational consumption and improve performance. Experimental results from complex reasoning benchmarks reveal that our method can significantly reduce API costs (up to 85%) while maintaining original performance. Alternatively, it achieves up to 4.5% higher accuracy compared to the baselines at the same cost. The datasets and code are available at https://github.com/john1226966735/Adaptive-Solver.}
}
@article{ERBOZ202587,
title = {Electromagnetic radiation and biophoton emission in neuronal communication and neurodegenerative diseases},
journal = {Progress in Biophysics and Molecular Biology},
volume = {195},
pages = {87-99},
year = {2025},
issn = {0079-6107},
doi = {https://doi.org/10.1016/j.pbiomolbio.2024.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S0079610724001159},
author = {Aysin Erboz and Elif Kesekler and Pier Luigi Gentili and Vladimir N. Uversky and Orkid Coskuner-Weber},
keywords = {Electromagnetic radiation, Biophotons, Neurodegenerative diseases, Neuron communication, Intrinsically disordered proteins},
abstract = {The intersection of electromagnetic radiation and neuronal communication, focusing on the potential role of biophoton emission in brain function and neurodegenerative diseases is an emerging research area. Traditionally, it is believed that neurons encode and communicate information via electrochemical impulses, generating electromagnetic fields detectable by EEG and MEG. Recent discoveries indicate that neurons may also emit biophotons, suggesting an additional communication channel alongside the regular synaptic interactions. This dual signaling system is analyzed for its potential in synchronizing neuronal activity and improving information transfer, with implications for brain-like computing systems. The clinical relevance is explored through the lens of neurodegenerative diseases and intrinsically disordered proteins, where oxidative stress may alter biophoton emission, offering clues for pathological conditions, such as Alzheimer's and Parkinson's diseases. The potential therapeutic use of Low-Level Laser Therapy (LLLT) is also examined for its ability to modulate biophoton activity and mitigate oxidative stress, presenting new opportunities for treatment. Here, we invite further exploration into the intricate roles the electromagnetic phenomena play in brain function, potentially leading to breakthroughs in computational neuroscience and medical therapies for neurodegenerative diseases.}
}
@article{PAZBARUCH2023101294,
title = {Cognitive abilities and creativity: The role of working memory and visual processing},
journal = {Thinking Skills and Creativity},
volume = {48},
pages = {101294},
year = {2023},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2023.101294},
url = {https://www.sciencedirect.com/science/article/pii/S1871187123000640},
author = {Nurit Paz-Baruch and Rotem Maor},
keywords = {Cognitive abilities, Creativity, Working memory, Visual processing},
abstract = {Recent studies have revealed the importance of cognitive abilities in creative thinking. However, most research addressed adults and only a few studies have examined the ways these correlations are manifested among young children. The present study explores the role of various cognitive abilities in creativity among school children. Measures of creativity, visual-spatial working memory (VSWM), verbal short-term memory (STM), working memory (WM), and visual processing (VP) were administered to 331 students in Grades 4 and 5. Cluster analysis was used to group students' creativity levels. A multivariate analysis of variance (MANOVA) was conducted to test differences in cognitive abilities across the three clusters. Group differences between high and moderate level creativity students and low creativity students were found regarding VP abilities in the following tests: VSWM, visual discrimination (VD), and Raven's Colored Progressive Matrices (RCPM). Group differences between high creativity students and low creativity students were also found on verbal STM and WM. Additionally, structural equation modeling (SEM) analysis revealed that VP can significantly account for unique variances associated with creativity, while verbal STM and WM are not significantly related to creativity. These findings enlighten the cognitive processes underlying creativity in young children.}
}
@article{KLATT2009536,
title = {Perspectives for process systems engineering—Personal views from academia and industry},
journal = {Computers & Chemical Engineering},
volume = {33},
number = {3},
pages = {536-550},
year = {2009},
note = {Selected Papers from the 17th European Symposium on Computer Aided Process Engineering held in Bucharest, Romania, May 2007},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2008.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S0098135408001737},
author = {Karsten-Ulrich Klatt and Wolfgang Marquardt},
keywords = {Review, Modeling, Design, Optimization, Control, Operations, Numerical algorithms, Software, Computer-aided process engineering (CAPE)},
abstract = {Process systems engineering (PSE) has been an active research field for almost 50 years. Its major achievements include methodologies and tools to support process modeling, simulation and optimization (MSO). Mature, commercially available technologies have been penetrating all fields of chemical engineering in academia as well as in industrial practice. MSO technologies have become a commodity, they are not a distinguishing feature of the PSE field any more. Consequently, PSE has to reassess and to reposition its future research agenda. Emphasis should be put on model-based applications in all PSE domains including product and process design, control and operations. Furthermore, systems thinking and systems problem solving have to be prioritized rather than the mere application of computational problem solving methods. This essay reflects on the past, present and future of PSE from an academic and industrial point of view. It redefines PSE as an active and future-proof research field which can play an active role in providing enabling technologies for product and process innovations in the chemical industries and beyond.}
}
@article{LEE2005261,
title = {Insights into nucleic acid reactivity through gas-phase experimental and computational studies},
journal = {International Journal of Mass Spectrometry},
volume = {240},
number = {3},
pages = {261-272},
year = {2005},
note = {Mass Spectrometry of Biopolymers: From Model Systems to Ribosomes},
issn = {1387-3806},
doi = {https://doi.org/10.1016/j.ijms.2004.09.020},
url = {https://www.sciencedirect.com/science/article/pii/S1387380604003975},
author = {Jeehiun K. Lee},
keywords = {Nucleic acids, RNA, DNA, Enzyme nucleobase, Acidity, Proton affinity},
abstract = {Accurate measurements of the acidities and basicities of nucleic bases and nucleic base derivatives is essential for understanding issues of fundamental importance in biological systems. Hydrogen bonding modulates recognition of DNA and RNA bases, and the interaction energy between two bonded complementary nucleobases is dependent on the intrinsic basicity and acidity of the acceptor and donor groups. In addition, understanding the intrinsic reactivity of nucleic bases can shed light on key biosynthetic mechanisms for which nucleobases are substrates. In this review, we highlight advances in our lab toward understanding the fundamental reactivity of DNA and RNA. In particular, we focus on our investigation of the gas phase acidities and basicities of natural and unnatural nucleobases, and the implications of our results for the mechanisms of nucleotide biosynthetic and repair enzymes.}
}
@article{KIMSEY1994113,
title = {Parallel computation of impact dynamics},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {119},
number = {1},
pages = {113-121},
year = {1994},
issn = {0045-7825},
doi = {https://doi.org/10.1016/0045-7825(94)00079-4},
url = {https://www.sciencedirect.com/science/article/pii/0045782594000794},
author = {K.D. Kimsey and M.A. Olson},
abstract = {This paper discusses a parallel algorithm and data structures for implementing multimaterial, two-step Eulerian finite difference solution schemes on hypercube architectures. Selected problems in impact dynamics have been modeled on the Connection Machine model CM5, and the results are compared with computational results reported in the literature, as well as direct comparison with experimental data.}
}
@article{MATHESON2020116697,
title = {The role of the motor system in generating creative thoughts},
journal = {NeuroImage},
volume = {213},
pages = {116697},
year = {2020},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2020.116697},
url = {https://www.sciencedirect.com/science/article/pii/S1053811920301841},
author = {Heath E. Matheson and Yoed N. Kenett},
keywords = {Motor system, Creativity, Simulations, Embodied cognition, Grounded cognition, Divergent thinking, Improvisation},
abstract = {Neurocognitive research is pertinent to developing mechanistic models of how humans generate creative thoughts. Such models usually overlook the role of the motor cortex in creative thinking. The framework of embodied or grounded cognition suggests that creative thoughts (e.g. using a shoe as a hammer, improvising a piano solo) are partially served by simulations of motor activity associated with tools and their use. The major hypothesis stemming from the embodied or grounded account is that, while the motor system is used to execute actions, simulations within this system also support higher-order cognition, creativity included. That is, the cognitive process of generating creative output, not just executing it, is deeply embedded in motor processes. Here, we highlight a collection of neuroimaging research that implicates the motor system in generating creative thoughts, including some evidence for its functionally necessary role in generating creative output. Specifically, the grounded or embodied framework suggests that generating creative output may, in part, rely on motor simulations of possible actions, and that these simulations may by partially implemented in the motor regions themselves. In such cases, action simulations (i.e. reactivating or re-using the motor system), do not result in overt action but instead are used to support higher-order cognitive goals like generating creative uses or improvising.}
}
@incollection{ZIMMERMAN2005255,
title = {VIII - Development of Theory with Computation},
editor = {M. Olivucci},
series = {Theoretical and Computational Chemistry},
publisher = {Elsevier},
volume = {16},
pages = {255-278},
year = {2005},
booktitle = {Computational Photochemistry},
issn = {1380-7323},
doi = {https://doi.org/10.1016/S1380-7323(05)80025-1},
url = {https://www.sciencedirect.com/science/article/pii/S1380732305800251},
author = {Howard E. Zimmerman}
}
@article{ATREIDES202135,
title = {E-governance with ethical living democracy},
journal = {Procedia Computer Science},
volume = {190},
pages = {35-39},
year = {2021},
note = {2020 Annual International Conference on Brain-Inspired Cognitive Architectures for Artificial Intelligence: Eleventh Annual Meeting of the BICA Society},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921012461},
author = {Kyrtin Atreides},
keywords = {mASI, AGI, e-governance, mediated artificial superintelligence, collective superintelligence, direct digital democracy, liquid democracy, EAP, Effective Altruistic Principles, Ethical Living Democracy, ELD},
abstract = {A new form of e-governance is proposed based on systems seen in biological life at all scales. This model of e-governance offers the performance of collective superintelligence, equally high ethical quality, and a substantial reduction in resource requirements for government functions. In addition, the problems seen in modern forms of government such as misrepresentation, corruption, lack of expertise, short-term thinking, political squabbling, and popularity contests may be rendered virtually obsolete by this approach. Lastly, this model of government generates a digital ecosystem of intelligent life which mirrors physical citizens, serving to bridge the emotional divide between physical and digital life, while also producing the first form of government able to keep pace with accelerating technological progress.}
}
@article{SUI2001529,
title = {Terrae Incognitae and Limits of Computation: Whither GIScience?},
journal = {Computers, Environment and Urban Systems},
volume = {25},
number = {6},
pages = {529-533},
year = {2001},
issn = {0198-9715},
doi = {https://doi.org/10.1016/S0198-9715(01)00027-8},
url = {https://www.sciencedirect.com/science/article/pii/S0198971501000278},
author = {Daniel Sui}
}
@article{MORGAN20052564,
title = {The visual computation of 2-D area by human observers},
journal = {Vision Research},
volume = {45},
number = {19},
pages = {2564-2570},
year = {2005},
issn = {0042-6989},
doi = {https://doi.org/10.1016/j.visres.2005.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S0042698905002075},
author = {M.J. Morgan},
keywords = {Psychophysics, Shape, Weber fraction},
abstract = {Normal human observers compared either the width, height or area of two simultaneously-presented shapes (the standard and the test), with a cue to indicate which decision had to be made. On ‘area’ trials, test width was a random variable, ensuring that neither shape (aspect ratio), width nor height by themselves was a reliable signal. Weber fractions for width and height of both ellipses and rectangles were in the range 5–10%, but for area they were higher (10–20%) than predicted from the combination of noisy width and height decisions. With ellipses, observers were more likely to overestimate width or height when the other dimension differed from the standard in the same direction (e.g. both greater). We conclude that observers have no access to high-precision codes for 2-D area, and that they base their decisions on a variety of heuristics derived from 1-D codes. A second experiment measured acuity for changes in aspect ratio. For ellipses, accuracy for aspect ratio was higher than predicted by the combination of noisy width and height signals; for rectangles it was worse, suggesting that 2-D curvature is a potent cue to shape.}
}
@article{CHEN2014740,
title = {On the Systematic Method to Enhance the Epiphany Ability of Individuals},
journal = {Procedia Computer Science},
volume = {31},
pages = {740-746},
year = {2014},
note = {2nd International Conference on Information Technology and Quantitative Management, ITQM 2014},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2014.05.322},
url = {https://www.sciencedirect.com/science/article/pii/S1877050914004992},
author = {Ailing Chen and Wei Liu and Zhihui Wu and Jun Zhang},
keywords = {Prototype heuristics, epiphany, extenics, Theory of Creativity, sytematic scheme ;},
abstract = {Epiphany is a crucial stage in the process of creative thinking. The prototype heuristic theory has proved that the individual epiphany ability depends on the individual's ability to get out of the fetter of mental fixation, activate the prototype and acquire the key heuristic information from the activated prototype. Based on this theory, this present research combines the findings of extenics, TRIZ and theory of creativity to have developed a systematic method on enhancing individual epiphany ability. Supported by information technology, the method takes theory of creativity as its methodology, extension strategy generation as its framework, element theory its database and knowledge management its feedback chain. The research aims to cultivate creative thinking and eventually enhance the creativity of individuals.}
}
@article{HUNTER2025100118,
title = {Is LIWC reliable, efficient, and effective for the analysis of large online datasets in forensic and security contexts?},
journal = {Applied Corpus Linguistics},
volume = {5},
number = {1},
pages = {100118},
year = {2025},
issn = {2666-7991},
doi = {https://doi.org/10.1016/j.acorp.2025.100118},
url = {https://www.sciencedirect.com/science/article/pii/S2666799125000012},
author = {Madison Hunter and Tim Grant},
keywords = {LIWC, Reliability, Computerized text analysis, Forensic linguistics, Discourse analysis},
abstract = {This article evaluates the reliability, efficiency, and effectiveness of Linguistic Inquiry and Word Count (LIWC; Boyd et al., 2022) for the analysis of a white nationalist forum. This is important because LIWC has been the computational tool of choice for scores of studies generally and many examining extremist content in a forensic or security context. Our purpose, therefore, is to understand whether LIWC can be depended upon for large-scale analyses; we initially examine this here using a small sample of posts from a set of just eight users and manually checking the program's automated codings of a subset of categories. Our results show that the LIWC coding cannot be relied upon – precision falls to as low as 49.6 % and recall as low as 41.7 % for some categories. It would be possible to engage in considerable manual correction of these results, but this undermines its purported efficiency for large datasets.}
}
@article{CAGLAYAN2015131,
title = {Making sense of eigenvalue–eigenvector relationships: Math majors’ linear algebra – Geometry connections in a dynamic environment},
journal = {The Journal of Mathematical Behavior},
volume = {40},
pages = {131-153},
year = {2015},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2015.08.003},
url = {https://www.sciencedirect.com/science/article/pii/S0732312315000504},
author = {Günhan Caglayan},
keywords = {Undergraduate mathematics education, Dynamic geometry software, Visualization, Representation, Connection, Linear algebra, Eigenvectors, Eigenvalues, Matrices},
abstract = {The present qualitative case study on mathematics majors’ visualization of eigenvector–eigenvalue concepts in a dynamic environment highlights the significance of student-generated representations in a theoretical framework drawn from Sierpinska's (2000) modes of thinking in linear algebra. Such an approach seemed to provide the research participants with mathematical freedom, which resulted in an awareness of the multiple ways that eigenvalue–eigenvector relationships could be visualized in a manner that widened students’ repertoire of meta-representational competences (diSessa, 2004) in coordination with their preferred modes of visualization. Students’ expression of visual fluency in the course of making sense of the eigenvalue problem Au=λu associated with a variety of matrices occurred in different, yet not necessarily hierarchical modes of visualizations that differed from matrix to matrix: (i) synthetic/analytic mode manifested in the process of detecting eigenvectors when the sought eigenvector and the matrix-applied product vector were aligned in the same/opposite directions; (ii) analytic arithmetic mode manifested in the case of singular matrices (in the determination of the zero eigenvalue) and invertible matrices with nonreal eigenvalues; (iii) analytic structural mode, though rarely occurred, manifested in making sense of the trajectory (circle, ellipse, line segment) of the matrix-applied product vector and relating trajectory behavior to matrix type. While the connection between the thinking modes (Sierpinska, 2000) and the concreteness–necessity–generalizability triad (Harel, 2000) was not sharp, math majors still frequently implemented the CNG principles, which proved facilitatory tools in the evolution of students’ thinking about the eigenvalue–eigenvector relationships.}
}
@incollection{BICKHARD2025169,
title = {The mentality of Homo Sapiens},
editor = {Mark H. Bickhard},
booktitle = {The Whole Person},
publisher = {Academic Press},
pages = {169-260},
year = {2025},
isbn = {978-0-443-33050-6},
doi = {https://doi.org/10.1016/B978-0-443-33050-6.00009-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780443330506000094},
author = {Mark H. Bickhard},
keywords = {interactive knowing, situation knowledge, presupposition, higher order motivation and values, themes, central nervous system as a microgenetic dynamic system, thinking and concepts, contact-content, indexicality, attention, will, learning, topologies in heuristic learning, “enactive”, “semantic”, episodic, and autobiographical memory, knowing levels, development, emotions, evolution of emotion processes, organizations of microgenesis processes, reflective consciousness, rehearsal, purpose, emergence of reflection in the brain, experiencing, experiencing of experiencing, higher order theory, Brentano, Searle, folk psychology},
abstract = {The central macro-evolutionary ratchet supports myriad further emergences as specializations, further developments, and various kinds of interactions among the differing processes. This chapter, in the section on interactive knowing, addresses, for example, situation knowledge, presupposition, higher order motivation and values, the important process characteristic of themes, central nervous system as a microgenetic dynamic system, thinking and concepts, the contact-content distinction, indexicality, attention, and will. Concerning learning, the function of topologies in heuristic learning is explored, multiple kinds of memory are addressed, such as “enactive”, “semantic”, episodic, and autobiographical, and some of the enabling and constraining developmental consequences of knowing levels are examined. In the section on emotions, basic principles of developmental differentiations of emotions are examined, and the evolution of emotion processes within organizations of microgenesis processes is framed. The discussion of reflective consciousness addresses enablings such as rehearsal and purpose, and extends the evolutionary analysis to the emergence of the possibility of reflection in the brain. Issues concerning experiencing, such as unities and boundaries, are addressed. Reflective experiencing of experiencing is presented as a resolution of a fundamental problem in the literature. Some further literature is addressed, such as higher order theory, Brentano, Searle, and folk psychology.}
}
@article{FERNANDEZFONTECHA2022101067,
title = {Examining the relations between semantic memory structure and creativity in second language},
journal = {Thinking Skills and Creativity},
volume = {45},
pages = {101067},
year = {2022},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2022.101067},
url = {https://www.sciencedirect.com/science/article/pii/S1871187122000700},
author = {Almudena Fernández-Fontecha and Yoed N. Kenett},
keywords = {Creativity, Semantic network, L2, Bilingualism, Semantic fluency},
abstract = {Creativity is related to a higher flexible semantic memory structure, which could explain greater fluency of ideas. Extensive research has identified a positive connection between creativity and bi-/multilingualism mainly in contexts where two languages or more concur in daily communicative interactions. Yet, creativity has received scant attention in regard to L2 (second or foreign language) acquisition that mainly takes place in classroom situations. The scarce research points to a positive relationship between creativity and L2 fluency – understood as the number of words produced. We apply computational network science analysis and Forward Flow methods to examine lexical organization patterns of a low creativity (LC) and high creativity (HC) group of 12th grade Spanish English as a Foreign Language (EFL) learners. The participants completed two fluency tasks, where they generated animal names in their L2, and also L1 – used here as a control measure. EFL proficiency was controlled. Our analyses revealed that the HC individuals were more fluent in L1 and L2, generated more remote responses, and exhibited a more flexible and efficiently structured semantic memory in both languages, with a greater effect of creativity in L2. Contrary to previous research, the L2 semantic memory network exhibited a less random organization. Differences in the L2 learning conditions are adduced as likely causes of this result.}
}
@article{OXMAN2006229,
title = {Theory and design in the first digital age},
journal = {Design Studies},
volume = {27},
number = {3},
pages = {229-265},
year = {2006},
note = {Digital Design},
issn = {0142-694X},
doi = {https://doi.org/10.1016/j.destud.2005.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X05000840},
author = {Rivka Oxman},
keywords = {digital design, design theory, design methodology, design thinking},
abstract = {Digital design and its growing impact on design and production practices have resulted in the need for a re-examination of current design theories and methodologies in order to explain and guide future research and development. The present research postulates the requirements for a conceptual framework and theoretical basis of digital design; reviews the recent theoretical and historical background; and defines a generic schema of design characteristics through which the paradigmatic classes of digital design are formulated. The implication of this research for the formulation of ‘digital design thinking’ is presented and discussed.}
}
@article{ZHOU2025110885,
title = {Developing a deep reinforcement learning model for safety risk prediction at subway construction sites},
journal = {Reliability Engineering & System Safety},
volume = {257},
pages = {110885},
year = {2025},
issn = {0951-8320},
doi = {https://doi.org/10.1016/j.ress.2025.110885},
url = {https://www.sciencedirect.com/science/article/pii/S0951832025000894},
author = {Zhipeng Zhou and Wen Zhuo and Jianqiang Cui and Haiying Luan and Yudi Chen and Dong Lin},
keywords = {Subway construction safety, Grounded theory, Deep reinforcement learning, Double deep Q-network, Permutation importance},
abstract = {Underground construction work is heavily affected by surrounding hydrogeology, adjacent pipelines, and existing subway lines, which can lead to a high degree of uncertainty and generate safety risk on site. In order to overcome rigid thinking of causal factors within a structured framework and incorporate features of different accidents, this study adopted grounded theory for the investigation on factors contributing to workplace accidents in subway construction. The deep reinforcement learning model of double deep Q-network (DDQN) was developed for predicting subway construction safety risk, which integrated the advantage of reinforcement learning in decision making with the advantage of deep learning in objection perception. The findings denoted that DDQN performed better than other machine learning models inclusive of random forest, extreme gradient boosting, k-nearest neighbor, and support vector machine. Contributing factors relevant to subway construction accidents were quantitatively analyzed using permutation importance of attributes. It was beneficial for determining how the 37 contributing factors had negative effects on subway construction safety risk. Safety measures for risk reduction and controlling could be optimized according to permutation importance of individual contributing factor, which paved a new way for the promotion of safety management performance at subway construction sites.}
}
@incollection{GARDNER202427,
title = {Chapter 2 - Ethics and the smart city},
editor = {Nicole Gardner},
booktitle = {Scaling the Smart City},
publisher = {Elsevier},
pages = {27-49},
year = {2024},
series = {Smart Cities},
isbn = {978-0-443-18452-9},
doi = {https://doi.org/10.1016/B978-0-443-18452-9.00004-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780443184529000045},
author = {Nicole Gardner},
keywords = {Design, process, Ethics, Ethics by design, Material ethics, Philosophy of technology, Postphenomenology, Practical ethics, Smart city, Urban technology, Value-sensitive design},
abstract = {Examining who (and what) stands to gain and lose in the smart city, and what kind of urban life smart cities create, are questions of a philosophical and ethical import. Since the early 2010s scholars and journalists have critiqued the smart city paradigm and examined its various projects and experimental initiatives to surface its ethical dimensions and significance. The smart city's rhetorical swing from tech-centric to human-centric and from smart city to smart citizen is construed here as a further effort to create the image of a more ethical smart city. Consequently, the topic of ethics has also intersected with the smart city in particular ways, and predominantly through the lens of data governance and the protection of privacy rights. This chapter argues for an expanded approach to smart city ethics. It proposes a focus on urban technology design to bridge the gap between a micro-ethical focus on data ethics and macro-level political-ethical critique. Bringing philosophical thinking on technology together with a design-led approach to urban technology is argued to provide a further way to draw out a potentially different set of ethical concerns and to explore how urban life can be lived with technology.}
}
@article{JOEL2002535,
title = {Actor–critic models of the basal ganglia: new anatomical and computational perspectives},
journal = {Neural Networks},
volume = {15},
number = {4},
pages = {535-547},
year = {2002},
issn = {0893-6080},
doi = {https://doi.org/10.1016/S0893-6080(02)00047-3},
url = {https://www.sciencedirect.com/science/article/pii/S0893608002000473},
author = {Daphna Joel and Yael Niv and Eytan Ruppin},
keywords = {Basal ganglia, Dopamine, Reinforcement learning, Actor–critic, Dimensionality reduction, Evolutionary computation, Behavioral switching, Striosomes/patches},
abstract = {A large number of computational models of information processing in the basal ganglia have been developed in recent years. Prominent in these are actor–critic models of basal ganglia functioning, which build on the strong resemblance between dopamine neuron activity and the temporal difference prediction error signal in the critic, and between dopamine-dependent long-term synaptic plasticity in the striatum and learning guided by a prediction error signal in the actor. We selectively review several actor–critic models of the basal ganglia with an emphasis on two important aspects: the way in which models of the critic reproduce the temporal dynamics of dopamine firing, and the extent to which models of the actor take into account known basal ganglia anatomy and physiology. To complement the efforts to relate basal ganglia mechanisms to reinforcement learning (RL), we introduce an alternative approach to modeling a critic network, which uses Evolutionary Computation techniques to ‘evolve’ an optimal RL mechanism, and relate the evolved mechanism to the basic model of the critic. We conclude our discussion of models of the critic by a critical discussion of the anatomical plausibility of implementations of a critic in basal ganglia circuitry, and conclude that such implementations build on assumptions that are inconsistent with the known anatomy of the basal ganglia. We return to the actor component of the actor–critic model, which is usually modeled at the striatal level with very little detail. We describe an alternative model of the basal ganglia which takes into account several important, and previously neglected, anatomical and physiological characteristics of basal ganglia–thalamocortical connectivity and suggests that the basal ganglia performs reinforcement-biased dimensionality reduction of cortical inputs. We further suggest that since such selective encoding may bias the representation at the level of the frontal cortex towards the selection of rewarded plans and actions, the reinforcement-driven dimensionality reduction framework may serve as a basis for basal ganglia actor models. We conclude with a short discussion of the dual role of the dopamine signal in RL and in behavioral switching.}
}
@article{ALOUPIS2015135,
title = {Classic Nintendo games are (computationally) hard},
journal = {Theoretical Computer Science},
volume = {586},
pages = {135-160},
year = {2015},
note = {Fun with Algorithms},
issn = {0304-3975},
doi = {https://doi.org/10.1016/j.tcs.2015.02.037},
url = {https://www.sciencedirect.com/science/article/pii/S0304397515001735},
author = {Greg Aloupis and Erik D. Demaine and Alan Guo and Giovanni Viglietta},
keywords = {Nintendo games, Video games, Computational complexity, NP-hardness, PSPACE-hardness},
abstract = {We prove NP-hardness results for five of Nintendo's largest video game franchises: Mario, Donkey Kong, Legend of Zelda, Metroid, and Pokémon. Our results apply to generalized versions of Super Mario Bros. 1–3, The Lost Levels, and Super Mario World; Donkey Kong Country 1–3; all Legend of Zelda games; all Metroid games; and all Pokémon role-playing games. In addition, we prove PSPACE-completeness of the Donkey Kong Country games and several Legend of Zelda games.}
}
@article{BENTON20001135,
title = {Computational modelling of interleaved first- and second-order motion sequences and translating 3f+4f beat patterns},
journal = {Vision Research},
volume = {40},
number = {9},
pages = {1135-1142},
year = {2000},
issn = {0042-6989},
doi = {https://doi.org/10.1016/S0042-6989(00)00026-2},
url = {https://www.sciencedirect.com/science/article/pii/S0042698900000262},
author = {Christopher P. Benton and Alan Johnston and Peter W. McOwan},
keywords = {Motion perception, Computational modelling, Second-order, Feature tracking},
abstract = {Despite detailed psychophysical, neurophysiological and electrophysiological investigation, the number and nature of independent and parallel motion processing mechanisms in the visual cortex remains controversial. Here we use computational modelling to evaluate evidence from two psychophysical studies collectively thought to demonstrate the existence of three separate and independent motion processing channels. We show that the pattern of psychophysical results can largely be accounted for by a single mechanism. The results demonstrate that a low-level luminance based approach can potentially provide a wider account of human motion processing than generally thought possible.}
}