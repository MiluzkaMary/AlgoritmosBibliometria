@article{GOMES2019411,
title = {State-of-the-art of transmission expansion planning: A survey from restructuring to renewable and distributed electricity markets},
journal = {International Journal of Electrical Power & Energy Systems},
volume = {111},
pages = {411-424},
year = {2019},
issn = {0142-0615},
doi = {https://doi.org/10.1016/j.ijepes.2019.04.035},
url = {https://www.sciencedirect.com/science/article/pii/S014206151831888X},
author = {Phillipe Vilaça Gomes and João Tomé Saraiva},
keywords = {Heuristics, Optimization, Mathematical programming, Metaheuristic, Transmission expansion planning},
abstract = {Transmission Expansion Planning (TEP) problem aims at identifying when and where new equipment as transmission lines, cables and transformers should be inserted on the grid. The transmission upgrade capacity is motivated by several factors as meeting the increasing electricity demand, increasing the reliability of the system and providing non-discriminatory access to cheap generation for consumers. However, TEP problems have been changing over the years as the electrical system evolves. In this way, this paper provides a detailed historical analysis of the evolution of the TEP over the years and the prospects for this challenging task. Furthermore, this study presents an outline review of more than 140 recent articles about TEP problems, literature insights and identified gaps as a critical thinking in how new tools and approaches on TEP can contribute for the new era of renewable and distributed electricity markets.}
}
@article{SHAN201032,
title = {Study on large time-delay constant temperature control system based on TEC},
journal = {The Journal of China Universities of Posts and Telecommunications},
volume = {17},
pages = {32-35},
year = {2010},
issn = {1005-8885},
doi = {https://doi.org/10.1016/S1005-8885(09)60586-0},
url = {https://www.sciencedirect.com/science/article/pii/S1005888509605860},
author = {Jiang-dong SHAN and Ge WU and Xiao-jian TIAN},
keywords = {thermoelectric cooler, large time-delay control system, proportion integration differentiation (PID) control, constant temperature control},
abstract = {This paper designes a diminutive constant temperature control system based on thermoelectric cooler (TEC). Considering that the system is a large time-delay control system, the paper proposes a new method to determine the transfer function of the controlled system which gets the transfer function by doing nonlinear fitting of the step response of the controlled system. The characteristics of system model which is established by the method are basically same as the actual constant temperature control system. This method provides a new way of thinking to the design of large time-delay control system.}
}
@article{2024100676,
title = {Erratum regarding missing declaration of competing interest statements in previously published articles},
journal = {International Journal of Child-Computer Interaction},
volume = {41},
pages = {100676},
year = {2024},
issn = {2212-8689},
doi = {https://doi.org/10.1016/j.ijcci.2024.100676},
url = {https://www.sciencedirect.com/science/article/pii/S221286892400045X}
}
@incollection{KATZ202453,
title = {Chapter Three - The role of big data and analytics in utilities innovation},
editor = {Reza Arghandeh and Yuxun Zhou},
booktitle = {Big Data Application in Power Systems (Second Edition)},
publisher = {Elsevier Science},
edition = {Second Edition},
pages = {53-68},
year = {2024},
isbn = {978-0-443-21524-7},
doi = {https://doi.org/10.1016/B978-0-443-21524-7.00003-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780443215247000037},
author = {Jeffrey S. Katz},
keywords = {Big data, Analytics, Data science, Smart grid, Power system simulation, Numerical weather analysis, Smarter energy research},
abstract = {The computational technology known as big data and its subsequent processing, analytics, are driving innovation in electric power system integration of renewable energy, outage prediction, processing of increasing volumes of smart grid data, as well as the velocity of such data. In the age of cybersecurity, the veracity of this data is also a factor. The almost concurrent rise of cognitive computing gives new importance to unstructured data such as drone images and text in maintenance reports. The intelligent connection of real-time numerical data with written and visual data gives rise to even more innovation. The benefits of high-precision weather modeling on power demand, grid damage, and solar- and wind-based generation are also considered.}
}
@article{NEDIC2019224,
title = {OP0284 PARE THE INFLUENCE OF FUNCTIONAL TRAINING AND PSYCHO-SOCIAL SUPPORT},
journal = {Annals of the Rheumatic Diseases},
volume = {78},
pages = {224},
year = {2019},
issn = {0003-4967},
doi = {https://doi.org/10.1136/annrheumdis-2019-eular.4256},
url = {https://www.sciencedirect.com/science/article/pii/S0003496724042614},
author = {Nenad Nedić and Mirjana Lapčević},
abstract = {Background
Treatment of chronic noncontagious diseases in which we include RMD implies usage of medicines and non medicine (changing bad lifestyle habits like losing nutrition, physical activity, smoking…).1 Unwanted cardiovascular and cerebrovascular states are the most common cause of shortening of live of people with RMD.2 None of the chemical drugs can replace physical activity. Physical activity dosage is individual, and depends on aerobic capability and heart rate increase, taking in account age, type of noncontagious diseases, level of tissue and organs damage as well as the type of work the person is engaged in. Patients with RMD have common symptoms, such as stiffness, fatigue, poor mobility, joint pain and muscle pain, anxiety and depression, and lack of fitness. In addition to physical medicine and rehabilitation and balneoklimatology, various forms of physical activity are recommended, such as walking, swimming, functional training. Today, moderate physical activity is known to help reduce fatigue, strengthen muscles and bones, improve flexibility and endurance of the joints, and improve general health. It is necessary to find the best combination of rest, activities and exercise programs to prevent deformities of the joints, the development of disability, improve the quality of life, and the mental health of patients with RMD.3
Objectives
1. By practicing Cigong, an increase in the volume of movement in the joint, the strengthening of joint muscles and the improvement of general condition, pain relief, fatigue reduction is achieved and also, it helps patients look and feel better. 2. The goal of Yoga is to neutralize and remove all obstacles that stand in the way and disturb the function of the body and the mind and achieves inner peace. 3. Changing the psychological state during exercise also led to a positive way of thinking. 4. Psycho-physical support for the people with RMD
Methods
From 2011, we organize two times a week functional training of Cigong.4 Since January 2016, twice a week persons with RMD have been practicing Yoga.5 From 2015, one per week four psychologists volunteer hold workshops for psycho-social support for persons with RMD.6,7 Participants of the training and psycho-social sessions took a survey.
Results
1. Joint pain reduction – 50% of total number of participants 2. Joint mobility increase – 95% of total number of participants 3. General fitness improvement – 73% of total number of participants 4. Less pronounced negative emotions – 59% of total number of participants
Conclusion
Practicing cigong, yoga and psycho-social support workshops help patients look better and feel better. Changing the psychological state during exercise also led to a positive way of thinking. All of this increased the effectiveness of drug treatment and improved the quality of life of patients with RMD.
References
[1] Dragojević R. Vodič za zdrav život: preporuke medicine i pouke mudrosti, Beograd 2017. [2] Lapcević M., Vuković M. Dimitrijevic I. Et all Uticaj medikamentnog I nemedikamentnog lečenja na smanjenje faktora rizika za kardiovaskularne I cerebrovaskularne događaje u interventnoj studiji. Srp Arh Celok Lek 2007. [3] Lapčević M, Prvanov D, Đorđević S. Procena kvaliteta života obolelih od hroničnih reumatskih oboljenja. Opšta medicina 2010. [4] Ilinka Acimovic, “The Influence of Health Qigong on the Subjectively Expressed Psychophysical State of Patients with Rheumatoid Arthritis, Rheum, Osteoporosis, Osteopenia” CHINESE MEDICINE AND CULTURE [5] Swami Kriyananda, “Demystifying Patanjali: The Yoga Sutras - The Wisdom of Paramhansa Yogananda”. Crystal Clarity Publishers, Nevada City, CA, 2013. [6] Lapčević M, et al. Socioeconomic and therapy factor influence on self-reported fatigue, anxiety and depression in rheumatoid arthritis patients. Rev Bras Reumatol. 2017. [7] Milić V. Analiza ličnosti I uticaj optimizma na pozitivan ishod lečenja; 2018.
Disclosure of Interests
None declared}
}
@article{YANG2024106650,
title = {Integrating parcel delivery schedules with public transport networks in urban co-modality systems},
journal = {Computers & Operations Research},
volume = {167},
pages = {106650},
year = {2024},
issn = {0305-0548},
doi = {https://doi.org/10.1016/j.cor.2024.106650},
url = {https://www.sciencedirect.com/science/article/pii/S0305054824001229},
author = {Xuan Yang and Xinyao Nie and Hao Luo and George Q. Huang},
keywords = {Logistics, Public transport, Co-modality, Parcel assignment},
abstract = {Co-modality transportation advocates using urban public transport to support urban freight operations. This study considers the implementation of co-modality in a fixed-route transit network comprising multiple lines following predetermined routes and schedules. We first develop a schedule-based parcel assignment model to formulate the synchronized co-modality transportation problem (SCTP). The effectiveness of the proposed arc-based meta-heuristic algorithm is substantiated through a comprehensive computational analysis, comparing its performance with that of an exact approach and genetic algorithm. Our findings reveal a nuanced trade-off between transportation efficiency and co-modal stop utilization, identifying a threshold beyond which additional stops do not improve efficiency but increase costs. We also discover a 'buckets effect' in co-modal capacities, suggesting that balanced vehicle and stop capacities are crucial for optimizing system performance. A case study with real urban transit data validates our model's potential for significant efficiency gains in co-modality transportation systems, offering actionable insights for urban logistics.}
}
@article{EBELING2023120768,
title = {A multi-dimensional framework to analyze group behavior based on political polarization},
journal = {Expert Systems with Applications},
volume = {233},
pages = {120768},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.120768},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423012708},
author = {Régis Ebeling and Jéferson Nobre and Karin Becker},
keywords = {Analysis framework, Political polarization, Group behavior, Topic modeling, Social network analysis, COVID-19},
abstract = {The recent wave of elections won by right-wing worldwide brings up increased discussions biased by political polarization, including in social media. Social media data enables the investigation of the contexts where political polarization occurs, enabling to derive insights into how it affects human behavior. Related work has shown how computing techniques can be leveraged to understand political polarization in restricted scenarios, but the complexity of this behavior can be better understood when considered from different viewpoints. This article describes a multi-dimensional analysis framework to study the behavior of groups on Twitter in politically polarized scenarios. It can be applied to various themes where groups display stances that can be politically biased, and it aggregates a wide range of computational techniques in an innovative way to provide rich insights. The framework includes guidelines and techniques to: (a) collect data on Twitter to represent the groups; (b) automatically infer the political leaning of users; (c) derive topological properties of the groups’ social network and analyze political influence; (d) identify topics representing concerns at coarse and fine-grained granularity levels using a hybrid topic modeling approach; (e) identify psychological aspects based on linguistic cues, and (f) analyze the sources of information disseminated by the groups. Applying the framework in two case studies related to COVID-19 revealed patterns of behavior common to ideologies. We confirmed that the stances were politically motivated and that both groups, left/right, were subject to the echo chamber effect. Comparatively, the social structure of the right-oriented groups is more connected, and they rely on politicians and social media for information spreading. Left-oriented groups are less connected and more prone to facts. The psychological aspects reveal that both groups are emotionally distressed in arguing about being right, given their beliefs.}
}
@article{2024100677,
title = {Erratum regarding missing Declaration of Competing Interest statements in previously published articles},
journal = {International Journal of Child-Computer Interaction},
volume = {41},
pages = {100677},
year = {2024},
issn = {2212-8689},
doi = {https://doi.org/10.1016/j.ijcci.2024.100677},
url = {https://www.sciencedirect.com/science/article/pii/S2212868924000461}
}
@article{THEODOROU20075697,
title = {Hierarchical modelling of polymeric materials},
journal = {Chemical Engineering Science},
volume = {62},
number = {21},
pages = {5697-5714},
year = {2007},
issn = {0009-2509},
doi = {https://doi.org/10.1016/j.ces.2007.04.048},
url = {https://www.sciencedirect.com/science/article/pii/S000925090700382X},
author = {Doros N. Theodorou},
keywords = {Polymers, Mathematical modelling, Simulation, Rheology, Nanostructure, Diffusion},
abstract = {Within the last 20 years, computer simulations of materials have evolved from an academic curiosity to a predictive tool for addressing structure–property–processing–performance relations that are critical to the design of new products and processes. Chemical engineers, with their problem-oriented thinking and their systems approach, have played a significant role in this development. The computational prediction of physical properties is particularly challenging for polymeric materials, because of the extremely broad spectra of length and time scales governing structure and molecular motion in these materials. This challenge can only be met through the development of hierarchical analysis and simulation strategies encompassing many interconnected levels, each level addressing phenomena over a specific window of time and length scales. In this paper we will briefly discuss the fundamental underpinnings and example applications of new methods and algorithms for the hierarchical modelling of polymers. Questions to be addressed include: How can one equilibrate atomistic models of long-chain polymer melts at all length scales and thereby predict thermodynamic and conformational properties reliably? How can one quantify the structure of entanglement networks present in these melts through topological analysis and relate it to rheological properties? Are there ways to predict the microphase-separated morphology and stress–strain behaviour of multicomponent block copolymer-based materials, such as pressure sensitive adhesives? Is it possible to anticipate changes in the barrier properties of glassy amorphous polymers used in packaging applications as a consequence of modifications in the chemical constitution of chains?}
}
@article{HEIRDSFIELD200257,
title = {Flexibility and inflexibility in accurate mental addition and subtraction: two case studies},
journal = {The Journal of Mathematical Behavior},
volume = {21},
number = {1},
pages = {57-74},
year = {2002},
issn = {0732-3123},
doi = {https://doi.org/10.1016/S0732-3123(02)00103-7},
url = {https://www.sciencedirect.com/science/article/pii/S0732312302001037},
author = {Ann M Heirdsfield and Tom J Cooper},
keywords = {Mental computation, Addition, Subtraction, Accuracy},
abstract = {This paper reports on a study of two children’s mental computation in addition and subtraction, and compares their mental architecture. Both students were identified as being accurate, however, one student used a variety of mental strategies (was flexible) while the other student used only one strategy that reflected the written procedure for each of the addition and subtraction algorithms taught in the classroom. Interviews were used to identify both children’s knowledge and ability with respect to number sense (including numeration, number and operations, basic facts, estimation), metacognition and affects. Frameworks were developed to show how these factors interacted to explain the two types of accuracy in mental addition and subtraction. Flexible accuracy was related to the presence of strong number sense knowledge integrated with metacognitive strategies and beliefs and beliefs about self and teaching; while inflexible accuracy was a result of compensation of inadequate knowledge supported by beliefs about self and teaching.}
}
@incollection{LIANG2023263,
title = {Teacher skills and knowledge for technology integration},
editor = {Robert J Tierney and Fazal Rizvi and Kadriye Ercikan},
booktitle = {International Encyclopedia of Education (Fourth Edition)},
publisher = {Elsevier},
edition = {Fourth Edition},
address = {Oxford},
pages = {263-271},
year = {2023},
isbn = {978-0-12-818629-9},
doi = {https://doi.org/10.1016/B978-0-12-818630-5.04037-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128186305040379},
author = {Leming Liang and Nancy Law},
keywords = {Technology-enhanced learning, Teacher skills, Teacher knowledge, TPACK, Teacher learning, AR/VR, AI, Ethics, Pedagogical innovation, Teacher leadership},
abstract = {With the pervasive use of digital technology in all aspects of our lives, and the rapid advances and deployment of new technologies in education, the use and teaching of technology in education is a necessary professional repertoire for teachers to ensure students' well-being. Influential education policy frameworks posit that teachers need competence in the integration of technology for agile delivery of teaching in blended/fully online modes and for fostering students' digital literacy. Future-oriented professional development programs need to go beyond individual learning and engage leadership in creating the necessary conditions for technology-enhanced learning innovations at institutional and system levels.}
}
@article{ANDIC2023490,
title = {A robust crow search algorithm based power system state estimation},
journal = {Energy Reports},
volume = {9},
pages = {490-501},
year = {2023},
note = {Proceedings of 2022 7th International Conference on Renewable Energy and Conservation},
issn = {2352-4847},
doi = {https://doi.org/10.1016/j.egyr.2023.09.075},
url = {https://www.sciencedirect.com/science/article/pii/S2352484723013124},
author = {Cenk Andic and Ali Ozturk and Belgin Turkay},
keywords = {Crow search algorithm, Power systems, State estimation},
abstract = {The State Estimation (SE) computational procedure plays a crucial role in modern electric power system security control by monitoring and analyzing operational conditions and predicting any emergency. In order to estimate state variables, Power System State Estimation (PSSE) takes into account the magnitudes and phases of voltage on each bus. To address the state estimation challenges in power systems, in this paper, we propose a novel application of the Crow Search Algorithm (CSA) specifically tailored for the state estimation problem. We have assessed the introduced algorithm using the frameworks of both the IEEE 14-bus and IEEE 30-bus test systems. The first formulation is the Weighted Least Square (WLS) method, and the second is the Weighted Least Absolute Value (WLAV) method, both of which are objective function formulations. By comparing the results, it is clear that CSA-based SE is superior to the other metaheuristic algorithms considered, namely Genetic Algorithm (GA), Particle Swarm Optimization (PSO), and Artificial Bee Swarm Optimization (ABSO). As a point of comparison, we use the Newton–Raphson method for calculating load flow. It has been shown that the proposed CSA-based SE technique has better accuracy than the other two algorithms in all different test systems. With this study, the power system is operated more accurately and reliably by the operators operating the system.}
}
@incollection{GOUGH2024547,
title = {25 - Mycelium-based materials for the built environment: a case study on simulation, fabrication and repurposing myco-materials},
editor = {Emina Kristina Petrović and Morten Gjerde and Fabricio Chicca and Guy Marriage},
booktitle = {Sustainability and Toxicity of Building Materials},
publisher = {Woodhead Publishing},
pages = {547-571},
year = {2024},
series = {Woodhead Publishing Series in Civil and Structural Engineering},
isbn = {978-0-323-98336-5},
doi = {https://doi.org/10.1016/B978-0-323-98336-5.00025-X},
url = {https://www.sciencedirect.com/science/article/pii/B978032398336500025X},
author = {Phillip Gough and Anastasia Globa and Dagmar Ingrid Elfriede Reinhardt},
keywords = {Biodegradable building materials, mycelium, circular economy, materials science, case study research, digital printing},
abstract = {Remanufacturing organic waste for composite materials is an opportunity to create circular economic approaches in construction. Research shows how the mycelium (root network) of some fungi, such as Reishi mushrooms, can be guided and formed to create sustainable, biodegradable composite myco-materials for a range of applications. As they degrade wood, paper or coffee waste, Reishi mushrooms create a new material with potential applications in architecture. Research into biocomposites and eco-materials such as mycelium integrates advanced digital fabrication processes, complex structures and algorithmically generated forms for packaging, thermal and sound insulation or as cladding and structural materials. Significantly, the capacity of mycelium composites to seamlessly return as resource material after use in a sustainable ecological cycle indicates the potential for a new approach to sustainability in building and construction processes across the lifespan of buildings. The adaptation of a living organism as an interactive architecture building module could allow strategies for signalling and negotiating climatic variations and local site conditions around the building. Moreover, the capacity of mycelia to not only thrive in contaminated industrial wastelands but to actively detoxify and colonise could further be employed for bioremediation. The research discussed here presents an empirical study into mycelium composites, using computational design and desktop 3D printing to identify strengths and limitations of material, moulds, growth and form. The case study demonstrates how substrate inclusions impact the formation of the material, opportunities to re-awaken inert myco-material for new growth and the quality of fine-detailed elements, using domestic technology and readily available materials. We contribute a taxonomy of existing uses and applications of myco-materials and a range of implications for the process of designing with myco-materials in architecture.}
}
@article{NASCIMENTO2023105421,
title = {Core–shell clustering approach for detection and analysis of coastal upwelling},
journal = {Computers & Geosciences},
volume = {179},
pages = {105421},
year = {2023},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2023.105421},
url = {https://www.sciencedirect.com/science/article/pii/S0098300423001255},
author = {Susana Nascimento and Alexandre Martins and Paulo Relvas and Joaquim F. Luís and Boris Mirkin},
keywords = {Spatio-temporal clustering, Time series segmentation, SST images, Coastal upwelling, Core–shell cluster},
abstract = {A comprehensive approach is presented to analyze season’s coastal upwelling represented by weekly sea surface temperature (SST) image grids. The proposed model, core–shell clustering, assumes that the season’s upwelling can be divided into shorter periods of stability, time ranges, consisting of constant core and variable shell parts. A one-by-one core–shell clustering algorithm is provided. The algorithm parameters are automatically derived from the least-squares clustering criterion. The approach applies to SST gridded data for sixteen successive years (2004–2019) of coastal upwelling in the western Iberian coast, the northernmost branch of the Canary Current Upwelling System. Our results show that at each season, there are 3 to 5 time intervals, the ranges, at which the upwelling presents stable core patterns of relatively cold water surrounded by somewhat larger shell areas of warmer waters. Based on other experimental computations performed by our team, we conclude that this pattern is not just a purely local phenomenon but has a more global meaning. Inter-annual time series analysis are consistent among themselves and with existing expert domain knowledge.}
}
@article{BATTLEDAY20151865,
title = {Modafinil for cognitive neuroenhancement in healthy non-sleep-deprived subjects: A systematic review},
journal = {European Neuropsychopharmacology},
volume = {25},
number = {11},
pages = {1865-1881},
year = {2015},
issn = {0924-977X},
doi = {https://doi.org/10.1016/j.euroneuro.2015.07.028},
url = {https://www.sciencedirect.com/science/article/pii/S0924977X15002497},
author = {R.M. Battleday and A.-K. Brem},
keywords = {Neuroenhancement, Modafinil, Cognitive, Psychometric, Enhancement, Nootropic},
abstract = {Modafinil is an FDA-approved eugeroic that directly increases cortical catecholamine levels, indirectly upregulates cerebral serotonin, glutamate, orexin, and histamine levels, and indirectly decreases cerebral gamma-amino-butrytic acid levels. In addition to its approved use treating excessive somnolence, modafinil is thought to be used widely off-prescription for cognitive enhancement. However, despite this popularity, there has been little consensus on the extent and nature of the cognitive effects of modafinil in healthy, non-sleep-deprived humans. This problem is compounded by methodological discrepancies within the literature, and reliance on psychometric tests designed to detect cognitive effects in ill rather than healthy populations. In order to provide an up-to-date systematic evaluation that addresses these concerns, we searched MEDLINE with the terms “modafinil” and “cognitive”, and reviewed all resultant primary studies in English from January 1990 until December 2014 investigating the cognitive actions of modafinil in healthy non-sleep-deprived humans. We found that whilst most studies employing basic testing paradigms show that modafinil intake enhances executive function, only half show improvements in attention and learning and memory, and a few even report impairments in divergent creative thinking. In contrast, when more complex assessments are used, modafinil appears to consistently engender enhancement of attention, executive functions, and learning. Importantly, we did not observe any preponderances for side effects or mood changes. Finally, in light of the methodological discrepancies encountered within this literature, we conclude with a series of recommendations on how to optimally detect valid, robust, and consistent effects in healthy populations that should aid future assessment of neuroenhancement.}
}
@article{BORIS2013113,
title = {Flux-Corrected Transport looks at forty},
journal = {Computers & Fluids},
volume = {84},
pages = {113-126},
year = {2013},
issn = {0045-7930},
doi = {https://doi.org/10.1016/j.compfluid.2013.05.004},
url = {https://www.sciencedirect.com/science/article/pii/S0045793013001874},
author = {Jay Paul Boris},
keywords = {Flux-Corrected Transport (FCT), Monotonicity, Positivity, Computational Fluid Dynamics (CFDs), Large Eddy Simulation (LES), Monotone Integrated Large Eddy Simulation (MILES), Implicit Large Eddy Simulation (ILES)},
abstract = {This year, 2013, marks the 40th anniversary of the journal article “Flux-Corrected Transport I. SHASTA, A Fluid Transport Algorithm That Works” by Jay Boris and David Book [1]. Flux-Corrected Transport (FCT) removed a serious roadblock to advances in Computational Fluid Dynamics (CFD) by enabling the accurate treatment of strong, time-dependent shock problems in blast, reactive-flow, and combustion physics, and in aerodynamics and astrophysics. Steep gradients in conserved fluid variables could now be convected across a computational grid without the appearance of spurious oscillations and physically impossible negative values. The nonlinear “flux-correction” algorithm introduced in FCT imposes the physical properties of conservation, locality, causality, and monotonicity on the numerical solutions for convection without adding a great deal of numerical diffusion. This article shows that implementing these physical properties in solving the continuity equation through high-resolution FCT also results in a serviceable Large-Eddy Simulation treatment of turbulent flows without need for additional “subgrid turbulence models.” We have named this simplified approach Monotone Integrated Large Eddy Simulation (MILES).}
}
@article{CRONIN2022100213,
title = {A review of in silico toxicology approaches to support the safety assessment of cosmetics-related materials},
journal = {Computational Toxicology},
volume = {21},
pages = {100213},
year = {2022},
issn = {2468-1113},
doi = {https://doi.org/10.1016/j.comtox.2022.100213},
url = {https://www.sciencedirect.com/science/article/pii/S2468111322000019},
author = {Mark T.D. Cronin and Steven J. Enoch and Judith C. Madden and James F. Rathman and Andrea-Nicole Richarz and Chihae Yang},
keywords = {Cosmetics, Risk assessment, , Computational, Read-across, Quantitative structure-activity relationship},
abstract = {In silico tools and resources are now used commonly in toxicology and to support the “Next Generation Risk Assessment” (NGRA) of cosmetics ingredients or materials. This review provides an overview of the approaches that are applied to assess the exposure and hazard of a cosmetic ingredient. For both hazard and exposure, databases of existing information are used routinely. In addition, for exposure, in silico approaches include the use of rules of thumb for systemic bioavailability as well as physiologically-based kinetics (PBK) and multi-scale models for estimating internal exposure at the organ or tissue level. (Internal) Thresholds of Toxicological Concern are applicable for the safety assessment of ingredients at low concentrations. The use of structural rules, (Quantitative) Structure-Activity Relationships ((Q)SARs) and read-across are the most typically applied modelling approaches to predict hazard. Data from exposure and hazard assessment are increasingly being brought together in NGRA to provide an overall assessment of the safety of a cosmetic ingredient. All in silico approaches are reviewed in terms of their maturity and robustness for use.}
}
@article{BALL2024,
title = {Trust but Verify: Lessons Learned for the Application of AI to Case-Based Clinical Decision-Making From Postmarketing Drug Safety Assessment at the US Food and Drug Administration},
journal = {Journal of Medical Internet Research},
volume = {26},
year = {2024},
issn = {1438-8871},
doi = {https://doi.org/10.2196/50274},
url = {https://www.sciencedirect.com/science/article/pii/S1438887124003078},
author = {Robert Ball and Andrew H Talal and Oanh Dang and Monica Muñoz and Marianthi Markatou},
keywords = {drug safety, artificial intelligence, machine learning, natural language processing, causal inference, case-based reasoning, clinical decision support},
abstract = {Adverse drug reactions are a common cause of morbidity in health care. The US Food and Drug Administration (FDA) evaluates individual case safety reports of adverse events (AEs) after submission to the FDA Adverse Event Reporting System as part of its surveillance activities. Over the past decade, the FDA has explored the application of artificial intelligence (AI) to evaluate these reports to improve the efficiency and scientific rigor of the process. However, a gap remains between AI algorithm development and deployment. This viewpoint aims to describe the lessons learned from our experience and research needed to address both general issues in case-based reasoning using AI and specific needs for individual case safety report assessment. Beginning with the recognition that the trustworthiness of the AI algorithm is the main determinant of its acceptance by human experts, we apply the Diffusion of Innovations theory to help explain why certain algorithms for evaluating AEs at the FDA were accepted by safety reviewers and others were not. This analysis reveals that the process by which clinicians decide from case reports whether a drug is likely to cause an AE is not well defined beyond general principles. This makes the development of high performing, transparent, and explainable AI algorithms challenging, leading to a lack of trust by the safety reviewers. Even accounting for the introduction of large language models, the pharmacovigilance community needs an improved understanding of causal inference and of the cognitive framework for determining the causal relationship between a drug and an AE. We describe specific future research directions that underpin facilitating implementation and trust in AI for drug safety applications, including improved methods for measuring and controlling of algorithmic uncertainty, computational reproducibility, and clear articulation of a cognitive framework for causal inference in case-based reasoning.}
}
@article{DILUZIO2023104418,
title = {A randomized deep neural network for emotion recognition with landmarks detection},
journal = {Biomedical Signal Processing and Control},
volume = {81},
pages = {104418},
year = {2023},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2022.104418},
url = {https://www.sciencedirect.com/science/article/pii/S1746809422008722},
author = {Francesco {Di Luzio} and Antonello Rosato and Massimo Panella},
keywords = {Emotion recognition, Randomized neural networks, Facial landmarks, Deep learning, Video sequences},
abstract = {In this paper, we present an innovative deep neural architecture employing parameter randomization in a complex classification model for emotion recognition. Actually, randomized deep neural networks represent an interesting alternative to exploring the efficiency-to-accuracy balance in real-life applications. Moreover, we also introduce the use of input frames composed of 468 facial landmarks coordinates and an innovative sampling procedure avoiding padding. The proposed randomized classifier is trained for emotion recognition on video sequences and the related accuracy is compared with a non-randomized version of the same model and with well-known benchmark architectures, demonstrating the robustness of the proposed approach in terms of classification accuracy and training time.}
}
@incollection{SINGH2025427,
title = {Chapter Seventeen - Internet of Things legal landscape: Privacy, security, liability, and regulatory issues},
editor = {Pethuru Raj and Kavita Saini and Brij B. Gupta},
series = {Advances in Computers},
publisher = {Elsevier},
volume = {138},
pages = {427-447},
year = {2025},
booktitle = {Post-Quantum Cryptography Algorithms and Approaches for IoT and Blockchain Security},
issn = {0065-2458},
doi = {https://doi.org/10.1016/bs.adcom.2025.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S0065245825000269},
author = {Sushma Singh and Ravi Chandra Prakash and M. Arvindhan},
keywords = {Internet of Things, Legal landscape, Privacy, Security, Liability, Regulatory challenges, Intellectual property, Emerging trends, Case studies},
abstract = {Rapid adoption of Internet of Things (IoT) technology has created a new era of physical-digital convergence. This chapter examines the complex legal landscape of the Internet of Things, including privacy, security, liability, and regulatory issues. The goal is to help policymakers and industry professionals comprehend the complex IoT legal landscape. The chapter opens by contextualizing the extraordinary convergence of technology and law and emphasizing the need for a proactive legal framework. A full examination of IoT privacy risks follows. This includes an overview of broad data collecting, informed consent issues, and the changing role of individuals in personal data control. Existing data protection rules are evaluated to ensure privacy in the IoT context. The next part examines the legal ramifications of IoT ecosystem security breaches. Manufacturing, software, and service providers’ roles in device and data security are examined. The debate includes cybersecurity rules and the changing legal attitude needed to combat security breaches. In the next section, IoT-related legal frameworks are examined for liability and accountability. To explain IoT liability, the section navigates the complex relationships between manufacturers, software developers, and service providers using case studies and precedents. The next part discusses IoT stakeholders’ regulatory compliance challenges. A review of present legislation’ ability to meet the dynamic nature of IoT technology is provided, along with suggestions for new regulatory frameworks that balance innovation and ethical and legal standards. The chapter examines IoT patents, trademarks, and copyrights. Exploring interoperability, licencing, and IP rights challenges provides practical insights for stakeholders navigating the legal complexity of the quickly evolving IoT. New IoT trends and their legal ramifications are examined throughout the chapter. An adaptive legal framework is needed to suit the changing technology landscape of artificial intelligence, blockchain, and IoT. Strategically interwoven real-world case studies demonstrate how legal principles apply to IoT implementations. These case studies demonstrate how legal principles are used and teach us from important cases. Internet of Things, Legal Landscape, Privacy, Security, Liability, Regulatory Challenges, IP, Emerging Trends, Case Studies.}
}
@article{KIM2025,
title = {Machine Learning–Based Prediction of Substance Use in Adolescents in Three Independent Worldwide Cohorts: Algorithm Development and Validation Study},
journal = {Journal of Medical Internet Research},
volume = {27},
year = {2025},
issn = {1438-8871},
doi = {https://doi.org/10.2196/62805},
url = {https://www.sciencedirect.com/science/article/pii/S1438887125002699},
author = {Soeun Kim and Hyejun Kim and Seokjun Kim and Hojae Lee and Ahmed Hammoodi and Yujin Choi and Hyeon Jin Kim and Lee Smith and Min Seo Kim and Guillaume Fond and Laurent Boyer and Sung Wook Baik and Hayeon Lee and Jaeyu Park and Rosie Kwon and Selin Woo and Dong Keon Yon},
keywords = {adolescents, machine learning, substance, prediction, XGBoost, random forest, ML, substance use, adolescents, adolescent, South Korea, United States, Norway, web-based survey, survey, risk behavior, smoking, alcohol, intervention, interventions},
abstract = {Background
To address gaps in global understanding of cultural and social variations, this study used a high-performance machine learning (ML) model to predict adolescent substance use across three national datasets.
Objective
This study aims to develop a generalizable predictive model for adolescent substance use using multinational datasets and ML.
Methods
The study used the Korea Youth Risk Behavior Web-Based Survey (KYRBS) from South Korea (n=1,098,641) to train ML models. For external validation, we used the Youth Risk Behavior Survey (YRBS) from the United States (n=2,511,916) and Norwegian nationwide Ungdata surveys (Ungdata) from Norway (n=700,660). After developing various ML models, we evaluated the final model’s performance using multiple metrics. We also assessed feature importance using traditional methods and further analyzed variable contributions through SHapley Additive exPlanation values.
Results
The study used nationwide adolescent datasets for ML model development and validation, analyzing data from 1,098,641 KYRBS adolescents, 2,511,916 YRBS participants, and 700,660 from Ungdata. The XGBoost model was the top performer on the KYRBS, achieving an area under receiver operating characteristic curve (AUROC) score of 80.61% (95% CI 79.63-81.59) and precision of 30.42 (95% CI 28.65-32.16) with detailed analysis on sensitivity of 31.30 (95% CI 29.47-33.20), specificity of 99.16 (95% CI 99.12-99.20), accuracy of 98.36 (95% CI 98.31-98.42), balanced accuracy of 65.23 (95% CI 64.31-66.17), F1-score of 30.85 (95% CI 29.25-32.51), and area under precision-recall curve of 32.14 (95% CI 30.34-33.95). The model achieved an AUROC score of 79.30% and a precision of 68.37% on the YRBS dataset, while in external validation using the Ungdata dataset, it recorded an AUROC score of 76.39% and a precision of 12.74%. Feature importance and SHapley Additive exPlanation value analyses identified smoking status, BMI, suicidal ideation, alcohol consumption, and feelings of sadness and despair as key contributors to the risk of substance use, with smoking status emerging as the most influential factor.
Conclusions
Based on multinational datasets from South Korea, the United States, and Norway, this study shows the potential of ML models, particularly the XGBoost model, in predicting adolescent substance use. These findings provide a solid basis for future research exploring additional influencing factors or developing targeted intervention strategies.}
}
@article{CAI20051145,
title = {BioSim—a biomedical character-based problem solving environment},
journal = {Future Generation Computer Systems},
volume = {21},
number = {7},
pages = {1145-1156},
year = {2005},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2004.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X04000469},
author = {Yang Cai and Ingo Snel and Betty Cheng and B. {Suman Bharathi} and Clementine Klein and Judith Klein-Seetharaman},
keywords = {Scientific visualization, Biological discovery, Game design, Problem solving, Artificial life, Education},
abstract = {Understanding and solving biomedical problems requires insight into the complex interactions between the components of biomedical systems by domain and non-domain experts. This is challenging because of the enormous amount of data and knowledge in this domain. Therefore, non-traditional educational tools have been developed such as a biological storytelling system, animations of biomedical processes and concepts, and interactive virtual laboratories. The next-generation problem solving tools need to be more interactive to include users with any background, while remaining sufficiently flexible to target open research problems at any level of abstraction, from the conformational changes of a protein to the interaction of the various biochemical pathways in our body. Here, we present an interactive and visual problem solving environment for the biomedical domain. We designed a biological world model, in which users can explore biological interactions by role-playing “characters” such as cells and molecules or as an observer in a “shielded vessel”, both with the option of networked collaboration between simultaneous users. The system architecture of these “characters” contains four main components: (1) bio-behavior is modeled using cellular automata; (2) bio-morphing uses vision-based shape tracking techniques to learn from recordings of real biological dynamics; (3) bio-sensing is based on molecular principles of recognition to identify objects, environmental conditions and progression in a process; (4) bio-dynamics implements mathematical models of cell growth and fluid-dynamic properties of biological solutions. The principles are implemented in a simple world model of the human vascular system and a biomedical problem that involves an infection by Neisseria meningitides where the biological characters are white and red blood cells and Neisseria cells. Our case studies show that the problem solving environment can inspire user's strategic, creative and innovative thinking.}
}
@article{HAO2025115545,
title = {Temperature history reconstruction in steel box girders using limited data and proper orthogonal decomposition-based dimension reduction representation},
journal = {Measurement},
volume = {240},
pages = {115545},
year = {2025},
issn = {0263-2241},
doi = {https://doi.org/10.1016/j.measurement.2024.115545},
url = {https://www.sciencedirect.com/science/article/pii/S0263224124014301},
author = {Jing Hao and Hailin Lu and Hongyou Cao and Yunlai Zhou},
keywords = {Temperature history reconstruction, High-frequency temperature component, Stochastic vector processes, Dimension reduction, Proper orthogonal decomposition},
abstract = {The monitoring temperature history of steel box girders inevitably contains gaps or anomalies due to the malfunctions of the structural health monitoring system. Traditional methods for reconstructing temperature histories face the challenge in accounting for the randomness of temperature variations caused by the uncertainty of complex environmental factors. This research proposes a framework for reconstructing the long-term temperature of steel box girders by combining the limited measurements with a proper orthogonal decomposition (POD) based dimension reduction representation approach, to account for the stochastic nature of daily temperature variations. Unlike the conventional Monte Carlo-based POD method, the developed POD-based dimension reduction representation approach effectively reduces the number of elementary random variables from thousands to two by introducing random functions serving as constraints, overcoming the challenge of high-dimensional random variables inherent in the Monte Carlo methods. The proposed approach divides the measured temperature histories into random high-frequency (HF) and deterministic low-frequency (LF) components and establishes the theoretical models of power spectral density and coherence functions of HF temperature components to accommodate the generation of HF temperature component samples, and finally reconstructs temperature samples by superimposing the LF components and generated HF component samples. The results from a practical example demonstrate that the statistical characteristics of representative HF temperature component samples generated by the POD-based dimension reduction representation align well with the corresponding targeted values, and the proposed method outperforms the traditional POD method, yielding a 60% efficiency enhancement without compromising computational accuracy. The developed framework owns apparent superiority in accuracy compared to the traditional POD and the long short-term memory methods, particularly in continuous and extensive missing data. Moreover, the reconstructed temperature samples with assigned probabilities present complete probability information from the level of total probability. These results advance the probabilistic methods in tackling long-term temperature history reconstruction.}
}
@article{CHAVAS2024476,
title = {Bridging the microscopic divide: a comprehensive overview of micro-crystallization and in vivo crystallography},
journal = {IUCrJ},
volume = {11},
number = {4},
pages = {476-485},
year = {2024},
issn = {2052-2525},
doi = {https://doi.org/10.1107/S205225252400513X},
url = {https://www.sciencedirect.com/science/article/pii/S205225252400054X},
author = {Leonard Michel Gabriel Chavas and Fasséli Coulibaly and Damià Garriga and E. N. Baker},
keywords = {micro-crystallization,  crystallography, structural biology, macromolecular research, X-ray diffraction, XFELs, MicroED},
abstract = {The 26th IUCr congress held in Melbourne brought discussions on micro-crystallization and in vivo crystallography within structural biology to the forefront, highlighting innovative approaches and collaborative efforts to advance macromolecular research.
A series of events underscoring the significant advancements in micro-crystallization and in vivo crystallography were held during the 26th IUCr Congress in Melbourne, positioning microcrystallography as a pivotal field within structural biology. Through collaborative discussions and the sharing of innovative methodologies, these sessions outlined frontier approaches in macromolecular crystallography. This review provides an overview of this rapidly moving field in light of the rich dialogues and forward-thinking proposals explored during the congress workshop and microsymposium. These advances in microcrystallography shed light on the potential to reshape current research paradigms and enhance our comprehension of biological mechanisms at the molecular scale.}
}
@article{IVANITSKY2009101,
title = {Brain science: On the way to solving the problem of consciousness},
journal = {International Journal of Psychophysiology},
volume = {73},
number = {2},
pages = {101-108},
year = {2009},
note = {Neural Processes in Clinical Psychophysiology},
issn = {0167-8760},
doi = {https://doi.org/10.1016/j.ijpsycho.2009.02.004},
url = {https://www.sciencedirect.com/science/article/pii/S0167876009001044},
author = {Alexey M. Ivanitsky and George A. Ivanitsky and Olga V. Sysoeva},
keywords = {Consciousness and brain problem, Event-related potentials, EEG rhythms, Semantic brain systems, Artificial intelligence},
abstract = {Four issues are discussed: the possible mechanism of subjective events, conscious versus unconscious brain functions, the rhythmic coding of mental operations and the possible brain basis of understanding.i.Several approaches have been developed to explain how subjective experience emerges from brain activity. One of them is the return of the nervous impulses to the sites of their primary projections, providing a synthesis of sensory information with memory and motivation [Ivanitsky, A.M., 1976. Brain Mechanisms of the Signal Evaluation. Medicina, Moscow 264 pp. (in Russian)]. Support for the existence of such a mechanism stems from studies upon the brain activity that subserves perception (visual and somato-sensory) and thought (verbal and imaginative). The cortical centres for information synthesis have been found. For perception, these are located in projection areas; for thinking — in frontal and temporal-parietal associative cortex. Closely related ideas were also developed by G. Edelman [Edelman, G.M., 1978. Group selection and phasic reentrant signaling: A theory of higher brain function. In: Eds. Edelman, G.M., Mountcastle, V.B. The Mindful Brain. Cortical Organization and the Group-selective Theory of Higher Brain Function. Cambridge, MA, MIT Press, pp 51–100.] in his re-entry theory of consciousness. Both theories emphasize the key role of memory and motivation in the origin of conscious function.ii.Conscious experience elucidates not all, but only salient brain functions. As a rule, voluntary control is switched on when additional cognitive resources are needed. Even a rather complicated mental operation, such as the discrimination between concrete and abstract words, could be executed very rapidly and implicitly; explicit analysis being engaged only in more difficult tasks. Furthermore, these two different kinds of mental operations, i.e., automatic and conscious, are predominantly associated with two different kinds of memory: a recognition memory for implicit analysis, and an episodic memory for explicit functions.iii.Rearrangements of EEG rhythms underlie mental functions. Certain rhythmical patterns are related with definite types of mental activity. The dependence of one upon the other is rather pronounced and expressive, so it becomes possible to recognize the type of mental operation being performed in mind with few seconds of the ongoing EEG, provided that the analysis of rhythms is accomplished using an artificial neural network.iv.It is commonly recognized that the computer, in contrast to the living brain, can calculate, yet cannot understand [Penrose, R., 1996. Shadows of the Mind: A Search for the Missing Science of Consciousness New York, Oxford, Oxford University Press 480 pp.]. Comprehension implies the comparison of new and old information that requires the ability to search for associations, grouping similar objects together, and distinguishing different objects from one another. However, these functions may also be implemented on a computer. Still, it is believed that computers perform these complicated operations without genuine understanding. Evidently, comprehension additionally has to be based upon some biologically significant ground. It is hypothesized that the subjective feeling of understanding appears when current information is attributed to a definite need, which is scaled in sign (+/−) coordinates. This coordinate system ceases the brain calculations, when “comprehension” is reached, i.e., the acceptable level of need satisfaction is attained.}
}
@article{HOME200255,
title = {Fluids and forces in eighteenth-century electricity},
journal = {Endeavour},
volume = {26},
number = {2},
pages = {55-59},
year = {2002},
issn = {0160-9327},
doi = {https://doi.org/10.1016/S0160-9327(02)01411-4},
url = {https://www.sciencedirect.com/science/article/pii/S0160932702014114},
author = {Roderick W. Home},
abstract = {Our understanding of the history of electricity in the eighteenth century has changed significantly since the early 1960s, when Thomas Kuhn presented it as a leading example to support his general view of the history of science. In particular, while the ideas of Benjamin Franklin are still seen as important, they are no longer seen as constituting a revolution in the theory of electricity. They appear instead as merely one stage in a long drawn-out process of evolution in electrical thinking.}
}
@article{BURCH2023101039,
title = {Investigating two teachers’ development of combinatorial meaning for algebraic structure},
journal = {The Journal of Mathematical Behavior},
volume = {70},
pages = {101039},
year = {2023},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2023.101039},
url = {https://www.sciencedirect.com/science/article/pii/S0732312323000093},
author = {Lori J. Burch},
keywords = {Mathematical meanings, Combinatorial reasoning, Algebraic reasoning, Polynomial operations, Secondary teachers},
abstract = {This paper reports on the results of a four-day teaching experiment that supported two algebra teachers to develop a combinatorial meaning for algebraic structure. The purpose of the teaching episodes was to support the teachers (a) to establish a combinatorial understanding for algebraic structure (Tillema & Burch, 2022) by generalizing the cubic identity, a+b3=a3+3a2b+3ab2+b3, as a symbolization of quantitative and combinatorial relationships out of a contextualized problem (Tillema & Gatza, 2016) and (b) to develop a combinatorial meaning as a mobilization of their understanding through a series of algebraic tasks (cf. Thompson et al., 2014). The findings from this study contribute to research literature on teachers’ mathematical meanings within secondary algebra by investigating how teachers’ combinatorial meanings developed and how differences in their combinatorial meanings impacted their algebraic reasoning. The findings demonstrate a combinatorial pathway for supporting the development of expanding and factoring as reversible polynomial operations (cf. Sangwin & Jones, 2017).}
}
@article{NELSON2019100758,
title = {Designing and transforming yield-stress fluids},
journal = {Current Opinion in Solid State and Materials Science},
volume = {23},
number = {5},
pages = {100758},
year = {2019},
issn = {1359-0286},
doi = {https://doi.org/10.1016/j.cossms.2019.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S1359028619300762},
author = {Arif Z. Nelson and Kenneth S. Schweizer and Brittany M. Rauzan and Ralph G. Nuzzo and Jan Vermant and Randy H. Ewoldt},
keywords = {Soft matter, Yield-stress fluid, Design, Engineering, Extension, Thixotropy, Elasticity, Colloids, Emulsions, Polymers, 3D printing, Chemistry, Physics, Rheology, Complex fluids},
abstract = {We review progress in designing and transforming multi-functional yield-stress fluids and give a perspective on the current state of knowledge that supports each step in the design process. We focus mainly on the rheological properties that make yield-stress fluids so useful and the trade-offs which need to be considered when working with these materials. Thinking in terms of “design with” and “design of” yield-stress fluids motivates how we can organize our scientific understanding of this field. “Design with” involves identification of rheological property requirements independent of the chemical formulation, e.g. for 3D direct-write printing which needs to accommodate a wide range of chemistry and material structures. “Design of” includes microstructural considerations: conceptual models relating formulation to properties, quantitative models of formulation-structure-property relations, and chemical transformation strategies for converting effective yield-stress fluids to be more useful solid engineering materials. Future research directions are suggested at the intersection of chemistry, soft-matter physics, and material science in the context of our desire to design useful rheologically-complex functional materials.}
}
@article{BUI2017115,
title = {Envisioning the future of ‘big data’ biomedicine},
journal = {Journal of Biomedical Informatics},
volume = {69},
pages = {115-117},
year = {2017},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2017.03.017},
url = {https://www.sciencedirect.com/science/article/pii/S1532046417300709},
author = {Alex A.T. Bui and John Darrell {Van Horn}},
keywords = {Biomedicine, Data science, Software, Computing, Training},
abstract = {Through the increasing availability of more efficient data collection procedures, biomedical scientists are now confronting ever larger sets of data, often finding themselves struggling to process and interpret what they have gathered. This, while still more data continues to accumulate. This torrent of biomedical information necessitates creative thinking about how the data are being generated, how they might be best managed, analyzed, and eventually how they can be transformed into further scientific understanding for improving patient care. Recognizing this as a major challenge, the National Institutes of Health (NIH) has spearheaded the “Big Data to Knowledge” (BD2K) program – the agency’s most ambitious biomedical informatics effort ever undertaken to date. In this commentary, we describe how the NIH has taken on “big data” science head-on, how a consortium of leading research centers are developing the means for handling large-scale data, and how such activities are being marshalled for the training of a new generation of biomedical data scientists. All in all, the NIH BD2K program seeks to position data science at the heart of 21st Century biomedical research.}
}
@article{LASSITER201927,
title = {Language and simplexity: A powers view},
journal = {Language Sciences},
volume = {71},
pages = {27-37},
year = {2019},
note = {Simplexity, agency and language},
issn = {0388-0001},
doi = {https://doi.org/10.1016/j.langsci.2018.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S0388000118300366},
author = {Charles Lassiter},
keywords = {Distributed language, Simplexity, Causal powers, Speech acts},
abstract = {The notion of simplexity is that complex problems are often solved by novel combinations of simple mechanisms. These solutions aren't simple; they're simplex. Language use, as a complex behavior, is ripe for simplex analysis. In this paper, I argue that the notion of powers—an organism's capacity to instigate or undergo change—is doubly useful. First, powers, as opposed to mental representations, are a suitable object for simplex analysis. So conceptualizing languaging in terms of powers gets us one step closer to a simplex analysis of language. But thinking of languaging in terms of powers has an additional payoff. Berthoz asserts that the concept of simplexity is related to the concept of meaning. How they're related is unclear. Conceptualizing languaging in terms of powers injects meaningfulness into lived world of the organism. Consequently, the concept of powers can act as a bridge between the concepts of meaningfulness and simplexity.}
}
@article{LORIMER2009152,
title = {Empathic accuracy in coach–athlete dyads who participate in team and individual sports},
journal = {Psychology of Sport and Exercise},
volume = {10},
number = {1},
pages = {152-158},
year = {2009},
issn = {1469-0292},
doi = {https://doi.org/10.1016/j.psychsport.2008.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S1469029208000526},
author = {Ross Lorimer and Sophia Jowett},
keywords = {Empathy, Understanding, Interaction, Coach–athlete dyads},
abstract = {Objective
The purpose of the present study was to investigate the empathic accuracy of coach–athlete dyads participating in team and individual sports.
Method
An adaptation of Ickes's [2001. Measuring empathic accuracy. In J. A. Hall & F. J. Bernieri (Eds.), Interpersonal sensitivity (pp. 219–242). Mahwah, NJ: Lawrence Erlbaum Associates] unstructured dyadic interaction paradigm was used to assess the empathic accuracy of 40 coach–athlete dyads. Accordingly, each dyad was filmed during a training session. The dyad members viewed selected video footage that displayed discrete interactions that had naturally occurred during that session. Dyad members reported what they remembered thinking/feeling while making inferences about what their partner's thought/felt at each point. Empathic accuracy was estimated by comparing self-reports and inferences.
Results
The results indicted that accuracy for coaches in individual sports was higher than coaches in team sports. Shared cognitive focus also differed between team and individual sports, and fully mediated the effect of sport-type on coach empathic accuracy. Moreover, coaches whose training sessions were longer demonstrated increased empathic accuracy. No differences were found for athletes.
Conclusions
The results suggest that the dynamics of the interaction between a coach and an athlete play a key role in how accurately they perceive each other.}
}
@incollection{HEFNER20163,
title = {Chapter 1 - A Brief History of Biological Distance Analysis},
editor = {Marin A. Pilloud and Joseph T. Hefner},
booktitle = {Biological Distance Analysis},
publisher = {Academic Press},
address = {San Diego},
pages = {3-22},
year = {2016},
isbn = {978-0-12-801966-5},
doi = {https://doi.org/10.1016/B978-0-12-801966-5.00001-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128019665000019},
author = {J.T. Hefner and M.A. Pilloud and J.E. Buikstra and C.C.M. Vogelsberg},
keywords = {aDNA, Analytical scales, Biodistance, Cranial nonmetric traits, Craniometrics, Kinship, Odontometrics, Typology},
abstract = {Biological distance, or biodistance, analysis employs data derived from skeletal remains to reflect population relatedness (similarity/dissimilarity) through the application of multivariate statistical methods. The approaches used in biodistance studies have changed markedly over recent centuries, exploring phenotypic expressions assumed to be informative. Biodistance analysis began as the study of anomalous variants in the human skull, but the field has transformed over the centuries now seeking to incorporate skeletal morphology in the interpretation of genetic affinity, providing insight into the genetics governing trait expression, and providing understanding into the role of developmental biology on the expression of morphological variants. As methodological approaches improve, so too has the application of these analyses. We present here a brief historical overview of biodistance analysis research, focusing on meta-themes in the field, shifts in thinking among researchers in biological anthropology, and several of the outside influences that impact biodistance analysis.}
}
@article{COLTHER2024100625,
title = {Artificial intelligence: Driving force in the evolution of human knowledge},
journal = {Journal of Innovation & Knowledge},
volume = {9},
number = {4},
pages = {100625},
year = {2024},
issn = {2444-569X},
doi = {https://doi.org/10.1016/j.jik.2024.100625},
url = {https://www.sciencedirect.com/science/article/pii/S2444569X24001641},
author = {Cristian Colther and Jean Pierre Doussoulin},
keywords = {Artificial intelligence, Evolution knowledge, Noosphere, Ethical considerations, Future scenarios},
abstract = {This article proposes that artificial intelligence (AI) is positioned as a key driver of a new evolutionary stage of human knowledge, complementing human intelligence and facilitating the creation and development of sophisticated collective intelligence, defined as the noosphere, understood as the sphere of collective human thought. The study reveals several key insights into the transformative potential of AI, including its capacity to accelerate, mediate, and diffuse human knowledge. It concludes that AI not only catalyzes the existence of the noosphere but also redefines the structures and mechanisms through which human knowledge is expanded and democratized. Additionally, the document presents potential risks and significant ethical, social, and legal challenges of an AI-mediated noosphere, offering recommendations and a research agenda around the topic, and limitations and proposals for improvement to be considered in the future.}
}
@incollection{SNYDER200089,
title = {Chapter 5 - Hope as a Common Factor across Psychotherapy Approaches: A Lesson from the Dodo's Verdict},
editor = {C.R. Snyder},
booktitle = {Handbook of Hope},
publisher = {Academic Press},
address = {San Diego},
pages = {89-108},
year = {2000},
isbn = {978-0-12-654050-5},
doi = {https://doi.org/10.1016/B978-012654050-5/50007-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780126540505500075},
author = {C.R. Snyder and Julia D. Taylor},
abstract = {Publisher Summary
Despite providing different explanations and targeting disparate symptoms, various psychological approaches for producing change appear to be equally effective. The chapter compares this phenomenon to the Dodo's verdict in “Alice in Wonderland,” in the end race. It explores the question—specifically, what mechanism (or mechanisms) underlie the equal, high efficacy produced by differing types of psychological interventions. Agency reflects people's thoughts about their capacity to use the pathways they have selected to reach their goals. Agency is crucial for the psychotherapy process because it provides mental energy so that a client can undertake various therapy-related activities. This type of goal-directed motivation is reflected in self-affirming mental statements. Operating across differing samples and methodologies, agentic thinking both initiates and helps sustain clients' improvements in psychotherapy. Furthermore, enlisting the literature on placebo effects to illustrate the impact of agentic thinking. The chapter demonstrates how agency alone propels clinical improvement. Agentic thought is the motivational force or engine in hope theory. All the mental energy imaginable, however, cannot guarantee successful goal attainment in psychotherapy. Perceptions that one can produce the routes to those goals is a second necessary component. The chapter also explores pathways thinking in the context of varying psychotherapies.}
}
@article{ROBERTS2020116758,
title = {Creative, internally-directed cognition is associated with reduced BOLD variability},
journal = {NeuroImage},
volume = {219},
pages = {116758},
year = {2020},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2020.116758},
url = {https://www.sciencedirect.com/science/article/pii/S1053811920302457},
author = {Reece P. Roberts and Cheryl L. Grady and Donna Rose Addis},
keywords = {Episodic simulation, Imagination, Creativity, BOLD variability},
abstract = {In a range of externally-directed tasks, intra-individual variability of fMRI BOLD signal has been shown to be a stronger predictor of cognitive performance than mean BOLD signal. BOLD variability’s strong association with cognitive performance is hypothesised to be due to it capturing the dynamic range of neural systems. Although increased BOLD variability is also speculated to play a role in internally-directed thought, particularly when creative and flexible cognition is required, there is a relative lack of research exploring whether BOLD variability is related to internally-directed cognition. Thus, we investigated the relationship between BOLD variability and a key component of creativity – divergent thinking – in various tasks that required participants to think flexibly. We also determined whether any associations between BOLD variability and creativity overlapped with, or differed, from associations between mean BOLD signal and creativity. First, we performed task Partial Least Squares (PLS) analyses that compared BOLD signal (either mean or variability) during two future imagination conditions that differed in the amount of cognitive flexibility required: a Congruent condition in which autobiographical details (people, places, objects) comprising an imagined event belonged to the same social sphere (e.g., university) and an Incongruent condition in which details belonged to different social spheres and required greater cognitive flexibility to integrate. Results indicated that the Incongruent condition was associated with a widespread reduction in both BOLD variability and mean signal (relative to the Congruent condition), but in largely non-overlapping regions. Next, we used behavioral PLS to determine whether individual differences in performance on future simulation tasks as well as the Alternate Uses Task relates to BOLD variability and mean BOLD signal. Better performance on these tasks was predominantly associated with increases in mean BOLD signal and decreases in BOLD variability, in a range of disparate brain regions. Together, the results suggest that, unlike tasks requiring externally-directed cognition, superior performance on tasks requiring creative internal mentation is associated with less (not more) variability.}
}
@article{BENDETOWICZ2017216,
title = {Brain morphometry predicts individual creative potential and the ability to combine remote ideas},
journal = {Cortex},
volume = {86},
pages = {216-229},
year = {2017},
note = {Is a "single" brain model sufficient?},
issn = {0010-9452},
doi = {https://doi.org/10.1016/j.cortex.2016.10.021},
url = {https://www.sciencedirect.com/science/article/pii/S0010945216303161},
author = {David Bendetowicz and Marika Urbanski and Clarisse Aichelburg and Richard Levy and Emmanuelle Volle},
keywords = {Creativity, Semantic associations, Rostral prefrontal, Frontal pole, Morphometry},
abstract = {For complex mental functions such as creative thinking, inter-individual variability is useful to better understand the underlying cognitive components and brain anatomy. Associative theories propose that creative individuals have flexible semantic associations, which allows remote elements to be formed into new combinations. However, the structural brain variability associated with the ability to combine remote associates has not been explored. To address this question, we performed a voxel-based morphometry (VBM) study and explored the anatomical connectivity of significant regions. We developed a Remote Combination Association Task adapted from Mednick's test, in which subjects had to find a solution word related to three cue words presented to them. In our adaptation of the task, we used free association norms to quantify the associative distance between the cue words and solution words, and we varied this distance. The tendency to solve the task with insight and the ability to evaluate the appropriateness of a proposed solution were also analysed. Fifty-four healthy volunteers performed this task and underwent a structural MRI. Structure–function relationships were analysed using regression models between grey matter (GM) volume and task performance. Significant clusters were mapped onto an atlas of white matter (WM) tracts. The ability to solve the task, which depended on the associative distance of the solution word, was associated with structural variation in the left rostrolateral prefrontal and posterior parietal regions; the left rostral prefrontal region was connected to distant regions through long-range pathways. By using a creative combination task in which the semantic distance between words varied, we revealed a brain network centred on the left frontal pole that appears to support the ability to combine information in new ways by bridging the semantic distance between pieces of information.}
}
@article{ARAGONES20141,
title = {Rhetoric and analogies},
journal = {Research in Economics},
volume = {68},
number = {1},
pages = {1-10},
year = {2014},
issn = {1090-9443},
doi = {https://doi.org/10.1016/j.rie.2013.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S1090944313000410},
author = {Enriqueta Aragones and Itzhak Gilboa and Andrew Postlewaite and David Schmeidler},
keywords = {Rhetoric, Analogies, Complexity},
abstract = {The art of rhetoric may be defined as changing other people's minds (opinions, beliefs) without providing them new information. One technique heavily used by rhetoric employs analogies. Using analogies, one may draw the listener's attention to similarities between cases and to re-organize existing information in a way that highlights certain regularities. In this paper we offer two models of analogies, discuss their theoretical equivalence, and show that finding good analogies is a computationally hard problem.}
}
@article{ALAGEEL20152003,
title = {Human Factors in the Design and Evaluation of Bioinformatics Tools},
journal = {Procedia Manufacturing},
volume = {3},
pages = {2003-2010},
year = {2015},
note = {6th International Conference on Applied Human Factors and Ergonomics (AHFE 2015) and the Affiliated Conferences, AHFE 2015},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2015.07.247},
url = {https://www.sciencedirect.com/science/article/pii/S2351978915002486},
author = {Naelah Al-Ageel and Areej Al-Wabil and Ghada Badr and Noura AlOmar},
keywords = {Bioinformatics tools, Human factors, Usability metrics, Heuristics evaluation},
abstract = {Human factors contribute significantly to the information visualization design considerations and usability evaluation process, and have been shown to play an important role in the design, development and quality assurance of bioinformatics tools. Despite the technological advances in bioinformatics computational methods, humans are an indispensable part of the data mining and decision making process. The complexity of biology data visualization can make perception and analysis a complex cognitive activity for professionals in the bioinformatics domain. Information Visualization (InfoVis) can provide valuable assistance for data analysis in bioinformatics by visually depicting sequences, genomes, alignments, and macromolecular structures. InfoVis coupled with interaction modalities of bioinformatics tools also impact the efficiency and effectiveness of decision-making tasks in applied bioinformatics computing. However, the way people perceive and interact with bioinformatics tools can strongly influence their understanding of the complex data as well as the perceived usability and accessibility of these systems. In this paper, we present a synthesis of research studies and initiatives that have recently examined human factors in interaction and visualization for bioinformatics tools, particularly in perception-based design. Although bioinformatics’ visualization and interaction design research that involves human factors is considered in its infancy, a plethora of potentially promising areas have yet to be explored. The aims of this paper are to review current human factors research in interaction, usability and visualization within bioinformatics tools to provide a basis for future investigations in systems and software engineering of bioinformatics tools, and to identify promising areas for future research directions in interaction design of bioinformatics tools.}
}
@incollection{SIEGEL20163,
title = {Chapter 1 - Introduction: Defining the Role of Statistics in Business},
editor = {Andrew F. Siegel},
booktitle = {Practical Business Statistics (Seventh Edition)},
publisher = {Academic Press},
edition = {Seventh Edition},
pages = {3-17},
year = {2016},
isbn = {978-0-12-804250-2},
doi = {https://doi.org/10.1016/B978-0-12-804250-2.00001-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128042502000018},
author = {Andrew F. Siegel},
abstract = {We begin this chapter with an overview of the competitive advantage provided by a knowledge of statistical methods, followed by some basic facts about statistics and probability and their role in business. Statistical activities can be grouped into five main activities (designing, exploring, modeling, estimating, and hypothesis testing), and one way to clarify statistical thinking is to be able to match the business task at hand with the correct collection of statistical methods. This chapter sets the stage for the rest of the book, which follows up with many important detailed procedures for accomplishing business goals that involve these activities. Next follows an overview of data mining of Big Data (which involves these main activities) and its importance in business. Then we distinguish the field of probability (where, based on assumptions, we reach conclusions about what is likely to happen—a useful exercise in business where nobody knows for sure what will happen) from the field of statistics (where we know from the data what happened, from which we infer conclusions about the system that produced these data) while recognizing that probability and statistics will work well together in future chapters. The chapter concludes with some words of advice on how to integrate statistical thinking with other business viewpoints and activities.}
}
@article{HEDE2015522,
title = {TRIZ and the Paradigms of Social Sustainability in Product Development Endeavors},
journal = {Procedia Engineering},
volume = {131},
pages = {522-538},
year = {2015},
note = {TRIZ and Knowledge-Based Innovation in Science and Industry},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2015.12.447},
url = {https://www.sciencedirect.com/science/article/pii/S1877705815043398},
author = {Shantesh Hede and Paula Verandas Ferreira and Manuel Nunes Lopes and Luis Alexandre Rocha},
keywords = {Sustainability, Decision Modeling, TRIZ, Product Development ;},
abstract = {The Business practices of an industrialized civilization are responsible for intensifying the dynamics of the interdependent environmental, social and economic domains of our ecosystem. The worldwide objective to accomplish Sustainability is invariably addressed by Policy makers and Institutions by means of moderately disparate co-relations between Environmental and Social considerations. The dimension of Social Sustainability has a direct co-relation towards the extended continuation of a globalized Enterprise. The stated co-relation is an interconnected and interdependent network comprising of growth in Innovation and Sustainability at the Environmental and Economic frontiers. From the standpoint of Innovation, the 20th century has been dominated by both TRIZ with OTSM and Kurzweil's Law of Accelerating Returns to steer the future of revolutionary innovations. Moreover, TRIZ and its evolved counterpart OTSM have been extensively utilized for macro-scale problem solving scenarios, while Kurzweil's Law has reached up to quantum scale whereby matter as we know exhibits an entire range of unique properties with a potential to dramatically transform our human civilization. Accordingly, the perceived limitations and vague applicability of TRIZ in sub-macro scale innovations has been discussed. The contemporary tools for project evaluation (e.g.: cost benefit analysis) and product development (e.g.: linear stage-gate process) quintessential for commercializing innovations are identified to be limited, both in scope and accuracy for delivering a long term ‘sustainable’ competitive advantage to an Enterprise. Consequently, the proposed conceptual Multifaceted Framework addresses the issue of social sustainability in Product Development. The underpinnings of Systems Thinking, TRIZ and OTSM, Complex Adaptive Systems, Socio-Economics & Human Behavior forms the fundamental basis of the proposed Multifaceted Framework. The novel perspective offered by the proposed Framework enables product development teams to overcome the inherent myopia and other limitations associated with the contemporary Environmental Life Cycle Analysis and Sustainability related Decision Models. An Expert opinion based evaluation technique in conjugation with a Multilayered Decision Modeling Method have been incorporated as a salient features in the proposed framework. The evaluation technique is utilized for assigning numerical values to the pertinent sustainability related criteria of the Multilayered Decision Model. The proposed Framework plays a crucial role in product development and decision modeling across the Idea Screening Phase (Stage 2) up to the Feasibility Analysis Phase (Stage 4). In addition, a modified version Taguchi Loss Function is included to exemplify a tangible relation between Product Quality parameters and Sustainability. The objective of the proposed framework is to provide an efficient, yet comprehensive evaluation as well as an effective product development strategy with a distinct and a holistic outlook on Social Sustainability.}
}
@incollection{GARDNER202451,
title = {Chapter 3 - Designing and prototyping smarter urban spaces},
editor = {Nicole Gardner},
booktitle = {Scaling the Smart City},
publisher = {Elsevier},
pages = {51-74},
year = {2024},
series = {Smart Cities},
isbn = {978-0-443-18452-9},
doi = {https://doi.org/10.1016/B978-0-443-18452-9.00009-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780443184529000094},
author = {Nicole Gardner},
keywords = {Cyber-physical system, Design, Design education, Design pedagogy, Design process, Human-computer interaction, Interaction, Interaction design, IoT, Physical computing, Prototyping, Smart city, Urban technology},
abstract = {This chapter outlines how the concept of the smart city is explored and challenged in an undergraduate design course that adopts a cross-scale framework to design and prototype urban technology projects. It sets out an integrated and interdisciplinary approach to urban technology design that combines context-oriented spatial design methods with physical computing and interaction principles. It construes the design and prototyping of urban technology projects as sociotechnical thought experiments that can materialize ethical concerns and explore alternate ways that urban life can be lived with technology. The chapter concludes by outlining the themes that organize selected existing and speculative urban technology projects in the following chapters of the book.}
}
@article{HUDA2024122380,
title = {Experts and intelligent systems for smart homes’ Transformation to Sustainable Smart Cities: A comprehensive review},
journal = {Expert Systems with Applications},
volume = {238},
pages = {122380},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.122380},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423028828},
author = {Noor Ul Huda and Ijaz Ahmed and Muhammad Adnan and Mansoor Ali and Faisal Naeem},
keywords = {Artificial intelligence, Automation, Block chain, Energy management, Expert intelligent systems, Management systems, Smart cities},
abstract = {In this constantly evolving landscape of urbanization, the relationship between technology and automation, in regards to sustainability, holds immense significance. The intricate strands of human intelligence are seamlessly interwoven with the fabric of technological progress, giving rise to exquisite patterns of synergy and collaborative innovation. Automation is just another step in this process which started with the industrial revolution and now has paved way towards urbanization. Smart homes or home automation is a subset of Internet of Things (IoT) based automation that has added into the comfort, ease, and quality of our living standards and is now being integrated to form the concept of Smart Cities. In the past decade, various techniques and processes of smart home automation have been proposed and implemented. To extend and translate the existing methods into new one, the understanding of the former is imperative to the research procedure. This review stands as a comprehensive exploration, diving into the pivotal role of intelligent systems and expert knowledge in driving the transformation of smart homes into sustainable smart cities. By meticulously analyzing and aggregating an array of contemporary techniques used in smart homes, this paper offers profound contributions to the intersection of urban evolution and technological innovation. The review’s holistic approach not only facilitates a deep understanding of smart homes’ contributions but also charts a course for innovative strategies in city planning, infrastructure, and technological integration. In bridging the gap between technology and sustainable urban development, this exploration underscores the transformative power of leveraging smart home techniques to lay the foundation for harmonious and forward-thinking smart cities. The technologies cover a wide range of methodologies and intelligent systems used for communication, security and management in an urban infrastructure. The paper focuses on analysis of the technology to provide an outlook into achieving the goal of sustainable smart cities and deal with challenges like scalability and big data computation. Our comprehensive analysis yields a holistic set of technology comparisons and illuminates the promising future prospects within this domain. The information is highly insightful in creating a bigger picture for adopting state of the art technologies like Federated Learning (FL), Digital Twin and Embedded Edge computing in better planning and infrastructure management in smart cities. These findings offer reliable and potent methods to chart not only the course of research but also to enhance these technologies for the betterment of mankind’s convenience and advancement.}
}
@article{BREIGER2018104,
title = {Capturing distinctions while mining text data: Toward low-tech formalization for text analysis},
journal = {Poetics},
volume = {68},
pages = {104-119},
year = {2018},
issn = {0304-422X},
doi = {https://doi.org/10.1016/j.poetic.2018.02.005},
url = {https://www.sciencedirect.com/science/article/pii/S0304422X17301584},
author = {Ronald L. Breiger and Robin Wagner-Pacifici and John W. Mohr},
keywords = {Text mining, Hermeneutics, National security, Computational sociology, Big data, Close reading},
abstract = {In this article we consider some low-tech approaches to text mining. Our goal is to articulate a RiCH (Reader in Control of Hermeneutics) style of text analysis that takes advantage of the digital affordances of modern reading practices and easily deployable computational tools while also preserving the primacy of the interpretive lens of the human reader. In the article we offer three analytical interventions that are suitable to the low-tech formalizations we propose: the first and most developed intervention tracks the (normally computationally ignored) “stop” words; the second identifies the use of strategic anxiety terms in the texts; and the third (less developed in this article) introduces the grammatical features of modality (including modalization statements of probability and usuality, and modulation statements regarding degrees of obligation and inclination). All three analytical interventions provide a productive tracking of various modes and degrees of strategic decisiveness, contradiction, uncertainty and indeterminacy in a corpus of recent U.S. National Security Strategy reports.}
}
@incollection{FLOTHER202583,
title = {Chapter 6 - Early quantum computing applications on the path towards precision medicine},
editor = {Laura Kelly and William P. Stanford},
booktitle = {Implementation of Personalized Precision Medicine},
publisher = {Academic Press},
pages = {83-96},
year = {2025},
isbn = {978-0-323-98808-7},
doi = {https://doi.org/10.1016/B978-0-323-98808-7.00001-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780323988087000011},
author = {Frederik F. Flöther},
keywords = {Precision medicine, Quantum computers, Artificial intelligence/machine learning, EHRs},
abstract = {The last few years have seen rapid progress in transitioning quantum computing from lab to industry. In healthcare and life sciences, more than 40 proof-of-concept experiments and studies have been conducted; an increasing number of these are even run on real quantum hardware. Major investments have been made with hundreds of millions of dollars already allocated towards quantum applications and hardware in medicine. In addition to pharmaceutical and life sciences uses, clinical and medical applications are now increasingly coming into the picture. This chapter focuses on three key use case areas associated with (precision) medicine, including genomics and clinical research, diagnostics, and treatments and interventions. Examples of organizations and the use cases they have been researching are given; ideas how the development of practical quantum computing applications can be further accelerated are described.}
}
@article{RENNA2023104420,
title = {A randomized controlled trial comparing two doses of emotion regulation therapy: Preliminary evidence that gains in attentional and metacognitive regulation reduce worry, rumination, and distress},
journal = {Behaviour Research and Therapy},
volume = {170},
pages = {104420},
year = {2023},
issn = {0005-7967},
doi = {https://doi.org/10.1016/j.brat.2023.104420},
url = {https://www.sciencedirect.com/science/article/pii/S0005796723001687},
author = {Megan E. Renna and Phillip E. Spaeth and Jean M. Quintero and Mia S. O'Toole and Christina F. Sandman and David M. Fresco and Douglas S. Mennin},
keywords = {Emotion regulation, Randomized controlled trial, Distress, Anxiety, Depression},
abstract = {Background
Emotion regulation therapy (ERT) promotes resilience in distress disorders by strengthening attentional and metacognitive capacities. Regulation skills are presented with the goal of ameliorating the perseverative negative thinking (PNT) that characterizes these disorders. This study tested ERT in a randomized controlled trial comparing the effectiveness of 16-session (ERT16) versus 8-session (ERT8) doses.
Method
Patients (N = 72) endorsing elevated worry and/or rumination and meeting diagnostic criteria for a distress disorder were randomized to ERT8 or ERT16. PNT, anxiety/depressive symptoms, functioning/quality of life, and treatment mechanisms (attention shifting, attention focusing, decentering, reappraisal) were measured at pre, mid, and post treatment. Clinical symptom severity was also assigned via diagnostic interview at each timepoint.
Results
ERT produced significant improvements across outcomes. ERT16 showed an advantage over ERT8 for distress disorder severity, worry, rumination, and attention shifting from pre-post treatment. Changes in ERT treatment mechanisms mediated changes in clinical improvement.
Conclusion
These findings provide evidence of the effectiveness of two doses of ERT in reducing PNT and distress through improvements in regulation skills.
Clinicaltrials.gov identifier
NCT04060940.}
}
@article{HESTER2019187,
title = {Simulation of integrative physiology for medical education},
journal = {Morphologie},
volume = {103},
number = {343},
pages = {187-193},
year = {2019},
issn = {1286-0115},
doi = {https://doi.org/10.1016/j.morpho.2019.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S1286011519300554},
author = {R.L. Hester and W. Pruett and J. Clemmer and A. Ruckdeschel},
keywords = {VPH, Simulation, Physiology, Healthcare, Electronic health record},
abstract = {Summary
Medical education is founded on the understanding of physiology. While lecture materials and reading contribute to the learning of physiology, the richness and complexity of the subject suggest that more active learning methods may provide a richer introduction to the science as it applies to the practice of medicine. Simulation has been previously used in basic science to better understand the interaction of physiological systems. In the current context, simulation generally refers to interactive case studies performed with a manikin or anatomic device. More recently, simulation has grown to encompass computational simulation: virtual models of physiology and pathophysiology where students can see in a mechanistic setting how tissues and organs interact with one another to respond to changes in their environment. In this manuscript, we discuss how simulation fits into the overall history of medical education, and detail two computational simulation products designed for medical education. The first of these is an acute simulator, JustPhysiology, which reduces the scope of a large model, HumMod, down to a more focused interface. The second is Sycamore, an electronic health record-delivered, real time simulator of patients designed to teach chronic patient care to students. These products represent a new type of tool for medical and allied health students to encourage active learning and integration of basic science knowledge into clinical situations.
Résumé
L’étude de la médecine est fondée entre autres sur la compréhension de la physiologie. Bien que l’apprentissage de la physiologie puisse se faire au moyen de cours magistraux et la lecture de contenus spécialisés, la richesse et la complexité du sujet laissent supposer que des méthodes d’apprentissage plus interactives puissent susciter une initiation plus élaborée de cette science et de son application à la pratique de la médecine. La simulation a précédemment été appliquée aux sciences fondamentales afin de mieux comprendre l’interaction entre systèmes physiologiques. Dans le contexte actuel, la simulation réfère en général à des études de cas interactives réalisées à l’aide d’un mannequin ou tout autre modèle anatomique. Plus récemment, la simulation s’est étendue à la simulation informatique incluant des modèles virtuels de physiologie et de physiopathologie à partir desquels les étudiants peuvent apprécier dans un contexte mécanistique comment les tissus et organes interagissent dans leur réponse à tout changement environnemental. Dans cet article nous présentons comment la simulation s’intègre dans l’histoire de l’éducation de la médecine et détaillons deux modèles de simulation informatique adaptés à l’éducation médicale. Le premier modèle, JustPhysiology, est un simulateur de courte durée qui réduit le champ d’action d’un simulateur plus complexe, HumMod, à une interface plus spécialisée. Le second outil est Sycamore, un dossier de santé électronique généré en temps réel et conçu pour un apprentissage de la pratique de soins médicaux en continu. Ces simulateurs informatiques représentent un nouvel outil pour les étudiants en médecine et autres professions de santé afin d’encourager un apprentissage actif et l’intégration de concepts scientifiques fondamentaux aux conditions cliniques.}
}
@article{KELLER2011174,
title = {Towards a science of informed matter},
journal = {Studies in History and Philosophy of Science Part C: Studies in History and Philosophy of Biological and Biomedical Sciences},
volume = {42},
number = {2},
pages = {174-179},
year = {2011},
note = {When Physics Meets Biology},
issn = {1369-8486},
doi = {https://doi.org/10.1016/j.shpsc.2010.11.024},
url = {https://www.sciencedirect.com/science/article/pii/S1369848610001172},
author = {Evelyn Fox Keller},
keywords = {Information, Self-assembly, Evolution, Selection, Embodiment, Supramolecular chemistry},
abstract = {Over the last couple of decades, a call has begun to resound in a number of distinct fields of inquiry for a reattachment of form to matter, for an understanding of ‘information’ as inherently embodied, or, as Jean-Marie Lehn calls it, for a “science of informed matter.” We hear this call most clearly in chemistry, in cognitive science, in molecular computation, and in robotics—all fields looking to biological processes to ground a new epistemology. The departure from the values of a more traditional epistemological culture can be seen most clearly in changing representations of biological development. Where for many years now, biological discourse has accepted a sharp distinction (borrowed directly from classical computer science) between information and matter, software and hardware, data and program, encoding and enactment, a new discourse has now begun to emerge in which these distinctions have little meaning. Perhaps ironically, much of this shift depends on drawing inspiration from just those biological processes which the discourse of disembodied information was intended to describe.}
}
@article{BASOV2024101869,
title = {Professional patios, emotional studios: Locating social ties in European art residences},
journal = {Poetics},
volume = {102},
pages = {101869},
year = {2024},
issn = {0304-422X},
doi = {https://doi.org/10.1016/j.poetic.2024.101869},
url = {https://www.sciencedirect.com/science/article/pii/S0304422X24000081},
author = {Nikita Basov and Dafne Muntanyola-Saura and Sergi Méndez and Oleksandra Nenko},
keywords = {Material space, Social network, Socio-material network analysis, Mixed method, Statistical modeling of ethnographic data, Artistic residence},
abstract = {To foster creativity through sociality, residences put artists together. At the same time, in their quest for originality, artists often opt for individualism. Little is known on how physical collocation in residences affects artistic sociality. Addressing this gap, we draw on a combination of interviews, observations, and surveys, analysed with an innovative mixture of abductive coding, computational space analysis, and statistical network modeling. This allows us to unveil how room sharing and object usage relate to friendships and collaborations between residents. Along with explicit individualism of artists, we spot plenty of social ties between them. And these ties are positively related to joint material embeddedness. Simultaneously, the two main types of residential zones – working studios and leisure areas – appear to encourage the types of social ties inverse to our expectations. Our findings inform the practice of artistic residence organising and the proposed approach enables explanatory analysis of the relation between material space and sociality in various settings.}
}
@article{REN2024122745,
title = {Pooling-based Visual Transformer with low complexity attention hashing for image retrieval},
journal = {Expert Systems with Applications},
volume = {241},
pages = {122745},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.122745},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423032475},
author = {Huan Ren and Jiangtao Guo and Shuli Cheng and Yongming Li},
keywords = {Pooling-based Visual Transformers, Attention, Image retrieval, Deep hash},
abstract = {Retrieving similar images is becoming an urgent need for us with the continuous growth of large-scale data. However, whether the dominant image retrieval methods are Convolutional Neural Networks (CNNs) or the recently emerging Visual Transformer (ViT), their complex computation, insufficient feature extraction, and mismatched weights greatly influence the efficiency and retrieval accuracy. In this paper, we propose a Pooling-based Visual Transformer with low complexity attention hashing (PTLCH) for image retrieval. First, a backbone network for Pooling-based Vision Transformer (PiT) feature learning is designed to combine the pooling in CNN and the ViT to achieve the purpose of spatial dimensionality reduction while learning rich semantic information. Second, a low complexity attention (LCA) module is incorporated into PiT, which works by combining the positional deviation with the key matrix and the value matrix and then matrix multiplying with the query matrix. LCA explores rich contextual information to enable network learning of more granular feature information. Finally, a new loss framework is proposed where we focus on the effect of difficult and erroneous samples on accuracy. By using different improved cross-entropy losses, better weights are assigned to the learning samples of our network, which effectively improves learning hash coding. We have conducted extensive experiments on three public datasets, CIFAR-10, ImageNet100, and MS-COCO, which have the highest mean average precision of 93.76%, 92.62%, and 90.60%, respectively.}
}
@incollection{SWARNALINGAM202535,
title = {Chapter 3 - Electroencephalographic evaluation of epileptogenicity: traditional versus novel biomarkers to guide surgery},
editor = {Aria Fallah and George M. Ibrahim and Alexander G. Weil},
booktitle = {Pediatric Epilepsy Surgery Techniques},
publisher = {Academic Press},
pages = {35-55},
year = {2025},
isbn = {978-0-323-95981-0},
doi = {https://doi.org/10.1016/B978-0-323-95981-0.00006-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780323959810000060},
author = {Eroshini Swarnalingam and Julia Jacobs},
keywords = {Medically refractory epilepsy, drug resistant epilepsy, epilepsy surgery, epileptogenic zone, invasive EEG, Stereo EEG, high-frequency oscillations, infra-slow activity, epileptogenicity index},
abstract = {Objectives
Precise localization of the epileptogenic zone (EZ) is a crucial step prior to any planned surgical intervention for medication-refractory epilepsy. However, in reality, a single gold standard biomarker of the EZ does not exist, emphasizing the need for novel biomarkers. The objectives of this chapter are to discuss some of these novel biomarkers utilized to guide epilepsy surgery and discuss their utility and evidence.
Methods
We discuss available evidence to support the use of some of the novel biomarkers of epileptogenicity and compare these with the use of more traditional biomarkers.
Results
There are two main types of developments in the biomarker for epileptic activity. Studies that expand the conventional frequency spectrum of cortical activity such as high-frequency oscillations or those that use computational analysis to assess epileptogenic networks such as the epileptogenicity index. Clinical evidence in most of the new biomarkers is limited to retrospective data analysis and most novel biomarkers require clinical trials prior to incorporating them into day-to-day pre surgical evaluation.
Conclusion
A better understating of the epileptogenic networks as a whole, rather than conceptualizing the EZ as a single focus, will lead to improved surgical outcomes. Currently no single biomarker can be considered a gold standard in outlining the epileptogenic network. Therefore, a combination of complementary investigative methods currently is the best approach to decide on brain areas that need to be removed for seizure free outcomes.}
}
@article{ZHANG20247,
title = {Large language model in electrocatalysis},
journal = {Chinese Journal of Catalysis},
volume = {59},
pages = {7-14},
year = {2024},
issn = {1872-2067},
doi = {https://doi.org/10.1016/S1872-2067(23)64612-1},
url = {https://www.sciencedirect.com/science/article/pii/S1872206723646121},
author = {Chengyi Zhang and Xingyu Wang and Ziyun Wang},
keywords = {Large language model, Electrocatalysis, Artificial intelligence, Multimodal large language model},
abstract = {ABSTRACT
Large language models have recently brought a massive storm on modern society in all fields. While many view them as mere search engines for specific answers or text refinement tools like a chatbot, their broader applications remain largely unexplored. These large language models, consisting of billions of interconnected neurons, derived from all knowledge of the human, possess the remarkable ability to engage in smooth and precise conversations with individuals across the globe. Human-like intelligence enables them to address modern challenges and display immense potential in various scientific domains. In this perspective, we delve into the potential applications of modern large language model and its future iterations within the field of catalysis, aiming to shed light on how these AI-driven models can contribute to a deeper understanding of catalysis science and the intelligent design of catalysts.}
}
@incollection{MORA2019215,
title = {Chapter 7 - The social shaping of smart cities},
editor = {Luca Mora and Mark Deakin},
booktitle = {Untangling Smart Cities},
publisher = {Elsevier},
pages = {215-234},
year = {2019},
isbn = {978-0-12-815477-9},
doi = {https://doi.org/10.1016/B978-0-12-815477-9.00007-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128154779000074},
author = {Luca Mora and Mark Deakin},
keywords = {Interdisciplinarity, Expectations, Social shaping, Open innovation, Technological advancement, Urban innovations, Technological change, Hype, Smart city, Smart city research, Lessons, Recommendations, Open community, Co-design, Collaboration, Collaborative environment, Quadruple helix, Triple helix, Organization dynamics, Urban innovation, Urban sustainability, Sustainable urban development, Hyped behaviors, Urban utopia, Expectations, Smart city development, Smart urbanism},
abstract = {This last chapter concludes the investigation by summing up the key lessons and recommendations that this book can offer to the community of stakeholders involved in smart city research, policy, and practice. The series of complementary analyses that the previous chapters report on demonstrate that, when untangled from the technocentric urban utopia pictured by the corporate sector, smart cities have the potential to develop into innovation systems that set the stage for a technology-enabled approach to urban sustainability. But realizing this opportunity requires to move beyond traditional boundaries, separate the hype from reality, and strengthen the focus on the social shaping of smart cities. The investigation demonstrates that, in order for such a social shaping to develop, the design of smart cities needs to be understood as a collective action in which two complementary forces are combined. On the one hand, the faith in the technological advancement exposed in the utopian thinking. On the other, the knowledge, skills, and interests of a quadruple-helix collaborative environment where the need for technological innovation in response to urban sustainability goals is not shaped by the corporate sector and its technocentric and market-oriented logic, but an open community whose actions serve the public interest and are based on a holistic interpretation of smart city development.}
}
@article{ISMAILOVA2020291,
title = {Hereditary information processes with semantic modeling structures},
journal = {Procedia Computer Science},
volume = {169},
pages = {291-296},
year = {2020},
note = {Postproceedings of the 10th Annual International Conference on Biologically Inspired Cognitive Architectures, BICA 2019 (Tenth Annual Meeting of the BICA Society), held August 15-19, 2019 in Seattle, Washington, USA},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.02.181},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920303045},
author = {Larisa Ismailova and Viacheslav Wolfengagen and Sergey Kosikov},
keywords = {semantic information processing, computational model, variable domains},
abstract = {In practice, when developing an information model, inheritance and composition mechanisms are used, which allows the model developer to extend the properties of the class. In this paper, we establish and use the difference between these two closely related representations when applied in aspect-oriented modeling. In particular, when an aspect is applied to extend the base class of the original model, the designer must choose to use composition. Depending on the composition order, indexing occurs, which can lead to the expansion of the base class by dynamic effects. With a different compositional order, a class narrowing occurs, since it becomes necessary to take into account an additional property. If you intend to define an alternative to a base class with advanced functionality, then inheritance should be used. The work demonstrates the power of the combined use of inheritance and composition, which allows us to develop an aspect-oriented modeling of a family of property transformations, in which a line of intermediate models of the working information process arises.}
}
@article{BRIAS2016151,
title = {Computing the reliability kernel of a time-variant system: Application to a corroded beam},
journal = {IFAC-PapersOnLine},
volume = {49},
number = {12},
pages = {151-155},
year = {2016},
note = {8th IFAC Conference on Manufacturing Modelling, Management and Control MIM 2016},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2016.07.566},
url = {https://www.sciencedirect.com/science/article/pii/S2405896316308242},
author = {A. Brias and J-D. Mathias and G. Deffuant},
keywords = {Discrete systems, Dynamic systems, Initial states, Reliability analysis, Reliability kernel, System failures, Transition matrix, Reliable design},
abstract = {Time-variant reliability analysis aims at assessing the probability of failure of a time-variant system within a given time horizon. We illustrate in this paper the computation of the reliability kernel which is the set of initial states for which the probability of failure remains under a threshold within the considered time horizon. This paper supposes that the time-variant system is discrete in time and space with given probabilities of transition between space states. We use a recursive relation for computing the cumulative probability of failure of the system, linking the probability of failure at time t with the probability of being at a given state x (for all possible states) at time t — 1. Applying this relation, it is possible to compute the probability of failure at any starting point in the state space and hence to derive the reliability kernel. The computation of this kernel gives informations about the system which can be further helpful in reliable design. The approach is illustrated on an example of a steel beam under corrosion.}
}
@article{OZTOP2022106240,
title = {Analysis of melting of phase change material block inserted to an open cavity},
journal = {International Communications in Heat and Mass Transfer},
volume = {137},
pages = {106240},
year = {2022},
issn = {0735-1933},
doi = {https://doi.org/10.1016/j.icheatmasstransfer.2022.106240},
url = {https://www.sciencedirect.com/science/article/pii/S0735193322003621},
author = {Hakan F. Öztop and Hakan Coşanay and Fatih Selimefendigil and Nidal Abu-Hamdeh},
keywords = {Partially open cavity, PCM, Melting, Computational, Finned heater},
abstract = {A numerical work has been conducted to explore the effects of opening parameters on melting of phase change material (PCM) during natural convection in a partially open enclosure. A finned heater is located on bottom wall while the remaining parts are insulated. Paraffin wax is used as PCM and two-dimensional time dependent analysis is performed by using the finite volume method for the parameters of location of opening and temperature difference. The governing parameters for the study are chosen for the range of Ra = 1.45 × 108 ≤ Ra ≤ Ra = 1.97 × 108, 0.25 ≤ w/H ≤ 0.75 and 0.25 ≤ c/H ≤ 0.75. It is found that both opening ratio and opening length are effective parameter on melting time and these can be used as control parameters for improving the energy efficiency. Also, heat transfer can be controlled by using PCM inserted block and opening parameters. Among different cases of opening ratios and locations of opening, the most favorable configuration is obtained at Ra = 1.97 × 108, w/H = 0.25, c/H = 0.25 while average heat transfer enhancement by about 60% is achieved. At the lowest and highest value of Rayleigh numbers, the most favorable location of the opening is obtained at c/H = 0.25 in order to have the highest reduction amount of phase completion time.}
}
@article{BETTINGER2023105459,
title = {Conceptual foundations of physiological regulation incorporating the free energy principle and self-organized criticality},
journal = {Neuroscience & Biobehavioral Reviews},
volume = {155},
pages = {105459},
year = {2023},
issn = {0149-7634},
doi = {https://doi.org/10.1016/j.neubiorev.2023.105459},
url = {https://www.sciencedirect.com/science/article/pii/S0149763423004281},
author = {Jesse S. Bettinger and Karl J. Friston},
keywords = {Physiological regulation, Homeostasis, Allostasis, Variational systems, Free energy principle, Criticality, Griffiths region, Complex adaptive systems, Dynamic stability, Metastability, Control theory, Neuro-immunology, Computational psychiatry, Resilience},
abstract = {Bettinger, J. S., K. J. Friston. Conceptual Foundations of Physiological Regulation incorporating the Free Energy Principle & Self-Organized Criticality. NEUROSCI BIOBEHAV REV 23(x) 144-XXX, 2022. Since the late nineteen-nineties, the concept of homeostasis has been contextualized within a broader class of "allostatic" dynamics characterized by a wider-berth of causal factors including social, psychological and environmental entailments; the fundamental nature of integrated brain-body dynamics; plus the role of anticipatory, top-down constraints supplied by intrinsic regulatory models. Many of these evidentiary factors are integral in original descriptions of homeostasis; subsequently integrated; and/or cite more-general operating principles of self-organization. As a result, the concept of allostasis may be generalized to a larger category of variational systems in biology, engineering and physics in terms of advances in complex systems, statistical mechanics and dynamics involving heterogenous (hierarchical/heterarchical, modular) systems like brain-networks and the internal milieu. This paper offers a three-part treatment. 1) interpret "allostasis" to emphasize a variational and relational foundation of physiological stability; 2) adapt the role of allostasis as "stability through change" to include a "return to stability" and 3) reframe the model of homeostasis with a conceptual model of criticality that licenses the upgrade to variational dynamics.}
}
@article{CRAGG201463,
title = {Skills underlying mathematics: The role of executive function in the development of mathematics proficiency},
journal = {Trends in Neuroscience and Education},
volume = {3},
number = {2},
pages = {63-68},
year = {2014},
issn = {2211-9493},
doi = {https://doi.org/10.1016/j.tine.2013.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S2211949313000422},
author = {Lucy Cragg and Camilla Gilmore},
keywords = {Mathematics, Executive function, Working memory, Development},
abstract = {The successful learning and performance of mathematics relies on a range of individual, social and educational factors. Recent research suggests that executive function skills, which include monitoring and manipulating information in mind (working memory), suppressing distracting information and unwanted responses (inhibition) and flexible thinking (shifting), play a critical role in the development of mathematics proficiency. This paper reviews the literature to assess concurrent relationships between mathematics and executive function skills, the role of executive function skills in the performance of mathematical calculations, and how executive function skills support the acquisition of new mathematics knowledge. In doing so, we highlight key theoretical issues within the field and identify future avenues for research.}
}
@article{MALOMO2024106108,
title = {Discontinuum models for the structural and seismic assessment of unreinforced masonry structures: a critical appraisal},
journal = {Structures},
volume = {62},
pages = {106108},
year = {2024},
issn = {2352-0124},
doi = {https://doi.org/10.1016/j.istruc.2024.106108},
url = {https://www.sciencedirect.com/science/article/pii/S2352012424002601},
author = {D. Malomo and B. Pulatsu},
keywords = {Unreinforced masonry, Structural analysis, Seismic analysis, Discontinuum analysis, Discrete Element Methods, DEM, AEM, NSCD, Computational modelling, Numerical modelling},
abstract = {In the last few decades, discontinuum (or discrete, discontinuous) numerical modelling strategies – i.e. those capable of representing the motion of multiple, intersecting discontinuities explicitly – have become increasingly popular for the structural and seismic assessment of unreinforced masonry (URM) structures. The automatic recognition of new contact points and prediction of large deformations up to complete separation are unique features of discontinuum-based models, making them particularly suitable for unit-by-unit simulations. The adaptation of discrete computational models, primarily used for analyzing rock mechanics and geomechanics problems, to the conservation, structural and earthquake engineering evaluation of URM assemblies is still ongoing, and recent advances in computer-aided technologies are accelerating significantly their adoption. Researchers have now developed fracture energy-based contact models tailored to unreinforced masonry mechanics, explored discontinuum analysis from the mortar joint- to the 3D building-level, combined discrete modelling strategies with analytical or continuum approaches, integrated the latest structural health monitoring and image-based developments into discontinuum-based analysis framework. Concurrently, new and still unsolved issues have also arisen, including the selection of appropriate damping schemes, degree of idealization and discretization strategies, identification of appropriate lab or onsite tests to infer meaningful equivalent mechanical input parameters. This paper offers to the research and industry communities an updated critical appraisal and practical guidelines on the use of discontinuum-based structural and seismic assessment strategies for URM structures, providing opportunities to uncover future key research paths. First, masonry mechanics and discontinuum-based idealization options are discussed by considering micro-, meso- and macro-scale modelling strategies. Pragmatic suggestions are provided to select appropriate input parameters essential to model masonry composite and its constituents at different scales. Then, discontinuum approaches are classified based on their formulation, focusing on the Distinct Element Method (DEM), Applied Element Method (AEM) and Non-Smooth Contact Dynamics (NSCD), and an overview of primary differences, capabilities, pros and cons are thoroughly discussed. Finally, previous discontinuum-based analyses of URM small-scale specimens, isolated planar or curved components, assemblies or complex structures are critically reviewed and compared in terms of adopted strategies and relevant outcomes. This paper presents to new and experienced analysts an in-depth summary of what modern discontinuum-based tools can provide to the structural and earthquake engineering fields, practical guidelines on implementing robust and meaningful modelling strategies at various scales, and potential future research directions.}
}
@article{LEE2024101413,
title = {Cognitive flexibility training for impact in real-world settings},
journal = {Current Opinion in Behavioral Sciences},
volume = {59},
pages = {101413},
year = {2024},
issn = {2352-1546},
doi = {https://doi.org/10.1016/j.cobeha.2024.101413},
url = {https://www.sciencedirect.com/science/article/pii/S2352154624000640},
author = {Liz Y Lee and Máiréad P Healy and Nastassja L Fischer and Ke Tong and Annabel SH Chen and Barbara J Sahakian and Zoe Kourtzi},
abstract = {Interacting with complex and dynamic environments challenges the brain’s ability to adapt to change. This key ability known as cognitive flexibility involves learning the structure of the environment, switching attention between features, dimensions and tasks, and adopting new rules in the face of uncertainty. Training cognitive flexibility has strong potential to improve adaptive behavior across the lifespan with impact in real-world settings (e.g. educational, clinical). Here, we review evidence on the role of cognitive training in improving executive functions and the factors that may enhance the effectiveness of training programs. We propose that personalized and adaptive training programs that focus on the multifaceted abilities comprising cognitive flexibility are key for promoting adaptive behavior and lifelong learning in real-world settings.}
}
@article{MOHAMED2023108104,
title = {A discrete-based multi-scale modeling approach for the propagation of seismic waves in soils},
journal = {Soil Dynamics and Earthquake Engineering},
volume = {173},
pages = {108104},
year = {2023},
issn = {0267-7261},
doi = {https://doi.org/10.1016/j.soildyn.2023.108104},
url = {https://www.sciencedirect.com/science/article/pii/S0267726123003494},
author = {Tarek Mohamed and Jérôme Duriez and Guillaume Veylon and Laurent Peyras and Patrick Soulat},
keywords = {Multi-scale, DEM, Toyoura sand, Seismic waves propagation, Bounding surface plasticity, Inertial effect},
abstract = {A three-dimensional multi-scale discrete–continuum model (Finite Volume Method × Discrete Element Method, FVM × DEM) is developed for a discrete-based description of the mechanical behavior of granular soils in boundary value problems (BVPs). In such a scheme, the constitutive response of the material is derived through direct DEM computations on a representative volume element attached to each mesh element. The developed multi-scale approach includes the inertial effect in the stress homogenization formulation and serves to study the mechanism of propagation of seismic waves, in comparison with a more classical BVP simulation that adopts an advanced bounding surface plasticity model “P2PSand”. We start with a detailed and fair calibration and validation of these two models against laboratory tests for Toyoura sand under monotonic and cyclic loading. Then, the performance of the two approaches is compared for the case of a seismic wave loading passing through a saturated soil column with different relative densities, revealing several differences between the results of the two models.}
}
@article{BOTTI2017481,
title = {Integrating ergonomics and lean manufacturing principles in a hybrid assembly line},
journal = {Computers & Industrial Engineering},
volume = {111},
pages = {481-491},
year = {2017},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2017.05.011},
url = {https://www.sciencedirect.com/science/article/pii/S0360835217302152},
author = {Lucia Botti and Cristina Mora and Alberto Regattieri},
keywords = {Lean manufacturing, Occupational safety, Ergonomics, Automation, Human factors, Hybrid assembly line},
abstract = {Lean manufacturing is a production method that was established in the wake of the Japanese Toyota Production System and rapidly established in the worldwide manufacturing industry. Lean characteristics combine just-in-time practices, work-in-progress and waste reduction, improvement strategies, defect-free production, and standardization. The primary goal of lean thinking is to improve profits and create value by minimizing waste. This study introduces a novel mathematical model to design lean processes in hybrid assembly lines. The aim was to provide an effective, efficient assembly line design tool that meets the lean principles and ergonomic requirements of safe assembly work. Given the production requirements, product characteristics and assembly tasks, the model defines the assembly process for hybrid assembly lines with both manual workers and automated assembly machines. Each assembly line solution ensures an acceptable risk level of repetitive movements, as required by current law. This model helps managers and practitioners to design hybrid assembly lines with both manual workers and automated assembly machines. The model was tested in a case study of an assembly line for hard shell tool cases. Results show that worker ergonomics is a key parameter of the assembly process design, as other lean manufacturing parameters, e.g. takt time, cycle time and work in progress.}
}
@article{WAN2025122214,
title = {Hesitant multiplicative best and worst method for multi-criteria group decision making},
journal = {Information Sciences},
volume = {715},
pages = {122214},
year = {2025},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2025.122214},
url = {https://www.sciencedirect.com/science/article/pii/S0020025525003469},
author = {Shu-Ping Wan and Xi-Nuo Chen and Jiu-Ying Dong and Yu Gao},
keywords = {Hesitant multiplicative preference relation, Multi-criteria decision-making, Best and worst method, Analytic hierarchy process},
abstract = {Best-worst method (BWM) has been extended in various uncertain scenarios owing to fewer comparisons and better reliability. This article utilizes hesitant multiplicative (HM) sets (HMSs) to express reference comparisons (RCs) and develops a novel HM BWM (HMBWM). We first define the multiplicative consistency for HM preference relation (HMPR). A fast and effective approach is designed to derive the priority weights (PWs) from an HMPR. To extend BW into HMPR, the score value of each criterion is computed to identify the best and worst criteria. Then, the PWs are acquired through constructing a 0–1 mixed goal programming model based on the HM RCs (HMRCs). The consistency ratio is given to judge the multiplicative consistency of HMRCs. An approach is proposed to enhance the consistency when the HMRCs are unacceptably consistent. Thereby, a novel HMBWM is proposed. On basis of HMBWM, this article further develops a novel method for group decision making (GDM) with HMPRs. The decision makers’ weights are determined by consistency ratio and the group PWs of alternatives are obtained by minimum relative entropy model. Four examples show that HMBWM possesses higher consistency and the proposed GDM method has stronger distinguishing ability, less computation workload and fewer modifications of elements.}
}
@article{LI2023297,
title = {BalanceHRNet: An effective network for bottom-up human pose estimation},
journal = {Neural Networks},
volume = {161},
pages = {297-305},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.01.036},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023000485},
author = {Yaoping Li and Shuangcheng Jia and Qian Li},
keywords = {Multi-branch structure, Fusion, Balance structure, Branch attention},
abstract = {In the study of human pose estimation, which is widely used in safety and sports scenes, the performance of deep learning methods is greatly reduced in high overlap rate and crowded scenes. Therefore, we propose a bottom-up model, called BalanceHRNet, which is based on balanced high-resolution module and a new branch attention module. BalanceHRNet draws on the multi-branch structure and fusion method of a popular model HigherHRNet. And our model overcomes the shortcoming of HigherHRNet that cannot obtain a large enough receptive field. Specifically, through the connecting structure in balanced high-resolution module, we can connect almost all convolutional layers and obtain a sufficiently large receptive field. At the same time, the multi-resolution representation can be maintained due to the use of balanced high-resolution module, which enable our model to recognize objects with richer scales and obtain more complex semantics information. And for branch fusion method, we design branch attention to obtain the importance of different branches at different stages. Finally, our model improves the accuracy while ensuring a smaller amount of computation than HigherHRNet. The CrowdPose dataset is used as test dataset, and HigherHRNet, AlphaPose, OpenPose and so on are taken as comparison models. The AP measured by BalanceHRNet is 63.0%, increased by 3.1% compared to best model — HigherHRNet. We also demonstrate the effectiveness of our network through the COCO(2017) keypoint detection dataset. Compared with HigherHRNet-w32, the AP of our model is improved by 1.6%.}
}
@article{CERONGARCIA20221,
title = {Jigsaw cooperative learning of multistage counter-current liquid-liquid extraction using Mathcad®},
journal = {Education for Chemical Engineers},
volume = {38},
pages = {1-13},
year = {2022},
issn = {1749-7728},
doi = {https://doi.org/10.1016/j.ece.2021.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S1749772821000488},
author = {Mª Carmen Cerón-García and Lorenzo López-Rosales and Juan José Gallardo-Rodríguez and Elvira Navarro-López and Asterio Sánchez-Mirón and Francisco García-Camacho},
keywords = {Liquid-liquid extraction, Mass transfer, Jigsaw cooperative learning, Computational tools, Mathcad®},
abstract = {This work shows the improvement in comprehending counter-current liquid-liquid extraction by applying jigsaw-type cooperative learning and the engineering math software Mathcad® in a chemical engineering course, part of the Chemistry degree. This study was performed on two different groups at the University of Almería (Spain) over three academic years. The students were divided into two groups: one half of the class followed a non-cooperative learning methodology (the control) while the other half were spread among the jigsaw cooperative groups following the methodology known as “Jigsaw Experts Groups”. A main template made with Mathcad® of a multistage counter-current liquid-liquid extraction was supplied to both the jigsaw and non-jigsaw groups. The assessment of this educational experience in the course revealed that the jigsaw group outperformed the control group. The use of Mathcad® proved to be very intuitive and effective in explaining these relatively complex problems and utilising it is highly recommended; we suggest it is used rather than classical and less intuitive graphical methods.}
}
@article{AMARAL20211,
title = {Overlapping but distinct: Distal connectivity dissociates hand and tool processing networks},
journal = {Cortex},
volume = {140},
pages = {1-13},
year = {2021},
issn = {0010-9452},
doi = {https://doi.org/10.1016/j.cortex.2021.03.011},
url = {https://www.sciencedirect.com/science/article/pii/S0010945221001088},
author = {Lénia Amaral and Fredrik Bergström and Jorge Almeida},
keywords = {Tools, Hands, Distal connectivity, Representation, Functional organization, fMRI},
abstract = {The processes and organizational principles of information involved in object recognition have been a subject of intense debate. These research efforts led to the understanding that local computations and feedforward/feedback connections are essential to our representations and their organization. Recent data, however, has demonstrated that distal computations also play a role in how information is locally processed. Here we focus on how long-range connectivity and local functional organization of information are related, by exploring regions that show overlapping category-preferences for two categories and testing whether their connections are related with distal representations in a category-specific way. We used an approach that relates functional connectivity with distal areas to local voxel-wise category-preferences. Specifically, we focused on two areas that show an overlap in category-preferences for tools and hands–the inferior parietal lobule/anterior intraparietal sulcus (IPL/aIPS) and the posterior middle temporal gyrus/lateral occipital temporal cortex (pMTG/LOTC) – and how connectivity from these two areas relate to voxel-wise category-preferences in two ventral temporal regions dedicated to the processing of tools and hands separately–the left medial fusiform gyrus and the fusiform body area respectively–as well as across the brain. We show that the functional connections of the two overlap areas correlate with categorical preferences for each category independently. These results show that regions that process both tools and hands maintain object topography in a category-specific way. This potentially allows for a category-specific flow of information that is pertinent to computing object representations.}
}
@article{TSAI20251293,
title = {VR Games for Teaching Lean Manufacturing Tools: A Case Study of Stool Manufacturing},
journal = {Procedia Computer Science},
volume = {253},
pages = {1293-1302},
year = {2025},
note = {6th International Conference on Industry 4.0 and Smart Manufacturing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2025.01.191},
url = {https://www.sciencedirect.com/science/article/pii/S1877050925001991},
author = {Tim Tsai and Zaneta Sarah Widjaja and Md Rakibul Hasan and Dhrumil Pithwa and Purna Sai {Teja Pinninti} and Rafiq Ahmad},
keywords = {Lean Manufacturing, Virtual Reality, Lean Tools, VR Games},
abstract = {This study investigates the efficacy of Virtual Reality (VR) in enhancing lean manufacturing training. By integrating VR with lean manufacturing principles, the aim is to compare performance and learning outcomes in traditional and lean scenarios. The research highlights the limitations of conventional training approaches in fully engaging learners and keeping pace with rapid technological advancements in manufacturing processes. Through the development of an interactive VR game focused on a stool manufacturing process, the study advances the use of PDCA framework to incorporate key lean manufacturing tools such as 5S Principles, Kanban, Poka-Yoke, and ergonomic improvements. The game development process is detailed, covering the preparation of 3D models, set up of virtual scenes, development of game function, design of user interface, and deployment. User testing reveals significant improvements in process efficiency and knowledge acquisition when employing lean-inspired scenarios within the VR environment. The study concludes with promising results, demonstrating the potential of VR in lean manufacturing training while also acknowledging the need for further research to validate these findings across a wider range of manufacturing processes.}
}
@article{HANOCH20021,
title = {“Neither an angel nor an ant”: Emotion as an aid to bounded rationality},
journal = {Journal of Economic Psychology},
volume = {23},
number = {1},
pages = {1-25},
year = {2002},
issn = {0167-4870},
doi = {https://doi.org/10.1016/S0167-4870(01)00065-4},
url = {https://www.sciencedirect.com/science/article/pii/S0167487001000654},
author = {Yaniv Hanoch},
keywords = {Bounded rationality, Emotion},
abstract = {The role of emotion as a source of bounded rationality has been largely ignored. Following Herbert Simon, economists as well as psychologists have mainly focused on cognitive constraints while neglecting to integrate the growing body of research on emotion which indicates that reason and emotion are interconnected. Accordingly, the present paper aims to bridge the existing gap. By establishing a link between the two domains of research, emotion and bounded rationality, it will be suggested that emotions work together with rational thinking in two distinct ways, and thereby function as an additional source of bounded rationality. The aim, therefore, is not to offer an alternative to bounded rationality; rather, the purpose is to elaborate and supplement themes emerging out of bounded rationality.}
}
@article{ANDRAS2021110734,
title = {Where do successful populations originate from?},
journal = {Journal of Theoretical Biology},
volume = {524},
pages = {110734},
year = {2021},
issn = {0022-5193},
doi = {https://doi.org/10.1016/j.jtbi.2021.110734},
url = {https://www.sciencedirect.com/science/article/pii/S0022519321001569},
author = {Peter Andras and Adam Stanton},
keywords = {Computational modelling, Socio-technical evolution, Socio-biological simulation, Human geography, Geography of speciation},
abstract = {In order to understand the dynamics of emergence and spreading of socio-technical innovations and population moves it is important to determine the place of origin of these populations. Here we focus on the role of geographical factors, such as land fertility and mountains in the context of human population evolution and distribution dynamics. We use a constrained diffusion-based computational model, computer simulations and the analysis of geographical and land-quality data. Our analysis shows that successful human populations, i.e. those which become dominant in their socio – geographical environment, originate from lands of many valleys with relatively low land fertility, which are close to areas of high land fertility. Many of the homelands predicted by our analysis match the assumed homelands of known successful populations (e.g. Bantus, Turkic, Maya). We also predict other likely homelands as well, where further archaeological, linguistic or genetic exploration may confirm the place of origin for populations with no currently identified urheimat. Our work is significant because it advances the understanding of human population dynamics by guiding the identification of the origin locations of successful populations.}
}
@article{TRELEAVEN198859,
title = {Parallel architecture overview},
journal = {Parallel Computing},
volume = {8},
number = {1},
pages = {59-70},
year = {1988},
note = {Proceedings of the International Conference on Vector and Parallel Processors in Computational Science III},
issn = {0167-8191},
doi = {https://doi.org/10.1016/0167-8191(88)90109-3},
url = {https://www.sciencedirect.com/science/article/pii/0167819188901093},
author = {Philip C. Treleaven},
keywords = {Computer architecture, parallel computing},
abstract = {An increasing number of parallel computer products are appearing in the market place. Their design motivations and market areas cover a broad spectrum: (i) Transaction Processing Systems, such as Parallel UNIX systems (e.g. SEQUENT Balance), for data processing applications; (ii) Numeric Supercomputers, such as Hypercube systems (e.g. INTEL iPSC), for scientific and engineering applications; (iii) VLSI Architectures, such as parallel microcomputers (e.g. INMOS Transputer), for exploiting very large scales of integration; (iv) High-Level Language Computers, such as Logic machines (e.g. FUJITSU Kabu-Wake), for symbolic computation; and (v) Neurocomputers, such as Connectionist computers (e.g. THINKING MACHINES Connection Machine), for general-purpose pattern matching applications. This survey paper gives an overview of these novel parallel computers and discusses the likely commercial impact of parallel computers.}
}
@article{BARTELL20051283,
title = {How hydrides misled chemists},
journal = {Spectrochimica Acta Part A: Molecular and Biomolecular Spectroscopy},
volume = {61},
number = {7},
pages = {1283-1286},
year = {2005},
note = {Honour Issue - Jim Durig},
issn = {1386-1425},
doi = {https://doi.org/10.1016/j.saa.2004.11.056},
url = {https://www.sciencedirect.com/science/article/pii/S1386142504006481},
author = {Lawrence S. Bartell},
keywords = {Models of molecular structure and force fields, Simple hydrides, Ligand-close-packing, Cautionary stories},
abstract = {Hydrogen-containing molecules are simple enough to be attractive subjects in experimental diffraction and spectroscopic studies and in quantum computations. Yet, the inferences about molecular structure and force fields originally drawn from studies of these subjects were significantly flawed. In recent developments the original models of structure invoked, such as hybridization, have been superseded. The reasons for this are briefly reviewed. What has emerged to account for molecular geometry, prevailing even over the popular VSEPR theory, is a model of geminal nonbonded interactions.}
}
@article{HERSHCOVICH2021107809,
title = {Thermal performance of sculptured tiles for building envelopes},
journal = {Building and Environment},
volume = {197},
pages = {107809},
year = {2021},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2021.107809},
url = {https://www.sciencedirect.com/science/article/pii/S036013232100216X},
author = {Cheli Hershcovich and René {van Hout} and Vladislav Rinsky and Michael Laufer and Yasha j. Grobman},
keywords = {Microclimate, Computational fluid dynamics simulations, Particle Image Velocimetry (PIV) experiments, Building envelope, Thermal insulation},
abstract = {Since effective thermal insulation of buildings could significantly decrease energy consumption worldwide, this study examined the potential of concrete tiles with complex geometries at improving thermal performance in building envelopes. The study focused on air flow characteristics that occur near the external surface of the tile, and their influence on the rate of heat transfer within the material. Six groups of sculptured tiles were developed, using a systematic approach (i.e., repetition of basic geometries), biomimicry, and inspiration from natural envelopes. Air flow behavior and heat transfer rates were examined at three different wind speeds, through both experiments and computational fluid dynamics simulations using a configuration of flow impingement that can be regarded as the worst case scenario. After successfully validating the data, additional numerical simulations were conducted for all developed tiles using the Star-CCM + commercial software. The results showed an improvement in the insulation performance of about half of all the tested cases. Moreover, significant improvements were seen in the geometries that mimicked animal fur, achieving heat transfer rates that were up to 24% lower than those achieved by smooth tiles. Our results indicate that the application of such tiles with increased thermal resistance could save on thermal insulation materials and improve the thermal performance of building façades.}
}
@article{ASAI1991323,
title = {Discipline Pascal with descriptive environment; precise writing to learn programming and to avoid errors},
journal = {Computers & Education},
volume = {16},
number = {4},
pages = {323-335},
year = {1991},
issn = {0360-1315},
doi = {https://doi.org/10.1016/0360-1315(91)90006-D},
url = {https://www.sciencedirect.com/science/article/pii/036013159190006D},
author = {Hitohisa Asai},
abstract = {Dijkstra's article “On the cruelty of really teaching computing science” [l] has encouraged me to review my thoughts and experience. In the teaching environments of an introductory programming course (CS1, CS2 and others), I have discovered a mental gap in the minds of some students, which has often led to difficulties in class. A Pascal statement contains highly condensed information. In order to deal with this type of information, the students must apply a certain thinking level. In a Pascal statement, much information is concealed behind the written words. For example, consider a variable in a program. The data type and locality of the variable are not explicitly expressed in a statement. In other words, this condensed information is hiding, e.g. invisible. Hence a degree of the student's ability to focus on it may fade away in his mind. If programming code demands writing precise information in a statement then the student's difficulty may be alleviated because he can see it. This would be an attempt to narrow the gap by writing Discipline Pascal code closer to the precise thinking level. I believe that this proposal would also bring benefits to experienced programmers.}
}
@article{SINCLAIR2004169,
title = {Improving computer-assisted instruction in teaching higher-order skills},
journal = {Computers & Education},
volume = {42},
number = {2},
pages = {169-180},
year = {2004},
issn = {0360-1315},
doi = {https://doi.org/10.1016/S0360-1315(03)00070-8},
url = {https://www.sciencedirect.com/science/article/pii/S0360131503000708},
author = {Kelsey J Sinclair and Carl E Renshaw and Holly A Taylor},
keywords = {Evaluation methodologies, Multimedia/hypermedia systems, Secondary education, Applications in subject areas},
abstract = {Computer-assisted instruction (CAI) has been shown to enhance rote memory skills and improve higher order critical thinking skills. The challenge now is to identify what aspects of CAI improve which specific higher-order skills. This study focuses on the effectiveness of using CAI to teach logarithmic graphing and dimensional analysis. Two groups of ninth graders participated in a one-class period laboratory. Experiment 1 compared a fully automated computer laboratory to an equivalent paper-and-pencil exercise. Experiment 2 compared the same automated computer laboratory in Experiment 1 with a revised, less automated computer version. Both the paper-and-pencil exercise and the less automated computer exercise required students to perform basic mathematical calculations. The results from a post-test revealed that very few students were able to master the complex task of dimensional analysis, but students who took the paper-based and revised, less automated version scored higher overall. These results imply that students required to perform basic calculations had a better understanding of the lab as a whole. These results suggest that until students master basic skills, they do not have the cognitive resources to concentrate on higher-order concepts. This is supported by cognitive load theory.}
}
@article{DRANKO2021738,
title = {Structural Analysis of Large-Scale Socio-Technical Systems Based on the Concept of Influence},
journal = {IFAC-PapersOnLine},
volume = {54},
number = {13},
pages = {738-743},
year = {2021},
note = {20th IFAC Conference on Technology, Culture, and International Stability TECIS 2021},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2021.10.540},
url = {https://www.sciencedirect.com/science/article/pii/S2405896321019789},
author = {O.I. Dranko and Yu.G. Rykov and A.A. Karandeev},
keywords = {Complex systems, Cognitive maps, Influence, Structural analysis, Russian Federation economics structure},
abstract = {The paper is devoted to studying the stability of complex systems, which include diversified and heterogeneous elements. In order to analyze the stability of such systems, it is proposed to develop the process of dynamic determination of the main factors. For this purpose, this paper uses a variant of cognitive modeling based on the concept of “influence”. The proposed approach implies, in a sense, a broader view of the concept of the “fuzzy cognitive map” introduced by B. Kosko. “Influence” does not mean “causality”, allows a broader interpretation range, and is calculated in a special way that makes it possible to prove rigorous theorems. A complex system is represented as a digraph, and the selection of the main factors is proposed to be based on the concept of “influence”, which is introduced as follows. All nodes of the graph are assigned an abstract property “significance”, which allows you to compare heterogeneous factors, and a particular computational procedure is introduced that allows this “significance” to be calculated. The “influence” of factors on each other is defined as a mathematically formalized response in accordance with the introduced computational procedure of the “significance” of the studied factor to the variation in the “significance” of input factors. For example, the analysis of the sustainability of a large-scale model of the Russian economy in terms of the strength of the “influence” is provided.}
}
@article{VIJAYALAKSHMI20222382,
title = {Topological indices relating some nanostructures},
journal = {Materials Today: Proceedings},
volume = {68},
pages = {2382-2386},
year = {2022},
note = {4th International Conference on Advances in Mechanical Engineering},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2022.09.106},
url = {https://www.sciencedirect.com/science/article/pii/S2214785322059144},
author = {K. {Vijaya Lakshmi} and N. Parvathi},
keywords = {Topological indices, Chemical graph theory, Molecular structure, Nanotechnology, Nanotubes, Nanosheet},
abstract = {The application of mathematics mainly graph theory has a significant role in the development of chemistry. Topological indices act as a bridge between mathematics and chemistry. The topological index is nothing but a numerical quantity that has yielded valuable insights in terms of molecular structure. In nanotechnology, the structural characterization of the molecular species and the nanoparticles are indicated by the indices. These indices identify the symmetry of molecular structures and provide them a scientific terminology to predict qualities like boiling temperatures, viscosity, and gyrating radius. The main objective of the current paper is to compute topological indices with the corresponding formulae, moreover, these indices help in predicting physicochemical properties without the involvement of the laboratory. The current research paper investigated the computation of degree-based topological indices for the nanotubes HAC5C6C7[a,b] and TU(C4C6C8[a,b].}
}
@article{KHODADADI2022104354,
title = {Design exploration by using a genetic algorithm and the Theory of Inventive Problem Solving (TRIZ)},
journal = {Automation in Construction},
volume = {141},
pages = {104354},
year = {2022},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2022.104354},
url = {https://www.sciencedirect.com/science/article/pii/S0926580522002278},
author = {Anahita Khodadadi and Peter {von Buelow}},
keywords = {Conceptual design, Design exploration, TRIZ, Genetic algorithm, Multi-objective design},
abstract = {This paper presents a computational design exploration method called GA+TRIZ, which aids designers in defining the design problem clearly, making a parametric model where pertinent variables are included, obtaining a series of suitable solutions, and resolving existing conflicts among design objectives. The goal is to include the designer's qualitative and performance-based quantitative design goals in the design process, while promoting innovative ideas for resolving contradictory design objectives. The method employed is a Genetic Algorithm (GA), earlier implemented in an automated design exploration process called ParaGen, in combination with the Theory of Inventive Problem Solving (TRIZ), a novel methodology to assist architects and structural engineers in the conceptual phase of design. The GA+TRIZ method promotes automated design exploration, investigation of unexpected solutions, and continuous interaction with the computational generating system. Finally, this paper presents two examples that illustrate how the GA+TRIZ method assists designers in problem structuring, design exploration, and decision-making.}
}
@article{ROY2022105849,
title = {EEG based stress analysis using rhythm specific spectral feature for video game play},
journal = {Computers in Biology and Medicine},
volume = {148},
pages = {105849},
year = {2022},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2022.105849},
url = {https://www.sciencedirect.com/science/article/pii/S0010482522006023},
author = {Shidhartho Roy and Monira Islam and Md. Salah Uddin Yusuf and Nushrat Jahan},
keywords = {Beta–Alpha ratio, EEG, Stress-relaxation modeling, Topography, Video gameplay},
abstract = {Background and objective:
For the emerging significance of mental stress, various research directives have been established over time to understand better the causes of stress and how to deal with it. In recent years, the rise of video gameplay has been unprecedented, further triggered by the lockdown imposed due to the COVID-19 pandemic. Several researchers and organizations have contributed to the practical analysis of the impacts of such extended periods of gameplay, which lacks coordinated studies to underline the outcomes and reflect those in future game designing and public awareness about video gameplay. Investigations have mainly focused on the “gameplay stress” based on physical syndromes. Some studies have analyzed the effects of video gameplay with Electroencephalogram (EEG), Magnetic resonance imaging (MRI), etc., without concentrating on the relaxation procedure after video gameplay.
Methods:
This paper presents an end-to-end stress analysis for video gaming stimuli using EEG. The power spectral density (PSD) of the Alpha and Beta bands is computed to calculate the Beta-to-Alpha ratio (BAR). The Alpha and Beta band power is computed, and the Beta-to-Alpha band power ratio (BAR) has been determined. In this article, BAR is used to denote mental stress. Subjects are chosen based on various factors such as gender, gameplay experience, age, and Body mass index (BMI). EEG is recorded using Scan SynAmps2 Express equipment. There are three types of video gameplay: strategic, puzzle, and combinational. Relaxation is accomplished in this study by using music of various pitches. Two types of regression analysis are done to mathematically model stress and relaxation curve. Brain topography is rendered to indicate the stressed and relaxed region of the brain.
Results:
In the relaxed state, the subjects have BAR 0.701, which is considered the baseline value. Non-gamer subjects have an average BAR of 2.403 for 1 h of strategic video gameplay, whereas gamers have 2.218 BAR concurrently. After 12 minutes of listening to low-pitch music, gamers achieved 0.709 BAR, which is nearly the baseline value. In comparison to Quartic regression, the 4PL symmetrical sigmoid function performs regression analysis with fewer parameters and computational power.
Conclusion:
Non-gamers experience more stress than gamers, whereas strategic games stress the human brain more. During gameplay, the beta band in the frontal region is mostly activated. For relaxation, low pitch music is the most useful medium. Residual stress is evident in the frontal lobe when the subjects have listened to high pitch music. Quartic regression and 4PL symmetrical sigmoid function have been employed to find the model parameters of the relaxation curve. Among them, quartic regression performs better in terms of Akaike information criterion (AIC) and R2 measure.}
}
@article{FANOUS2023105658,
title = {Challenges and prospects of climate change impact assessment on mangrove environments through mathematical models},
journal = {Environmental Modelling & Software},
volume = {162},
pages = {105658},
year = {2023},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2023.105658},
url = {https://www.sciencedirect.com/science/article/pii/S1364815223000440},
author = {Majdi Fanous and Jonathan M. Eden and Renji Remesan and Alireza Daneshkhah},
keywords = {Mangrove environments, Climate change, Hydro-morphodynamic modelling, Adaptation policies, Machine learning, Data-driven modelling},
abstract = {The impacts of climate change, especially sea-level rise, are an increasing threat to the world’s coastal regions. Following recommendations made by the United Nations about the preservation of mangrove environments, particularly given their potential for effective natural defence against wave-driven hazards, a series of experiments have been conducted to quantify the ability of mangroves to counter climate change impacts. To date, these experiments have been limited by computational cost and inability to model multiple scenarios. With improved data quality and availability, machine learning has enormous potential to supplement, or even replace, existing numerical methods. This article presents both an outline of the importance of protecting mangrove environments and a review of methods currently used to quantify the capacity of mangroves to adapt to climate change impacts. In view of the limitations of existing numerical methods, the article also discusses the potential of machine learning as an efficient and effective alternative.}
}
@article{SURESHBABU2006277,
title = {Modeling and simulation in signal transduction pathways: a systems biology approach},
journal = {Biochimie},
volume = {88},
number = {3},
pages = {277-283},
year = {2006},
issn = {0300-9084},
doi = {https://doi.org/10.1016/j.biochi.2005.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S0300908405001999},
author = {C.V. {Suresh Babu} and Eun {Joo Song} and Young Sook Yoo},
keywords = {Biological systems, Signal transduction, Systems biology, Modeling and simulation},
abstract = {Modeling, the heart of systems biology, of complex processes (example: signal transduction) is a wide scientific discipline where many approaches from different areas are confronted with the aim of better understanding, identifying and modeling of complex data coming from various sources. The purpose of this paper is to introduce the basic steps of systems biology view towards signaling pathways, which mainly deals with the computational tools. The paper emphasizes the modeling and simulation approach in the signal transduction pathways using the topologies of the biochemical reactions with an overview of the different types of software platforms. Finally, we demonstrated the epidermal growth factor receptor signaling pathway model as an example to study the growth factor mediated signaling system with biological experiments. This paper will enables new comers to underline the strengths of the computational approaches towards signal transduction, as well as to highlight the systems biology research directions.}
}
@article{WU2024100295,
title = {Analyzing K-12 AI education: A large language model study of classroom instruction on learning theories, pedagogy, tools, and AI literacy},
journal = {Computers and Education: Artificial Intelligence},
volume = {7},
pages = {100295},
year = {2024},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2024.100295},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X24000985},
author = {Di Wu and Meng Chen and Xu Chen and Xing Liu},
keywords = {AI education, Large language models, Pedagogical approaches, AI literacy},
abstract = {There is growing recognition among researchers and stakeholders about the significant impact of artificial intelligence (AI) technology on classroom instruction. As a crucial element in developing AI literacy, AI education in K-12 schools is increasingly gaining attention. However, most existing research on K-12 AI education relies on experiential methodologies and suffers from a lack of quantitative analysis based on extensive classroom data, hindering a comprehensive depiction of AI education's current state at these educational levels. To address this gap, this article employs the advanced semantic understanding capabilities of large language models (LLMs) to create an intelligent analysis framework that identifies learning theories, pedagogical approaches, learning tools, and levels of AI literacy in AI classroom instruction. Compared with the results of manual analysis, analysis based on LLMs can achieve more than 90% consistency. Our findings, based on the analysis of 98 classroom instruction videos in central Chinese cities, reveal that current AI classroom instruction insufficiently foster AI literacy, with only 35.71% addressing higher-level skills such as evaluating and creating AI. AI ethics are even less commonly addressed, featured in just 5.1% of classroom instruction. We classified AI classroom instruction into three categories: conceptual (50%), heuristic (18.37%), and experimental (31.63%). Correlation analysis suggests a significant relationship between the adoption of pedagogical approaches and the development of advanced AI literacy. Specifically, integrating Project-based/Problem-based learning (PBL) with Collaborative learning appears effective in cultivating the capacity to evaluate and create AI.}
}
@article{YANG2024111470,
title = {A lattice-theoretic model of three-way conflict analysis},
journal = {Knowledge-Based Systems},
volume = {288},
pages = {111470},
year = {2024},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.111470},
url = {https://www.sciencedirect.com/science/article/pii/S0950705124001059},
author = {Han Yang and Yiyu Yao and Keyun Qin},
keywords = {MS.F-bilattice, Three-way decision, Three-way conflict analysis},
abstract = {Pawlak conflict analysis uses a three-valued situation table for representing the ratings of a set of agents on a set of issues. This paper examines a lattice-theoretic basis of three-way conflict analysis. Qualitatively, we adopt a triangle, namely, an MS.F-bilattice, to characterize the structures of agents ratings, which gives an intuitive and effective tool for ordering a single agent and a pair of agents. We consider a strength ordering and a rating ordering to construct MS.F-bilattices. By applying the principles of three-way decision as thinking in threes, we trisect, according to the rating ordering, the nine pairs of ratings into three regions: potential opposition (PO), potential conflict (PC), and potential support (PS) regions. For each region, according to the strength ordering, we construct the weak, medium, and strong three subregions. Quantitatively, we introduce opposition-alliance and support-alliance measures based on the rating ordering for one issue to trisect these pairs of ratings into PO, PC, and PS regions. We study opposition strength, conflict strength, and support strength measures based on strength ordering for one issue to trisect each of the three regions into three subregions. Finally, we extend the five types of measures for a set of issues. The lattice-theoretic model of three-way conflict analysis clarifies the semantics of pairs of ratings by two agents and gives a different perspective on trisection methods in conflict analysis. To demonstrate the value of the proposed methods, we analyze a case study of the development planning of the Gansu Province of China.}
}
@article{LOPEZ2015289,
title = {DAMQT 2.0: A new version of the DAMQT package for the analysis of electron density in molecules},
journal = {Computer Physics Communications},
volume = {192},
pages = {289-294},
year = {2015},
issn = {0010-4655},
doi = {https://doi.org/10.1016/j.cpc.2015.02.027},
url = {https://www.sciencedirect.com/science/article/pii/S0010465515000855},
author = {Rafael López and Jaime Fernández Rico and Guillermo Ramírez and Ignacio Ema and David Zorrilla},
keywords = {Electron density, Electrostatic potential, Electric field, Hellmann–Feynman forces, Density deformations},
abstract = {DAMQT 2.0 is a new version of the DAMQT package for the analysis of electron density in molecules and the fast computation of the density, density deformations, electrostatic potential and field, and Hellmann–Feynman forces. Algorithms for the partition of the electron density and the computation of related properties like density deformations, electrostatic potential and field and Hellmann–Feynman forces have been improved and their codes, fully rewritten. MPI versions of the most computational demanding modules are now included in the package for parallel computation. The Graphical User Interface has been also enhanced, with new features including a 2D plotter and significant improvements in the 3D viewer.
Program summary
Program title: DAMQT 2.0 Catalogue identifier: AEDL_v2_0 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/AEDL_v2_0.html Program obtainable from: CPC Program Library, Queen’s University, Belfast, N. Ireland Licensing provisions: GPLv3 No. of lines in distributed program, including test data, etc.: 317,270 No. of bytes in distributed program, including test data, etc.: 40,193,220 Distribution format: tar.gz Programming language: Fortran90 and C++. Computer: Any. Operating system: Linux, Windows (7, 8). RAM: 200 Mbytes Classification: 16.1. Catalogue identifier of previous version: AEDL_v1_0 Journal reference of previous version: Comput. Phys. Comm. 180(2009)1654 External routines: Qt (4.8 or higher), OpenGL (3.x or higher), freeGLUT 2.8.x Nature of problem: Analysis of the molecular electron density and density deformations, including fast evaluation of electrostatic potential, electric field and Hellmann–Feynman forces on nuclei. Solution method: The method of Deformed Atoms in Molecules, reported elsewhere [1], is used for partitioning the molecular electron density into atomic fragments, which are further expanded in spherical harmonics times radial factors. The partition is used for defining molecular density deformations and for the fast calculation of several properties associated to density. Restrictions: Density must come from an LCAO calculation (any level) with spherical (not Cartesian) Slater or Gaussian functions. Unusual features: The program contains an OPEN statement to binary files (stream) in several files. This statement has not a standard syntax in Fortran 90. Two possibilities are considered in conditional compilation: Intel’s ifort and Fortran2003 standard. The latter is applied to compilers other than ifort (gfortran uses this one, for instance). Additional comments: Quick-start guide and User’s manual in PDF format included in the package. User’s manual is also accessible from the Graphical User Interface. The distribution file for this program is over 40 Mbytes and therefore is not delivered directly when downloaded or Email is requested. Instead an html file giving details of how the program can be obtained is sent. Running time: Largely dependent on the system size and the module run (from fractions of a second to hours). References:[1]J. Fernández Rico, R. López, I. Ema and G. Ramírez, J. Mol. Struct. Theochem 727 (2005) 115.}
}
@article{LI2024933,
title = {A Novel Quantifiable Weight Model for the Digital Twin Technology in Enterprise},
journal = {Procedia Computer Science},
volume = {247},
pages = {933-942},
year = {2024},
note = {The 11th International Conference on Applications and Techniques in Cyber Intelligence},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.10.113},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924029132},
author = {Yanchao Li},
keywords = {Quantifiable weight model, digital twin technology, main core},
abstract = {The performance evaluation involves a variety of composite factors, and most factors are subjectively determined by the people, so the fuzziness of this evaluation is inevitable. This paper firstly focuses on this issue based on the dynamic influence of digital twin technology input for different kinds of digital twin technology activities. Then this paper carries out a detailed analysis and research on the main core and criterion of digital twin technology. Thirdly this paper puts forward the following suggestions so as to give full play to the role of digital twin technology input of three different kinds in activities. Lastly this paper constructs a novel quantifiable weight model of performance evaluation by applying fuzzy mathematics theory. The calculation results indicate that this model can keep an eye on the evolution rule of digital twin technology to timely change the system are the optimal plans and measures based on weight analysis, and their weights are 0.336, 0.334 and 0.330 respectively, and the rationality of quantifiable weight behavior has a significant impact on the technology development.}
}
@incollection{CAULLER1992199,
title = {Chapter 8 - Functions of Very Distal Dendrites: Experimental and Computational Studies of Layer I Synapses on Neocortical Pyramidal Cells},
editor = {THOMAS MCKENNA and JOEL DAVIS and STEVEN F. ZORNETZER},
booktitle = {Single Neuron Computation},
publisher = {Academic Press},
address = {San Diego},
pages = {199-229},
year = {1992},
series = {Neural Networks: Foundations to Applications},
isbn = {978-0-12-484815-3},
doi = {https://doi.org/10.1016/B978-0-12-484815-3.50014-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780124848153500141},
author = {LARRY J. CAULLER and BARRY W. CONNORS},
abstract = {Publisher Summary
This chapter reviews an approach that combines quantitative morphology, physiology, and computational analysis to understandthe functions of a complex synaptic–neuronal interaction in the cortex. A variation of the in vitro slice of rat somatosensory neocortex examines the effectiveness of layer 1 inputs to pyramidal cells whose bodies lie 0.5–1 mm deeper, in layers 3 or 5. The horizontal fibers in layer 1 (HL1) were isolated by disconnecting all deeper horizontal fibers with a cut perpendicular to the surface, extending from just below layer 1 downward through subcortical white matter and Layer 1 was stimulated on one side of the cut and the response mediated by HL1 fibers passing to the other side was recorded extracellularly and intracellularly. Backward cortico–cortical projections, which end largely on distal apical dendrites in layer 1, are important for higher cortical functions. By isolating horizontal afferents to layer 1 in an in vitro neocortical slice, layer 1 synapses can strongly excite pyramidal cells as deep as layer 5b.}
}
@article{KAMEUGNE2025505,
title = {Quadratic horizontally elastic not-first/not-last filtering algorithm for cumulative constraint},
journal = {European Journal of Operational Research},
volume = {320},
number = {3},
pages = {505-515},
year = {2025},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2024.09.003},
url = {https://www.sciencedirect.com/science/article/pii/S0377221724006945},
author = {Roger Kameugne and Sévérine Fetgo Betmbe and Thierry Noulamo},
keywords = {Not-first/not-last algorithm, Profile data structure, TimeTable data structure, Cumulative scheduling, Constraint programming, Horizontally elastic scheduling, RCPSP},
abstract = {The not-first/not-last rule is a pendant of the edge finding rule, generally embedded in the cumulative constraint during constraint-based scheduling. It is combined with other filtering rules for more pruning of the tree search. In this paper, the Profile data structure in which tasks are scheduled in a horizontally elastic way is used to strengthen the classic not-first/not-last rule. Potential not-first task intervals are selected using criteria (specified later in the paper), and the Profile data structure is applied to selected task intervals. We prove that this new rule subsumes the classic not-first rule. A quadratic filtering algorithm is proposed for the new rule, thus improving the complexity of the horizontally elastic not-first/not-last algorithm from O(n3) to O(n2). The fixed part of external tasks that overlap with the selected task intervals is considered during the computation of the earliest completion time of task intervals. This improvement increases the filtering power of the algorithm while remaining quadratic. Experimental results, on a well-known suite of benchmark instances of Resource-Constrained Project Scheduling Problems (RCPSPs), show that the propounded algorithms are competitive with the state-of-the-art not-first algorithms in terms of tree search and running time reduction.}
}
@incollection{GILLESPIE2024124,
title = {Differentiation},
editor = {Samuel M. Scheiner},
booktitle = {Encyclopedia of Biodiversity (Third Edition)},
publisher = {Academic Press},
edition = {Third Edition},
address = {Oxford},
pages = {124-133},
year = {2024},
isbn = {978-0-323-98434-8},
doi = {https://doi.org/10.1016/B978-0-12-822562-2.00167-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128225622001675},
author = {Rosemary G. Gillespie},
keywords = {Adaptive landscape, Cline, Coalescent process, Gene flow, Hybrid zone, Local adaptation, Natural selection, Neutral theory, Population structure and Speciation},
abstract = {Differentiation generally considers the accumulation of genetic differences between populations or species but can be applied more broadly to the diversification of genes, organisms, and populations. This article examines how elementary evolutionary and ecological processes lead to differentiation in both the narrow and broad senses and introduces successively more complex processes involved in differentiation. Most mutations are selectively neutral and neutral evolution can be considered the default process of genomic change, with natural selection being measurable from changes in allele frequencies that lead to deviation from neutrality. While the basic tenets of genetic differentiation are well established, the advent of new genomic technologies and associated computational tools have provided unprecedented insights into the mechanism of genetic differentiation and how these might lead to the formation of species. Recent work has highlighted in particular the importance of genetic admixture events and genomic interactions in shaping the process of differentiation.}
}
@article{ZHAO201568,
title = {Approximate methods for optimal replacement, maintenance, and inspection policies},
journal = {Reliability Engineering & System Safety},
volume = {144},
pages = {68-73},
year = {2015},
issn = {0951-8320},
doi = {https://doi.org/10.1016/j.ress.2015.07.005},
url = {https://www.sciencedirect.com/science/article/pii/S0951832015001957},
author = {Xufeng Zhao and Khalifa N. Al-Khalifa and Toshio Nakagawa},
keywords = {Hazard function, Mean time to failure, Age replacement, Imperfect maintenance, Approximate inspection},
abstract = {It might be difficult sometimes to derive theoretical and numerical solutions for analytical maintenance modelings due to the computational complexity. This paper takes up several approximate models in maintenance theory, by using the cumulative hazard function H(t) and the newly proposed asymptotic MTTF (Mean Time to Failure) skilfully. We firstly denote by tx the time when the expected number of failures is x. Using H(tx)=x, we estimate failure times, model age and periodic replacements, and sequential imperfect maintenance. Motivated by the asymptotic method of computation of MTTF, we secondly model the expected cost rate for a parallel system when replacement is made at system failure, and give approximate computations for the sequential inspection policy. Optimizations of each model are obtained approximately in an easier way. When failure times have a Weibull distribution, it is shown from numerical examples that the obtained approximate optimal solutions have good approximations of the exact ones.}
}
@article{STROM2024111887,
title = {Diborane anharmonic vibrational frequencies and Intensities: Experiment and theory},
journal = {Journal of Molecular Spectroscopy},
volume = {400},
pages = {111887},
year = {2024},
issn = {0022-2852},
doi = {https://doi.org/10.1016/j.jms.2024.111887},
url = {https://www.sciencedirect.com/science/article/pii/S0022285224000146},
author = {Aaron I. Strom and Ibrahim Muddasser and Guntram Rauhut and David T. Anderson},
keywords = {Matrix Isolation Spectroscopy, Anharmonic Vibrational Dynamics, Infrared Spectroscopy, Computational Spectroscopy, Diborane, Fermi Resonance, Darling-Dennison Resonance},
abstract = {The vibrational dynamics of diborane have been extensively studied both theoretically and experimentally ever since the bridge structure of diborane was established in the 1950s. Numerous infrared and several Raman spectroscopic studies have followed in the ensuing years at ever increasing levels of spectral resolution. In parallel, ab initio computations of the underlying potential energy surface have progressed as well as the methods to calculate the anharmonic vibration dynamics beyond the double harmonic approximation. Nevertheless, even 70 years after the bridge structure of diborane was established, there are still significant discrepancies between experiment and theory for the fundamental vibrational frequencies of diborane. In this work we use parahydrogen (pH2) matrix isolation infrared spectroscopy to characterize six fundamental vibrations of B2H6 and B2D6 and compare them with results from configuration-selective vibrational configuration interaction theory. The calculated frequencies and intensities are in very good agreement with the pH2 matrix isolation spectra, even several combination bands are well reproduced. We believe that the reason discrepancies have existed for so long is related to the large amount of anharmonicity that is associated with the bridge BH stretching modes. However, the calculated frequencies and intensities reported here for the vibrational modes of all three boron isotopologues of B2H6 and B2D6 are within ± 2.00 cm−1 and ± 1.44 cm−1, respectively, of the experimental frequencies and therefore a refined vibrational assignment of diborane has been achieved.}
}
@article{MCGEE201440,
title = {The pragmatics of paragraphing English argumentative text},
journal = {Journal of Pragmatics},
volume = {68},
pages = {40-72},
year = {2014},
issn = {0378-2166},
doi = {https://doi.org/10.1016/j.pragma.2014.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S0378216614000770},
author = {Iain McGee},
keywords = {Paragraphing, Lexical cohesion, Argumentative text, Textual colligation, Foregrounding, Discourse signaling, Rhetorical devices, Computational Linguistics},
abstract = {Computational linguistic work into the paragraph and paragraphing has highlighted the significant role that intra-paragraph lexical cohesion plays in ‘marking off’ one paragraph unit from another. The goal of the research reported on in this paper is to consider, in some detail, the relationship that exists between the lexical repetition patterns in an argumentative text (as identified by a computational procedure), the genre moves within it, the actual paragraphing of the texts, and the textual colligation features of the paragraphs. The Link Set Median procedure (Berber-Sardinha, 1997, Berber-Sardinha, 2001, Berber-Sardinha, 2002) is used to document exact, inflectional and derivational lexical repetition usage across 10 short English argumentative texts, and to predict where segmentations originally occurred in the texts. The resulting data are then analyzed in the light of diverse research interests into the paragraph, and classified accordingly. A comparison of these results is made with data where there is either a marginal or no difference in the link set medians of adjacent sentences across paragraph junctures within the same texts. It is suggested that this novel approach of analyzing computational data from multiple paragraph-specific research interests results in a clearer picture of paragraphing practice emerging.}
}
@article{UWIZEYE2016647,
title = {A comprehensive framework to assess the sustainability of nutrient use in global livestock supply chains},
journal = {Journal of Cleaner Production},
volume = {129},
pages = {647-658},
year = {2016},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2016.03.108},
url = {https://www.sciencedirect.com/science/article/pii/S0959652616301792},
author = {Aimable Uwizeye and Pierre J. Gerber and Rogier P.O. Schulte and Imke J.M. {de Boer}},
keywords = {Nitrogen, Phosphorus, Nutrient use efficiency, Life-cycle thinking, Livestock supply chain, Soil nutrient stock change},
abstract = {The assessment of the performance of nutrient use along livestock supply chains can help to identify targeted nutrient management interventions, with a goal to benchmark and to monitor the improvement of production practices. It is necessary, therefore, to develop indicators that are capable to describe all nutrient dynamics and management along the chain. This paper proposed a comprehensive framework, based on life-cycle thinking, to assess the sustainability of nitrogen and phosphorus use. The proposed framework represents nutrient flows in typical livestock supply chain from the “cradle-to-primary-processing-gate”, including crop/pasture production, animal production, and primary processing stage as well as the transportation of feed materials, live-animals or animal products. In addition, three indicators, including the life-cycle nutrient use efficiency (life-cycle-NUE), life-cycle net nutrient balance (life-cycle-NNB) and nutrient hotspot index (NHI) were proposed and tested in a case study of mixed dairy supply chains in Europe. Proposed indicators were found to be suitable to describe different aspects of nitrogen and phosphorus dynamics and, therefore, were all needed. Moreover, the disaggregation of life-cycle-NUE and life-cycle-NNB has been investigated and the uncertainties related to the choice of the method used to estimate changes in nutrient soil stock have been discussed. Given these uncertainties, the choice of method to compute the proposed indicators is determined by data availability and by the goal and scope of the exercise.}
}
@article{BUESODEBARRIO2025101019,
title = {Executable contracts for Elixir},
journal = {Journal of Logical and Algebraic Methods in Programming},
volume = {142},
pages = {101019},
year = {2025},
issn = {2352-2208},
doi = {https://doi.org/10.1016/j.jlamp.2024.101019},
url = {https://www.sciencedirect.com/science/article/pii/S2352220824000737},
author = {Luis Eduardo {Bueso de Barrio} and Lars-Åke Fredlund and Ángel Herranz and Julio Mariño and Clara {Benac Earle}},
abstract = {This article presents the design of a library for attaching and checking executable contracts to code written in the Elixir programming language. In addition to classical contract constructs such as preconditions and postconditions, the library allows specifying exceptional behaviour (i.e., which exceptions are thrown and under which conditions), detecting non-termination issues in recursive functions by specifying a strictly decreasing order in function arguments, and associating timers with function calls to detect slow computations. The library also focuses on language-specific features, enabling the association of contracts with the reception of messages sent by processes and the attachment of constraints to variable names (useful due to variable shadowing in Elixir). Moreover, stateful contracts (i.e., with a model state) permit specifying the behaviour of stateful APIs whose operations can be linearized. Using the stateful contracts, a monitor can be employed to check that the observed state can be explained in terms of possible linearizations.}
}
@article{JOHANNESJOSEFIJEN202173,
title = {An adaptive temporal-causal network model to analyse extinction of communication over time},
journal = {Cognitive Systems Research},
volume = {68},
pages = {73-83},
year = {2021},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2020.08.013},
url = {https://www.sciencedirect.com/science/article/pii/S1389041721000231},
author = {Lucas {Johannes José Fijen} and Julio {Joaquín López González} and Jan Treur},
keywords = {Extinction of communication, Network modeling, Adaptive network, Social simulation},
abstract = {The persistence of information communicated between humans is difficult to measure as it is affected by many features. This paper presents an approach to computationally model the cognitive processes of information sharing to describe persistence or extinction of communication in Twitter over time. The adaptive mental network model explains, for example, how an individual can experience information overflow on a topic, and how this affects the sharing of information. Parameter tuning by Simulated Annealing is used to identify characteristics of the network model that fit to empirical data from Twitter. The data collected is related to the independentism in Catalunya, Spain, which is considered a global issue with repercussion in Europe.}
}
@article{SCHOEN2025102018,
title = {Improving the teaching and learning of statistics},
journal = {Learning and Instruction},
volume = {95},
pages = {102018},
year = {2025},
issn = {0959-4752},
doi = {https://doi.org/10.1016/j.learninstruc.2024.102018},
url = {https://www.sciencedirect.com/science/article/pii/S0959475224001452},
author = {Robert C. Schoen and Christopher Rhoads and Alexandra Perez and Tim Jacobbe and Lanrong Li},
keywords = {Curriculum, Teacher professional development, Statistics education, Many-facet rasch, Hierarchical linear modeling},
abstract = {Structured Abstract
Background
Statistical literacy is more important now than ever. Mathematics teachers are often expected to teach statistics, but statistics and mathematics differ in important ways. The mathematics teaching workforce needs more opportunities to learn statistics and how to teach it accurately and effectively.
Aims
This study was designed to estimate the effects of an intervention. The intervention consisted of a combination of an inquiry-oriented curriculum replacement unit and teacher learning opportunities in statistics and probability. Primary outcomes of interest were instructional practice and student understanding of statistics and probability.
Sample
The study sample included seventh-grade teachers and their students (age 13) in a single, urban school district in the southeastern United States. There were 74 classrooms represented in the analytic sample for the instructional outcome and 2,283 students in the analytic sample for the student outcome.
Methods
Schools were randomly assigned to the treatment or control conditions with equal probability of assignment to condition. Treatment-condition teachers participated in four days of professional learning workshops focused on teaching a 20-day curriculum unit. The Instructional Quality Assessment was used to measure instructional practice. The Levels of Conceptual Understanding in Statistics assessment instrument was used to measure student learning outcomes. Data analysis used hierarchical linear modeling.
Results
Positive, statistically significant effects on both instructional practice (ES = .99) and student understanding of statistics (ES = .25) were found.
Conclusions
The study results indicate that the inquiry-oriented lessons in the curriculum—with the support of teacher-learning opportunities—can improve instruction and increase student learning in statistics.}
}
@article{SCOTNEY2020102981,
title = {The form of a ‘half-baked’ creative idea: Empirical explorations into the structure of ill-defined mental representations},
journal = {Acta Psychologica},
volume = {203},
pages = {102981},
year = {2020},
issn = {0001-6918},
doi = {https://doi.org/10.1016/j.actpsy.2019.102981},
url = {https://www.sciencedirect.com/science/article/pii/S0001691819303129},
author = {Victoria S. Scotney and Jasmine Schwartz and Nicole Carbert and Adam Saab and Liane Gabora},
keywords = {Analogy, Art, Creative process, Honing, Mental representation, Structure mapping},
abstract = {Creative thought is conventionally believed to involve searching memory and generating multiple independent candidate ideas followed by selection and refinement of the most promising. Honing theory, which grew out of the quantum approach to describing how concepts interact, posits that what appears to be discrete, separate ideas are actually different projections of the same underlying mental representation, which can be described as a superposition state, and which may take different outward forms when reflected upon from different perspectives. As creative thought proceeds, this representation loses potentiality to be viewed from different perspectives and manifest as different outcomes. Honing theory yields different predictions from conventional theories about the mental representation of an idea midway through the creative process. These predictions were pitted against one another in two studies: one closed-ended and one open-ended. In the first study, participants were interrupted midway through solving an analogy problem and wrote down what they were thinking in terms of a solution. In the second, participants were instructed to create a painting that expressed their true essence and describe how they conceived of the painting. For both studies, naïve judges categorized these responses as supportive of either the conventional view or the honing theory view. The results of both studies were significantly more consistent with the predictions of honing theory. Some implications for creative cognition, and cognition in general, are discussed.}
}
@article{URBINA2022100031,
title = {The commoditization of AI for molecule design},
journal = {Artificial Intelligence in the Life Sciences},
volume = {2},
pages = {100031},
year = {2022},
issn = {2667-3185},
doi = {https://doi.org/10.1016/j.ailsci.2022.100031},
url = {https://www.sciencedirect.com/science/article/pii/S2667318522000022},
author = {Fabio Urbina and Sean Ekins},
keywords = {Artificial intelligence, Design-make-test, Machine learning, Molecule design, Recurrent neural networks},
abstract = {Anyone involved in designing or finding molecules in the life sciences over the past few years has witnessed a dramatic change in how we now work due to the COVID-19 pandemic. Computational technologies like artificial intelligence (AI) seemed to become ubiquitous in 2020 and have been increasingly applied as scientists worked from home and were separated from the laboratory and their colleagues. This shift may be more permanent as the future of molecule design across different industries will increasingly require machine learning models for design and optimization of molecules as they become “designed by AI”. AI and machine learning has essentially become a commodity within the pharmaceutical industry. This perspective will briefly describe our personal opinions of how machine learning has evolved and is being applied to model different molecule properties that crosses industries in their utility and ultimately suggests the potential for tight integration of AI into equipment and automated experimental pipelines. It will also describe how many groups have implemented generative models covering different architectures, for de novo design of molecules. We also highlight some of the companies at the forefront of using AI to demonstrate how machine learning has impacted and influenced our work. Finally, we will peer into the future and suggest some of the areas that represent the most interesting technologies that may shape the future of molecule design, highlighting how we can help increase the efficiency of the design-make-test cycle which is currently a major focus across industries.}
}
@article{CHEN2019381,
title = {How Big Data and High-performance Computing Drive Brain Science},
journal = {Genomics, Proteomics & Bioinformatics},
volume = {17},
number = {4},
pages = {381-392},
year = {2019},
note = {Big Data in Brain Science},
issn = {1672-0229},
doi = {https://doi.org/10.1016/j.gpb.2019.09.003},
url = {https://www.sciencedirect.com/science/article/pii/S1672022919301561},
author = {Shanyu Chen and Zhipeng He and Xinyin Han and Xiaoyu He and Ruilin Li and Haidong Zhu and Dan Zhao and Chuangchuang Dai and Yu Zhang and Zhonghua Lu and Xuebin Chi and Beifang Niu},
keywords = {Brain science, Big data, High-performance computing, Brain connectomes, Deep learning},
abstract = {Brain science accelerates the study of intelligence and behavior, contributes fundamental insights into human cognition, and offers prospective treatments for brain disease. Faced with the challenges posed by imaging technologies and deep learning computational models, big data and high-performance computing (HPC) play essential roles in studying brain function, brain diseases, and large-scale brain models or connectomes. We review the driving forces behind big data and HPC methods applied to brain science, including deep learning, powerful data analysis capabilities, and computational performance solutions, each of which can be used to improve diagnostic accuracy and research output. This work reinforces predictions that big data and HPC will continue to improve brain science by making ultrahigh-performance analysis possible, by improving data standardization and sharing, and by providing new neuromorphic insights.}
}
@article{CUMMING2012923,
title = {Better compounds faster: the development and exploitation of a desktop predictive chemistry toolkit},
journal = {Drug Discovery Today},
volume = {17},
number = {17},
pages = {923-927},
year = {2012},
issn = {1359-6446},
doi = {https://doi.org/10.1016/j.drudis.2012.03.003},
url = {https://www.sciencedirect.com/science/article/pii/S1359644612000840},
author = {John G. Cumming and Jon Winter and Andrew Poirrette},
abstract = {Today's drug designer has access to vast quantities of data and an impressive array of sophisticated computational methods. At the same time, there is increasing pressure on the pharmaceutical industry to improve its productivity and reduce candidate drug attrition. We set out to develop a highly integrated suite of design and data analysis tools underpinned by the best predictive chemistry methods and models, with the aim of enabling multi-disciplinary compound design teams to make better informed design decisions. In this article we address the challenges of developing a powerful, flexible and user-friendly toolkit, and of maximising its exploitation by the design community. We describe the impact the toolkit has had on drug discovery projects and give our perspective on the future direction of this activity.}
}
@article{MOHAMMED2025110933,
title = {Artificial intelligence approaches in predicting the mechanical properties of natural fiber-reinforced concrete: A comprehensive review},
journal = {Engineering Applications of Artificial Intelligence},
volume = {153},
pages = {110933},
year = {2025},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2025.110933},
url = {https://www.sciencedirect.com/science/article/pii/S0952197625009339},
author = {Mohammed Mohammed and Jawad K. Oleiwi and Aeshah M. Mohammed and Azlin F. Osman and Tijjani Adam and Bashir O. Betar and Subash C.B. Gopinath},
keywords = {Implemented artificial intelligence, Natural fiber-reinforced concrete, Mechanical properties, Applications artificial intelligence, Sustainability},
abstract = {Implementing artificial intelligence (AI) techniques in predicting the mechanical properties of natural fiber-reinforced concrete (NFRC) has emerged as a transformative approach, offering significant advancements over traditional modeling methods. Construction materials science increasingly leverages advanced technologies to enhance composites like natural fiber-reinforced concrete (NFRC). However, the varied nature of natural fibers and their interactions within concrete matrices present significant challenges in accurately predicting the NFRC's mechanical properties. This comprehensive review highlights the innovative use of artificial intelligence (AI) techniques to address challenges in predicting material reliability. It explores the application of various AI methodologies, including machine learning (ML) techniques such as artificial neural networks (ANN) and support vector machines (SVM), along with pattern recognition (PR) and deep learning (DL). These approaches are utilized to tackle the challenges posed by the variability of natural fibers and their interactions within concrete matrices. These techniques have proven highly effective in accurately predicting the mechanical behavior of NFRC, marking significant advancements in the field. This review paper aims to summarize techniques for applying the AI methods mentioned in the NFRC. Firstly, we present a general introduction to AI and NFRC, highlighting the significance of AI in predicting the mechanical properties of NFRC. After that, a comparison between ML, PR, and DL in the field is discussed, and a review of recent applications of AI in the field is provided. Further, the advantages of employing such algorithmic methods are discussed in detail. Finally, future directions for employing ML, PR, and DL are presented, and their limitations are discussed.}
}
@article{AJIMATI2025112300,
title = {Adoption of low-code and no-code development: A systematic literature review and future research agenda},
journal = {Journal of Systems and Software},
volume = {222},
pages = {112300},
year = {2025},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2024.112300},
url = {https://www.sciencedirect.com/science/article/pii/S0164121224003443},
author = {Matthew Oladeji Ajimati and Noel Carroll and Mary Maher},
keywords = {Citizen development, Low-code, No-code, Digital transformation, Systematic literature review},
abstract = {Context
Low-code/no-code (LCNC) is an emerging technology trend that extends software development beyond professionalsoftware engineers, making it accessible to individuals throughout organizations and society.
Objective
We aim to provide a systematic review of the current research on the adoption of LCNC technologies within citizen development (CD) practices for digital transformation (DT), and to propose a research agenda for this field.
Method
This review is primarily conducted using a multi-phase systematic literature review of publications from the past five years, i.e., between 2017 and 2023.
Results
We identified 40 primary studies that describes the application of LCNC development and CD practices, the theoretical lenses/frameworks used, and the associated benefits and challenges.
Conclusion
In this study, we present three key contributions. First, we provide a comprehensive review of the benefits, challenges, theoretical perspectives, and methods used to explore LCNC and CD adoption. Second, we introduce a framework designed to guide managers in effectively adopting LCNC and CD practices. Finally, our systematic review uncovers gaps in existing research and identifies opportunities for further exploration, which paves the way for a future research agenda.}
}
@article{CHRISLEY2008119,
title = {Philosophical foundations of artificial consciousness},
journal = {Artificial Intelligence in Medicine},
volume = {44},
number = {2},
pages = {119-137},
year = {2008},
note = {Artificial Consciousness},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2008.07.011},
url = {https://www.sciencedirect.com/science/article/pii/S0933365708001000},
author = {Ron Chrisley},
keywords = {Artificial consciousness, Machine consciousness, Prosthetic artificial intelligence, Synthetic phenomenology, Interactive empiricism, Heterophenomenology},
abstract = {Summary
Objective
Consciousness is often thought to be that aspect of mind that is least amenable to being understood or replicated by artificial intelligence (AI). The first-personal, subjective, what-it-is-like-to-be-something nature of consciousness is thought to be untouchable by the computations, algorithms, processing and functions of AI method. Since AI is the most promising avenue toward artificial consciousness (AC), the conclusion many draw is that AC is even more doomed than AI supposedly is. The objective of this paper is to evaluate the soundness of this inference.
Methods
The results are achieved by means of conceptual analysis and argumentation.
Results and conclusions
It is shown that pessimism concerning the theoretical possibility of artificial consciousness is unfounded, based as it is on misunderstandings of AI, and a lack of awareness of the possible roles AI might play in accounting for or reproducing consciousness. This is done by making some foundational distinctions relevant to AC, and using them to show that some common reasons given for AC scepticism do not touch some of the (usually neglected) possibilities for AC, such as prosthetic, discriminative, practically necessary, and lagom (necessary-but-not-sufficient) AC. Along the way three strands of the author's work in AC – interactive empiricism, synthetic phenomenology, and ontologically conservative heterophenomenology – are used to illustrate and motivate the distinctions and the defences of AC they make possible.}
}