@article{ZHOU2019244,
title = {Lightweight IoT-based authentication scheme in cloud computing circumstance},
journal = {Future Generation Computer Systems},
volume = {91},
pages = {244-251},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.08.038},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X18307878},
author = {Lu Zhou and Xiong Li and Kuo-Hui Yeh and Chunhua Su and Wayne Chiu},
keywords = {Internet-of-things (IoT), Cloud computing, Authentication, Proverif, User tracking},
abstract = {Recently, authentication technologies integrated with the Internet of Things (IoT) and cloud computing have been promptly investigated for secure data retrieval and robust access control on large-scale IoT networks. However, it does not have a best practice for simultaneously deploying IoT and cloud computing with robust security. In this study, we present a novel authentication scheme for IoT-based architectures combined with cloud servers. To pursue the best efficiency, lightweight crypto-modules, such as one-way hash function and exclusive-or operation, are adopted in our authentication scheme. It not only removes the computation burden but also makes our proposed scheme suitable for resource-limited objects, such as sensors or IoT devices. Through the formal verification delivered by Proverif, the security robustness of the proposed authentication scheme is guaranteed. Furthermore, the performance evaluation presents the practicability of our proposed scheme in which a user-acceptable computation cost is achieved.}
}
@article{UMEREZ2001159,
title = {Howard Pattee's theoretical biology — a radical epistemological stance to approach life, evolution and complexity},
journal = {Biosystems},
volume = {60},
number = {1},
pages = {159-177},
year = {2001},
issn = {0303-2647},
doi = {https://doi.org/10.1016/S0303-2647(01)00114-9},
url = {https://www.sciencedirect.com/science/article/pii/S0303264701001149},
author = {Jon Umerez},
keywords = {Epistemological stance, Epistemic cut, Semantic closure, Code, Symbol},
abstract = {This paper offers a short review of Pattee's main contributions to science and philosophy. With no intention of being exhaustive, an account of Pattee's work is presented which discusses some of his ideas and their reception. This is done through an analysis centered in what is thought to be his main contribution: the elaboration of an internal epistemic stance to better understand life, evolution and complexity. Having introduced this core idea as a sort of a posteriori cohesive element of a complex but highly coherent and complete system of thinking, further specific elements are also reviewed.}
}
@article{CRAIG20233427,
title = {FEFOS: a method to derive oxide formation energies from oxidation states††Electronic supplementary information (ESI) available. See DOI: https://doi.org/10.1039/d3cy00107e},
journal = {Catalysis Science & Technology},
volume = {13},
number = {11},
pages = {3427-3435},
year = {2023},
issn = {2044-4753},
doi = {https://doi.org/10.1039/d3cy00107e},
url = {https://www.sciencedirect.com/science/article/pii/S2044475323006822},
author = {Michael John Craig and Felix Kleuker and Michal Bajdich and Max García-Melchor},
abstract = {ABSTRACT
Herein we report a method to extract formation energies from oxidation states, which we call FEFOS. This new scheme predicts the formation energies of binary oxides through analyzing unary oxide formation energies as a function of their oxidation states. Taking averages of fitted quadratic equations that represent how elements respond to oxidation and reduction, the weights of these averages are determined by constraining the compound to be neutral. The application of FEFOS results in mean absolute errors of ca. 0.10 eV per atom when tested against Materials Project data for oxides with general formulas A1−zBzO, A1−zBzO1.5, and A1−zBzO2 with specific coordinations. Our FEFOS method not only allows for the prediction of binary oxide formation energies with low variance and high interpretability, but also compares well with state-of-the-art deep learning methods without being biased by training data and the need for large resources to compute it. Finally, we discuss the potential applications of the FEFOS method in tackling the problem of inverse catalyst design.}
}
@article{POOBALAN2025101667,
title = {A novel and secured email classification using deep neural network with bidirectional long short-term memory},
journal = {Computer Speech & Language},
volume = {89},
pages = {101667},
year = {2025},
issn = {0885-2308},
doi = {https://doi.org/10.1016/j.csl.2024.101667},
url = {https://www.sciencedirect.com/science/article/pii/S0885230824000500},
author = {A. Poobalan and K. Ganapriya and K. Kalaivani and K. Parthiban},
keywords = {Email classification, DNN-BiLSTM, AES algorithm, Rabit algorithm, Random forests (RF)},
abstract = {Email data has some characteristics that are different from other social media data, such as a large range of answers, formal language, notable length variations, high degrees of anomalies, and indirect relationships. The main goal in this research is to develop a robust and computationally efficient classifier that can distinguish between spam and regular email content. The benchmark Enron dataset, which is accessible to the public, was used for the tests. The six distinct Enron data sets we acquired were combined to generate the final seven Enron data sets. The dataset undergoes early preprocessing to remove superfluous sentences. The proposed model Bidirectional Long Short-Term Memory (BiLSTM) apply spam labels and to examine email documents for spam. On seven Enron datasets, DNN-BiLSTM performs better than other classifiers in the performance comparison in terms of accuracy. DNN-BiLSTM and convolutional neural networks demonstrated that they can classify spam with 96.39 % and 98.69 % accuracy, respectively, in comparison to other machine learning classifiers. The risks associated with cloud data management and potential security flaws are also covered in the paper. This research presents hybrid encryption as a means of protecting cloud data while preserving privacy by using the hybrid AES-Rabit encryption algorithm which is based on symmetric session key exchange.}
}
@incollection{KLOCKING202597,
title = {Geochemical databases},
editor = {Ariel Anbar and Dominique Weis},
booktitle = {Treatise on Geochemistry (Third edition)},
publisher = {Elsevier},
edition = {Third edition},
address = {Oxford},
pages = {97-135},
year = {2025},
isbn = {978-0-323-99763-8},
doi = {https://doi.org/10.1016/B978-0-323-99762-1.00123-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780323997621001236},
author = {Marthe Klöcking and Kerstin A. Lehnert and Lesley Wyborn},
keywords = {Artificial intelligence, CARE, Community standards, Data ethics, Data management, Databases, FAIR, Geochemistry, Machine learning, Machine readable data, Repository, TRUST},
abstract = {Geochemistry is a data-driven discipline. Modern laboratories produce highly diverse data, and the recent exponential increase in data volumes is challenging established practices and capabilities for organizing, analyzing, preserving, and accessing these data. At the same time, sophisticated computational techniques, including machine learning, are increasingly applied to geochemical research questions, which require easy access to large volumes of high-quality, well-organized, and standardized data. Data management has been important since the beginning of geochemistry but has recently become a necessity for the discipline to thrive in the age of digitalization and artificial intelligence. This paper summarizes the landscape of geochemical databases, distinguishing different types of data systems based on their purpose, and their evolution in a historic context. We apply the life cycle model of geochemical data; explain the relevance of current standards, practices, and policies that determine the design of modern geochemical databases and data management; the ethics of data reuse such as data ownership, data attribution, and data citation; and finally create a vision for the future of geochemical databases: data being born digital, connected to agreed community standards, and contributing to global democratization of geochemical data.}
}
@article{RAJPUT2021104270,
title = {VLSI implementation of transcendental function hyperbolic tangent for deep neural network accelerators},
journal = {Microprocessors and Microsystems},
volume = {84},
pages = {104270},
year = {2021},
issn = {0141-9331},
doi = {https://doi.org/10.1016/j.micpro.2021.104270},
url = {https://www.sciencedirect.com/science/article/pii/S014193312100435X},
author = {Gunjan Rajput and Gopal Raut and Mahesh Chandra and Santosh Kumar Vishvakarma},
keywords = {Activation function, Artificial neural network, Hyperbolic tangent (tanh), Digital implementation, Combinational logic},
abstract = {Extensive use of neural network applications prompted researchers to customize a design to speed up their computation based on ASIC implementation. The choice of activation function (AF) in a neural network is an essential requirement. Accurate design architecture of an AF in a digital network faces various challenges as these AF require more hardware resources because of its non-linear nature. This paper proposed an efficient approximation scheme for hyperbolic tangent (tanh) function which purely based on combinational design architecture. The approximation is based on mathematical analysis by considering maximum allowable error in a neural network. The results prove that the proposed combinational design of an AF is efficient in terms of area, power and delay with negligible accuracy loss on MNIST and CIFAR-10 benchmark datasets. Post synthesis results show that the proposed design area is reduced by 66% and delay is reduced by nearly 16% compared to state-of-the-art.}
}
@article{HALL20156607,
title = {Understanding Sector Dependencies in the Stabilization and Reconstruction of Nation-states},
journal = {Procedia Manufacturing},
volume = {3},
pages = {6607-6614},
year = {2015},
note = {6th International Conference on Applied Human Factors and Ergonomics (AHFE 2015) and the Affiliated Conferences, AHFE 2015},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2015.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S2351978915009932},
author = {Steven Hall and Curtis Blais},
keywords = {Multi-agent, system dynamics, modeling and simulation, panarchy, resiliency, stabilitiy, reconstruction and humanitarian operations.},
abstract = {The United States Army is undergoing a re-definition of its Civil Affairs officer positions. A recent project to define the educational requirements for an Army Civil Affairs Officer (38G) identified an educational requirement to help officers understand the complex ways in which the operations that advance the achievement of one stabilization objective often hinder the achievement of other objectives. The system level thinking was seen to be frequently insufficiently ingrained amongst Civil Affairs Officers (and the leaders they advised), who were both often perceived to be inclined, in the face of the complexities of the situation on the ground, to become too narrowly focused on achieving their specific assigned responsibilities, limiting their ability to see how the mission effectiveness of what they were recommending would be influenced by the state and trajectory of other Sectors and how, in turn, their recommendations would influence the mission effectiveness of other Sector stewards. While system dynamics modeling has proven itself to be effective in capturing and effectively communicating feedback loops that define such non-linear (and non-intuitive) systems they do not, in themselves, provide sufficient modeling richness to comprehensively capture the critical spatial (geographical) determinants of a successful state reconstruction process. For these purposes, a multi-agent cellular automata model is recommended both as a vehicle for introducing students to the complex nature of the state reconstruction process and, eventually, for use in the field by deployed Civilian Affairs Officers at all levels. This paper describes the problem and the modeling approach to address it.}
}
@article{ARGYROUDIS2022100387,
title = {Digital technologies can enhance climate resilience of critical infrastructure},
journal = {Climate Risk Management},
volume = {35},
pages = {100387},
year = {2022},
issn = {2212-0963},
doi = {https://doi.org/10.1016/j.crm.2021.100387},
url = {https://www.sciencedirect.com/science/article/pii/S2212096321001169},
author = {Sotirios A. Argyroudis and Stergios Aristoteles Mitoulis and Eleni Chatzi and Jack W. Baker and Ioannis Brilakis and Konstantinos Gkoumas and Michalis Vousdoukas and William Hynes and Savina Carluccio and Oceane Keou and Dan M. Frangopol and Igor Linkov},
keywords = {Emerging digital technologies, Data-driven, Critical infrastructure, Climate change, Sustainable development goals (SDGs)},
abstract = {Delivering infrastructure, resilient to multiple natural hazards and climate change, is fundamental to continued economic prosperity and social coherence. This is a strategic priority of the United Nations Sustainable Development Goals (SDGs), the World Bank, the Organisation for Economic Co-operation and Development (OECD), public policies and global initiatives. The operability and functionality of critical infrastructure are continuously challenged by multiple stressors, increasing demands and ageing, whilst their interconnectedness and dependencies pose additional challenges. Emerging and disruptive digital technologies have the potential to enhance climate resilience of critical infrastructure, by providing rapid and accurate assessment of asset condition and support decision-making and adaptation. In this pursuit, it is imperative to adopt multidisciplinary roadmaps and deploy computational, communication and other digital technologies, tools and monitoring systems. Nevertheless, the potential of these emerging technologies remains largely unexploited, as there is a lack of consensus, integrated approaches and legislation in support of their use. In this perspective paper, we discuss the main challenges and enablers of climate-resilient infrastructure and we identify how available roadmaps, tools and emerging digital technologies, e.g. Internet of Things, digital twins, point clouds, Artificial Intelligence, Building Information Modelling, can be placed at the service of a safer world. We show how digital technologies will lead to infrastructure of enhanced resilience, by delivering efficient and reliable decision-making, in a proactive and/or reactive manner, prior, during and after hazard occurrences. In this respect, we discuss how emerging technologies significantly reduce the uncertainties in all phases of infrastructure resilience evaluations. Thus, building climate-resilient infrastructure, aided by digital technologies, will underpin critical activities globally, contribute to Net Zero target and hence safeguard our societies and economies. To achieve this we set an agenda, which is aligned with the relevant SDGs and highlights the urgent need to deliver holistic and inclusive standards and legislation, supported by coordinated alliances, to fully utilise emerging digital technologies.}
}
@article{MILIK201022,
title = {On Efficient Implementation of Search Algorithm for Genome Patterns},
journal = {IFAC Proceedings Volumes},
volume = {43},
number = {24},
pages = {22-27},
year = {2010},
note = {10th IFAC Workshop on Programmable Devices and Embedded Systems},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20101006-2-PL-4019.00006},
url = {https://www.sciencedirect.com/science/article/pii/S1474667015309812},
author = {Adam Milik and Andrzej Pulka},
keywords = {Dynamic programming, Computational methods, Pattern identification, Pattern recognition, Parallel processing, Pipeline processing},
abstract = {The presented paper describes the implementation of the computation algorithm on modern, complex programmable hardware devices. The presented algorithm originates from computation biology and works on very long chains of symbols, which come from reference patterns of the genome. The software solutions in the field are very limited and need large time and space resources. Main research efforts have been done to investigate the properties of the searching algorithm. Especially the influence of the penalty values assigned for the mismatch, the insertion and the deletion on the algorithm has been analyzed. This allows obtaining completely new algorithm that offers extremely efficient implementation and exhibits outstanding performance. The different FPGA generations have been considered as target families for the searching algorithm based on the dynamic programming idea. The obtained results are very promising and show the dominance of the dedicated platforms over the general purpose PC-based systems.}
}
@article{ASAHIRO202016,
title = {Graph orientation with splits},
journal = {Theoretical Computer Science},
volume = {844},
pages = {16-25},
year = {2020},
issn = {0304-3975},
doi = {https://doi.org/10.1016/j.tcs.2020.07.013},
url = {https://www.sciencedirect.com/science/article/pii/S030439752030387X},
author = {Yuichi Asahiro and Jesper Jansson and Eiji Miyano and Hesam Nikpey and Hirotaka Ono},
keywords = {Graph orientation, Maximum flow, Vertex cover, Partition, Algorithm, Computational complexity},
abstract = {The Minimum Maximum Outdegree Problem (MMO) is to assign a direction to every edge in an input undirected, edge-weighted graph so that the maximum weighted outdegree taken over all vertices becomes as small as possible. In this paper, we introduce a new variant of MMO called the p-Split Minimum Maximum Outdegree Problem (p-Split-MMO) in which one is allowed to perform a sequence of p split operations on the vertices before orienting the edges, for some specified non-negative integer p, and study its computational complexity.}
}
@incollection{ASHBY2016211,
title = {Chapter 14 - The Vision: A Circular Materials Economy},
editor = {Michael F. Ashby},
booktitle = {Materials and Sustainable Development},
publisher = {Butterworth-Heinemann},
address = {Boston},
pages = {211-239},
year = {2016},
isbn = {978-0-08-100176-9},
doi = {https://doi.org/10.1016/B978-0-08-100176-9.00014-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780081001769000141},
author = {Michael F. Ashby},
keywords = {Active stock, Circularity metrics, Material efficiency, Natural and industrial ecology, Product life extension, Product–service systems, Reuse, repair and recycling, Take-back schemes},
abstract = {We live at present with a largely linear materials economy. Our use of natural resources is characterized by the sequence “take – make – use – dispose” as materials progress from mine, through product, to landfill. Increasing population, rising affluence and the limited capacity for the planet to provide resources and absorb waste argue for a transition towards a more circular way of using materials. When products come to the end of their lives the materials they contain are still there. Repair, reuse and recycling (the three “Rs”) can return these to active use. Repair, reuse and recycling are not new ideas; they have been used for centuries to recirculate materials and, in less-developed economies, they still are. But in developed nations they have dwindled as the cost of materials fell and that of labor rose over time, making all three Rs uneconomic. So what is novel about the contemporary idea of a circular materials economy? Haven’t we been there before? The “circularity” concept is a way thinking that looks not just for efficiencies but also for new ways of providing the functions we need. In the last decade momentum has gathered about this transition. The idea of deploying rather than consuming materials, of using them not once but many times, and of redesign to make this a reality has economic as well as environmental appeal. Governments now sign up to programs to foster circular economic ideas and mechanisms begin to appear to advance them. This chapter examines the background, the successes and the difficulties of implementing a circular materials economy.}
}
@article{MCQUEEN2021120575,
title = {Do we really understand how drug eluted from stents modulates arterial healing?},
journal = {International Journal of Pharmaceutics},
volume = {601},
pages = {120575},
year = {2021},
issn = {0378-5173},
doi = {https://doi.org/10.1016/j.ijpharm.2021.120575},
url = {https://www.sciencedirect.com/science/article/pii/S037851732100380X},
author = {Alistair McQueen and Javier Escuer and Ankush Aggarwal and Simon Kennedy and Christopher McCormick and Keith Oldroyd and Sean McGinty},
keywords = {Pharmacodynamics, Ligand-receptor interactions, Drug-eluting stents, Smooth Muscle Cells, Cell proliferation, Mathematical Modelling},
abstract = {The advent of drug-eluting stents (DES) has revolutionised the treatment of coronary artery disease. These devices, coated with anti-proliferative drugs, are deployed into stenosed or occluded vessels, compressing the plaque to restore natural blood flow, whilst simultaneously combating the evolution of restenotic tissue. Since the development of the first stent, extensive research has investigated how further advancements in stent technology can improve patient outcome. Mathematical and computational modelling has featured heavily, with models focussing on structural mechanics, computational fluid dynamics, drug elution kinetics and subsequent binding within the arterial wall; often considered separately. Smooth Muscle Cell (SMC) proliferation and neointimal growth are key features of the healing process following stent deployment. However, models which depict the action of drug on these processes are lacking. In this article, we start by reviewing current models of cell growth, which predominantly emanate from cancer research, and available published data on SMC proliferation, before presenting a series of mathematical models of varying complexity to detail the action of drug on SMC growth in vitro. Our results highlight that, at least for Sodium Salicylate and Paclitaxel, the current state-of-the-art nonlinear saturable binding model is incapable of capturing the proliferative response of SMCs across a range of drug doses and exposure times. Our findings potentially have important implications on the interpretation of current computational models and their future use to optimise and control drug release from DES and drug-coated balloons.}
}
@article{CHOI2025,
title = {Association Between Shift Working and Brain Morphometric Changes in Workers: A Voxel-wise Comparison},
journal = {Safety and Health at Work},
year = {2025},
issn = {2093-7911},
doi = {https://doi.org/10.1016/j.shaw.2025.02.004},
url = {https://www.sciencedirect.com/science/article/pii/S2093791125000113},
author = {Joon Yul Choi and Sungmin Kim and Yongho Lee and Dohyeon Kim and Wanhyung Lee},
keywords = {Brain MRI, Neuroplasticity, Shift work, Voxel-wise comparison},
abstract = {Objective
There is abundant evidence from observational studies linking various health problems to shift work, but there is a lack of brain-based neurological evidence. Therefore, we examined morphometric changes on brain magnetic resonance imaging (MRI) between shift and non-shift workers.
Methods
A total 111 healthy workers participated in this study and underwent brain MRI, with the analysis incorporating merged workers' health surveillance data from regional hospital workers. Voxel-based morphometry analysis was used to investigate regional changes in the gray matter volume. To investigate the association of structural changes between shift workers and non-shift workers, a general linear model and threshold-free cluster enhancement were used with covariates, including total intracranial volume, age, and sex.
Results
After family-wise error correction, non-shift workers exhibited a significantly larger cerebellar region (p < 0.05) than shift workers. Conversely, the inferior parietal gyrus was found to be significantly larger in shift workers than in non-shift workers with family-wise error correction.
Conclusions
We observed increased clusters in the brains of both shift and non-shift workers, suggesting that the acquired occupational environment, including the shift work schedule, could influence brain neuroplasticity, which is an important consideration for occupational health.}
}
@article{KLEEBARILLAS2015455,
title = {A comparative study and validation of state estimation algorithms for Li-ion batteries in battery management systems},
journal = {Applied Energy},
volume = {155},
pages = {455-462},
year = {2015},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2015.05.102},
url = {https://www.sciencedirect.com/science/article/pii/S0306261915007357},
author = {Joaquín {Klee Barillas} and Jiahao Li and Clemens Günther and Michael A. Danzer},
keywords = {Lithium-ion battery, Battery management system, State of charge estimation, Robustness analysis, Sliding-mode observer, Kalman-based SOC estimation},
abstract = {To increase lifetime, safety, and energy usage battery management systems (BMS) for Li-ion batteries have to be capable of estimating the state of charge (SOC) of the battery cells with a very low estimation error. The accurate SOC estimation and the real time reliability are critical issues for a BMS. In general an increasing complexity of the estimation methods leads to higher accuracy. On the other hand it also leads to a higher computational load and may exceed the BMS limitations or increase its costs. An approach to evaluate and verify estimation algorithms is presented as a requisite prior the release of the battery system. The approach consists of an analysis concerning the SOC estimation accuracy, the code properties, complexity, the computation time, and the memory usage. Furthermore, a study for estimation methods is proposed for their evaluation and validation with respect to convergence behavior, parameter sensitivity, initialization error, and performance. In this work, the introduced analysis is demonstrated with four of the most published model-based estimation algorithms including Luenberger observer, sliding-mode observer, Extended Kalman Filter and Sigma-point Kalman Filter. The experiments under dynamic current conditions are used to verify the real time functionality of the BMS. The results show that a simple estimation method like the sliding-mode observer can compete with the Kalman-based methods presenting less computational time and memory usage. Depending on the battery system’s application the estimation algorithm has to be selected to fulfill the specific requirements of the BMS.}
}
@incollection{HEILMAN201319,
title = {Chapter 2 - Visual artistic creativity and the brain},
editor = {Stanley Finger and Dahlia W. Zaidel and François Boller and Julien Bogousslavsky},
series = {Progress in Brain Research},
publisher = {Elsevier},
volume = {204},
pages = {19-43},
year = {2013},
booktitle = {The Fine Arts, Neurology, and Neuroscience},
issn = {0079-6123},
doi = {https://doi.org/10.1016/B978-0-444-63287-6.00002-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780444632876000026},
author = {Kenneth M. Heilman and Lealani Mae Acosta},
keywords = {artistic creativity, hemispheric functions, visuospatial skills, creative innovation, divergent thinking, imagery, global and focal attention},
abstract = {Creativity is the development of a new or novel understanding—insight that leads to the expression of orderly relationships (e.g., finding and revealing the thread that unites). Visual artistic creativity plays an important role in the quality of human lives, and the goal of this chapter is to describe some of the brain mechanisms that may be important in visual artistic creativity. The initial major means of learning how the brain mediates any activity is to understand the anatomy and physiology that may support these processes. A further understanding of specific cognitive activities and behaviors may be gained by studying patients who have diseases of the brain and how these diseases influence these functions. Physiological recording such as electroencephalography and brain imaging techniques such as PET and fMRI have also allowed us to gain a better understanding of the brain mechanisms important in visual creativity. In this chapter, we discuss anatomic and physiological studies, as well as neuropsychological studies of healthy artists and patients with neurological disease that have helped us gain some insight into the brain mechanisms that mediate artistic creativity.}
}
@article{CALDEIRA2025102657,
title = {Model compression techniques in biometrics applications: A survey},
journal = {Information Fusion},
volume = {114},
pages = {102657},
year = {2025},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2024.102657},
url = {https://www.sciencedirect.com/science/article/pii/S1566253524004354},
author = {Eduarda Caldeira and Pedro C. Neto and Marco Huber and Naser Damer and Ana F. Sequeira},
keywords = {Compression, Knowledge distillation, Quantization, Pruning, Biometrics, Bias},
abstract = {The development of deep learning algorithms has extensively empowered humanity’s task automatization capacity. However, the huge improvement in the performance of these models is highly correlated with their increasing level of complexity, limiting their usefulness in human-oriented applications, which are usually deployed in resource-constrained devices. This led to the development of compression techniques that drastically reduce the computational and memory costs of deep learning models without significant performance degradation. These compressed models are especially essential when implementing multi-model fusion solutions where multiple models are required to operate simultaneously. This paper aims to systematize the current literature on this topic by presenting a comprehensive survey of model compression techniques in biometrics applications, namely quantization, knowledge distillation and pruning. We conduct a critical analysis of the comparative value of these techniques, focusing on their advantages and disadvantages and presenting suggestions for future work directions that can potentially improve the current methods. Additionally, we discuss and analyze the link between model bias and model compression, highlighting the need to direct compression research toward model fairness in future works.}
}
@article{ABDELAZIZ2024100615,
title = {A scoping review of artificial intelligence within pharmacy education},
journal = {American Journal of Pharmaceutical Education},
volume = {88},
number = {1},
pages = {100615},
year = {2024},
issn = {0002-9459},
doi = {https://doi.org/10.1016/j.ajpe.2023.100615},
url = {https://www.sciencedirect.com/science/article/pii/S0002945923045539},
author = {May H. {Abdel Aziz} and Casey Rowe and Robin Southwood and Anna Nogid and Sarah Berman and Kyle Gustafson},
keywords = {Pharmacy education, Artificial intelligence, Deep learning, Machine learning},
abstract = {Objectives
This scoping review aimed to summarize the available literature on the use of artificial intelligence (AI) in pharmacy education and identify gaps where additional research is needed.
Findings
Seven studies specifically addressing the use of AI in pharmacy education were identified. Of these 7 studies, 5 focused on AI use in the context of teaching and learning, 1 on the prediction of academic performance for admissions, and the final study focused on using AI text generation to elucidate the benefits and limitations of ChatGPT use in pharmacy education.
Summary
There are currently a limited number of available publications that describe AI use in pharmacy education. Several challenges exist regarding the use of AI in pharmacy education, including the need for faculty expertise and time, limited generalizability of tools, limited outcomes data, and several legal and ethical concerns. As AI use increases and implementation becomes more standardized, opportunities will be created for the inclusion of AI in pharmacy education.}
}
@article{HE2025103404,
title = {Thermal imaging monitoring based on image texture feature analysis for simulating gymnastics teaching images in universities},
journal = {Thermal Science and Engineering Progress},
volume = {60},
pages = {103404},
year = {2025},
issn = {2451-9049},
doi = {https://doi.org/10.1016/j.tsep.2025.103404},
url = {https://www.sciencedirect.com/science/article/pii/S2451904925001945},
author = {Yuxin He and Dongcheng Ma},
keywords = {Image texture feature analysis, Thermal imaging monitoring, College gymnastics teaching, Image simulation},
abstract = {With the development of science and technology, thermal imaging technology is used as a non-contact monitoring means. This study designed and developed a gymnastics teaching image simulation system based on thermal imaging monitoring technology. The system aims to extract key image texture features by analyzing the thermal imaging images of gymnasts in the training process, so as to realize real-time monitoring and evaluation of athletes’ physical conditions. In this paper, image processing technology is used to extract key texture features from thermal imaging images, and the quantitative index reflecting athletes’ physical condition is extracted from thermal imaging images, and it is applied to the simulation of gymnastics teaching images. Through the analysis of image texture features, researchers successfully extracted the key indicators reflecting the athlete’s physical condition, and applied these indicators to the simulation of gymnastics teaching images. The experimental results show that the image simulation system based on thermal imaging monitoring can provide more accurate and objective evaluation results, which is helpful for coaches to better understand the training status of athletes and adjust the training plan in time.}
}
@article{BROCAS2022331,
title = {Adverse selection and contingent reasoning in preadolescents and teenagers},
journal = {Games and Economic Behavior},
volume = {133},
pages = {331-351},
year = {2022},
issn = {0899-8256},
doi = {https://doi.org/10.1016/j.geb.2022.03.010},
url = {https://www.sciencedirect.com/science/article/pii/S0899825622000616},
author = {Isabelle Brocas and Juan D. Carrillo},
keywords = {Developmental decision-making, Lab-in-the-field experiment, Contingent reasoning, Winner's curse},
abstract = {We study from a developmental viewpoint the ability to perform contingent reasoning and the cognitive abilities that facilitate optimal behavior. Individuals from 11 to 17 years old participate in a simplified version of the two-value, deterministic “acquire-a-company” adverse selection game (Charness and Levin, 2009; Martínez-Marquina et al., 2019). We find that even our youngest subjects understand well the basic principles of contingent reasoning (offer the reservation price of one of the sellers), although they do not necessarily choose the optimal price. Performance improves steadily and significantly over the developmental window but it is not facilitated by repeated exposure or feedback. High cognitive ability–measured by a high performance in a working memory task–is necessary to behave optimally in the simplest settings but it is not sufficient to solve the most complex situations.}
}
@article{YUAN2023107911,
title = {Surface profile evolution model for titanium alloy machined using abrasive waterjet},
journal = {International Journal of Mechanical Sciences},
volume = {240},
pages = {107911},
year = {2023},
issn = {0020-7403},
doi = {https://doi.org/10.1016/j.ijmecsci.2022.107911},
url = {https://www.sciencedirect.com/science/article/pii/S0020740322007895},
author = {Yemin Yuan and Jianfeng Chen and Hang Gao},
keywords = {Abrasive waterjet, Ti-6Al-4V alloy, Computational fluid dynamics (CFD), Stagnation zone, Surface profile evolution},
abstract = {The surface profile evolution model, which was initially developed for glass and polymers, can accurately predict a channel profile cross-section produced by abrasive jet (AJ) machining. In this study, the model is modified and applied for estimating the profiles of a Ti-6Al-4V alloy eroded by an abrasive waterjet (AWJ). First, the velocity and mass fraction distributions of the gas–liquid–solid phases in the AWJ at the nozzle exit were derived and compared, and several improvements were proposed, such as considering the divergence angle of the jet and particles, as well as the length of the jet core area, to precisely construct a theoretical connection of the erosion efficiency distribution before impacting the workpiece. Computational fluid dynamics (CFD) simulations were then performed to investigate the behaviour of the erosion jets during surface evolution. The results revealed that the jet diffusion provoked by the stagnation zone effect became more pronounced as the surface profile depth deepened, which led to jet directional deflection and suppressed the erosion capacities of the AWJ. Therefore, a central erosion depth function was introduced to correct this detrimental effect with the intention of obtaining an accurate channel profile. In addition, a second-order single-step fitting function was suggested to eliminate the fluctuations caused by uneven abrasive particles and the problem of reduced erosion efficiency due to channel depth variation. Finally, based on the determination of the parameters affecting the channel profile, a normalised centre erosion rate function, which only depends on the channel depth and is isolated from the material properties and the standoff distance, was recommended to simplify the calculation. The erosion function conforming to a Gaussian surface was fitted using MATLAB (R2019b, MathWorks, USA). The results demonstrated that the channel profiles predicted by the surface evolution model were consistent with the measured profiles, with an average error of 11.4%.}
}
@article{LOVE2017113,
title = {On languaging and languages},
journal = {Language Sciences},
volume = {61},
pages = {113-147},
year = {2017},
note = {Orders of Language: A festschrift for Nigel Love},
issn = {0388-0001},
doi = {https://doi.org/10.1016/j.langsci.2017.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S0388000117301080},
author = {Nigel Love},
keywords = {Languaging, Linguistic reflexivity, Metalanguage, Ontology of languages, Verbatim repetition, Writing},
abstract = {I consider the ontology of languages and the linguistic units said to constitute them, in the light of a speculative sketch of how languaging about language might give rise to the idea of a language. The focus is principally on the role of reflexivity and the development of writing in facilitating the decontextualisation, abstraction and reification of linguistic units and languages themselves. The main trend in modern linguistics has been to take the products of these processes as realia, and to retroject them on to languagers as the basis for their languaging activities: I touch on some of the deleterious effects of this on theorising about the acquisition, storage and production of language. Finally, I consider how in thinking about these matters the concept of different ‘orders’ of language has been and might be interpreted and deployed. Whether or not this concept has a useful role to play in formulating them, the ideas assembled here are offered in the hope that they might serve as a platform from which to debate the significance and implications of the stultifying effect our modes of metalanguaging have so far had on inquiry into our engagement with language.}
}
@article{YONGBO2025100388,
title = {Design and FPGA Implementation of a Zero IF I/Q Blind Calibration Algorithm},
journal = {Next Research},
pages = {100388},
year = {2025},
issn = {3050-4759},
doi = {https://doi.org/10.1016/j.nexres.2025.100388},
url = {https://www.sciencedirect.com/science/article/pii/S3050475925002581},
author = {Liao Yongbo and Li Lang and Li Linhan and Liang Jiangshan and Li Mengyou and Chen Rui and Chen Xiongfei and Wang Menghao and Wen Wu},
keywords = {Zero Intermediate Frequency, I/Q Imbalance, FastICA Algorithm, FPGA Implementation},
abstract = {SUMMARY
The aim of this study is to validate the implementation of a digital domain correction method on hardware with basis of the FastICA algorithm. This algorithm compensates for I/Q imbalance by separating independent components from mixed signals, while introducing differential thinking to adjust correction parameters in real-time to adapt to the processing of streaming signals. The effectiveness of the proposed algorithm was verified through simulation and hardware testing. The results showed that, under 1MHz single tone signal input and 100MHz sampling frequency, after correction, the image suppression ratio increased from 13.5dB to 55.8dB, and in hardware testing, it increased to 51.4dB. It can be seen that the image suppression module designed in this study can effectively suppress DC offset and image interference, improve the performance of zero IF transceivers, and confirm an effective I/Q imbalance correction method.}
}
@article{OSWALD2025100552,
title = {Understanding individual differences in non-ordinary state of consciousness: Relationship between phenomenological experiences and autonomic nervous system},
journal = {International Journal of Clinical and Health Psychology},
volume = {25},
number = {1},
pages = {100552},
year = {2025},
issn = {1697-2600},
doi = {https://doi.org/10.1016/j.ijchp.2025.100552},
url = {https://www.sciencedirect.com/science/article/pii/S1697260025000109},
author = {Victor Oswald and Karim Jerbi and Corine Sombrun and Annen Jitka and Charlotte Martial and Olivia Gosseries and Audrey Vanhaudenhuyse},
keywords = {Non-ordinary states of consciousness, Auto-induced cognitive trance, Heart rate variability, Phenomenological experiences, Machine learning, Inter-individual differences},
abstract = {Non-ordinary states of consciousness offer a unique opportunity to explore the interplay between phenomenological experiences and physiological processes. This study investigated individual differences in phenomenological and autonomic nervous system changes between a resting state condition and a non-ordinary state of consciousness (auto-induced cognitive trance, AICT). Specifically, it examined the relationship between self-reported experiences (e.g., absorption, visual representations) and heart rate variability (HRV). Twenty-seven participants underwent electrocardiography recordings and completed self-report questionnaires during rest and AICT. A machine learning framework distinguished the rest and AICT states based on self-reported measures and HRV metrics. A linear mixed-effects model assessed inter-individual differences in HRV and self-reported phenomenology between the two states. Finally, the relationship between relative change in HRV and self-reported experiences was explored. Results showed changes in self-reported phenomenology (accuracy=86 %; p<.001) and HRV (accuracy=73 %; p<.001) characterizing the AICT state compared to rest. The baseline level in phenomenology or HRV was associated with change amplitude during AICT. Moreover, relative change in HRV was associated with change in phenomenology. The findings suggest that inter-individual differences at rest revealed a functional mechanism between phenomenology and the autonomic nervous system during non-ordinary states of consciousness, offering a novel perspective on how physiological mechanisms shape subjective experiences.}
}
@article{BRAITHWAITE201640,
title = {Non-formal mechanisms in mathematical cognitive development: The case of arithmetic},
journal = {Cognition},
volume = {149},
pages = {40-55},
year = {2016},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2016.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S001002771630004X},
author = {David W. Braithwaite and Robert L. Goldstone and Han L.J. {van der Maas} and David H. Landy},
keywords = {Mathematical cognitive development, Concrete to abstract shift, Arithmetic, Syntax, Perception, Mathematics education},
abstract = {The idea that cognitive development involves a shift towards abstraction has a long history in psychology. One incarnation of this idea holds that development in the domain of mathematics involves a shift from non-formal mechanisms to formal rules and axioms. Contrary to this view, the present study provides evidence that reliance on non-formal mechanisms may actually increase with age. Participants – Dutch primary school children – evaluated three-term arithmetic expressions in which violation of formally correct order of evaluation led to errors, termed foil errors. Participants solved the problems as part of their regular mathematics practice through an online study platform, and data were collected from over 50,000 children representing approximately 10% of all primary schools in the Netherlands, suggesting that the results have high external validity. Foil errors were more common for problems in which formally lower-priority sub-expressions were spaced close together, and also for problems in which such sub-expressions were relatively easy to calculate. We interpret these effects as resulting from reliance on two non-formal mechanisms, perceptual grouping and opportunistic selection, to determine order of evaluation. Critically, these effects reliably increased with participants’ grade level, suggesting that these mechanisms are not phased out but actually become more important over development, even when they cause systematic violations of formal rules. This conclusion presents a challenge for the shift towards abstraction view as a description of cognitive development in arithmetic. Implications of this result for educational practice are discussed.}
}
@article{ASTLE2024105539,
title = {Understanding divergence: Placing developmental neuroscience in its dynamic context},
journal = {Neuroscience & Biobehavioral Reviews},
volume = {157},
pages = {105539},
year = {2024},
issn = {0149-7634},
doi = {https://doi.org/10.1016/j.neubiorev.2024.105539},
url = {https://www.sciencedirect.com/science/article/pii/S0149763424000071},
author = {Duncan E. Astle and Dani S. Bassett and Essi Viding},
keywords = {Development, Systems neuroscience, Neurodevelopmental condition, Mental health, Computational neuroscience},
abstract = {Neurodevelopment is not merely a process of brain maturation, but an adaptation to constraints unique to each individual and to the environments we co-create. However, our theoretical and methodological toolkits often ignore this reality. There is growing awareness that a shift is needed that allows us to study divergence of brain and behaviour across conventional categorical boundaries. However, we argue that in future our study of divergence must also incorporate the developmental dynamics that capture the emergence of those neurodevelopmental differences. This crucial step will require adjustments in study design and methodology. If our ultimate aim is to incorporate the developmental dynamics that capture how, and ultimately when, divergence takes place then we will need an analytic toolkit equal to these ambitions. We argue that the over reliance on group averages has been a conceptual dead-end with regard to the neurodevelopmental differences. This is in part because any individual differences and developmental dynamics are inevitably lost within the group average. Instead, analytic approaches which are themselves new, or simply newly applied within this context, may allow us to shift our theoretical and methodological frameworks from groups to individuals. Likewise, methods capable of modelling complex dynamic systems may allow us to understand the emergent dynamics only possible at the level of an interacting neural system.}
}
@article{DRABECK202591,
title = {Disability in ecology and evolution},
journal = {Trends in Ecology & Evolution},
volume = {40},
number = {2},
pages = {91-95},
year = {2025},
issn = {0169-5347},
doi = {https://doi.org/10.1016/j.tree.2024.12.009},
url = {https://www.sciencedirect.com/science/article/pii/S0169534724003173},
author = {Danielle Drabeck and Chris Rensing and Kat {Van der Poorten}}
}
@article{COSTABILE2021126306,
title = {A 2D-SWEs framework for efficient catchment-scale simulations: Hydrodynamic scaling properties of river networks and implications for non-uniform grids generation},
journal = {Journal of Hydrology},
volume = {599},
pages = {126306},
year = {2021},
issn = {0022-1694},
doi = {https://doi.org/10.1016/j.jhydrol.2021.126306},
url = {https://www.sciencedirect.com/science/article/pii/S002216942100353X},
author = {Pierfranco Costabile and Carmelina Costanzo},
keywords = {2D shallow water equations, Surface runoff, River networks, Non-uniform grids, Channel heads, Scaling laws},
abstract = {The application of two-dimensional shallow-water equations models (2D-SWEs) for the description of hydrodynamic-based surface runoff computations is becoming a reference approach in rainfall-runoff simulations at the catchment scale. Due to their ability in generation of flow patterns throughout the basin, they can be used not only as an advanced method for flood mapping studies and hazard assessment but also as an innovative tool for the analysis of river drainage networks, opening new perspectives for several environmental processes. In particular, in this work we put the river networks in a 2D-SWEs framework, meaning that the traditional tree-like fluvial structure, represented by a skeleton composed of a set of lines, is replaced by a collection of points discretizing the 2-D geometry of the river structure itself, for which the values of the hydrodynamic values are provided by the numerical simulations. This approach is used here to derive a new scaling property that relates the specific discharge threshold, used to identify the river network cells, to the total areas of the network cells themselves. The hydrodynamic and geomorphological interpretation of this power law function and the influence of grid resolution, on some relevant parameters of this curve, have inspired the development of a heuristic procedure for non-uniform grid generation, able to detect the most hydrodynamically active areas of the basins for which the grid refinement process makes sense. Moreover, information related to how much grid refinement is needed is provided as well. The performances of this procedure are very promising in terms of accuracy of simulated discharges, hydrodynamic behaviour of the river network and flooded areas, reducing significantly the computational times in respect to the use of fine uniform grids.}
}
@article{ROLISON2022105401,
title = {Developmental differences in description-based versus experience-based decision making under risk in children},
journal = {Journal of Experimental Child Psychology},
volume = {219},
pages = {105401},
year = {2022},
issn = {0022-0965},
doi = {https://doi.org/10.1016/j.jecp.2022.105401},
url = {https://www.sciencedirect.com/science/article/pii/S0022096522000303},
author = {Jonathan J. Rolison and Thorsten Pachur and Teresa McCormack and Aidan Feeney},
keywords = {Decision making under risk, Children, Computational modeling, Description-based decision making, Experience-based decision making, Risk taking},
abstract = {The willingness to take a risk is shaped by temperaments and cognitive abilities, both of which develop rapidly during childhood. In the adult developmental literature, a distinction is drawn between description-based tasks, which provide explicit choice–reward information, and experience-based tasks, which require decisions from past experience, each emphasizing different cognitive demands. Although developmental trends have been investigated for both types of decisions, few studies have compared description-based and experience-based decision making in the same sample of children. In the current study, children (N = 112; 5–9 years of age) completed both description-based and experience-based decision tasks tailored for use with young children. Child temperament was reported by the children’s primary teacher. Behavioral measures suggested that the willingness to take a risk in a description-based task increased with age, whereas it decreased in an experience-based task. However, computational modeling alongside further inspection of the behavioral data suggested that these opposite developmental trends across the two types of tasks both were associated with related capacities: older (vs. younger) children’s higher sensitivity to experienced losses and higher outcome sensitivity to described rewards and losses. From the temperamental characteristics, higher attentional focusing was linked with a higher learning rate on the experience-based task and a bias to accept gambles in the gain domain on the description-based task. Our findings demonstrate the importance of comparing children’s behavior across qualitatively different tasks rather than studying a single behavior in isolation.}
}
@incollection{SCHAUB2022555,
title = {Chapter 22 - Conclusions},
editor = {Michael Schaub and Marc Kéry},
booktitle = {Integrated Population Models},
publisher = {Academic Press},
pages = {555-563},
year = {2022},
isbn = {978-0-323-90810-8},
doi = {https://doi.org/10.1016/B978-0-12-820564-8.24002-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128205648240025},
author = {Michael Schaub and Marc Kéry},
keywords = {Continuous time scale, Full annual cycle integrated population model, Future developments, Individual heterogeneity, Long-term ecological research, Multispecies integrated population model, Sampling design, Spatial integrated population model, Spatial scale},
abstract = {In this final chapter, we first look back and briefly summarize what we have learned in this book. We then look forward and sketch out possible avenues of future research into integrated population models (IPMs) and where it may or should go. We especially foresee likely future developments in three areas. The first is in developing alternative formulations of the population, or process, model in an IPM, which currently is mostly a classical matrix population model. In the future, we expect to see refinements along some or all of the spatial, temporal, and individual axes of fundamental demographic information—that is, a general shift away from discrete to more continuous scales along these dimensions of the description of population dynamics. In particular, we think a more widespread “spatialization” of IPMs is imminent. We also think that IPMs for two or more species with explicit links among them will increasingly be developed because they allow the study of interactions among species at a very basic mechanistic level. The second area of likely future progress in IPMs deals with the observation model, especially the Gaussian error model in the state-space model for population counts. In a sense, this model is a misspecification that cannot explicitly account for the false-positives and false-negatives that now are so commonly included in the capture-recapture class of models. The third area where we envision future progress in IPMs is with more fundamental statistical and computational work. We expect further progress in algorithm fitting, goodness-of-fit testing, models that account for dependence among the components of joint likelihood, and the study and development of more effective sampling designs. Finally, we are excited to see many more applications of existing and future IPMs to improve our scientific conclusions and conservation and wildlife management decisions.}
}
@article{ZHU2024100138,
title = {Exploring the impact of ChatGPT on art creation and collaboration: Benefits, challenges and ethical implications},
journal = {Telematics and Informatics Reports},
volume = {14},
pages = {100138},
year = {2024},
issn = {2772-5030},
doi = {https://doi.org/10.1016/j.teler.2024.100138},
url = {https://www.sciencedirect.com/science/article/pii/S2772503024000240},
author = {Sijin Zhu and Zheng Wang and Yuan Zhuang and Yuyang Jiang and Mengyao Guo and Xiaolin Zhang and Ze Gao},
keywords = {Creative AI, HumanAI collaboration, Language models, Interactive AI literacy},
abstract = {This paper examines the chaos caused by introducing advanced language models, specifically ChatGPT, to art. Our focus is on the potential impact of ChatGPT on art creation and collaboration. We explore how it has been utilized to generate art and assist in creative writing and how it facilitates collaboration between artists. This exploration includes an investigation into the use of AI in creating art, music, and literature, emphasizing ChatGPT’s role in generating poetry and prose and its ability to provide valuable suggestions for sentence structure and word choice in creative writing. We conduct case studies and interviews with diverse artists and AI experts to understand the benefits and challenges of using ChatGPT in the creative process. Our findings reveal that artists find ChatGPT helpful in generating new ideas, overcoming creative blocks, and improving the quality of their work. It enables remote collaboration between artists by providing a real-time communication and idea-sharing platform. However, ethical concerns relating to authorship ownership and authenticity have emerged. Artists fear using ChatGPT may lead to losing their artistic identity and ownership of their work. While our data suggests that ChatGPT holds the potential to transform the art world, careful consideration must be given to the ethical implications of AI in art. We recommend future research to focus on developing guidelines for the responsible use of AI in art, safeguarding artists’ rights, and preserving artistic authenticity.}
}
@incollection{VODOVOTZ201563,
title = {Chapter 3.2 - Dynamic Knowledge Representation and the Power of Model Making},
editor = {Yoram Vodovotz and Gary An},
booktitle = {Translational Systems Biology},
publisher = {Academic Press},
address = {Boston},
pages = {63-68},
year = {2015},
isbn = {978-0-12-397884-4},
doi = {https://doi.org/10.1016/B978-0-12-397884-4.00009-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780123978844000094},
author = {Yoram Vodovotz and Gary An},
keywords = {Systems biology, mathematical modeling, computational biology, computational modeling, knowledge representation, conceptual model},
abstract = {This chapter focuses on describing the primary tool used in Translational Systems Biology: dynamic computational modeling. This chapter discusses the conceptual basis and rationale for modeling, with particular emphasis on the role of dynamic computational and mathematical models in biomedical research. We introduce the concept of using models as means of Dynamic Knowledge Representation, with the scientific target of facilitating the visualization, instantiation, evaluation, and falsification of biological hypotheses. We compare and contrast the use of modeling and simulation for this purpose versus the development and use of “engineering grade” quantitative models, noting specifically that given the state of biological knowledge, biomedical Dynamic Knowledge Representation is aimed at facilitating discovery, as opposed to the engineering goal of optimizing solutions. We discuss the fundamental step in model construction, mapping, and explain its role in the use and potential interpretation of both biological proxy models and computational models. We introduce the concept of Conceptual Model Verification, and its role as a means of accelerating the Scientific Cycle.}
}
@article{BOULGAKOV2020154,
title = {Bringing Microscopy-By-Sequencing into View},
journal = {Trends in Biotechnology},
volume = {38},
number = {2},
pages = {154-162},
year = {2020},
issn = {0167-7799},
doi = {https://doi.org/10.1016/j.tibtech.2019.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S0167779919301349},
author = {Alexander A. Boulgakov and Andrew D. Ellington and Edward M. Marcotte},
keywords = {DNA microscopy, next-generation sequencing, barcoding, localization, oligonucleotides},
abstract = {The spatial distribution of molecules and cells is fundamental to understanding biological systems. Traditionally, microscopies based on electromagnetic waves such as visible light have been used to localize cellular components by direct visualization. However, these techniques suffer from limitations of transmissibility and throughput. Complementary to optical approaches, biochemical techniques such as crosslinking can colocalize molecules without suffering the same limitations. However, biochemical approaches are often unable to combine individual colocalizations into a map across entire cells or tissues. Microscopy-by-sequencing techniques aim to biochemically colocalize DNA-barcoded molecules and, by tracking their thus unique identities, reconcile all colocalizations into a global spatial map. Here, we review this new field and discuss its enormous potential to answer a broad spectrum of questions.}
}
@article{WENZLAFF200127,
title = {Mental control after dysphoria: Evidence of a suppressed, depressive bias},
journal = {Behavior Therapy},
volume = {32},
number = {1},
pages = {27-45},
year = {2001},
issn = {0005-7894},
doi = {https://doi.org/10.1016/S0005-7894(01)80042-3},
url = {https://www.sciencedirect.com/science/article/pii/S0005789401800423},
author = {Richard M. Wenzlaff and Ann R. Eisenberg},
abstract = {Previous research has generally failed to find persistent negative thinking following a depressive episode, suggesting that negative thoughts may simply be by-products of the emotional disturbance. The present research examined the idea that a persistent depressive bias does exist, but it is obscured by thought suppression. Mental control theory suggests that suppressed thoughts can be detected by assessing cognition before the effortful process of distraction is implemented. To test this prediction, formerly dysphoric, chronically dysphoric, and nondysphoric control groups interpreted audio recordings of words—some of which included homophones with emotional alternatives relevant to depression (e.g., weak/week). Participants wrote down each word either immediately or after a 10-sec delay. Although formerly dysphoric individuals did not display a depressive bias in the delayed condition, their immediate responses revealed a depressive bias. As predicted, the emergence of a negative bias was associated with high levels of chronic thought suppression.}
}
@article{AHMADI2022101232,
title = {Energy efficiency improvement and emission reduction potential of domestic gas burners through re-orientating the angle and position of burner holes: Experimental and numerical study},
journal = {Thermal Science and Engineering Progress},
volume = {32},
pages = {101232},
year = {2022},
issn = {2451-9049},
doi = {https://doi.org/10.1016/j.tsep.2022.101232},
url = {https://www.sciencedirect.com/science/article/pii/S2451904922000397},
author = {Ali Akbar Ahmadi and Alireza Rahbari and Mostafa Mohamadi},
keywords = {Experimental and numerical study, Domestic burners, Burner geometry, Turbulent combustion, Thermal efficiency},
abstract = {The trend towards enhancing the thermal performance of domestic cooking burners necessitates developing a new design for such devices. With this picture in mind, this paper numerically and experimentally investigates the effect of burner head design configurations on the energy efficiency and CO emission of domestic gas burners. The results of a three-dimensional steady-state computational fluid dynamics (CFD) model is validated with the experimental data according to Volunteers in Technical Assistance (VITA) standard in cold start, hot start, and Simmer condition for two types of burners. Having the model validated, a step-by-step approach has been undertaken to improve the design of these reference cases, resulted in a total number of nine burner configurations analysed in this research. This is followed by determining the influence of introduced geometries on the thermal efficiency of burners. Based on the insights from the numerical model, the most efficient burner exhibits 3.3–22.2% higher thermal efficiency and 20.2–32.6% lower CO emission—depending on the gas flow rate—relative to the conventional burners. The optimised design can be implemented into existing burners with relatively little need for reconstruction.}
}
@article{COOKE2020138,
title = {Diverse perspectives on interdisciplinarity from Members of the College of the Royal Society of Canada},
journal = {FACETS},
volume = {5},
number = {1},
pages = {138-165},
year = {2020},
issn = {2371-1671},
doi = {https://doi.org/10.1139/facets-2019-0044},
url = {https://www.sciencedirect.com/science/article/pii/S2371167120000551},
author = {Steven J. Cooke and Vivian M. Nguyen and Dimitry Anastakis and Shannon D. Scott and Merritt R. Turetsky and Alidad Amirfazli and Alison Hearn and Cynthia E. Milton and Laura Loewen and Eric E. Smith and D. Ryan Norris and Kim L. Lavoie and Alice Aiken and Daniel Ansari and Alissa N. Antle and Molly Babel and Jane Bailey and Daniel M. Bernstein and Rachel Birnbaum and Carrie Bourassa and Antonio Calcagno and Aurélie Campana and Bing Chen and Karen Collins and Catherine E. Connelly and Myriam Denov and Benoît Dupont and Eric George and Irene Gregory-Eaves and Steven High and Josephine M. Hill and Philip L. Jackson and Nathalie Jette and Mark Jurdjevic and Anita Kothari and Paul Khairy and Sylvie A. Lamoureux and Kiera Ladner and Christian R. Landry and François Légaré and Nadia Lehoux and Christian Leuprecht and Angela R. Lieverse and Artur Luczak and Mark L. Mallory and Erin Manning and Ali Mazalek and Stuart J. Murray and Lenore L. Newman and Valerie Oosterveld and Patrice Potvin and Sheryl Reimer-Kirkham and Jennifer Rowsell and Dawn Stacey and Susan L. Tighe and David J. Vocadlo and Anne E. Wilson and Andrew Woolford and Jules M. Blais},
keywords = {interdisciplinarity, academic institutions, universities, funding, scholarly activity, boundary crossing, barriers},
abstract = {Various multiple-disciplinary terms and concepts (although most commonly “interdisciplinarity,” which is used herein) are used to frame education, scholarship, research, and interactions within and outside academia. In principle, the premise of interdisciplinarity may appear to have many strengths; yet, the extent to which interdisciplinarity is embraced by the current generation of academics, the benefits and risks for doing so, and the barriers and facilitators to achieving interdisciplinarity, represent inherent challenges. Much has been written on the topic of interdisciplinarity, but to our knowledge there have been few attempts to consider and present diverse perspectives from scholars, artists, and scientists in a cohesive manner. As a team of 57 members from the Canadian College of New Scholars, Artists, and Scientists of the Royal Society of Canada (the College) who self-identify as being engaged or interested in interdisciplinarity, we provide diverse intellectual, cultural, and social perspectives. The goal of this paper is to share our collective wisdom on this topic with the broader community and to stimulate discourse and debate on the merits and challenges associated with interdisciplinarity. Perhaps the clearest message emerging from this exercise is that working across established boundaries of scholarly communities is rewarding, necessary, and is more likely to result in impact. However, there are barriers that limit the ease with which this can occur (e.g., lack of institutional structures and funding to facilitate cross-disciplinary exploration). Occasionally, there can be significant risk associated with doing interdisciplinary work (e.g., lack of adequate measurement or recognition of work by disciplinary peers). Solving many of the world’s complex and pressing problems (e.g., climate change, sustainable agriculture, the burden of chronic disease, and aging populations) demands thinking and working across long-standing, but in some ways restrictive, academic boundaries. Academic institutions and key support structures, especially funding bodies, will play an important role in helping to realize what is readily apparent to all who contributed to this paper—that interdisciplinarity is essential for solving complex problems; it is the new norm. Failure to empower and encourage those doing this research will serve as a great impediment to training, knowledge, and addressing societal issues.}
}
@article{JANG2022103225,
title = {Generative Design by Reinforcement Learning: Enhancing the Diversity of Topology Optimization Designs},
journal = {Computer-Aided Design},
volume = {146},
pages = {103225},
year = {2022},
issn = {0010-4485},
doi = {https://doi.org/10.1016/j.cad.2022.103225},
url = {https://www.sciencedirect.com/science/article/pii/S0010448522000239},
author = {Seowoo Jang and Soyoung Yoo and Namwoo Kang},
keywords = {Generative design, Topology optimization, Deep learning, Reinforcement learning, Design diversity},
abstract = {Generative design refers to computational design methods that can automatically conduct design exploration under constraints defined by designers. Among many approaches, topology optimization-based generative designs aim to explore diverse topology designs, which cannot be represented by conventional parametric design approaches. Recently, data-driven topology optimization research has started to exploit artificial intelligence, such as deep learning or machine learning, to improve the capability of design exploration. This study proposes a reinforcement learning (RL) based generative design process, with reward functions maximizing the diversity of topology designs. We formulate generative design as a sequential problem of finding optimal design parameter combinations in accordance with a given reference design. Proximal Policy Optimization is used as the learning framework, which is demonstrated in the case study of an automotive wheel design problem. To reduce the heavy computational burden of the wheel topology optimization process required by our RL formulation, we approximate the optimization process with neural networks. With efficient data preprocessing/augmentation and neural architecture, the neural networks achieve a generalized performance and symmetricity-reserving characteristics. We show that RL-based generative design produces a large number of diverse designs within a short inference time by exploiting GPU in a fully automated manner. It is different from the previous approach using CPU which takes much more processing time and involving human intervention.}
}
@incollection{DALY20173,
title = {8.02 - Molecular Logic Gates as Fluorescent Sensors},
editor = {Jerry L. Atwood},
booktitle = {Comprehensive Supramolecular Chemistry II},
publisher = {Elsevier},
address = {Oxford},
pages = {3-19},
year = {2017},
isbn = {978-0-12-803199-5},
doi = {https://doi.org/10.1016/B978-0-12-409547-2.12626-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780124095472126265},
author = {B. Daly and V.A.D. Silverson and C.Y. Yao and Z.Q. Chen and A.P. {de Silva}},
keywords = {AND Logic, Fluorescent Sensors, IMPLICATION Logic, INHIBIT Logic, Intracellular AND Logic, Logic Gates, NAND Logic, XOR and XNOR Logic, YES Logic},
abstract = {Some recent developments in the use of molecular logic gates as fluorescent sensors are described. The discussion is classified in terms of the Boolean logical assignment of the sensor system. Even simple fluorescent sensors can be recognized as single-input logic gates. Several YES gates launch the analysis of examples. A consideration of various sensors driven by double inputs and higher multiple inputs then follows. Attention is particularly drawn to the appearance of double-input logical sensor molecules, which successfully operate within living cells—a milieu where conventional semiconductor-based logic devices would struggle on the grounds of compatibility and size. The value of molecular logical thinking in the understanding of fluorescent sensor behavior is emphasized throughout.}
}
@article{NAWAZ2024121481,
title = {CoffeeNet: A deep learning approach for coffee plant leaves diseases recognition},
journal = {Expert Systems with Applications},
volume = {237},
pages = {121481},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.121481},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423019838},
author = {Marriam Nawaz and Tahira Nazir and Ali Javed and Sherif {Tawfik Amin} and Fathe Jeribi and Ali Tahir},
keywords = {CenterNet, Coffee plant disease, Classification, Deep learning, ResNet},
abstract = {Coffee is regarded as the highest consumed drink around the globe and has accounted as a major source of income in the regions where it is cultivated. To meet the coffee marketplace's requirements around the globe, cultivators must boost and analyze its cultivation and quality. Several factors like environmental changes and plant diseases are the major hindrance to increasing the yield of coffee. The development in the field of computer vision has facilitated the earliest diagnostic of diseased plant samples, however, the incidence of various image distortions i.e., color, light, size, orientation changes, and similarity in the healthy and diseased portions of examined samples are the major challenges in the effective recognition of various coffee plant leaf infections. The proposed work is focused to overwhelm the mentioned limitations by proposing a novel and effective DL model called the CoffeeNet. Explicitly, an improved CenterNet approach is proposed by introducing spatial-channel attention strategy-based ResNet-50 model for the computation of deep and disease-specific sample characteristics which are then classified by the 1-step detector of the CenterNet framework. We investigated the localization and cataloging outcomes of the suggested method on the Arabica coffee leaf repository which contains the images captured in the more realistic and complicated environmental constraints. The CoffeeNet model acquires a classification accuracy number of 98.54%, along with an mAP of 0.97 that is presenting the usefulness of our technique in localizing and categorizing various sorts of coffee plant leaf disorders.}
}
@article{PANG201667,
title = {A hierarchical alternative updated adaptive Volterra filter with pipelined architecture},
journal = {Digital Signal Processing},
volume = {56},
pages = {67-78},
year = {2016},
issn = {1051-2004},
doi = {https://doi.org/10.1016/j.dsp.2016.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S1051200416000506},
author = {Yanjie Pang and Jiashu Zhang},
keywords = {Nonlinear filter, Hierarchical pipelined structure, Alternative update mechanism, Volterra filter},
abstract = {The pipelined adaptive Volterra filters (PAVFs) with a two-layer structure constitute a class of good low-complexity filters. They can efficiently reduce the computational complexity of the conventional adaptive Volterra filter. Their major drawbacks are low convergence rate and high steady-state error caused by the coupling effect between the two layers. In order to remove the coupling effect and improve the performance of PAVFs, we present a novel hierarchical pipelined adaptive Volterra filter (HPAVF)-based alternative update mechanism. The HPAVFs with hierarchical decoupled normalized least mean square (HDNLMS) algorithms are derived to adaptively update weights of its nonlinear and linear subsections. The computational complexity of HPAVF is also analyzed. Simulations of nonlinear system adaptive identification, nonlinear channel equalization, and speech prediction show that the proposed HPAVF with different independent weight vectors in nonlinear subsection has superior performance to conventional Volterra filters, diagonally truncated Volterra filters, and PAVFs in terms of initial convergence, steady-state error, and computational complexity.}
}
@article{LI2023106560,
title = {High energy capacity or high power rating: Which is the more important performance metric for battery energy storage systems at different penetrations of variable renewables?},
journal = {Journal of Energy Storage},
volume = {59},
pages = {106560},
year = {2023},
issn = {2352-152X},
doi = {https://doi.org/10.1016/j.est.2022.106560},
url = {https://www.sciencedirect.com/science/article/pii/S2352152X2202549X},
author = {Mingquan Li and Rui Shan and Ahmed Abdulla and Jialin Tian and Shuo Gao},
keywords = {Energy storage, Energy-to-power ratio (EPR), Decarbonization, Carbon emissions, Renewable integration, Low-carbon transition},
abstract = {Studies exploring the role and value of energy storage in deep decarbonization often overlook the balance between the energy capacity and the power rating of storage systems—a key performance parameter that can affect every part of storage operation. Here, we quantitatively evaluate the system-wide impacts of battery storage systems with various energy-to-power ratios (EPRs) and at different levels of renewable penetration. We take Jiangsu province in China as our case study, due to its high electricity consumption and aggressive renewable energy targets. Our results show the evolving role of storage: as renewable penetration increases, higher EPRs are favored, as they lead to system-wide cost reductions, lower GHG emissions, and higher power system reliability. Whereas existing studies make exogenous assumptions about the lifetime of storage, we show that lifetimes across EPRs and renewable scenarios span 10 to 20 years. Existing research can thus send false signals to investors and grid planners, delaying the deployment of storage and retarding the energy transition. By showing how different EPRs yield different benefits at different stages of the energy transition, our results help investors, policy makers, and system planners design forward-thinking and dynamic policies that encourage prudent storage uptake.}
}
@incollection{POULTON20013,
title = {Chapter 1 A brief history},
editor = {Mary M. Poulton},
series = {Handbook of Geophysical Exploration: Seismic Exploration},
publisher = {Pergamon},
volume = {30},
pages = {3-18},
year = {2001},
booktitle = {Computational neural networks for geophysical data processing},
issn = {0950-1401},
doi = {https://doi.org/10.1016/S0950-1401(01)80015-X},
url = {https://www.sciencedirect.com/science/article/pii/S095014010180015X},
author = {Mary M. Poulton},
abstract = {Publisher Summary
Computational neural networks are not just the grist of science fiction writers anymore nor are they a temporary success that will soon fade from use. The field of computational neural networks has matured in the last decade and found so many industrial applications that the notion of using a neural network to solve a particular problem no longer needs a “sales pitch” to management in many companies. Neural networks are now being routinely used in process control, manufacturing, quality control, product design, financial analysis, fraud detection, loan approval, voice and handwriting recognition, and data mining to name just a few application areas. The resurgence of neural network research is often attributed to the publication of a nonlinear network algorithm that overcame many of the limitations of the Perceptron and ADALINE. Many industrial applications of neural networks can claim significant increases in productivity, reduced costs, improved quality, or new products. This chapter present neural networks to the geophysicists as a serious computational tool—a tool with great potential and great limitations.}
}
@article{RASHEED201686,
title = {Theoretical accounts to practical models: Grounding phenomenon for abstract words in cognitive robots},
journal = {Cognitive Systems Research},
volume = {40},
pages = {86-98},
year = {2016},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2016.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S1389041715300310},
author = {Nadia Rasheed and Shamsudin H.M. Amin and U. Sultana and Rabia Shakoor and Naila Zareen and Abdul Rauf Bhatti},
keywords = {Grounded cognition, Symbol grounding problem, Cognitive robotics, Connectionist computation},
abstract = {This review concentrates on the issue of acquisition of abstract words in a cognitive robot with the grounding principle, from relevant theories to practical models of agents and robots. Most cognitive robotics models developed for grounding of language take inspiration from the findings of neuroscience and psychology to get the theoretical skeleton of these models. To better understand these modelling approaches, it is indispensable to work from the base (theoretical accounts) to the top (computational models). Therefore in this paper, succinct definition of abstract words is presented first, and then the symbol grounding issue and accounts of grounded cognition for abstract words are given. The next section discusses the computational modelling approaches for abstract words grounding phenomenon. Finally, important cognitive robotics models are reviewed. This paper also points out the strengths and weaknesses of relevant hypotheses and models for the representation of abstract words in the grounded cognition framework and helps the understanding of issues such as where and why modelling efforts stand to address this problem in comparison with theoretical findings.}
}
@article{COLOMBINI2022104631,
title = {Safety evaluations on unignited high-pressure methane jets impacting a spherical obstacle},
journal = {Journal of Loss Prevention in the Process Industries},
volume = {74},
pages = {104631},
year = {2022},
issn = {0950-4230},
doi = {https://doi.org/10.1016/j.jlp.2021.104631},
url = {https://www.sciencedirect.com/science/article/pii/S0950423021002400},
author = {Cristian Colombini and Edoardo Carminati and Andrea Parisi and Renato Rota and Valentina Busini},
keywords = {High-pressure release, Methane, Spherical obstacle influence, Risk assessment, CFD, Analytical correlation},
abstract = {Nowadays methane is a fossil fuel widely used both in industries and in civil appliances. From the safety point of view, due to its flammability, its use implies hazards for people and assets. The hazardous area related to a high-pressure jet of methane arising from an accidental loss of containment requires the estimation of the distance at which the methane concentration falls below the Lower Flammability limit. Such a topic is well covered in the literature when considering free jet conditions, i.e., jets that do not interact with any equipment or surface. The same cannot be said for high pressure jets impacting an obstacle. In this context, the present work focuses on studying high pressure methane jets impacting spherical obstacles by means of Computational Fluid Dynamics with the aim of giving some insights about such a jet-obstacle interaction, possibly providing a brief by-hand procedure that, only based on known scenario information, allows to estimate the maximum extent of the unignited high-pressure jet when interacting with a spherical obstacle.}
}
@article{MIENYE2025181,
title = {ChatGPT in Education: A Review of Ethical Challenges and Approaches to Enhancing Transparency and Privacy},
journal = {Procedia Computer Science},
volume = {254},
pages = {181-190},
year = {2025},
note = {International Conference on Digital Sovereignty (ICDS)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2025.02.077},
url = {https://www.sciencedirect.com/science/article/pii/S1877050925004272},
author = {Ibomoiye Domor Mienye and Theo G. Swart},
keywords = {ChatGPT, Education, Ethics, LLMs, Privacy, Transparency},
abstract = {The integration of ChatGPT and large language models (LLMs) into education has created new possibilities for personalized learning, tutoring, and automation of administrative tasks. However, these advancements also present ethical challenges. This paper critically examines the ethical implications of deploying ChatGPT in educational settings, with a focus on data privacy, the opaque nature of AI decision-making, and the risks of biased outputs. To address these issues, we outline actionable approaches, including Explainable AI (XAI) techniques and privacy-preserving strategies, aimed at enabling transparency and protecting student data. We also outline frameworks that support human oversight and governance to maintain trust and accountability in Al-driven educational tools.}
}
@article{BENTO2025120579,
title = {Risk analysis in ocean and maritime engineering},
journal = {Ocean Engineering},
volume = {322},
pages = {120579},
year = {2025},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2025.120579},
url = {https://www.sciencedirect.com/science/article/pii/S002980182500294X},
author = {Ana Margarida Bento and Tiago Fazeres-Ferradosa and Paulo Rosa-Santos and Francisco Taveira-Pinto}
}
@article{VARGA202491,
title = {Foundations of Programmable Process Structures for the unified modeling and simulation of agricultural and aquacultural systems},
journal = {Information Processing in Agriculture},
volume = {11},
number = {1},
pages = {91-108},
year = {2024},
issn = {2214-3173},
doi = {https://doi.org/10.1016/j.inpa.2022.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S2214317322000737},
author = {Monika Varga and Bela Csukas},
keywords = {Unified process model, Meta-prototype-based architecture, Transition-based structure representation, Locally programmable functionality prototypes, Agricultural systems, Aquacultural systems},
abstract = {This research paper defines the theoretical foundations and computational implementation of a non-conventional modeling and simulation methodology, inspired by the needs of problem solving for biological, agricultural, aquacultural and environmental systems. The challenging practical problem is to develop a framework for automatic generation of causally right and balance-based, unified models that can also be applied for the effective coupling amongst the various (sophisticated field-specific, sensor data processing-based, upper level optimization-driven, etc.) models. The scientific problem addressed in this innovation is to develop Programmable Process Structures (PPS) by combining functional basis of systems theory, structural approach of net theory and computational principles of agent based modeling. PPS offers a novel framework for the automatic generation of easily extensible and connectible, unified models for the underlying complex systems. PPS models can be generated from one state and one transition meta-prototypes and from the transition oriented description of process structure. The models consist of unified state and transition elements. The local program containing prototype elements, derived also from the meta-prototypes, are responsible for the case-specific calculations. The integrity and consistency of PPS architecture are based on the meta-prototypes, prepared to distinguish between the conservation-laws-based measures and the signals. The simulation is based on data flows amongst the state and transition elements, as well as on the unification based data transfer between these elements and their calculating prototypes. This architecture and its AI language-based (Prolog) implementation support the integration of various field- and task-specific models, conveniently. The better understanding is helped by a simple example. The capabilities of the recently consolidated general methodology are discussed on the basis of some preliminary applications, focusing on the recently studied agricultural and aquacultural cases.}
}
@article{R2023100760,
title = {Stellar parameter estimation in O-type stars using artificial neural networks},
journal = {Astronomy and Computing},
volume = {45},
pages = {100760},
year = {2023},
issn = {2213-1337},
doi = {https://doi.org/10.1016/j.ascom.2023.100760},
url = {https://www.sciencedirect.com/science/article/pii/S2213133723000756},
author = {M. Flores R. and L.J. Corral and C.R. Fierro-Santillán and S.G. Navarro},
keywords = {Methods: Data analysis, Deep learning, Stars: Fundamental parameters, Astronomical databases: Miscellaneous},
abstract = {This work presents the results of the implementation of a deep learning system capable of estimating the effective temperature and surface gravity of O-type stars. The proposed system was trained with a database of 5,557 synthetic spectra computed with the stellar atmosphere code CMFGEN that covers stars with Teff from ∼20,000 K to ∼58,000 K, log(L/L⊙) from 4.3 to 6.3 dex, logg from 2.4 to 4.2 dex, and mass from 9 to 120 M⊙. Important advantages proposed in this paper include using a set of equivalent width measurements over the optical region of the stellar spectra, which avoids processing the full spectra with the inherent computational cost and allows it to apply the same trained system over different spectra resolutions. The validation of the system was performed by processing a sample of twenty O-type stars taken from the IACOB database, and a subgroup of eleven stars of those twenty taken from The Galactic O-Star Spectroscopic Catalog (GOSC) with lower resolution. As complementary work, we show the results of a synthetic spectra fitting process with the aim of simplifying the comparison with other estimations and parameter fitting from the literature.}
}
@article{BURIGANA2024102875,
title = {Bayesian networks and knowledge structures in cognitive assessment: Remarks on basic comparable aspects},
journal = {Journal of Mathematical Psychology},
volume = {123},
pages = {102875},
year = {2024},
issn = {0022-2496},
doi = {https://doi.org/10.1016/j.jmp.2024.102875},
url = {https://www.sciencedirect.com/science/article/pii/S0022249624000440},
author = {Luigi Burigana},
keywords = {Knowledge assessment, Knowledge structure, Bayesian network, Probabilistic graphical model, Probabilistic inference},
abstract = {Two theories of current interest and of mathematical and computational substance concerning knowledge assessment in education are discussed. These are the theory of knowledge structures and the theory of Bayesian networks as specifically related to educational assessment. In four separate sections, the two theories are compared by considering the sets of variables involved in their models, the set-theoretical and relational constructs defined on those variables, the probabilistic assumptions and properties, and the problems addressed by the theories in constructing their models. For the comparison, a common-base system of symbols and terms is adopted, which overcomes the peculiarities of expression in the corresponding streams of literature. This system gives us a better recognition of the similarities and differences between the two paradigms, and a precise appreciation of their arguments and abilities.}
}
@article{MORISHITA2023102079,
title = {Data assimilation and control system for adaptive model predictive control},
journal = {Journal of Computational Science},
volume = {72},
pages = {102079},
year = {2023},
issn = {1877-7503},
doi = {https://doi.org/10.1016/j.jocs.2023.102079},
url = {https://www.sciencedirect.com/science/article/pii/S1877750323001394},
author = {Y. Morishita and S. Murakami and M. Yokoyama and G. Ueno},
keywords = {Data assimilation, Model-based control, Fusion plasma, ASTI},
abstract = {Model-based control of complex systems is a challenging task, particularly when the system model involves many uncertain elements. To achieve model predictive control of complex systems, we require a method that sequentially reduces uncertainties in the system model using observations and estimates control inputs under the model uncertainties. In this work, we propose an extended data assimilation framework, named data assimilation and control system (DACS), to integrate data assimilation and optimal control-input estimation. The DACS framework comprises a prediction step and three filtering steps and provides adaptive model predictive control algorithms. Since the DACS framework does not require additional prediction steps, the framework can even be applied to a large system in which iterative model prediction is prohibitive due to computational burden. Through numerical experiments in controlling virtual (numerically created) fusion plasma, we demonstrate the effectiveness of DACS and reveal the characteristics of the control performance related to the choice of hyper parameters and the discrepancies between the system model and the real system.}
}
@article{ZU2023107200,
title = {Random walk numerical scheme for the steady-state of stochastic differential equations},
journal = {Communications in Nonlinear Science and Numerical Simulation},
volume = {121},
pages = {107200},
year = {2023},
issn = {1007-5704},
doi = {https://doi.org/10.1016/j.cnsns.2023.107200},
url = {https://www.sciencedirect.com/science/article/pii/S1007570423001181},
author = {Jian Zu},
keywords = {Continuous-time random walk, Stochastic differential equation, Steady state, Invariant distribution},
abstract = {The continuous-time random walk (CTRW) scheme is a time-continuous and space-discretization method to obtain the numerical solution of stochastic differential equations (SDEs). Compared with the traditional time-discretization scheme, it has the advantages of numerical stability and can alleviate the curse of dimensionality. This paper proposes an improved version of the CTRW scheme for the numerical solution of SDEs. By compensating the artificial diffusion caused by the Poisson approximation of the drift term of the SDE, the improved CTRW scheme has significantly better performance in the weak noise case, especially in approximating the invariant probability measure. Numerical studies show that the improved CTRW scheme has more accuracy than the existing one but takes less computation time. In addition, it has better accuracy of the mean holding time. We also modify the hybrid Fokker–Planck solver proposed for the CTRW scheme to compute the invariant probability measure.}
}
@article{OVERLAN2017320,
title = {Learning abstract visual concepts via probabilistic program induction in a Language of Thought},
journal = {Cognition},
volume = {168},
pages = {320-334},
year = {2017},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2017.07.005},
url = {https://www.sciencedirect.com/science/article/pii/S0010027717302020},
author = {Matthew C. Overlan and Robert A. Jacobs and Steven T. Piantadosi},
keywords = {Concept learning, Visual learning, Language of Thought, Computational modeling, Behavioral experiment},
abstract = {The ability to learn abstract concepts is a powerful component of human cognition. It has been argued that variable binding is the key element enabling this ability, but the computational aspects of variable binding remain poorly understood. Here, we address this shortcoming by formalizing the Hierarchical Language of Thought (HLOT) model of rule learning. Given a set of data items, the model uses Bayesian inference to infer a probability distribution over stochastic programs that implement variable binding. Because the model makes use of symbolic variables as well as Bayesian inference and programs with stochastic primitives, it combines many of the advantages of both symbolic and statistical approaches to cognitive modeling. To evaluate the model, we conducted an experiment in which human subjects viewed training items and then judged which test items belong to the same concept as the training items. We found that the HLOT model provides a close match to human generalization patterns, significantly outperforming two variants of the Generalized Context Model, one variant based on string similarity and the other based on visual similarity using features from a deep convolutional neural network. Additional results suggest that variable binding happens automatically, implying that binding operations do not add complexity to peoples’ hypothesized rules. Overall, this work demonstrates that a cognitive model combining symbolic variables with Bayesian inference and stochastic program primitives provides a new perspective for understanding people’s patterns of generalization.}
}
@incollection{SHERIDAN201023,
title = {Chapter 2 - The System Perspective on Human Factors in Aviation},
editor = {Eduardo Salas and Dan Maurino},
booktitle = {Human Factors in Aviation (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {San Diego},
pages = {23-63},
year = {2010},
isbn = {978-0-12-374518-7},
doi = {https://doi.org/10.1016/B978-0-12-374518-7.00002-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012374518700002X},
author = {Thomas B. Sheridan},
abstract = {Publisher Summary
This chapter reviews the system perspective in terms of its origins and fundamental quantitative ideas. An appreciation of these basic concepts adds rigor to analysis and synthesis of human-machine systems, and in particular to such systems in aviation. The chapter represents an effort to remind the reader of the meaning of “system,” where it comes from, and what it implies for research, design, construction, operation, and evaluation in aviation, especially with regard to the human role in aviation. Human factors professionals, pilots, and operational personnel in air traffic management and related practitioners who know about “systems” only as a general and often vague term for something complex can benefit from knowing a bit of the history, the people, and the quantitative substance that underlies the terminology. The chapter begins by defining what is meant by a system, then discusses the history of the idea, the major contributors and what they contributed, and what made the systems idea different from previous ideas in technology. It goes on to give examples of systems thinking applied to design, development, and manufacturing of aviation systems in consideration of the people involved. Salient system models such as control, decision, information, and reliability are then explicated.}
}
@article{HUEY2022101211,
title = {Assessing the impact of standards-based grading policy changes on student performance and practice work completion in secondary mathematics},
journal = {Studies in Educational Evaluation},
volume = {75},
pages = {101211},
year = {2022},
issn = {0191-491X},
doi = {https://doi.org/10.1016/j.stueduc.2022.101211},
url = {https://www.sciencedirect.com/science/article/pii/S0191491X22000888},
author = {Maryann E. Huey and Patrick R. Silvey and Amy G. Vaughan and Asa L. Fisher},
keywords = {Grading, Standards-based grading, Mathematics, Secondary, Motivation, High-achieving students},
abstract = {We report upon an intervention study conducted over two academic calendar years involving high-achieving, grade 8 and 9 students (n = 122 and 123 respectively) enrolled in a year-long geometry course. The study assesses the impact of a change in grading policy, namely removing practice work from grade computations, on student performance levels and behaviors. After the change in grading policy was implemented, findings reveal that performance decreased on some, but not all, standards assessed. Completion rates of practice work also decreased overall. Potential causes are discussed as well as implications for implementing aspects of standards-based grading systems in secondary mathematics classrooms.}
}
@article{MOGLIA2017173,
title = {A review of Agent-Based Modelling of technology diffusion with special reference to residential energy efficiency},
journal = {Sustainable Cities and Society},
volume = {31},
pages = {173-182},
year = {2017},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2017.03.006},
url = {https://www.sciencedirect.com/science/article/pii/S2210670716306813},
author = {Magnus Moglia and Stephen Cook and James McGregor},
keywords = {Agent-Based Modelling, Diffusion of innovation, HVAC, Lighting, Appliances},
abstract = {Residential energy efficiency is an important strategy for reducing greenhouse gas emissions. There are many technologies that help improve residential energy efficiency, and in fact, increased energy efficiency has already helped reduce global greenhouse gas emissions significantly in the past. However, with greater innovation, further improvements can be made and improving energy efficiency is an ongoing activity. Policymakers around the world are putting strategies in place to speed up the adoption of energy efficient technologies and practices, but ultimately this process is based on choice by residents themselves. Human decision making and choice however is a very complex issue, and complex computational tools are required in order to analyse and/or predict the impact of various policies. Traditionally, equation-based models such as Bass and Choice models have been used to describe the diffusion of technologies in a population, but certain limitations have been identified. This article explores what these limitations are in the context of energy efficient residential technologies and how an alternative computational and empirical paradigm, Agent-Based Modelling (ABM), can help resolve some of these limitations. As such, this is a review article into how ABM can support analysis of strategies to catalyse greater uptake of energy efficiency in the residential sector.}
}
@article{XING2024103704,
title = {Financial risk tolerance profiling from text},
journal = {Information Processing & Management},
volume = {61},
number = {4},
pages = {103704},
year = {2024},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2024.103704},
url = {https://www.sciencedirect.com/science/article/pii/S0306457324000645},
author = {Frank Xing},
keywords = {Artificial intelligence in finance, Risk tolerance, Risk profiling, Text mining, Convolutional neural network},
abstract = {Traditionally, individual financial risk tolerance information is gathered via questionnaires or similar structured psychometric tools. Our abundant digital footprint, as an unstructured alternative, is less investigated. Leveraging such information can potentially support large-scale and cost-efficient financial services. Therefore, I explore the possibility of building a computational model that distills risk tolerance information from user texts in this study, and discuss the design principles discovered from empirical results and their implications. Specifically, a new quaternary classification task is defined for text mining-based risk profiling. Experiments show that pre-trained large language models set a baseline micro-F1 of circa 0.34. Using a convolutional neural network (CNN), the reported system achieves a micro-F1 of circa 0.51, which significantly outperforms the baselines, and is a circa 4% further improvement over the standard CNN configurations (micro-F1 of circa 0.47). Textual feature richness and supervised learning are found to be the key contributors to model performances, while other machine learning strategies suggested by previous research (data augmentation and multi-tasking) are less effective. The findings confirm user texts to be a useful risk profiling resource and provide several insights on this task.}
}
@article{CHEEMA2022100123,
title = {Augmented Intelligence to Identify Patients With Advanced Heart Failure in an Integrated Health System},
journal = {JACC: Advances},
volume = {1},
number = {4},
pages = {100123},
year = {2022},
issn = {2772-963X},
doi = {https://doi.org/10.1016/j.jacadv.2022.100123},
url = {https://www.sciencedirect.com/science/article/pii/S2772963X22001739},
author = {Baljash Cheema and R. Kannan Mutharasan and Aditya Sharma and Maia Jacobs and Kaleigh Powers and Susan Lehrer and Firas H. Wehbe and Jason Ronald and Lindsay Pifer and Jonathan D. Rich and Kambiz Ghafourian and Anjan Tibrewala and Patrick McCarthy and Yuan Luo and Duc T. Pham and Jane E. Wilcox and Faraz S. Ahmad},
keywords = {advanced heart failure, artificial intelligence, augmented intelligence, electronic health record, integrated healthcare system, machine learning},
abstract = {Background
Timely referral for specialist evaluation in patients with advanced heart failure (HF) is a Class 1 recommendation. However, the transition from stage C HF to advanced or stage D HF often goes undetected in routine care, resulting in delayed referral and higher mortality rates.
Objectives
The authors sought to develop an augmented intelligence-enabled workflow using machine learning to identify patients with stage D HF and streamline referral.
Methods
We extracted data on HF patients with encounters from January 1, 2007, to November 30, 2020, from a HF registry within a regional, integrated health system. We created an ensemble machine learning model to predict stage C or stage D HF and integrated the results within the electronic health record.
Results
In a retrospective data set of 14,846 patients, the model had a good positive predictive value (60%) and low sensitivity (25%) for identifying stage D HF in a 100-person, physician-reviewed, holdout test set. During prospective implementation of the workflow from April 1, 2021, to February 15, 2022, 416 patients were reviewed by a clinical coordinator, with agreement between the model and the coordinator in 50.3% of stage D predictions. Twenty-four patients have been scheduled for evaluation in a HF clinic, 4 patients started an evaluation for advanced therapies, and 1 patient received a left ventricular assist device.
Conclusions
An augmented intelligence-enabled workflow was integrated into clinical operations to identify patients with advanced HF. Endeavors such as this require a multidisciplinary team with experience in design thinking, informatics, quality improvement, operations, and health information technology, as well as dedicated resources to monitor and improve performance over time.}
}
@article{BARTH2009441,
title = {Children’s multiplicative transformations of discrete and continuous quantities},
journal = {Journal of Experimental Child Psychology},
volume = {103},
number = {4},
pages = {441-454},
year = {2009},
note = {Special Issue: Typical Development of Numerical Cognition},
issn = {0022-0965},
doi = {https://doi.org/10.1016/j.jecp.2009.01.014},
url = {https://www.sciencedirect.com/science/article/pii/S0022096509000289},
author = {Hilary Barth and Andrew Baron and Elizabeth Spelke and Susan Carey},
keywords = {Ratio sensitivity, Ratios, Multiplicative operations, Doubling, Halving, Numerical cognition},
abstract = {Recent studies have documented an evolutionarily primitive, early emerging cognitive system for the mental representation of numerical quantity (the analog magnitude system). Studies with nonhuman primates, human infants, and preschoolers have shown this system to support computations of numerical ordering, addition, and subtraction involving whole number concepts prior to arithmetic training. Here we report evidence that this system supports children’s predictions about the outcomes of halving and perhaps also doubling transformations. A total of 138 kindergartners and first graders were asked to reason about the quantity resulting from the doubling or halving of an initial numerosity (of a set of dots) or an initial length (of a bar). Controls for dot size, total dot area, and dot density ensured that children were responding to the number of dots in the arrays. Prior to formal instruction in symbolic multiplication, division, or rational number, halving (and perhaps doubling) computations appear to be deployed over discrete and possibly continuous quantities. The ability to apply simple multiplicative transformations to analog magnitude representations of quantity may form a part of the toolkit that children use to construct later concepts of rational number.}
}
@article{HOGAN200855,
title = {Advancing the dialogue between inner and outer empiricism: A comment on O’Nualláin},
journal = {New Ideas in Psychology},
volume = {26},
number = {1},
pages = {55-68},
year = {2008},
issn = {0732-118X},
doi = {https://doi.org/10.1016/j.newideapsych.2007.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S0732118X07000293},
author = {Michael J. Hogan},
keywords = {Consciousness, Inner empiricism, Outer empiricism, Evolution, No-mind, Mythos, Logos},
abstract = {In a recent contribution to New Ideas in Psychology, Seán O’Nualláin draws out a distinction between inner and outer empiricism, and suggests that consciousness research can benefit from analysis in both directions, that is, via the exploration of facts and relations that facilitate a third-person understanding of consciousness (by reference to an analysis of the structures, processes, and functions of the brain) and via the direct exploration of conscious experience itself, both in terms of its computational (content filled) and non-computational (content empty) aspects. In positing a substrate of subjectivity independent of the contents of consciousness (and, more specifically, a state of “nothingness”), Ò’Nualláin follows a long tradition deeply rooted in mythical, religious, and esoteric schools of belief and practice. Although there is considerable debate amongst philosophers, psychologists, and neuroscientists as to whether or not a non-computational view of consciousness is viable, O’Nualláin accepts that such a possibility does exist. Further, he suggests that a dialogue between the inner and outer empiricists will be fruitful. In this comment I, critique Ò’Nualláin's initial thoughts on the subject and draw out a series of useful distinctions that will help to advance the dialogue between inner and outer empiricism. Critical amongst these distinctions is explicit reference to (1) ontological and epistemological interdependencies in consciousness research, and (2) states of consciousness that describe the transition from “mindfulness” through “nothingness” to “no-mind”.}
}
@article{HYLAND2007437,
title = {The Category Theoretic Understanding of Universal Algebra: Lawvere Theories and Monads},
journal = {Electronic Notes in Theoretical Computer Science},
volume = {172},
pages = {437-458},
year = {2007},
note = {Computation, Meaning, and Logic: Articles dedicated to Gordon Plotkin},
issn = {1571-0661},
doi = {https://doi.org/10.1016/j.entcs.2007.02.019},
url = {https://www.sciencedirect.com/science/article/pii/S1571066107000874},
author = {Martin Hyland and John Power},
keywords = {Universal algebra, Lawvere theory, monad, computational effect},
abstract = {Lawvere theories and monads have been the two main category theoretic formulations of universal algebra, Lawvere theories arising in 1963 and the connection with monads being established a few years later. Monads, although mathematically the less direct and less malleable formulation, rapidly gained precedence. A generation later, the definition of monad began to appear extensively in theoretical computer science in order to model computational effects, without reference to universal algebra. But since then, the relevance of universal algebra to computational effects has been recognised, leading to renewed prominence of the notion of Lawvere theory, now in a computational setting. This development has formed a major part of Gordon Plotkin's mature work, and we study its history here, in particular asking why Lawvere theories were eclipsed by monads in the 1960's, and how the renewed interest in them in a computer science setting might develop in future.}
}
@article{BAUCELLS201329,
title = {Guided decisions processes},
journal = {EURO Journal on Decision Processes},
volume = {1},
number = {1},
pages = {29-44},
year = {2013},
issn = {2193-9438},
doi = {https://doi.org/10.1007/s40070-013-0003-8},
url = {https://www.sciencedirect.com/science/article/pii/S2193943821000108},
author = {Manel Baucells and Rakesh K. Sarin},
keywords = {Decision analysis, Behavioral decision making, Narrow bracket, Insurance, Multi-attribute decisions},
abstract = {The heuristics and bias research program has convincingly demonstrated that our judgments and choices are prone to systematic errors. Decision analysis requires coherent judgments about beliefs (probabilities) and tastes (utilities), and a rational procedure to combine them so that choices maximize subjective expected utility. A guided decision process is a middle-of-the-road between decision analysis and intuitive judgments in which the emphasis is on improving decisions through simple decision rules. These rules reduce cost of thinking, or decision effort, for the myriad decisions that one faces in daily life; but at the same time, they are personalized to the individual and produce near optimal choices. We discuss the principles behind the guided decision processes research program, and illustrate the approach using several examples.}
}
@article{DORR2017322,
title = {Common errors in reasoning about the future: Three informal fallacies},
journal = {Technological Forecasting and Social Change},
volume = {116},
pages = {322-330},
year = {2017},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2016.06.018},
url = {https://www.sciencedirect.com/science/article/pii/S0040162516301275},
author = {Adam Dorr},
keywords = {Technological progress, Accelerating change, Computing, Fallacy, Errors in reasoning},
abstract = {The continued exponential growth of the price-performance of computing is likely to effectuate technologies that radically transform both the global economy and the human condition over the course of this century. Conventional visions of the next 50years fail to realistically account for the full implications of accelerating technological change driven by the exponential growth of computing, and as a result are deeply flawed. These flawed visions are, in part, a consequence of three interrelated errors in reasoning: 1) the linear projection fallacy, 2) the ceteris paribus fallacy, and 3) the arrival fallacy. Each of these informal fallacies is likely a manifestation of shortcomings in our intuitions about complex dynamic systems. Recognizing these errors and identifying when and where they affect our own reasoning is an important first step toward thinking more realistically about the future.}
}
@article{TRAUTTEUR2007106,
title = {A note on discreteness and virtuality in analog computing},
journal = {Theoretical Computer Science},
volume = {371},
number = {1},
pages = {106-114},
year = {2007},
note = {Computing and the Natural Sciences},
issn = {0304-3975},
doi = {https://doi.org/10.1016/j.tcs.2006.10.017},
url = {https://www.sciencedirect.com/science/article/pii/S0304397506007699},
author = {Giuseppe Trautteur and Guglielmo Tamburrini},
keywords = {Analog computing, Virtual machine, Cognitive modelling},
abstract = {The need for physically motivated discreteness and finiteness conditions emerges in models of both analog and digital computing that are genuinely concerned with physically realizable computational processes. This is brought out by a critical examination of notional analog superTuring devices which involve physically untenable idealizations about the perfect functioning of analog apparatuses and infinite precision of physical measurements. The capability for virtual behaviour, that is, the capability of interpreting, storing, transforming, creating the code, and thereby mimicking the behaviour of (Turing) machines, is used here to introduce a new dimension in the discussion of the analog–digital watershed. In the light of recent results on the analog simulation of digital computing, we examine the role of virtuality as a discriminating factor between these two species of computing, and immerse this problem in the context of natural computing. Is virtuality instantiated in parts of the natural world other than computer technology? This broad issue is examined in connection with the computational modelling of brain and mental information processing.}
}
@article{CARVAJALRODRIGUEZ201576,
title = {Incorporación de la programación informática en el currículum de Biología},
journal = {Magister},
volume = {27},
number = {2},
pages = {76-82},
year = {2015},
issn = {0212-6796},
doi = {https://doi.org/10.1016/j.magis.2015.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S0212679615000286},
author = {Antonio Carvajal-Rodríguez},
keywords = {Docencia, Bioinformática, Biología computacional, Python, Teaching, Bioinformatics, Computational biology, Python},
abstract = {Resumen
La investigación en biología ha cambiado radicalmente debido al efecto combinado de los avances en biotecnología y ciencias de la computación. En consecuencia, la biología computacional y la bioinformática son tan esenciales para la biología del siglo xxi como la biología molecular lo fue en el anterior. Sin embargo, las competencias correspondientes a razonamiento matemático y computacional en el currículo de Biología apenas han cambiado en los últimos 25 años. La formación del biólogo debería ser tan sofisticada desde el punto de vista computacional como la del físico o la del ingeniero. La incorporación de estos cambios requiere tanto de un mayor esfuerzo de integración de las asignaturas cuantitativas existentes en el ámbito de los problemas biológicos como de la contextualización de las asignaturas propias de la biología desde un punto de vista más formal y de modelización. En este trabajo se revisan algunos de los esfuerzos que en este sentido se están haciendo en el panorama internacional y se presenta también la experiencia del autor en el diseño e impartición de un curso de iniciación a la programación para biólogos usando una metodología de aprendizaje basado en problemas.
The joint effect of biotechnology and computing has changed the research in biology. Consequently, computational biology is as essential for 21st-century biologists as molecular biology was in the 20th. However, Biology curricula have little emphasis in quantitative thinking and computation. The education for biologists should become as sophisticated as the computational education of physicists and engineers. The necessary changes to reach this goal require the connection of mathematics and quantitative subjects with real biological problems and at the same time, teaching some biological subjects from a modeling and computational perspective. In the present work, some of the current international effort in this path is reviewed and additionally, the author's experience when teaching an introduction to programming for biologists is presented.}
}
@article{DEGRANDE201960,
title = {To add or to multiply? An investigation of the role of preference in children's solutions of word problems},
journal = {Learning and Instruction},
volume = {61},
pages = {60-71},
year = {2019},
issn = {0959-4752},
doi = {https://doi.org/10.1016/j.learninstruc.2019.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S0959475218304511},
author = {Tine Degrande and Lieven Verschaffel and Wim {Van Dooren}},
keywords = {Word problem solving, Additive reasoning, Multiplicative reasoning, Preference, Skill},
abstract = {Previous research has shown that upper primary school children frequently erroneously solve additive word problems multiplicatively, while younger children frequently erroneously solve multiplicative word problems additively. It has been suggested that children's preference for additive or multiplicative relations explains these errors, besides their lacking skills, but this claim has not been tested empirically yet. Therefore, we administered four test instruments (a word problem test, a preference test, and two tests measuring additive and multiplicative computation and discrimination skill) to 246 third to sixth graders. Previous research results on errors in word problems, as well as on preference were replicated and systematized. Further, they were extended by explaining this erroneous word problem solving behavior by preference, for those children who unmistakably had acquired the necessary computation and discrimination skills. This finding provides strong evidence for the unique additional role of children's preference in erroneous additive or multiplicative word problem solving behavior.}
}
@article{BENTLEY2000465,
title = {Statistics and archaeology in Israel},
journal = {Computational Statistics & Data Analysis},
volume = {32},
number = {3},
pages = {465-483},
year = {2000},
issn = {0167-9473},
doi = {https://doi.org/10.1016/S0167-9473(99)00094-8},
url = {https://www.sciencedirect.com/science/article/pii/S0167947399000948},
author = {Jim Bentley and Tammi J Schneider},
keywords = {Variability, Density estimation, Kriging, Mapping: Surface survey, Archaeology Archaeometrics},
abstract = {While the field of statistics is fairly young, the field of archaeology is quite old. Modern archaeology prides itself on its ability to glean maximum information about the past from minimal information collected in the present. This paper attempts to show how the application of statistical thinking and techniques can aid the archaeologist in retrieving as much information as possible from artifacts; thus allowing the archaeologists to leave the majority of a site for future generations. In the past few years, archaeologists working in Israel have joined forces with statisticians in an attempt to generate more accurate recordings of archaeological information than is currently the standard in the Middle East. Careful application of statistical methods has reduced collection time and improved the display of archaeological information. An understanding of statistical concepts such as variability and density estimation has already been shown to be of use to archaeologists. Conversely, the use of examples from the field have proven to be of use in motivating humanities students to learn about statistical thinking. Archaeology has also provided a field in which students of statistics may apply their new found knowledge. The combination of statistics and archaeology is clearly of benefit to both disciplines.}
}
@article{KARUNA2019161,
title = {Capital markets research in accounting: Lessons learnt and future implications},
journal = {Pacific-Basin Finance Journal},
volume = {55},
pages = {161-168},
year = {2019},
issn = {0927-538X},
doi = {https://doi.org/10.1016/j.pacfin.2019.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S0927538X19301398},
author = {Christo Karuna},
abstract = {I review the capital markets literature in accounting by describing the journey taken by researchers since the inception of this stream of research in the late 1960s. Based on a discussion of topics related to the relation between earnings and stock returns, I show how thinking has evolved depending on changing paradigms, methodologies, and data availability. What is clear from a review of the literature is that the usefulness of earnings in determining firm value is both contextual and broadening over time with changes in the global environment. Thus, more research needs to be conducted on a broader notion of earnings that appeals to not just the shareholder but a wide range of firm stakeholders.}
}
@article{CHEAH2025100363,
title = {Integrating generative artificial intelligence in K-12 education: Examining teachers’ preparedness, practices, and barriers},
journal = {Computers and Education: Artificial Intelligence},
volume = {8},
pages = {100363},
year = {2025},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2025.100363},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X25000037},
author = {Yin Hong Cheah and Jingru Lu and Juhee Kim},
keywords = {Generative artificial intelligence, In-service teachers, Preparedness, Practices, Barriers, K-12 education},
abstract = {Despite the growing body of research on developing K-12 teachers' generative AI (GenAI) knowledge and skills, its integration into daily teaching practices remains underexplored. Using a snowball sampling method, this study examined the preparedness, practices, and barriers encountered by 89 U.S. teachers in the state of Idaho. Participants were predominantly White, female teachers serving in rural schools. A mixed-methods analysis of survey responses revealed that teachers were generally underprepared for integrating GenAI, with fewer than half incorporating it into their educational practices. Unlike the widespread classroom integration patterns observed with general educational technologies, teachers in this study tended to use GenAI for out-of-classroom duties (i.e., lesson preparation, assessment, and administrative tasks) rather than for real-time teaching and learning. These preferences could be attributed to key barriers teachers faced, including doubts about GenAI's ability to manage risks (i.e., technology value beliefs), reduced human interaction in instruction (i.e., pedagogical beliefs), ethical considerations, and the absence of policies and guidance. This study highlights the need to develop support systems and targeted policies to facilitate teachers' GenAI integration, offering implications for Idaho's education system and the broader U.S. context.}
}
@article{GUPTA2005267,
title = {Power-law distribution in a learning process: competition, learning and natural selection},
journal = {Physica A: Statistical Mechanics and its Applications},
volume = {345},
number = {1},
pages = {267-274},
year = {2005},
issn = {0378-4371},
doi = {https://doi.org/10.1016/j.physa.2004.07.010},
url = {https://www.sciencedirect.com/science/article/pii/S0378437104009860},
author = {Hari M. Gupta and José R. Campanha},
keywords = {Power-law, Learning, Natural selection},
abstract = {In the present work, we propose a model for the statistical distribution of people versus number of steps acquired by them in a learning process, based on competition, learning and natural selection. We consider that learning ability is normally distributed. We found that the number of people versus step acquired by them in a learning process is given through a power law. As competition, learning and selection is also at the core of all economical and social systems, we consider that power-law scaling is a quantitative description of this process in social systems. This gives an alternative thinking in holistic properties of complex systems.}
}
@article{PELOROSSO2020101867,
title = {Modeling and urban planning: A systematic review of performance-based approaches},
journal = {Sustainable Cities and Society},
volume = {52},
pages = {101867},
year = {2020},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2019.101867},
url = {https://www.sciencedirect.com/science/article/pii/S2210670719316968},
author = {Raffaele Pelorosso},
keywords = {Systems thinking, Thermodynamics of open systems, Standards, Spatial planning, Model classification},
abstract = {New planning approaches based on performance measures of the urban system are emerging to face the current challenges to the sustainability of cities. Through modelling, planners can understand the general behavior of the system and, consequently, decide the strategic allocation of land uses and human activities with respect to performances of the considered processes and the socio-ecological and economic uncertainties. Thus, model-based planning approaches present strong similarities with the performance-based planning (PBP) approaches and modelling can represent a valuable tool for the evolution and expansion of PBP. In this paper, a systematic review has explored a) the contribution of modelling within PBP approaches in moving cities towards sustainability; b) the applicability for modeling in PBP in urban contexts. Twelve operational examples of model-based urban planning and PBP have been identified in energy, water infrastructure, land use and ecological planning areas. A scoring system for potential model applicability in urban planning was tested in the sampled case studies. Moreover, several critical elements in the relation between modeling approaches and PBP have been identified. Finally, a discussion on the system performance concept as a new urban planning paradigm has been proposed.}
}
@article{RODRIGUES2021406,
title = {Convolutional Neural Network for Respiratory Mechanics Estimation during Pressure Support Ventilation},
journal = {IFAC-PapersOnLine},
volume = {54},
number = {15},
pages = {406-411},
year = {2021},
note = {11th IFAC Symposium on Biological and Medical Systems BMS 2021},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2021.10.290},
url = {https://www.sciencedirect.com/science/article/pii/S2405896321016955},
author = {Adriano S. Rodrigues and Marcos R.O.A. Maximo and Marcus H. Victor},
keywords = {Mechanical Ventilation, Respiratory Mechanics, Respiratory Effort, Deep Learning, Convolutional Neural Networks},
abstract = {In mechanically ventilated patients, some lung injuries can be reduced or avoided with therapy individualization, while the lung function is evaluated continuously, breath by breath. However, obtaining information on respiratory mechanics (respiratory system resistance and compliance) in the presence of respiratory effort is challenging, even if using invasive and complex procedures. The contribution of this work is to predict both respiratory system resistance and compliance over time using a convolutional neural network (CNN) and estimate the respiratory effort profile using the respiratory dynamics. Therefore, the approach used in this work was to generate a large amount of simulated data to feed a CNN so it could learn how to predict the correct values of the respiratory system resistance and compliance. Then, the respiratory effort was estimated by solving a first-order linear model. The main results showed a normalized mean squared error of 5.7% for the respiratory system resistance and 11.56% for compliance from Bland-Altman plots derived from the computational simulator. Finally, the method was validated using real data from an active lung simulator within which respiratory mechanics varied, and some ventilator settings were adjusted to mimic actual patient situations. The active lung simulator effort profile was obtained with a normalized mean squared error of 8.31% considering the use of an active lung simulator. The results have shown that the simulated data were valuable for the CNN training, while the performance over the real data suggested that the network was generalized accordingly for estimating respiratory parameters and effort profile.}
}
@article{JANG2020107524,
title = {Hemispheric asymmetries in processing numerical meaning in arithmetic},
journal = {Neuropsychologia},
volume = {146},
pages = {107524},
year = {2020},
issn = {0028-3932},
doi = {https://doi.org/10.1016/j.neuropsychologia.2020.107524},
url = {https://www.sciencedirect.com/science/article/pii/S0028393220301974},
author = {Selim Jang and Daniel C. Hyde},
keywords = {Arithmetic, Numerical cognition, Cerebral hemispheres, Late positivity, Distance effect},
abstract = {Hemispheric asymmetries in arithmetic have been hypothesized based on neuropsychological, developmental, and neuroimaging work. However, it has been challenging to separate asymmetries related to arithmetic specifically, from those associated general cognitive or linguistic processes. Here we attempt to experimentally isolate the processing of numerical meaning in arithmetic problems from language and memory retrieval by employing novel non-symbolic addition problems, where participants estimated the sum of two dot arrays and judged whether a probe dot array was the correct sum of the first two arrays. Furthermore, we experimentally manipulated which hemisphere receive the probe array first using a visual half-field paradigm while recording event-related potentials (ERP). We find that neural sensitivity to numerical meaning in arithmetic arises under left but not right visual field presentation during early and middle portions of the late positive complex (LPC, 400-800 ms). Furthermore, we find that subsequent accuracy for judgements of whether the probe is the correct sum is better under right visual field presentation than left, suggesting a left hemisphere advantage for integrating information for categorization or decision making related to arithmetic. Finally, neural signatures of operational momentum, or differential sensitivity to whether the probe was greater or less than the sum, occurred at a later portion of the LPC (800-1000 ms) and regardless of visual field of presentation, suggesting a temporal and functional dissociation between magnitude and ordinal processing in arithmetic. Together these results provide novel evidence for differences in timing and hemispheric lateralization for several cognitive processes involved in arithmetic thinking.}
}
@article{RAYAMORENO2024125385,
title = {Degradation of the ZT thermoelectric figure of merit in silicon when nanostructuring: From bulk to nanowires},
journal = {International Journal of Heat and Mass Transfer},
volume = {225},
pages = {125385},
year = {2024},
issn = {0017-9310},
doi = {https://doi.org/10.1016/j.ijheatmasstransfer.2024.125385},
url = {https://www.sciencedirect.com/science/article/pii/S0017931024002163},
author = {Martí Raya-Moreno and Riccardo Rurali and Xavier Cartoixà},
keywords = {Thermoelectrics, Nanowires, Phonon drag, , Coupled e-ph Boltzmann transport equation},
abstract = {Since the landmark paper by Hicks and Dresselhaus [Phys. Rev. B 47, 16631(R) (1993)], there has been a general consensus that one-dimensional nanoscale conductors, i.e. nanowires, provide the long sought paradigm to implement the so-called phonon-glass electron-crystal material, which results in large improvements in the thermoelectric figure of merit ZT. Despite some encouraging—though isolated—experimental results, this idea has never been subjected to a rigorous scrutiny and the effect of the coupled dynamics of electrons and phonons has usually been oversimplified. To bypass these limitations, we have calculated the effective thermoelectric parameters for silicon nanowires (SiNWs) by iteratively solving the coupled electron-phonon Boltzmann transport equation (EPBTE) supplied with first-principles data. This allows for an unprecedented precision in determining the correct dependence of the thermoelectric parameters with system size; including, but not limited to, the figure of merit and its enhancement or degradation due to nanostructuring. Indeed, we demonstrate that the commonly used relaxation time approximation (RTA), or the uncoupled beyond the RTA (iterative) solution fail to describe the correct effect of nanostructuring on the thermoelectric properties and efficiency in SiNWs due to the strong contribution of phonon drag to the Seebeck coefficient, so that the use of fully coupled solution of the EPBTE is essential to obtain the correct effect of nanostructuring. Most importantly, we show that, contrarily to what commonly argued, resorting to NWs is not necessarily beneficial for ZT. Indeed, in a wide range of diameters nanostructuring diminishes the Seebeck coefficient faster than the decrease in thermal conductivity, due to the suppression of very long wavelength phonons responsible for the largest contribution to the phonon drag component of the Seebeck coefficient. This penalty to ZT can be mitigated if the NWs have a very rough surface, providing additional reduction to the thermal conductivity. Additionally, we demonstrate that our methodology provides improved data sets for an accurate determination of doping concentration in NWs through electrical-based inference and excellent agreement with the available experimental data.}
}
@article{RENDONCASTRILLON2023104,
title = {Training strategies from the undergraduate degree in chemical engineering focused on bioprocesses using PBL in the last decade},
journal = {Education for Chemical Engineers},
volume = {44},
pages = {104-116},
year = {2023},
issn = {1749-7728},
doi = {https://doi.org/10.1016/j.ece.2023.05.008},
url = {https://www.sciencedirect.com/science/article/pii/S1749772823000258},
author = {Leidy Rendón-Castrillón and Margarita Ramírez-Carmona and Carlos Ocampo-López},
keywords = {Research hotbed, Biotechnology, Green chemistry, Circular economy, Sustainability, ABET, Engineering education},
abstract = {Global engineering education addresses the development of professional competencies in undergraduates to prepare professionals capable of solving complex technical problems under social, environmental, and economic challenges. In this work, training was carried out to incorporate the bioprocess research of the chemical engineering students at Universidad Pontificia Bolivariana in Medellin, Colombia, using a project-based learning methodology (PBL). An open call was made to the students, and they were challenged to build a prototype which they had to support together with a written report as evidence for their admission to the research hotbed and assign them research projects in bioprocesses. In the last decade, 276 students participated in the hotbed generating 21 conference presentations, four software, 14 research articles, and 16 academic awards. In parallel, a survey was conducted to analyze the perception of graduates participating in the hotbed according to a list of 17 competency criteria relevant to the chemical engineering program. It was found that the average perception is at the highest levels (4−5), which indicates that most of the graduates value the significant contribution made by the CIBIOT hotbed to the development of a professional in experimentation, communication, and acquisition of new knowledge.}
}
@article{UENO2011385,
title = {Lichtheim 2: Synthesizing Aphasia and the Neural Basis of Language in a Neurocomputational Model of the Dual Dorsal-Ventral Language Pathways},
journal = {Neuron},
volume = {72},
number = {2},
pages = {385-396},
year = {2011},
issn = {0896-6273},
doi = {https://doi.org/10.1016/j.neuron.2011.09.013},
url = {https://www.sciencedirect.com/science/article/pii/S0896627311008348},
author = {Taiji Ueno and Satoru Saito and Timothy T. Rogers and Matthew A. Lambon Ralph},
abstract = {Summary
Traditional neurological models of language were based on a single neural pathway (the dorsal pathway underpinned by the arcuate fasciculus). Contemporary neuroscience indicates that anterior temporal regions and the “ventral” language pathway also make a significant contribution, yet there is no computationally-implemented model of the dual pathway, nor any synthesis of normal and aphasic behavior. The “Lichtheim 2” model was implemented by developing a new variety of computational model which reproduces and explains normal and patient data but also incorporates neuroanatomical information into its architecture. By bridging the “mind-brain” gap in this way, the resultant “neurocomputational” model provides a unique opportunity to explore the relationship between lesion location and behavioral deficits, and to provide a platform for simulating functional neuroimaging data.}
}
@article{WALLENTIN2017165,
title = {Dynamic hybrid modelling: Switching between AB and SD designs of a predator-prey model},
journal = {Ecological Modelling},
volume = {345},
pages = {165-175},
year = {2017},
issn = {0304-3800},
doi = {https://doi.org/10.1016/j.ecolmodel.2016.11.007},
url = {https://www.sciencedirect.com/science/article/pii/S0304380016303714},
author = {Gudrun Wallentin and Christian Neuwirth},
keywords = {Hybrid model, Multi-paradigmatic modelling, Agent-based model, System-dynamics model, Predator-prey system},
abstract = {Entities and processes in complex systems are of diverse nature and operate at various spatial and temporal scales. Hybrid agent-based (AB) and system dynamics (SD) models have been suggested to capture the essence of these systems in a natural and computationally efficient way. However, the integration of the equation-based SD and individual-based AB models is not least challenged by considerable conceptual differences between these models. Examples of tightly integrated and dynamically switching hybrid models are rare. The aim of this paper is to expand on theoretical frameworks of hybrid agent-based and system dynamics models in ecology to support the model design process of dynamically switching hybrid models. We suggested six alternative model designs that switched between the two modelling paradigms. By the example of a fish-plankton lake ecosystem we demonstrated that a well-designed switching hybrid model can be a performant modelling approach that retains relevant spatial and attributive information. Important findings with respect to optimising computational versus predictive performance were (1) the most plausible results were produced by a spatially explicit design based on spatial plankton stocks and fish switching between individual agents and aggregate school-agents, (2) higher levels of aggregation did not necessarily result in higher computational performance, and (3) adaptive, emergence-based triggers for the paradigm switches minimised information loss and could connect hierarchical and spatial scales. In conclusion, we argue to reach beyond efficiency-oriented considerations and use emergent super-individuals as structural elements of dynamically switching hybrid models.}
}
@incollection{BOGLE20031,
title = {Computer aided biochemical process engineering},
editor = {Andrzej Kraslawski and Ilkka Turunen},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {14},
pages = {1-10},
year = {2003},
booktitle = {European Symposium on Computer Aided Process Engineering-13},
issn = {1570-7946},
doi = {https://doi.org/10.1016/S1570-7946(03)80082-2},
url = {https://www.sciencedirect.com/science/article/pii/S1570794603800822},
author = {I.D.L. Bogle},
abstract = {The growth of the biochemical industries is heating up in Europe after not meeting the initial expectations. CAPE tools have made some impact and progress on computer aided synthesis and design of biochemical processes is demonstrated on a process for the production of a hormone. Systems thinking is being recognised by the life science community and to gain genuinely optimal process solutions it is necessary to design right through from product and function to metabolism and manufacturing process. The opportunities for CAPE experts to contribute in the explosion of interest in the Life Sciences is strong if we think of the ‘Process’ in CAPE as any process involving physical or (bio-)chemical change.}
}
@article{RUI2024100711,
title = {Simulation of e-learning in vocal network teaching experience system based on intelligent Internet of things technology},
journal = {Entertainment Computing},
volume = {50},
pages = {100711},
year = {2024},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2024.100711},
url = {https://www.sciencedirect.com/science/article/pii/S187595212400079X},
author = {Yu Rui},
keywords = {Intelligent technology, Internet of Things, E-learning, Vocal music network, Teaching experience, System simulation},
abstract = {With the rapid development of smart Internet of Things technology, education is also starting to use this technology to improve the learning experience. As an art subject, vocal music teaching has many limitations in the traditional face-to-face teaching methods, and e-learning provides new possibilities for vocal music network teaching. We analyze the problems and challenges existing in the traditional face-to-face mode of vocal music teaching, and then based on the intelligent Internet of Things technology, we design a vocal music network teaching experience system. The system combines sound acquisition equipment, intelligent audio processing algorithm, virtual classroom and other technologies to realize the simulation experience of online vocal music teaching. We have developed an intelligent audio processing algorithm for analyzing and processing students’ singing sounds. This algorithm can detect problems in tone, timbre, rhythm, and more, and provide real-time feedback and advice. In this way, students can understand the shortcomings of their own singing skills, and make timely adjustments and improvements. This study shows that e-learning based on intelligent Internet of Things technology has important application value in vocal music network teaching. By simulating the classroom environment and providing real-time feedback, students can obtain a better learning experience and improve their vocal skills.}
}
@article{MCDONALD201955,
title = {Cognitive bots and algorithmic humans: toward a shared understanding of social intelligence},
journal = {Current Opinion in Behavioral Sciences},
volume = {29},
pages = {55-62},
year = {2019},
note = {Artificial Intelligence},
issn = {2352-1546},
doi = {https://doi.org/10.1016/j.cobeha.2019.04.013},
url = {https://www.sciencedirect.com/science/article/pii/S2352154618301979},
author = {Kelsey R McDonald and John M Pearson},
abstract = {Questions of social behavior are simultaneously among the most fundamental in neuroscience and the most challenging in artificial intelligence. Yet despite decades of work, a unified perspective from the cognitive and computational approaches to the problem has yet to emerge. Recently, however, excitement around the challenges posed to reinforcement learning by multiplayer video games, coupled with the adoption of more complex modeling strategies in social neuroscience, has broadened the interface between the two fields. Here, we review recent progress from both directions, arguing that advances in artificial intelligence provide neuroscientists with valuable tools for modeling social interactions. At the same time, the study of humans as efficient social learners can inform the design of new algorithms for multi-agent systems. We conclude by encouraging a joint approach that incorporates the best of both domains to advance a shared picture of social intelligence.}
}
@incollection{CLAUSER20231,
title = {Past, present, and future of educational measurement},
editor = {Robert J Tierney and Fazal Rizvi and Kadriye Ercikan},
booktitle = {International Encyclopedia of Education (Fourth Edition)},
publisher = {Elsevier},
edition = {Fourth Edition},
address = {Oxford},
pages = {1-14},
year = {2023},
isbn = {978-0-12-818629-9},
doi = {https://doi.org/10.1016/B978-0-12-818630-5.10001-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128186305100016},
author = {Brian E. Clauser and Melissa J. Margolis},
keywords = {Karl Pearson, Francis Galton, Classical test theory, Item response theory, Generalizability theory, Frederic Lord, Lee Cronbach, Coefficient alpha, Validity theory, Charles Spearman, Eugenics movement, Spearman-Brown formula, Alfred Binet, Army Alpha test, Georg Rasch, Intelligence testing},
abstract = {This article provides an overview of the past, present, and future of educational measurement. We begin by examining the historical events in the 1800s that led to the development of a coherent mathematical theory of test scores in the first half of the 20th century. In this section we describe the contributions of Francis Galton, Karl Pearson, Charles Spearman, Truman Kelley, and Lee Cronbach. In addition to outlining the theoretical contributions of these researchers, we describe the rise of large-scale testing beginning with the Army Alpha test in 1917 and the administration of IQ tests to millions of school children in the decade that followed. We continue by discussing the current state of educational measurement theory and practice including the development and widespread use of item response theory, generalizability theory, validity theory, and large-scale national and international achievement testing to evaluate educational systems. Finally, we consider directions and developments that are likely to define the future of the field. These directions include increased use of computational power in assessment, the use of new sources of data (referred to as process data), automated systems to create test materials, and an increased emphasis on fairness.}
}
@article{ZHANG20231815,
title = {An intention inference method for the space non-cooperative target based on BiGRU-Self Attention},
journal = {Advances in Space Research},
volume = {72},
number = {5},
pages = {1815-1828},
year = {2023},
issn = {0273-1177},
doi = {https://doi.org/10.1016/j.asr.2023.04.032},
url = {https://www.sciencedirect.com/science/article/pii/S0273117723003101},
author = {Honglin Zhang and Jianjun Luo and Yuan Gao and Weihua Ma},
keywords = {Space non-cooperative target, Intention inference, Time series, BiGRU, Self-attention mechanism},
abstract = {Intention inference for space non-cooperative targets is the key to space situational awareness and assistant decision for collision avoidance. Given that the problem of target intention inference is essential to learn the dynamically changing time-series characteristics of space non-cooperative target intentions and infer their relative motion patterns for threat warning, this paper adopts a deep learning-based approach, introduces a bidirectional propagation mechanism and self-attention mechanism based on Gated Recurrent Unit (GRU) and proposes a bidirectional Gated Recurrent Unit (BiGRU)-Self Attention-based space non-cooperative target intention inference model. BiGRU is used to learn deep information in time-series characteristics of the space non-cooperative target, and self-attention mechanism is used to adaptively extract and assign weights to key characteristics to capture the internal correlations in time-series information, thus improving model performance. The line-of-sight measurements are used as the characteristics of target intention inference, and the typical target motion intentions are defined. Subsequently, the proposed model is trained and tested on the test set, with the accuracy reaching 97.1%. Besides, the effectiveness and advantages of the proposed model are verified by the simulation of a case study and comparison evaluations. The results demonstrate that our proposed model could significantly improve the accuracy, computational efficiency, and noise resistance for the space non-cooperative target intention inference compared with the existing intention inference models.}
}
@article{XIE20189,
title = {Detecting leadership in peer-moderated online collaborative learning through text mining and social network analysis},
journal = {The Internet and Higher Education},
volume = {38},
pages = {9-17},
year = {2018},
issn = {1096-7516},
doi = {https://doi.org/10.1016/j.iheduc.2018.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S1096751618300332},
author = {Kui Xie and Gennaro {Di Tosto} and Lin Lu and Young Suk Cho},
keywords = {Leadership, Computer-supported collaborative learning, Text mining, Social network analysis, Learning analytics, Online learning},
abstract = {Structured tasks and peer-moderated discussions are pedagogical models that have shown unique benefits for online collaborative learning. Students appointed with leadership roles are able to positively affect the dynamics in their groups by engaging with participants, raising questions, and advancing problem solving. To help monitoring and controlling the latent social dynamics associated with leadership behavior, we propose a methodological approach that makes use of computational techniques to mine the content of online communications and analyze group structure to identify students who behave as leaders. Through text mining and social network analysis, we systematically process the discussion posts made by students from four sections of an online course in an American university. The results allow us to quantify each individual's contribution and summarize their engagement in the form of a leadership index. The proposed methodology, when compared to judgements made by experts who manually coded samples of the data, is shown to have comparable performances, but, being fully automated, has the potential to be easily replicable. The summary offered by the leadership index is intended as actionable information that can guide just-in-time interventions together with other tools based on learning analytics.}
}
@article{YAMAKAWA2021478,
title = {The whole brain architecture approach: Accelerating the development of artificial general intelligence by referring to the brain},
journal = {Neural Networks},
volume = {144},
pages = {478-495},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021003543},
author = {Hiroshi Yamakawa},
keywords = {Brain reference architecture, Structure-constrained interface decomposition method, Brain information flow, Hypothetical component diagram, Brain-inspired artificial general intelligence, Whole-brain architecture},
abstract = {The vastness of the design space that is created by the combination of numerous computational mechanisms, including machine learning, is an obstacle to creating artificial general intelligence (AGI). Brain-inspired AGI development; that is, the reduction of the design space to resemble a biological brain more closely, is a promising approach for solving this problem. However, it is difficult for an individual to design a software program that corresponds to the entire brain as the neuroscientific data that are required to understand the architecture of the brain are extensive and complicated. The whole-brain architecture approach divides the brain-inspired AGI development process into the task of designing the brain reference architecture (BRA), which provides the flow of information and a diagram of the corresponding components, and the task of developing each component using the BRA. This is known as BRA-driven development. Another difficulty lies in the extraction of the operating principles that are necessary for reproducing the cognitive–behavioral function of the brain from neuroscience data. Therefore, this study proposes structure-constrained interface decomposition (SCID), which is a hypothesis-building method for creating a hypothetical component diagram that is consistent with neuroscientific findings. The application of this approach has been initiated for constructing various regions of the brain. In the future, we will examine methods for evaluating the biological plausibility of brain-inspired software. This evaluation will also be used to prioritize different computational mechanisms, which should be integrated and associated with the same regions of the brain.}
}
@incollection{ZHANG2022363,
title = {Chapter 18 - KPF: A retrospective view on urban planning AI for 2020},
editor = {Imdat As and Prithwish Basu and Pratap Talwar},
booktitle = {Artificial Intelligence in Urban Planning and Design},
publisher = {Elsevier},
pages = {363-380},
year = {2022},
isbn = {978-0-12-823941-4},
doi = {https://doi.org/10.1016/B978-0-12-823941-4.00004-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128239414000044},
author = {Snoweria Zhang and Kate Ringo and Richard Chou and Brandon Pachuca and Eric Pietraszkiewicz and Luc Wilson},
keywords = {Computational design, Digital twin, Urban design, Future history, City planning},
abstract = {Architectural historians have been fascinated by the year 1000, as the expectation of an impending apocalypse drove the sharp contrast between a dearth of construction before and a booming market after. One thousand years later, residents of 2020 found themselves at the crossroads again with the effects of climate change looming as a global threat. We constructed this chapter as a piece of a future, speculative, and historical document that examines the use of AI in urban planning and design in 2020. As historians from 2120, we study the evolution of tools at this critical junction with the backdrop of a confluence of crises. From explorative visual interfaces, open data initiatives, and computational design to AI that augments and collaborates with humans in the design and development of the city, we present case studies of both the technology and the projects that demonstrate some of the first applications of AI in negotiating the threat of climate change. Through these first examples, we trace the development of tools and corresponding trends in urban AI to the present year of 2120. The speculative narrative frame allows for an explication of the current urban design workflow using AI alongside an opportunity to conjecture where we believe AI development in design and planning ought to be. City makers in 2020 were not involved in the development of AI technologies. This work can act to inspire technologists who are envisioning the future of the city.}
}
@article{OSEIBRYSON20121156,
title = {A context-aware data mining process model based framework for supporting evaluation of data mining results},
journal = {Expert Systems with Applications},
volume = {39},
number = {1},
pages = {1156-1164},
year = {2012},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2011.07.117},
url = {https://www.sciencedirect.com/science/article/pii/S0957417411010797},
author = {Kweku-Muata Osei-Bryson},
keywords = {Context, Data mining process, KDDM, Evaluation, Decision analysis, Multi-criteria decision analysis, Post-processing},
abstract = {The knowledge discovery via data mining process (KDDM) is a multiple phase that aims to at a minimum semi-automatically extract new knowledge from existing datasets. For many data mining tasks, the evaluation phase is a challenging one for various reasons. Given this challenge several studies have presented techniques that could be used for the semi-automated evaluation of data mining results. When taken together, these studies suggest the possibility of a common multi-criteria evaluation framework. The use of such a multi-criteria evaluation framework, however, requires that relevant objectives, measures and preference function be identified. This implies that the context of the DM problem is particularly important for the evaluation phase of the KDDM process. Our framework utilizes and integrates a pair of established tightly coupled techniques (i.e. Value Focused Thinking (VFT) and the Goal–Question–Metric (GQM) methods) as well as established techniques from multi-criteria decision analysis in order to explicate and utilize context information in order to facilitate semi-automated evaluation.}
}
@article{DELEON2021281,
title = {Assessing the Efficacy of Tier 2 Mathematics Intervention for Spanish Primary School Students},
journal = {Early Childhood Research Quarterly},
volume = {56},
pages = {281-293},
year = {2021},
issn = {0885-2006},
doi = {https://doi.org/10.1016/j.ecresq.2021.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S0885200621000508},
author = {Sara C. {de León} and Juan E. Jiménez and Nuria Gutiérrez and Juan Andrés Hernández-Cabrera},
keywords = {RtI model, math, early grades, Tier 2, at-risk},
abstract = {This study explored the efficacy of a Tier 2 intervention within the context of the Response to Intervention (RtI) model implemented by Spanish first- to third-grade primary school teachers to improve at-risk students’ early math skills. Teachers were instructed in the administration of a math curriculum-based measure composed of 5 isolated measures (quantity discrimination, missing number, single-digit computation, multidigit computation, and place value) to identify at-risk students and to monitor their progress; and in the implementation of a systematic and explicit instructional program to improve basic math skills in at-risk students. Implementation fidelity was analyzed using direct observations and self-reports. The intervention was conducted with adequate fidelity and had a significant positive impact on all grades. Significant differences were found between experimental and control students at risk of math failure in the improvement rate of quantity discrimination, missing number, and place value in all grades. Experimental at-risk students showed a monthly improvement, assessed using a combination of screening and progress monitoring measures. In conclusion, Spanish first to third graders at risk of math failure benefited from a Tier 2 intervention based on basic math skills, implemented by in-service teachers.}
}
@article{LIU2024100744,
title = {Application of entertainment E-learning mode based on Apriori algorithm in intelligent English reading assistance mode},
journal = {Entertainment Computing},
volume = {51},
pages = {100744},
year = {2024},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2024.100744},
url = {https://www.sciencedirect.com/science/article/pii/S1875952124001125},
author = {Shanshan Liu},
keywords = {Apriori algorithm, Entertainment E-learning model, Intelligent English teaching, Auxiliary reading mode},
abstract = {With the assistance of digital media, entertainment oriented E-learning models can effectively enhance students’ learning enthusiasm. This article analyzes the application of entertainment E-learning mode based on Apriori algorithm in intelligent English reading assistance mode. At present, English reading teaching faces some problems, such as outdated teaching methods, passive learning among students, excessive emphasis on imparting grammar knowledge while neglecting the improvement of students’ reading skills and strategies, and so on. Therefore, this article conducts research on intelligent English reading comprehension tools based on semantic analysis and Apriori algorithm. This paper proposes a recreational E-learning model based on Apriori algorithm. Based on Apriori algorithm, students’ interests and preferences on different learning resources and entertainment elements are mined and incorporated into the learning model design. Then, a set of entertaining English reading assistant model is designed, which uses a variety of entertainment elements, such as gamified learning, interactive activities and reward mechanism, to increase students’ learning participation and enthusiasm. This article adopts the idea of LSA algorithm to construct a BERT semantic analysis model. We treat nodes in the network as word items and then use singular value decomposition algorithm to decompose the word document matrix. Secondly, the original association rule Apriori algorithm was optimized, and the optimized association rule Apriori algorithm effectively solved the problem of excessive computation in traditional algorithms. Finally, based on semantic analysis and Apriori algorithm, this article designs an intelligent English reading comprehension tool, mainly analyzing the practical application of the system and greatly improving the efficiency of English reading teaching.}
}
@article{BRAMSON2023105397,
title = {Emotion regulation from an action-control perspective},
journal = {Neuroscience & Biobehavioral Reviews},
volume = {153},
pages = {105397},
year = {2023},
issn = {0149-7634},
doi = {https://doi.org/10.1016/j.neubiorev.2023.105397},
url = {https://www.sciencedirect.com/science/article/pii/S0149763423003664},
author = {Bob Bramson and Ivan Toni and Karin Roelofs},
keywords = {Emotion control, Emotion regulation, Emotional-action selection, Forward modelling},
abstract = {Despite increasing interest in emotional processes in cognitive science, theories on emotion regulation have remained rather isolated, predominantly focused on cognitive regulation strategies such as reappraisal. However, recent neurocognitive evidence suggests that early emotion regulation may involve sensorimotor control in addition to other emotion-regulation processes. We propose an action-oriented view of emotion regulation, in which feedforward predictions develop from action-selection mechanisms. Those can account for acute emotional-action control as well as more abstract instances of emotion regulation such as cognitive reappraisal. We argue the latter occurs in absence of overt motor output, yet in the presence of full-blown autonomic, visceral, and subjective changes. This provides an integrated framework with testable neuro-computational predictions and concrete starting points for intervention to improve emotion control in affective disorders.}
}
@incollection{DETALLE2017495,
title = {2.20 - Translational Aspects in Drug Discovery},
editor = {Samuel Chackalamannil and David Rotella and Simon E. Ward},
booktitle = {Comprehensive Medicinal Chemistry III},
publisher = {Elsevier},
address = {Oxford},
pages = {495-529},
year = {2017},
isbn = {978-0-12-803201-5},
doi = {https://doi.org/10.1016/B978-0-12-409547-2.12335-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780124095472123352},
author = {L. Detalle and K. Vanheusden and M.L. Sargentini-Maier and T. Stöhr},
keywords = {Animal model, Biomarker, Imaging, Modeling, Simulation, Translational medicine},
abstract = {The efficiency of drug development has seen a constant decline. This observation is somewhat paradoxical since during the same time there have been huge advancements in drug discovery and development technologies that made it much cheaper, faster, and easier to identify new drug targets and new drug molecules. Translational Science or Translational Medicine (TM) has arisen as an important discipline in modern drug discovery and development. It was triggered by the fact that many promising drugs failed in clinical trials. The challenge was thus to enhance the predictivity of the preclinical models and to design exploratory clinical trial designs and methodologies to test promising molecules earlier and faster. Despite some advancements, the number of drugs that finally receive regulatory approval is still at a low level. The main reason for this drug failure rate was a lack of efficacy observed in clinical trials of drug candidates that showed great promise in drug discovery. There may be two main factors responsible for this: (1) the industrialization of drug discovery and development led to huge specialized departments that operate in isolation. (2) Tools for successful translational research have only been developed in the last one or two decades. We will describe the tools used in translational research, that is, biomarkers, animal models, imaging, in silico modeling, and simulations. Their use will be illustrated with examples and tips of how to implement those into daily project work. We believe, however, that TM is more than these tools and technologies. It is not yet another discipline or department, it is a way of thinking that should become part of every discipline involved in drug development. Thus, in addition to describing the tools and how best to use them, we will elaborate how to design a translational research strategy and exemplify with some case studies as to how this has been successfully implemented in the past.}
}
@article{BARTON201242,
title = {Looking for the future in the past: Long-term change in socioecological systems},
journal = {Ecological Modelling},
volume = {241},
pages = {42-53},
year = {2012},
note = {Modeling Across Millennia: Interdisciplinary Paths to Ancient socio-ecological Systems},
issn = {0304-3800},
doi = {https://doi.org/10.1016/j.ecolmodel.2012.02.010},
url = {https://www.sciencedirect.com/science/article/pii/S0304380012000786},
author = {C. Michael Barton and Isaac I.T. Ullah and Sean M. Bergin and Helena Mitasova and Hessam Sarjoughian},
keywords = {Socio-ecological systems, Coupled modeling, Agent-based modeling, Surface process modeling, Simulation, Prehistoric Mediterranean, Archaeology, Agricultural land-use},
abstract = {The archaeological record has been described as a key to the long-term consequences of human action that can help guide our decisions today. Yet the sparse and incomplete nature of this record often makes it impossible to inferentially reconstruct past societies in sufficient detail for them to serve as more than very general cautionary tales of coupled socio-ecological systems. However, when formal and computational modeling is used to experimentally simulate human socioecological dynamics, the empirical archaeological record can be used to validate and improve dynamic models of long term change. In this way, knowledge generated by archaeology can play a unique and valuable role in developing the tools to make more informed decisions that will shape our future. The Mediterranean Landscape Dynamics project offers an example of using the past to develop and test computational models of interactions between land-use and landscape evolution that ultimately may help guide decision-making.}
}
@article{VEKSLER20169,
title = {The performance comparison problem: Universal task access for cross-framework evaluation, Turing tests, grand challenges, and cognitive decathlons},
journal = {Biologically Inspired Cognitive Architectures},
volume = {18},
pages = {9-22},
year = {2016},
issn = {2212-683X},
doi = {https://doi.org/10.1016/j.bica.2016.10.003},
url = {https://www.sciencedirect.com/science/article/pii/S2212683X16300810},
author = {Vladislav D. Veksler and Norbou Buchler and Christian Lebiere and Don Morrison and Troy Kelley},
keywords = {Grand challenge, Cognitive decathlon, Turing test, Performance comparison, Simulation, API, Standards},
abstract = {A driver for achieving human-level AI and high-fidelity cognitive architectures is the ability to easily test and compare the performance and behavior of computational agents/models to humans and to one another. One major difficulty in setting up and getting participation in large-scale cognitive decathlon and grand challenge competitions, or even smaller scale cross-framework evaluation and Turing testing, is that there is no standard interface protocol that enables and facilitates human and computational agent “plug-and-play” participation across various tasks. We identify three major issues. First, human-readable task interfaces aren’t often translated into machine-readable form. Second, in the cases where a task interface is made available in a machine-readable protocol, the protocol is often task-specific, and differs from other task protocols. Finally, where both human and machine-readable versions of the task interface exist, the two versions often differ in content. This makes the bar of entry extremely high for comparison of humans and multiple computational frameworks across multiple tasks. This paper proposes a standard approach to task design where all task interactions adhere to a standard API. We provide examples of how this method can be employed to gather human and computational simulation data in text-and-button tasks, visual and animated tasks, and in real-time robotics tasks.}
}
@article{SUCHOW2017522,
title = {Evolution in Mind: Evolutionary Dynamics, Cognitive Processes, and Bayesian Inference},
journal = {Trends in Cognitive Sciences},
volume = {21},
number = {7},
pages = {522-530},
year = {2017},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2017.04.005},
url = {https://www.sciencedirect.com/science/article/pii/S1364661317300773},
author = {Jordan W. Suchow and David D. Bourgin and Thomas L. Griffiths},
keywords = {Bayesian inference, cognitive processes, creativity, evolution, learning, memory},
abstract = {Evolutionary theory describes the dynamics of population change in settings affected by reproduction, selection, mutation, and drift. In the context of human cognition, evolutionary theory is most often invoked to explain the origins of capacities such as language, metacognition, and spatial reasoning, framing them as functional adaptations to an ancestral environment. However, evolutionary theory is useful for understanding the mind in a second way: as a mathematical framework for describing evolving populations of thoughts, ideas, and memories within a single mind. In fact, deep correspondences exist between the mathematics of evolution and of learning, with perhaps the deepest being an equivalence between certain evolutionary dynamics and Bayesian inference. This equivalence permits reinterpretation of evolutionary processes as algorithms for Bayesian inference and has relevance for understanding diverse cognitive capacities, including memory and creativity.}
}
@article{BARON2022113861,
title = {Might pain be experienced in the brainstem rather than in the cerebral cortex?},
journal = {Behavioural Brain Research},
volume = {427},
pages = {113861},
year = {2022},
issn = {0166-4328},
doi = {https://doi.org/10.1016/j.bbr.2022.113861},
url = {https://www.sciencedirect.com/science/article/pii/S0166432822001292},
author = {Mark Baron and Marshall Devor},
keywords = {Anesthesia, Brain evolution, Consciousness, Coma, Mesopontine tegmentum, MPTA},
abstract = {It is nearly axiomatic that pain, among other examples of conscious experience, is an outcome of still-uncertain forms of neural processing that occur in the cerebral cortex, and specifically within thalamo-cortical networks. This belief rests largely on the dramatic relative expansion of the cortex in the course of primate evolution, in humans in particular, and on the fact that direct activation of sensory representations in the cortex evokes a corresponding conscious percept. Here we assemble evidence, drawn from a number of sources, suggesting that pain experience is unlike the other senses and may not, in fact, be an expression of cortical processing. These include the virtual inability to evoke pain by cortical stimulation, the rarity of painful auras in epileptic patients and outcomes of cortical lesions. And yet, pain perception is clearly a function of a conscious brain. Indeed, it is perhaps the most archetypical example of conscious experience. This draws us to conclude that conscious experience, at least as realized in the pain system, is seated subcortically, perhaps even in the “primitive” brainstem. Our conjecture is that the massive expansion of the cortex over the course of evolution was not driven by the adaptive value of implementing consciousness. Rather, the cortex evolved because of the adaptive value of providing an already existing subcortical generator of consciousness with a feed of critical information that requires the computationally intensive capability of the cerebral cortex.}
}
@article{SHAWKY2023100547,
title = {Adaptive chaotic map-based key extraction for efficient cross-layer authentication in VANETs},
journal = {Vehicular Communications},
volume = {39},
pages = {100547},
year = {2023},
issn = {2214-2096},
doi = {https://doi.org/10.1016/j.vehcom.2022.100547},
url = {https://www.sciencedirect.com/science/article/pii/S2214209622000948},
author = {Mahmoud A. Shawky and Muhammad Usman and Muhammad Ali Imran and Qammer H. Abbasi and Shuja Ansari and Ahmad Taha},
keywords = {Chebyshev chaotic mapping, Cross-layer authentication, Doppler emulation, Physical-layer signatures, Secret key extraction, Vehicular ad-hoc networks},
abstract = {Vehicle-to-everything (V2X) communication is expected to offer users available and ultra-reliable transmission, particularly for critical applications related to safety and autonomy. In this context, establishing a secure and resilient authentication process with low latency and high functionality may not be achieved using conventional cryptographic methodologies due to their significant computation costs. Recent research has focused on employing the physical (PHY) characteristics of wireless channels to develop efficient discrimination techniques to overcome the shortcomings of crypto-based authentication. This paper presents a cross-layer authentication scheme for multicarrier communication, leveraging the spatially/temporally correlated wireless channel features to facilitate key verification without exposing its secrecy. By mapping the time-stamped hashed key and masking it with channel phase responses, we create a PHY-layer signature, allowing for verifying the sender's identity while employing the correlated channel responses between subcarriers to verify messages' integrity. Furthermore, we developed a Diffie-Hellman secret key extraction algorithm that employs the computationally intractable problems of the Chebyshev chaotic mapping for channel probing. Thus, terminals can extract high entropy shared keys that can be used to create dynamic PHY-layer signatures, supporting forward and backward secrecy. We evaluated the scheme's security strength against active/passive attacks. Besides theoretical analysis, we designed a 3-Dimensional (3D) scattering Doppler emulator to investigate the scheme's performance at different speeds of a moving vehicle and signal-to-noise ratios (SNRs) for a realistic vehicular channel. Theoretical and hardware implementation analyses proved the capability of the proposed scheme to support high detection probability at SNR ≥ 0 dB and speed ≤ 45 m/s.}
}
@article{OBRIEN2021104184,
title = {Misplaced trust: When trust in science fosters belief in pseudoscience and the benefits of critical evaluation},
journal = {Journal of Experimental Social Psychology},
volume = {96},
pages = {104184},
year = {2021},
issn = {0022-1031},
doi = {https://doi.org/10.1016/j.jesp.2021.104184},
url = {https://www.sciencedirect.com/science/article/pii/S0022103121000871},
author = {Thomas C. O'Brien and Ryan Palmer and Dolores Albarracin},
keywords = {Misinformation, Trust in science, Critical thinking, Methodological literacy},
abstract = {At a time when pseudoscience threatens the survival of communities, understanding this vulnerability, and how to reduce it, is paramount. Four preregistered experiments (N = 532, N = 472, N = 605, N = 382) with online U.S. samples introduced false claims concerning a (fictional) virus created as a bioweapon, mirroring conspiracy theories about COVID-19, and carcinogenic effects of GMOs (Genetically Modified Organisms). We identify two critical determinants of vulnerability to pseudoscience. First, participants who trust science are more likely to believe and disseminate false claims that contain scientific references than false claims that do not. Second, reminding participants of the value of critical evaluation reduces belief in false claims, whereas reminders of the value of trusting science do not. We conclude that trust in science, although desirable in many ways, makes people vulnerable to pseudoscience. These findings have implications for science broadly and the application of psychological science to curbing misinformation during the COVID-19 pandemic.}
}
@article{LIU2020102176,
title = {Using a new approach for revealing the spatiotemporal patterns of functional urban polycentricity: A case study in the Tokyo metropolitan area},
journal = {Sustainable Cities and Society},
volume = {59},
pages = {102176},
year = {2020},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2020.102176},
url = {https://www.sciencedirect.com/science/article/pii/S2210670720301633},
author = {Kai Liu and Yuji Murayama and Toshiaki Ichinose},
keywords = {Functional urban area detection, Functional urban polycentricity, Multi-step Decision-making Newman algorithm, Tokyo Master Plan, Tokyo metropolitan area},
abstract = {This research designs a new approach by modifying the Fast-Newman algorithm for better implementing the process of detecting functional urban areas (FUAs) and further revealing the spatiotemporal patterns of functional urban polycentricity through 20 view-windows of each FUA in the Tokyo metropolitan area (TMA) by using geo-tagged big data. Through the 20 view-windows of our GIS microscope, it is possible to uncover patterns of functional connections and daily urban rhythms under the same layout of the functional urban structure in the TMA. Furthermore, our findings can elucidate the double-sided thinking by combining the explanations of functional urban polycentricity with the policy effects of the Tokyo Master Plan (TMP) from the perspective of area-byarea analysis across the entire TMA. Our results imply that the functional urban structure of the TMA is a four-level, annular pattern. The TMP still has room for the improvement toward sustainable urban planning in the TMA. Based on the investigation on the patterns of functional urban polycentricity, this research has obtained many hints and clues for improving the TMP. Rethinking the effectiveness of the TMP can also provide trustworthy academic verification and provide suggestions about concrete amendments that can enlighten future urban planning.}
}
@article{LEWTON202538,
title = {“Now is the time to realise useful autonomous quantum machines”},
journal = {New Scientist},
volume = {265},
number = {3534},
pages = {38-41},
year = {2025},
issn = {0262-4079},
doi = {https://doi.org/10.1016/S0262-4079(25)00433-6},
url = {https://www.sciencedirect.com/science/article/pii/S0262407925004336},
author = {Thomas Lewton},
abstract = {Quantum technology is still in its infancy, says Nicole Yunger Halpern. But, she tells Thomas Lewton, she intends to change that}
}
@article{MCDANIELS200333,
title = {Decision structuring to alleviate embedding in environmental valuation},
journal = {Ecological Economics},
volume = {46},
number = {1},
pages = {33-46},
year = {2003},
issn = {0921-8009},
doi = {https://doi.org/10.1016/S0921-8009(03)00103-4},
url = {https://www.sciencedirect.com/science/article/pii/S0921800903001034},
author = {Timothy L McDaniels and Robin Gregory and Joseph Arvai and Ratana Chuenpagdee},
keywords = {Embedding, Structured decision process, Environmental valuation, Value-focused thinking, Small group process, Fisheries valuation},
abstract = {Embedding is the widely-observed phenomenon that a good is assigned a higher value if evaluated on its own rather than as part of a more inclusive set. Embedding is considered a serious problem affecting the quality of many environmental management and health risk policy judgments. This paper presents the results of an experiment involving of a structured, small-group approach for conducting environmental policy evaluations. It focuses on eliciting problem-specific values and discussion among participants about the pros and cons of multiple project alternatives, in the context of tradeoffs between fisheries production and electricity generation from dams. Study results show a significant reduction in embedding, which is viewed as an improvement in the quality of the preference judgments compared with a standard contingent valuation (CV) approach.}
}
@article{WOLFF20124051,
title = {Constraints in the generation of photonic Wannier functions},
journal = {Physica B: Condensed Matter},
volume = {407},
number = {20},
pages = {4051-4055},
year = {2012},
note = {Proceedings of the conference - Wave Propagation: From Electrons to Photonic Crystals and Metamaterials},
issn = {0921-4526},
doi = {https://doi.org/10.1016/j.physb.2012.03.022},
url = {https://www.sciencedirect.com/science/article/pii/S0921452612002554},
author = {Christian Wolff and Kurt Busch},
keywords = {Photonic Crystals, Wannier functions},
abstract = {We report on the generation of maximally localized photonic Wannier functions under constraints. This allows us to impress certain symmetry properties of the underlying Photonic Crystal onto the Wannier functions. This added flexibility enhances the utility of the Wannier function approach to Photonic Crystal circuits by providing deeper physical insight and making computations more efficient.}
}
@article{WONG2021105042,
title = {Empathic accuracy of young boys and girls in ongoing parent–child interactions: Performance and (mis)perception},
journal = {Journal of Experimental Child Psychology},
volume = {203},
pages = {105042},
year = {2021},
issn = {0022-0965},
doi = {https://doi.org/10.1016/j.jecp.2020.105042},
url = {https://www.sciencedirect.com/science/article/pii/S0022096520304963},
author = {Wang Ivy Wong and Wai Bong Patrick Tsui and Tik-Sze Carrey Siu},
keywords = {Interpersonal interaction, Performance estimation, Social cognition, Gender, Empathic accuracy, Parent and child},
abstract = {Understanding others accurately is crucial in relationships and learning. Research shows that adults face challenges in empathic accuracy, that is, the ability to read the content of others’ moment-to-moment mental states during interactions. Although young children possess some empathic understanding, the extent of their empathic accuracy is uncharted. Using a new SSP, 106 Chinese children aged 60 to 80 months (M = 70 months) were assessed on their ability to infer the mental states of adults in ongoing parent–child interactions. Replicating and extending extant findings on adults and adolescents, the children’s inferences were found to be, at least computationally on a scale of .00 to 1.00, more often inaccurate than accurate regardless of the gender of the targets or participants (overall accuracy rate = .28). However, both the children and their primary caregivers overestimated the children’s performance. In addition, although the primary caregivers expected girls to outperform boys, no gender difference in empathic accuracy was found when controlling for verbal fluency. Drawing on the findings of this first-ever application of the empathic accuracy paradigm in young children, the implications of empathic accuracy performance and misperceptions about such accuracy are discussed.}
}
@article{WITTKUHN2021367,
title = {Replay in minds and machines},
journal = {Neuroscience & Biobehavioral Reviews},
volume = {129},
pages = {367-388},
year = {2021},
issn = {0149-7634},
doi = {https://doi.org/10.1016/j.neubiorev.2021.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S0149763421003444},
author = {Lennart Wittkuhn and Samson Chien and Sam Hall-McMaster and Nicolas W. Schuck},
keywords = {Replay, Reinforcement learning, Machine learning, Representation learning, Decision-making},
abstract = {Experience-related brain activity patterns reactivate during sleep, wakeful rest, and brief pauses from active behavior. In parallel, machine learning research has found that experience replay can lead to substantial performance improvements in artificial agents. Together, these lines of research suggest that replay has a variety of computational benefits for decision-making and learning. Here, we provide an overview of putative computational functions of replay as suggested by machine learning and neuroscientific research. We show that replay can lead to faster learning, less forgetting, reorganization or augmentation of experiences, and support planning and generalization. In addition, we highlight the benefits of reactivating abstracted internal representations rather than veridical memories, and discuss how replay could provide a mechanism to build internal representations that improve learning and decision-making.}
}