@article{ZHUAN2012248,
title = {The Assimilation Rule on the Parameters of Feedback System},
journal = {AASRI Procedia},
volume = {1},
pages = {248-260},
year = {2012},
note = {AASRI Conference on Computational Intelligence and Bioinformatics},
issn = {2212-6716},
doi = {https://doi.org/10.1016/j.aasri.2012.06.039},
url = {https://www.sciencedirect.com/science/article/pii/S2212671612000406},
author = {Ping Zhuan and Su Yun Gan and Shi Ming Zhou},
keywords = {Feedback, Assimilation factor, Supporting structure, Assimilation of parameters, Evolutionary mechanism},
abstract = {In this paper, the reaction mechanism of system feedback on the changes of external environmental parameters has been discussed. And the conclusions have been attributed to the assimilation rule. According to the research results of the circuit system, assimilated factors should be defined at first--- the parts which have been isolated from the system equivalent parameters and the external parameters have been also contained. Afterwards, the analog inductive method has been adopted to conduct the overall feasibility study for the establishment of the rules. Then, several new ideas have been also provided in accordance with the applications of assimilation rules in the fields of biology, cognitive science and social organizations, etc. Finally, the block diagram has been provided so as to give a comprehensive overview for the thinking ideas of system.}
}
@article{SHYER2025204018,
title = {Transcending the hegemony of the molecular machine through an organic renewal of biology and biomedicine},
journal = {Cells & Development},
pages = {204018},
year = {2025},
issn = {2667-2901},
doi = {https://doi.org/10.1016/j.cdev.2025.204018},
url = {https://www.sciencedirect.com/science/article/pii/S2667290125000257},
author = {Amy E. Shyer and Alan R. Rodrigues},
keywords = {Supracellular morphological self-organization, systems biology, molecular machine, organicism, cancer, tissue organization field theory, chronic disease},
abstract = {The dominant approach to the study of living systems in the 20th century into today has been that of a reductionist approach focused on genetics and biochemistry. The hunt for genes and the elucidation of their biochemical outputs has organized funding in research, educational curricula, academic promotion, and the distribution of prestige through awards. Such reductionism has gone hand in hand with an ontology of the machine. We will discuss how viewing life as if it emanated from a set of molecular machines is the main bottleneck in addressing key questions in biology. We will discuss how moving beyond it is not contingent on new technologies but rather a refreshed perspective of life that can be termed “organic”. Furthermore, we suggest that the study of how form arises, morphogenesis, is the key to an organic renewal of biology and biomedicine. Although morphogenesis is currently seen as a subsidiary branch of developmental biology as well as the consequence of molecular patterning processes at the subcellular scale, we will argue that morphology and its self-organizing capacity at the supracellular scale is the fundamental nexus in embryonic development as well as disease. We see the inability to appreciate form through an organic supracellular perspective as the principal bottleneck for making inroads into health issues such as cancer and the chronic disease epidemic.}
}
@article{ABRAHAM200738,
title = {Creative cognition: The diverse operations and the prospect of applying a cognitive neuroscience perspective},
journal = {Methods},
volume = {42},
number = {1},
pages = {38-48},
year = {2007},
note = {Neurocognitive Mechanisms of Creativity: A Toolkit},
issn = {1046-2023},
doi = {https://doi.org/10.1016/j.ymeth.2006.12.007},
url = {https://www.sciencedirect.com/science/article/pii/S1046202306002994},
author = {Anna Abraham and Sabine Windmann},
keywords = {Creative cognition, Conceptual expansion, Creative imagery, Constraints of examples, Insight, Alternate uses task, Cognitive neuroscience, Neuropsychology, Top-down and bottom-up processes},
abstract = {Creativity is defined quite simply as “the ability to create” in most lexicons, but, in reality, this is a complex and heterogeneous construct about which there is much to be discovered. The cognitive approach to investigating creativity recognizes and seeks to understand this complexity by investigating the component processes involved in creative thinking. The cognitive neuroscience approach, which has only limitedly been applied in the study of creativity, should ideally build on these ideas in uncovering the neural substrates of these processes. Following an introduction into the early experimental ideas and the cognitive approach to creativity, we discuss the theoretical background and behavioral methods for testing various processes of creative cognition, including conceptual expansion, the constraining influence of examples, creative imagery and insight. The complex relations between the underlying component processes of originality and relevance across these tasks are presented thereafter. We then outline how some of these conceptual distinctions can be evaluated by neuroscientific evidence and elaborate on the neuropsychological approach in the study of creativity. Given the current state of affairs, our recommendation is that despite methodological difficulties that are associated with investigating creativity, adopting the cognitive neuroscience perspective is a highly promising framework for validating and expanding on the critical issues that have been raised in this paper.}
}
@article{STEGER2021127,
title = {Mental models of a social-ecological system facilitate social learning among a diverse management team},
journal = {Environmental Science & Policy},
volume = {122},
pages = {127-138},
year = {2021},
issn = {1462-9011},
doi = {https://doi.org/10.1016/j.envsci.2021.04.006},
url = {https://www.sciencedirect.com/science/article/pii/S1462901121001039},
author = {Cara Steger and Kflay Gebrehiwot and Shambel Alemu Chengere and Jake Marinkovich and Bikila Warkineh Dullo and Sisay Wube Zewde and Julia A. Klein},
keywords = {Participatory modeling, Social learning, Collaborative environmental management, Community-based conservation, Social-ecological systems, Ethiopia},
abstract = {Managing social-ecological systems increasingly requires collaboration among diverse teams with a wide range of worldviews and perspectives. Increased attention to the social and cultural factors that shape environmental outcomes is needed for these collaborative teams to function effectively. Mental models are cognitive representations of the external world which guide an individual’s thinking, decision-making, and behavior. They are critical elements of collaborative environmental management because they shape our understanding of social-ecological systems, our perceptions of environmental problems, and our preferences for certain management actions. In this paper, we describe an iterative process of constructing and revising mental models at both individual and small group levels over the course of a year in a community-based conservation area in the Ethiopian highlands. We compared mental models of the conservation area from four groups involved in management to identify commonalities and differences in the way people conceptualize the area. While we found high variability in mental models both within and across groups, most participants perceived social, economic, and political variables to be the key drivers of change in this system. Economic variables were also identified as key sensitivities, along with biotic and livelihood variables. However, groups differed considerably in how they thought about relationships between these variables, particularly political and economic variables. We used interviews with participants to assess how they learned throughout the mental modeling process, finding evidence of changes to stakeholder relationships, system understanding, and the time horizons used in planning. Women farmers differed from other groups at multiple stages in our process, both in the structure of the models they produced and in the social learning they experienced. Our study was strengthened by the iterative process that allowed individuals and small groups to reflect on their own understanding and share it with others, resulting in increased communication, mutual respect, and understanding among members of the management team. These findings point to the complementarity of both individual and group-level mental modeling for nuanced system understanding, and emphasize the need for diverse perspectives in collaborative environmental management in order for holistic understanding of both problems and solutions to emerge.}
}
@article{SACOUTO202297,
title = {Using brain inspired principles to unsupervisedly learn good representations for visual pattern recognition},
journal = {Neurocomputing},
volume = {495},
pages = {97-104},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2022.04.130},
url = {https://www.sciencedirect.com/science/article/pii/S0925231222005306},
author = {Luis Sa-Couto and Andreas Wichert},
keywords = {Hubel Wiesel’s Hypothesis, Brain inspired architectures, Invariant pattern recognition, Deep learning},
abstract = {Although deep learning has solved difficult problems in visual pattern recognition, it is mostly successful in tasks where there are lots of labeled training data available. Furthermore, the global back-propagation based training rule and the amount of employed layers represents a departure from biological inspiration. The brain is able to perform most of these tasks in a very general way from limited to no labeled data. For these reasons it is still a key research question to look into computational principles in the brain that can help guide models to unsupervisedly learn good representations which can then be used to perform tasks like classification. To that end, we start by recalling four key brain-inspired principles that relate to simple vision: modeling ”whats” and ”wheres” separately; including a time component; context dependency; and layer-wise learning. Then, we take these principles and use them to convey an a priori structure to our model that makes the learning problem easier. With that, our model is able to generate such high quality representations for the MNIST data set. We compare the obtained results with similar recent works and verify extremely competitive results.}
}
@article{JANG2024132519,
title = {Comparative study on gradient-free optimization methods for inverse source-term estimation of radioactive dispersion from nuclear accidents},
journal = {Journal of Hazardous Materials},
volume = {461},
pages = {132519},
year = {2024},
issn = {0304-3894},
doi = {https://doi.org/10.1016/j.jhazmat.2023.132519},
url = {https://www.sciencedirect.com/science/article/pii/S0304389423018022},
author = {Siho Jang and Juryong Park and Hyun-Ha Lee and Chun-Sil Jin and Eung Soo Kim},
keywords = {Gradient-free optimization, Multi-units & multiple radionuclides release scenario, Improving source-term estimation, Environmental radioactivity monitoring, GPU parallelization},
abstract = {In this study, we rigorously assess the performance of three gradient-free optimization algorithms—Ensemble Kalman Inversion (EKI), Particle Swarm Optimization (PSO), and Genetic Algorithm (GA)—for estimating source terms in diverse radionuclide release scenarios. Our analysis encompasses both single and multiple sources with varying radionuclide compositions, delving into the influence of decay constants and radioactivity on source estimation accuracy. Although estimating a single radionuclide from a single source exhibits outstanding results, estimating multiple radionuclides from a single source proves more arduous due to the limited information available for discerning gamma dose rates. Contrary to expectations, increasing the number of observation stations does not consistently improve the likelihood of finding accurate solutions in ill-posed inverse problems. Impressively, under our simulation settings, EKI demonstrates competitive performance in terms of convergence, accuracy, and runtime compared to PSO and GA, with GPU parallelization further bolstering computational efficiency. We explore strategies for enhancing source term estimation, including incorporating prior information, applying uncertainty removal techniques, and optimizing observation placement. Additionally, this study underscores the intricate role of relative error in determining multi-radionuclide estimation accuracy from gamma dose measurements. By employing the Gaussian plume model under steady-state conditions, our research lays the groundwork for future applications of Lagrangian dispersion models with real-time data integration. The insights gleaned from our study promise to advance environmental radioactivity monitoring and catalyze the development of cutting-edge, real-time source estimation technologies in full-scale systems.}
}
@article{WALTER2016597,
title = {The financial Logos: The framing of financial decision-making by mathematical modelling},
journal = {Research in International Business and Finance},
volume = {37},
pages = {597-604},
year = {2016},
issn = {0275-5319},
doi = {https://doi.org/10.1016/j.ribaf.2016.01.022},
url = {https://www.sciencedirect.com/science/article/pii/S0275531916300228},
author = {Christian Walter},
keywords = {Performativity, Mathematisation, Mathematical modelling, Financialisation, Ethics, Finance},
abstract = {This paper introduces the notion of “financial Logos”, defined as a structuring discourse embedded in management tools and beliefs of financial practices. I hypothesize that this discourse contains a specific representation of risk mathematically modelled by probability measures. Next I use a performativity based approach to describe the concrete action of the financial Logos on financial practices: the framing of financial decision-making by mathematical modelling. I argue that it is not possible to think of a given financial practice without epistemologically and sociologically thinking of the contribution of the mathematical modelling to this practice. I conclude with consequences for ethics of finance: extending ethics of action to epistemic ethics, I suggest that, in finance, any preference in mathematical modelling is also a preference in ethics.}
}
@article{AMOORE2024102547,
title = {The deep border},
journal = {Political Geography},
volume = {109},
pages = {102547},
year = {2024},
issn = {0962-6298},
doi = {https://doi.org/10.1016/j.polgeo.2021.102547},
url = {https://www.sciencedirect.com/science/article/pii/S0962629821002079},
author = {Louise Amoore},
keywords = {Borders, Machine learning, Immigration, Computation, Algorithms, Biometric},
abstract = {Deep neural network algorithms are becoming intimately involved in the politics of the border, and are themselves bordering devices in that they classify, divide and demarcate boundaries in data. Deep learning involves much more than the deployment of technologies at the border, and is reordering what the border means, how the boundaries of political community can be imagined. Where the biometric border rendered the border mobile through its inscription in the body, the deep border generates the racialized body in novel forms that extend the reach of state violence. The deep border is written through the machine learning models that make the world in their own image – as clusters of attributes and feature spaces from which data examples can be drawn. The ‘depth’ that becomes imaginable in computer science models of the indefinite multiplication of layers in a neural network begins to resonate with state desires for a reach into the attributes of population. The border is spatially reimagined as a set of always possible functions, features, and clusters – as a ‘line of best fit’ where the fraught politics of the border can be condensed and resolved.}
}
@article{CHEN2021105850,
title = {Coupled crash mechanics and biomechanics of aircraft structures and passengers},
journal = {Communications in Nonlinear Science and Numerical Simulation},
volume = {101},
pages = {105850},
year = {2021},
issn = {1007-5704},
doi = {https://doi.org/10.1016/j.cnsns.2021.105850},
url = {https://www.sciencedirect.com/science/article/pii/S1007570421001623},
author = {Goong Chen and Jing Yang and Alexey Sergeev and Mingwei Wang and Chunqiu Wei and Jean Yeh and Philip J. Morris and Noah J. Fournier and Yining Chen and Xingong Cheng and Donghui Yang and Shuhuang Xiang and Marlan O. Scully},
keywords = {Aircraft crash mechanics, Passenger biomechanics, Aircraft structural components andfixtures, Injury analysis, LS-DYNA modeling, Supercomputer simulations, Vibration},
abstract = {The DYCAST (Dynamic Crash Analysis of Structures) experiments that started at NASA Langley Research Center during the late 1970s have greatly influenced the methodology and thinking of aircraft crashworthiness and survivability studies, and was continued and refined at other aerospace establishments. Nevertheless, so far most of the existing work has emphasized the impact damage to the aircraft section. Issues related to potential passenger injuries have not been properly addressed in the literature, to the best of our knowledge. Here, we study the DYCAST problem integrally by treating and combining impact damage and passenger injuries altogether. We develop the biomechanics by way of modal analysis of passenger dummy motions coupled with the vibration of aircraft structures in order to understand their basic interactions. Two types of mechanical dummies are used in this study. Such a modal analysis can help identify basic injury types, but is valid only in the constructed models, linear regime. However, we are able to extend the linear elastic model to a nonlinear elastoplastic computational model by using the versatile software LS-DYNA as the platform. Computer simulations are carried out on the supercomputer clusters and the numerical results are rendered into video animations for visualization and analysis. One can see, for example, how the passenger-dummy interactive motions with the fuselage and fixtures and the potential injuries caused in the event of general aircraft crashes on a fractal domain.}
}
@article{RUIZ201575,
title = {A transformational creativity tool to support chocolate designers},
journal = {Pattern Recognition Letters},
volume = {67},
pages = {75-80},
year = {2015},
note = {Cognitive Systems for Knowledge Discovery},
issn = {0167-8655},
doi = {https://doi.org/10.1016/j.patrec.2015.05.012},
url = {https://www.sciencedirect.com/science/article/pii/S0167865515001579},
author = {Francisco J. Ruiz and Cristóbal Raya and Albert Samà and Núria Agell},
keywords = {Cognigtive system, Creativity, Creativity support system},
abstract = {A new formulation of the central ideas of Boden's well-established theory on combinational, exploratory and transformational creativity is presented. This new formulation, based on the idea of conceptual space, redefines some terms and includes several types of concept properties (appropriateness and relevance), whose relationship facilitates the computational implementation of the transformational creativity mechanism. The presented formulation is applied to a real case of chocolate designing in which a novel and flavorful combination of chocolate and fruit is generated. The experimentation was conducted jointly with a Spanish chocolate chef. Experimental results prove the relationship between appropriateness and relevance in different frameworks and show that the formulation presented is not only useful for understanding how the creative mechanisms of design works but also facilitates its implementation in real cases to support creativity processes.}
}
@article{WANG2023100113,
title = {Scheduling power-to-ammonia plants considering uncertainty and periodicity of electricity prices},
journal = {Smart Energy},
volume = {11},
pages = {100113},
year = {2023},
issn = {2666-9552},
doi = {https://doi.org/10.1016/j.segy.2023.100113},
url = {https://www.sciencedirect.com/science/article/pii/S2666955223000205},
author = {Shunchao Wang and Pengfei Zhang and Tuo Zhuo and Hua Ye},
keywords = {Power-to-ammonia, Markov decision process, Hydrogen, Haber-Bosch reactor},
abstract = {Developing affordable and scalable energy storage solutions are essential to decarbonizing power systems. The conversion of renewable electricity into chemical energy carriers such as ammonia has attracted extensive attention from academia and industry. Many Power-to-Ammonia (PtA) plants have been conceptualized and developed worldwide in recent years. The PtA plant is an integration of multiple electrochemical processes, each with a distinct set of operational constraints and cost structure. One of the problems in the operation of PtA plants is the optimal scheduling of the hydrogen buffer in PtA plants considering the operational characteristics of electrochemical processes and the volatility and uncertainty of electricity prices. In this paper, a two-stage Markov-Decision-Process (MDP) approach is proposed. The computational challenges brought by the infinite optimization horizon and non-concavity of cost functions are resolved. The first stage solution is based on the periodic MDP approach, which captures the periodic structure of electricity prices. The second stage solution gives optimal real-time decisions based on a rolling-horizon MDP approach. Numerical results show that the accurate representations of the cost functions and the optimization horizon using the proposed method are necessary, while the linearization of cost functions and the truncation of the optimization horizon lead to notable deviations from the optimality.}
}
@article{DAVEY2012e139,
title = {Results from a study with Threshold Concepts in two chemical engineering undergraduate courses},
journal = {Education for Chemical Engineers},
volume = {7},
number = {3},
pages = {e139-e152},
year = {2012},
issn = {1749-7728},
doi = {https://doi.org/10.1016/j.ece.2012.05.004},
url = {https://www.sciencedirect.com/science/article/pii/S1749772812000127},
author = {K.R. Davey},
keywords = {Threshold Concepts, Undergraduate peer presentations, Enhancement of teaching and learning in core chemical engineering, Learning and teaching in higher education},
abstract = {A new study in peer presentation of Threshold Concepts as the focus of learning in two core chemical engineering undergraduate courses has shown that students benefit from an explanatory and illustrative presentation they give to their class peers in place of the traditional lecturer. The methodology was that the lecturer identified a (progressively linked) inventory of Threshold Concepts and had students critically prepare and then explain these in brief (3–5min) presentation-and-question sessions to their cohort. The inventory was informed by Rowbottom's (2007) notion of looking for abilities for which a concept is necessary. The two courses were a level III core course on separations processing with 74 students and a level IV elective in specialist heat transfer with 15 students. Students welcomed and highly valued this type of learning with more than 90% agreeing that it improved understanding of the course material both because it revealed things better than their experiences in lectures and because it promoted a mental organisation of necessary course ideas. It is concluded that peer presentations of Threshold Concepts is a useful and economic instrument to overcoming traditional barriers to student learning. The findings could be readily applied to other courses in distinctive chemical engineering thinking and practise.}
}
@article{CHEN2022100602,
title = {Modern views of machine learning for precision psychiatry},
journal = {Patterns},
volume = {3},
number = {11},
pages = {100602},
year = {2022},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2022.100602},
url = {https://www.sciencedirect.com/science/article/pii/S2666389922002276},
author = {Zhe Sage Chen and Prathamesh (Param) Kulkarni and Isaac R. Galatzer-Levy and Benedetta Bigio and Carla Nasca and Yu Zhang},
keywords = {machine learning, ML, artificial intelligence, AI, deep learning, precision psychiatry, digital psychiatry, computational psychiatry, neuroimaging, neurobiomarker, molecular biomarker, digital phenotyping, multi-modal data fusion, neuromodulation, causality, explainable AI, XAI, teletherapy},
abstract = {Summary
In light of the National Institute of Mental Health (NIMH)’s Research Domain Criteria (RDoC), the advent of functional neuroimaging, novel technologies and methods provide new opportunities to develop precise and personalized prognosis and diagnosis of mental disorders. Machine learning (ML) and artificial intelligence (AI) technologies are playing an increasingly critical role in the new era of precision psychiatry. Combining ML/AI with neuromodulation technologies can potentially provide explainable solutions in clinical practice and effective therapeutic treatment. Advanced wearable and mobile technologies also call for the new role of ML/AI for digital phenotyping in mobile mental health. In this review, we provide a comprehensive review of ML methodologies and applications by combining neuroimaging, neuromodulation, and advanced mobile technologies in psychiatry practice. We further review the role of ML in molecular phenotyping and cross-species biomarker identification in precision psychiatry. We also discuss explainable AI (XAI) and neuromodulation in a closed human-in-the-loop manner and highlight the ML potential in multi-media information extraction and multi-modal data fusion. Finally, we discuss conceptual and practical challenges in precision psychiatry and highlight ML opportunities in future research.}
}
@article{UCAR2017249,
title = {Managing disruptions in the multi-depot vehicle scheduling problem},
journal = {Transportation Research Part B: Methodological},
volume = {105},
pages = {249-269},
year = {2017},
issn = {0191-2615},
doi = {https://doi.org/10.1016/j.trb.2017.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S0191261516305495},
author = {Ezgi Uçar and Ş. {İlker Birbil} and İbrahim Muter},
keywords = {Multi-depot vehicle scheduling, Robust planning, Column-and-row generation},
abstract = {We consider two types of disruptions arising in the multi-depot vehicle scheduling; the delays and the extra trips. These disruptions may or may not occur during operations, and hence they need to be indirectly incorporated into the planned schedule by anticipating their likely occurrence times. We present a unique recovery method to handle these potential disruptions. Our method is based on partially swapping two planned routes in such a way that the effect on the planned schedule is minimal, if these disruptions are actually realized. The mathematical programming model for the multi-depot vehicle scheduling problem, which incorporates these robustness considerations, possesses a special structure. This special structure causes the conventional column generation method fall short as the resulting problem grows also row-wise when columns are generated. We design an exact simultaneous column-and-row generation algorithm to find a valid lower-bound. The novel aspect of this algorithm is the pricing subproblem, which generates pairs of routes that form recovery solutions. Compromising on exactness, we modify this algorithm in order to enable it to solve practical-sized instances efficiently. This heuristic algorithm is shown to provide very tight bounds on the randomly generated instances in a short computation time.}
}
@article{LACHANCE2001503,
title = {Helping students build a path of understanding from ratio and proportion to decimal notation},
journal = {The Journal of Mathematical Behavior},
volume = {20},
number = {4},
pages = {503-526},
year = {2001},
issn = {0732-3123},
doi = {https://doi.org/10.1016/S0732-3123(02)00087-1},
url = {https://www.sciencedirect.com/science/article/pii/S0732312302000871},
author = {Andrea Lachance and Jere Confrey},
keywords = {Decimal notation, Ratio and proportion, Multiplicative thinking},
abstract = {Various studies have shown that students of all levels struggle to understand decimal numbers. This paper discusses a novel approach to increasing students’ conceptual understanding of decimal numbers. Rather than approach decimal notation as a discrete and separate mathematical topic, this approach enables students to work with contextual problems to gain a solid understanding of ratio and proportion. Using their understanding of ratio and proportion as a foundation, students can then build connected and related understandings of fractions, decimals and percents. The study discussed in this paper illustrates that grounding decimal instruction in the broader context of ratio can help students gain deeper conceptual understandings of decimal notation as well as fractions and percents.}
}
@incollection{MEHTA2025549,
title = {16 - State of the art in machine learning for the purpose of optimizing and predicting the properties of polymeric nanocomposites},
editor = {Alokesh Pramanik and Animesh Basak and Yu Dong and Chander Prakash and J. Paulo Davim},
booktitle = {Nanocomposite Manufacturing Technologies},
publisher = {Woodhead Publishing},
pages = {549-573},
year = {2025},
series = {Woodhead Publishing Reviews: Mechanical Engineering Series},
isbn = {978-0-12-824329-9},
doi = {https://doi.org/10.1016/B978-0-12-824329-9.00016-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128243299000164},
author = {Amrinder Mehta and Hitesh Vasudev and Chander Prakash and Alokesh Pramanik and Animesh Basak and S. Shankar},
keywords = {Nanocomposite materials, machine learning, polymeric nanocomposites, thermal properties, nanofiller, matrix},
abstract = {Polymer nanocomposites are made up of a continuous matrix phase and a nano-reinforcement phase that is spread throughout the matrix. The mechanical, electrical, and thermal properties of these materials have seen major advancements as a result of these materials. They are currently put to use in a wide number of technological applications, some of which may be found in the automotive, aeronautical, aerospace, maritime, and civil sectors, respectively. Aspect ratio, geometry, size, orientation, and dispersion are examples of some of the factors that contribute to the reinforcing effect of the nanofiller. The processes of melt-blending, compression molding, solution processing, and in-situ polymerization are the ones that are utilized most commonly when it comes to the creation of polymeric nanocomposites. In the field of material science, the creation of raw computational tools for use in the design of innovative materials has been largely superseded by the use of coupled approaches. Machine learning (ML) is a subset of artificial intelligence that allows computers to automatically improve themselves by learning from their previous mistakes and obtaining new information. This is accomplished through a process known as “machine learning.” ML makes it possible to successfully analyze the behavior of the produced composites by using a wider number of different approaches. In the case of polymeric nanocomposites, we are able to make educated guesses regarding a wide variety of multifunctional features. Because it is educated on enormous amounts of data, ML also helps to keep the cost of the models down.}
}
@article{DERBEL2014731,
title = {Distributed localized bi-objective search},
journal = {European Journal of Operational Research},
volume = {239},
number = {3},
pages = {731-743},
year = {2014},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2014.05.040},
url = {https://www.sciencedirect.com/science/article/pii/S0377221714004639},
author = {Bilel Derbel and Jérémie Humeau and Arnaud Liefooghe and Sébastien Verel},
keywords = {Multiple objective programming, Combinatorial optimization, Parallel and distributed computing, Evolutionary computation},
abstract = {We propose a new distributed heuristic for approximating the Pareto set of bi-objective optimization problems. Our approach is at the crossroads of parallel cooperative computation, objective space decomposition, and adaptive search. Given a number of computing nodes, we self-coordinate them locally, in order to cooperatively search different regions of the Pareto front. This offers a trade-off between a fully independent approach, where each node would operate independently of the others, and a fully centralized approach, where a global knowledge of the entire population is required at every step. More specifically, the population of solutions is structured and mapped into computing nodes. As local information, every node uses only the positions of its neighbors in the objective space and evolves its local solution based on what we term a ‘localized fitness function’. This has the effect of making the distributed search evolve, over all nodes, to a high quality approximation set, with minimum communications. We deploy our distributed algorithm using a computer cluster of hundreds of cores and study its properties and performance on ρMNK-landscapes. Through extensive large-scale experiments, our approach is shown to be very effective in terms of approximation quality, computational time and scalability.}
}
@article{HE2025105245,
title = {A systematic review of the use of log-based process data in computer-based assessments},
journal = {Computers & Education},
volume = {228},
pages = {105245},
year = {2025},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2025.105245},
url = {https://www.sciencedirect.com/science/article/pii/S0360131525000132},
author = {Surina He and Ying Cui},
keywords = {Computer-based assessment, Log-based process data, Systematic review},
abstract = {In recent decades, log-based process data has been increasingly used in computer-based assessments to examine test-takers' response patterns and latent traits. This study provides a systematic review of the use of log-based process data in computer-based assessments. Following the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) guideline, we identified 2548 publications, of which 330 were finally included in this study after careful screening and full-text review. The results of this study can assist researchers in better understanding: (1) what are the trends in using log-based process data in computer-based assessments, (2) which process indicators have been constructed from raw log files, (3) what latent constructs have been inferred from process indicators and at what inferential levels, and (4) what are the benefits, challenges, and future recommendations for using log-based process data. By examining these questions, we conclude that the use of log-based process data in computer-based assessment shows many potentials for enhancing the assessment. Therefore, more study using log-based process data in various fields is encouraged to better understand test-takers’ underlying response processes during assessments. Additionally, there is also a considerable demand for validating process indicators and the generalizability of findings.}
}
@article{COHEN2013620,
title = {Autoantibody repertoires, natural biomarkers, and system controllers},
journal = {Trends in Immunology},
volume = {34},
number = {12},
pages = {620-625},
year = {2013},
issn = {1471-4906},
doi = {https://doi.org/10.1016/j.it.2013.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S147149061300080X},
author = {Irun R. Cohen},
abstract = {The immune system is composed of networks of interacting cells and molecules; therefore, to understand and control immune behavior we need to adopt the thinking and tools of systems immunology. This review describes the use of an antigen microarray device and informatics to profile the repertoires of autoantibodies in health and disease. Autoantibody profiling provides an insight into the biomarkers used by the immune system in its dialog with the body. Heat shock protein 60 (HSP60) and HSP70 are cited as examples of key hubs in physiological regulatory networks; HSP molecules and peptides can be viewed as natural regulators because the immune system itself deploys them to modulate inflammatory reactions. The discovery of such natural biomarkers paves the way towards natural control.}
}
@article{AMBUJ2025110119,
title = {Intelligent path planning for autonomous ground vehicles in dynamic environments utilizing adaptive Neuro-Fuzzy control},
journal = {Engineering Applications of Artificial Intelligence},
volume = {144},
pages = {110119},
year = {2025},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2025.110119},
url = {https://www.sciencedirect.com/science/article/pii/S0952197625001198},
author = { Ambuj and Rajendra Machavaram},
keywords = {Autonomous ground vehicles, Path planning, A∗ algorithm, Adaptive neuro-fuzzy inference system, Hybrid control strategy, Real-time navigation, Dynamic environments},
abstract = {Autonomous Ground Vehicles (AGVs) are increasingly deployed across diverse industries, where enhancing their operational efficiency is critical, particularly in dynamic environments. This study proposes a hybrid control strategy that integrates an improved A∗ algorithm for path planning with a Proportional-Integral-Derivative (PID) controller adaptively tuned by an Adaptive Neuro-Fuzzy Inference System (ANFIS) for path correction. The enhanced A∗ algorithm, combined with the Dynamic Window Approach (DWA), significantly reduces computational overhead while improving pathfinding speed by incorporating the vehicle's kinematic and dynamic constraints. To address non-linearities in AGV movement, the ANFIS framework continuously fine-tunes PID parameters in real-time based on sensor feedback, improving the system's ability to correct path deviations in complex terrains. Experimental results demonstrate the efficacy of the proposed method. The enhanced A∗ algorithm achieves an average path search time of 2.52 s, significantly faster than the traditional A∗ algorithm's 5.56 s. It also reduces the average search grid size from 160 to 100, yielding a shorter path length of 27.44 m compared to 32.25 m, reflecting a more efficient path search process. Additionally, the ANFIS-PID control algorithm achieves a convergence time of 0.038 s, ensuring smooth path correction with robust stability under varying load conditions. Comparisons with state-of-the-art techniques, including Rapidly-Exploring Random Trees (RRT) and Probabilistic Roadmap Algorithm, highlight the enhanced A∗ algorithm's competitive performance, particularly in resource-constrained, real-time applications. The integration of ANFIS with PID control enhances AGV navigation by enabling adaptive, real-time path correction, improving performance in dynamic environments across agricultural, industrial, logistics, and autonomous transportation applications.}
}
@incollection{HERNANDEZGARCIA2021307,
title = {Chapter 16 - Smart and informal? Self-organization and everyday},
editor = {Alessandro Aurigi and Nancy Odendaal},
booktitle = {Shaping Smart for Better Cities},
publisher = {Academic Press},
pages = {307-319},
year = {2021},
isbn = {978-0-12-818636-7},
doi = {https://doi.org/10.1016/B978-0-12-818636-7.00012-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128186367000123},
author = {Jaime Hernández-García and Iliana Hernández-García},
keywords = {Smart cities, Smart technologies, Informal settlements, Self-organization, Everyday, Complexity},
abstract = {With the aim to offer an alternative understanding of smart cities, this chapter explores the relationship between smart and informal characteristics, presenting a discussion of two concepts arguably found in both smart and informal types of urban development: self-organization and the everyday. For this purpose, this chapter discusses the social and spatial production of informal settlements—how these areas show high degrees of self-organization based on everyday actions and interactions. In line with Rauws (2016), observers can see smart cities as networks of knowledge, actions, and selection of choices; yet this view also aligns with the actions informal settlers in Latin America take to produce their own living environments via self-organization and everyday practices. The chapter suggests how smart technologies can utilize computational logics to help measure and interpret these self-organized systems, as well as help decipher everyday creativity, based on uncertainty, autonomy, and freedom. An urban area may possess no formal planning processes, yet residents’ bottom-up social and spatial initiatives give shape to their settlements and to the city. In this sense the use of smart technologies can bring heightened understandings to informality; therefore not only the smart but also the informal can undergo reconceptualizing. We suggest viewing the smart and the informal as collective and adaptive self-organized systems fuelled by everyday practices where the social emerges as everyday creativity.}
}
@article{SHAWKY2023103476,
title = {Blockchain-based secret key extraction for efficient and secure authentication in VANETs},
journal = {Journal of Information Security and Applications},
volume = {74},
pages = {103476},
year = {2023},
issn = {2214-2126},
doi = {https://doi.org/10.1016/j.jisa.2023.103476},
url = {https://www.sciencedirect.com/science/article/pii/S2214212623000601},
author = {Mahmoud A. Shawky and Muhammad Usman and David Flynn and Muhammad Ali Imran and Qammer H. Abbasi and Shuja Ansari and Ahmad Taha},
keywords = {AVISPA simulation, BAN-logic, Key reconciliation, Public key infrastructure, Secret key extraction, Smart contracts-based blockchain},
abstract = {Intelligent transportation systems are an emerging technology that facilitates real-time vehicle-to-everything communication. Hence, securing and authenticating data packets for intra- and inter-vehicle communication are fundamental security services in vehicular ad-hoc networks (VANETs). However, public-key cryptography (PKC) is commonly used in signature-based authentication, which consumes significant computation resources and communication bandwidth for signatures generation and verification, and key distribution. Therefore, physical layer-based secret key extraction has emerged as an effective candidate for key agreement, exploiting the randomness and reciprocity features of wireless channels. However, the imperfect channel reciprocity generates discrepancies in the extracted key, and existing reconciliation algorithms suffer from significant communication costs and security issues. In this paper, PKC-based authentication is used for initial legitimacy detection and exchanging authenticated probing packets. Accordingly, we propose a blockchain-based reconciliation technique that allows the trusted third party (TTP) to publish the correction sequence of the mismatched bits through a transaction using a smart contract. The smart contract functions enable the TTP to map the transaction address to vehicle-related information and allow vehicles to obtain the transaction contents securely. The obtained shared key is then used for symmetric key cryptography (SKC)-based authentication for subsequent transmissions, saving significant computation and communication costs. The correctness and security robustness of the scheme are proved using Burrows–Abadi–Needham (BAN)-logic and Automated Validation of Internet Security Protocols and Applications (AVISPA) simulator. We also discussed the scheme’s resistance to typical attacks. The scheme’s performance in terms of packet delay and loss ratio is evaluated using the network simulator (OMNeT++). Finally, the computation analysis shows that the scheme saves ∼99% of the time required to verify 1000 messages compared to existing PKC-based schemes.}
}
@article{AGRAWAL20241,
title = {A systematic review on metaheuristic approaches for autonomous path planning of unmanned aerial vehicles},
journal = {Drone Systems and Applications},
volume = {12},
pages = {1-28},
year = {2024},
issn = {2564-4939},
doi = {https://doi.org/10.1139/dsa-2023-0093},
url = {https://www.sciencedirect.com/science/article/pii/S2564493924000158},
author = {Sameer Agrawal and Bhumeshwar K. Patle and Sudarshan Sanap},
keywords = {artificial intelligence, path planning, metaheuristic algorithms, UAV, mobile robot navigation},
abstract = {In the path planning of UAVs, autonomous decision-making and control are challenging tasks in the uncertain 3D environment consisting of static and dynamic obstacles. Hence, the selection of appropriate path-planning approaches is essential. In the proposed work, we have considered the meta-heuristic approaches only for an in-depth review. Metaheuristic approaches have been remarkably known for solving complex problems, optimal solutions, and lesser computational complexity compared to deterministic approaches that produce an inefficient solution. An in-depth review has been made by considering the approaches used for path planning, their advantages, disadvantages, applications, the type of time domain (offline or online), type of environment (simulation or real time), hybridization with other approaches, single or multiple UAV system, and obstacle handled (static or dynamic). It is observed that current meta-heuristic methods face constraints like inadequate convergence rates, entrapment in local optima, and complex operations, necessitating continuous development of novel approaches. Implementation of path-planning approaches are very much limited to simulation study over experimental analysis. Hybrid algorithms emerge as a potential solution for tackling these hurdles and optimizing UAV navigation, particularly in dynamic environments involving multiple UAVs. The paper highlights key research gaps, trends, along with prospects in the field of research.}
}
@article{BERTO2024101278,
title = {A motivational-based learning model for mobile robots},
journal = {Cognitive Systems Research},
volume = {88},
pages = {101278},
year = {2024},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2024.101278},
url = {https://www.sciencedirect.com/science/article/pii/S138904172400072X},
author = {Letícia Berto and Paula Costa and Alexandre Simões and Ricardo Gudwin and Esther Colombini},
keywords = {Motivation, Action selection and planning, Models of internal states, Internal reinforces},
abstract = {Humans have needs motivating their behavior according to intensity and context. However, we also create preferences associated with each action’s perceived pleasure, which is susceptible to changes over time. This makes decision-making more complex, requiring learning to balance needs and preferences according to the context. To understand how this process works and enable the development of robots with a motivational-based learning model, we computationally model a motivation theory proposed by Hull. In this model, the agent (an abstraction of a mobile robot) is motivated to keep itself in a state of homeostasis. We introduced hedonic dimensions to explore the impact of preferences on decision-making and employed reinforcement learning to train our motivated-based agents. In our experiments, we deploy three agents with distinct energy decay rates, simulating different metabolic rates, within two diverse environments. We investigate the influence of these conditions on their strategies, movement patterns, and overall behavior. The findings reveal that agents excel at learning more effective strategies when the environment allows for choices that align with their metabolic requirements. Furthermore, we observe that incorporating pleasure as a component of the motivational mechanism affects behavior learning, particularly for agents with regular metabolisms depending on the environment. Our study also unveils that, when confronted with survival challenges, agents prioritize immediate needs over pleasure and equilibrium. These insights shed light on how robotic agents can adapt and make informed decisions in demanding scenarios, demonstrating the intricate interplay between motivation, pleasure, and environmental context in autonomous systems.}
}
@article{TYFLOPOULOS2019979,
title = {Messing with boundaries - quantifying the potential loss by pre-set parameters in topology optimization},
journal = {Procedia CIRP},
volume = {84},
pages = {979-985},
year = {2019},
note = {29th CIRP Design Conference 2019, 08-10 May 2019, Póvoa de Varzim, Portgal},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2019.04.307},
url = {https://www.sciencedirect.com/science/article/pii/S2212827119309552},
author = {Evangelos Tyflopoulos and Martin Steinert},
keywords = {topology optimization, SIMP, additive manufacturing, product development, design, finite element analysis},
abstract = {Additive manufacturing can increase the flexibility in the design phase of product development and that, in its turn, has changed the designer’s way of thinking. The design problem has reformulated; from designs that were not possible to be constructed, due to lack of equipment and technology, to constructions that the designer could not think to design. Topology optimization and generative design are useful tools in the hands of designer that can help him/her in the pursuit of the global optimum of a construction and in the choice of an alternative design solution respectively. However, topology optimization results are always depended on the given boundary conditions and restrictions. In other words, the designer’s decisions can affect the results of topology optimization and can easily lead to a local and not a global solution. In this paper, an identification and categorization of the most important parameters, that can affect the topology optimization results, were conducted. The main focus of the implemented research was on the pre-processing of topology optimization and especially on the designer’s decisions. The applied topology optimization approach here was a simple compliance optimization based on the SIMP interpolation methodology (Solid Isotropic Material with Penalization) and it was executed with the use of the commercial software Tosca (Abaqus). Different alternative designs of a wall bracket were used as a case study to test the sensitivity of the optimization algorithm and quantify the potential loss.}
}
@article{MARIA2007695,
title = {Emotional agents: A modeling and an application},
journal = {Information and Software Technology},
volume = {49},
number = {7},
pages = {695-716},
year = {2007},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2006.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S0950584906001030},
author = {Khulood Abu Maria and Raed Abu Zitar},
keywords = {Agent, Emotions, Behavior, Personality},
abstract = {This paper proposes modeling of artificial emotions through agents based on symbolic approach. The symbolic approach utilizes symbolic emotional rule-based systems (rule base that generated emotions) with continuous interactions with environment and an internal “thinking” machinery that comes as a result of series of inferences, evaluation, evolution processes, adaptation, learning, and emotions. We build two models for agent based systems; one is supported with artificial emotions and the other one without emotions. We use both in solving a bench mark problem; “The Orphanage Care Problem”. The two systems are simulated and results are compared. Our study shows that systems with proper model of emotions can perform in many cases better than systems without emotions. We try to shed the light here on how artificial emotions can be modeled in a simple rule-based agent systems and if emotions as they exist in “real intelligence” can be helpful for “artificial intelligence”. Agent architectures are presented as a generic blueprint on which the design of agents can be based. Our focus is on the functional design, including flow of information and control. With this information provided, the generic blueprints of architectures should not be difficult to implement agents, thus putting these theoretical models into practice. We build the agents using this architecture, and many experiments and analysis are shown.}
}
@article{LIU2025108569,
title = {Enhancing student GAI literacy in digital multimodal composing through development and validation of a scale},
journal = {Computers in Human Behavior},
volume = {166},
pages = {108569},
year = {2025},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2025.108569},
url = {https://www.sciencedirect.com/science/article/pii/S0747563225000160},
author = {Meilu Liu and Lawrence Jun Zhang and Donglan Zhang},
keywords = {Generative artificial intelligence, Digital multimodal composing, Literacy},
abstract = {It is widely acknowledged that Generative Artificial Intelligence (GAI) has exerted a greater influence on EFL learners' digital multimodal composing (DMC) process. GAI focuses on creating new textual and multimodal content using large language models (LLMs), and it puts different demands on EFL learners. Although much research has been conducted on EFL learners' AI literacy in various socio-cultural contexts, more attention should now be paid to EFL learners' GAI literacy in the DMC context, a new autonomous model of literacy. It should be noted that even though some studies may concentrate on users' perceptions and experiences with GAI, which may be closely tied to GAI literacy, there lacks the development of a scale for assessing GAI literacy in DMC. Thus, this study attempted to fill these research gaps by developing and validating an applicable and generalizable instrument to measure Chinese EFL learners' multimodal GAI literacy in their DMC process. Two subsamples (n1 = 296, n2 = 294) were randomly invited to respond to the GAIDMCS, and the data were subjected to exploratory factor analysis (EFA) and confirmatory factor analysis (CFA) to test the validity and reliability of the instrument. The findings suggested that a four-factor solution with 17 items can help explain Chinese EFL learners’ GAI literacy in DMC in terms of affective learning, behavior learning, cognitive learning, and ethical learning. Our GAI literacy in DMC scale may help improve GAI education for researchers and practitioners by providing a comprehensive and plausible framework that can serve as an outline for further syllabus design.}
}
@article{GALLAGHER2023100127,
title = {Navigating the uncertainty of precision cancer screening: The role of shared decision-making},
journal = {PEC Innovation},
volume = {2},
pages = {100127},
year = {2023},
issn = {2772-6282},
doi = {https://doi.org/10.1016/j.pecinn.2023.100127},
url = {https://www.sciencedirect.com/science/article/pii/S2772628223000079},
author = {Joseph H. Gallagher and Jason L. Vassy and Marla L. Clayman},
keywords = {Shared decision making, Polygenic risk scores, Cancer screening, Genetic counseling, Genomic testing, Patient-provider communication},
abstract = {Objective
Describe how applying a shared decision making (SDM) lens to the implementation of new technologies can improve patient-centeredness.
Methods
This paper argues that the emergence of polygenic risk scores (PRS) for cancer screening presents an illustrative opportunity to include SDM when novel technologies enter clinical care.
Results
PRS are novel tools that indicate an individual’s genetic risk of a given disease relative to the population. PRS are anticipated to help identify individuals most and least likely to benefit from screening. However, PRS have several types of uncertainty, including validity across populations, disparate computational methods, and inclusion of different genomic data across laboratories.
Conclusion
Implementing SDM alongside new technologies could prove useful for their ethical and patient-centered utilization. SDM’s importance as an approach to decision-making will not diminish, as evidence, uncertainty, and patient values will remain intrinsic to the art and science of clinical care.
Innovation
SDM can help providers and patients navigate the considerable uncertainty inherent in implementing new technologies, enabling decision-making based on existing evidence and patient values.}
}
@article{JOHNSON2023104327,
title = {Why is biomedical informatics hard? A fundamental framework},
journal = {Journal of Biomedical Informatics},
volume = {140},
pages = {104327},
year = {2023},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2023.104327},
url = {https://www.sciencedirect.com/science/article/pii/S1532046423000485},
author = {Todd R. Johnson and Elmer V. Bernstam},
keywords = {Biomedical informatics, Scientific discipline, Data, Information, Knowledge, Definition, Philosophy of information},
abstract = {Building on previous work to define the scientific discipline of biomedical informatics, we present a framework that categorizes fundamental challenges into groups based on data, information, and knowledge, along with the transitions between these levels. We define each level and argue that the framework provides a basis for separating informatics problems from non-informatics problems, identifying fundamental challenges in biomedical informatics, and provides guidance regarding the search for general, reusable solutions to informatics problems. We distinguish between processing data (symbols) and processing meaning. Computational systems, that are the basis for modern information technology (IT), process data. In contrast, many important challenges in biomedicine, such as providing clinical decision support, require processing meaning, not data. Biomedical informatics is hard because of the fundamental mismatch between many biomedical problems and the capabilities of current technology.}
}
@article{RIZZI19871,
title = {Selected topics in the theory and practice of computational fluid dynamics},
journal = {Journal of Computational Physics},
volume = {72},
number = {1},
pages = {1-69},
year = {1987},
issn = {0021-9991},
doi = {https://doi.org/10.1016/0021-9991(87)90072-6},
url = {https://www.sciencedirect.com/science/article/pii/0021999187900726},
author = {Arthur Rizzi and Björn Engquist},
abstract = {Computational fluid dynamics (CFD) is a large branch of scientific computing that lately has undergone explosive growth. It draws upon elements from related disciplines: fluid mechanics, numerical analysis, theory of partial differential equations, computer science, and computational geometry. By selecting certain topics we try to trace the way the dramatic growth came about and to illustrate the interplay of the related disciplines. The scope is broad and the emphasis is on discussing the underlying fundamentals in order to present an overall perspective on CFD. The focus is on the evolution of nonsmooth features in inviscid flows, primarily macroscale discontinuities like shock waves and vortex sheets admitted as solutions to the Euler equations, but also with some view to their possible unstable progression into small-scale features, ending ultimately in turbulence. Some of the current finite-difference methods, and the theory they are based upon, which are used to treat these problems are reviewed, and different grid generation techniques are introduced. Together with some principles for using advanced supercomputers, we also discuss how the methods are implemented on these machines. A number of computed results, some of them new and of large scale with up to one million grid points, are presented which reflect the limits of the theory and the current status of the field.}
}
@article{BESTER2024820,
title = {Complementary conservation of South African crop wild relatives for plant improvement},
journal = {South African Journal of Botany},
volume = {174},
pages = {820-829},
year = {2024},
issn = {0254-6299},
doi = {https://doi.org/10.1016/j.sajb.2024.09.041},
url = {https://www.sciencedirect.com/science/article/pii/S0254629924005945},
author = {C Bester and NC {Le Maitre} and M Visser and WC Botes},
keywords = {Crop wild relatives (CWR), Complementary conservation, Plant breeding, Southern African development community (SADC), CAPFITOGEN},
abstract = {Crop Wild Relatives (CWR) are good sources of unexplored genetic diversity that can assist plant breeders to increase the yield and resilience of their crops. These species are valuable plant genetic resources (PGR) that have been used in more than 4,157 documented cases of plant improvement to date. South Africa has 258 prioritized CWR, selected based on their distribution, threat status and potential as gene donors. In light of ongoing habitat destruction, global warming and mismanagement of resources, the conservation of these PGR is vital. Complementary conservation approaches allow for the continuous development of CWR, while harnessing and applying the available diversity in plant breeding programs. The South African National Biodiversity Strategy and Action Plan (NBSAP) strives to utilize conservation resources to build and maintain an effective complementary, in situ to ex situ conservation pipeline. As part of the Southern African Development Community (SADC), South Africa has access to numerous resources that can assist to protect its rich floral diversity, including the SADC Plant Genetic Resource Centre (SPGRC), the SADC CWR Project and CAPFITOGEN3.}
}
@article{PLANT20173335,
title = {Can a systems approach produce a better understanding of mood disorders?},
journal = {Biochimica et Biophysica Acta (BBA) - General Subjects},
volume = {1861},
number = {1, Part A},
pages = {3335-3344},
year = {2017},
issn = {0304-4165},
doi = {https://doi.org/10.1016/j.bbagen.2016.08.016},
url = {https://www.sciencedirect.com/science/article/pii/S0304416516303051},
author = {Nick Plant},
keywords = {Affective disorder, Bipolar disorder, Computational biology, Drug development, Systems biology},
abstract = {Background
One in twenty-five people suffer from a mood disorder. Current treatments are sub-optimal with poor patient response and uncertain modes-of-action. There is thus a need to better understand underlying mechanisms that determine mood, and how these go wrong in affective disorders. Systems biology approaches have yielded important biological discoveries for other complex diseases such as cancer, and their potential in affective disorders will be reviewed.
Scope of review
This review will provide a general background to affective disorders, plus an outline of experimental and computational systems biology. The current application of these approaches in understanding affective disorders will be considered, and future recommendations made.
Major conclusions
Experimental systems biology has been applied to the study of affective disorders, especially at the genome and transcriptomic levels. However, data generation has been slowed by a lack of human tissue or suitable animal models. At present, computational systems biology has only be applied to understanding affective disorders on a few occasions. These studies provide sufficient novel biological insight to motivate further use of computational biology in this field.
General significance
In common with many complex diseases much time and money has been spent on the generation of large-scale experimental datasets. The next step is to use the emerging computational approaches, predominantly developed in the field of oncology, to leverage the most biological insight from these datasets. This will lead to the critical breakthroughs required for more effective diagnosis, stratification and treatment of affective disorders.}
}
@article{CHEN2020103670,
title = {A visual learning analytics (VLA) approach to video-based teacher professional development: Impact on teachers’ beliefs, self-efficacy, and classroom talk practice},
journal = {Computers & Education},
volume = {144},
pages = {103670},
year = {2020},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2019.103670},
url = {https://www.sciencedirect.com/science/article/pii/S0360131519302234},
author = {Gaowei Chen},
keywords = {Teacher professional development, Data science applications in education, Improving classroom teaching, Pedagogical issues},
abstract = {To address the challenge of overwhelming data inherent in classroom lesson videos, this study proposed a visual learning analytics (VLA) approach to video-based teacher professional development (TPD). Using a two-year experimental design, 46 secondary mathematics teachers were divided randomly into a treatment group (N = 24) and a control group (N = 22) to learn about and integrate academically productive talk into their teaching. The treatment teachers participated in a VLA-supported TPD program, while the control teachers participated in conventional knowledge-based workshops. Results show that teachers in the treatment group had more positive beliefs and higher self-efficacy in the post-test and delayed-post-test, while the control group improved, but not significantly, in their beliefs about the usefulness of classroom talk. In addition, although the control group made a significant improvement in their self-efficacy in guiding classroom talk in the post-test, this improvement was not sustained to the delayed post-test. Moreover, the coding of classroom teaching behaviour revealed that teachers in the treatment group relative to the control group significantly increased their use of academically productive talk in the post-test lessons to encourage the students' elaboration, reasoning, and thinking with others in the classroom. The results suggest that, while attending knowledge-based workshops had, to some degree, positive effects on the control teachers' beliefs and self-efficacy, these effects were not sustainable over time. In contrast, the use of visual learning analytics to support the treatment group's reflection on the classroom data not only had significant and sustained effects on the teachers' beliefs and self-efficacy but also significantly influenced their actual classroom teaching behaviour. Implications for designing VLA to support teacher learning and professional development are discussed.}
}
@article{LIU2024110132,
title = {Multimodal brain-controlled system for rehabilitation training: Combining asynchronous online brain–computer interface and exoskeleton},
journal = {Journal of Neuroscience Methods},
volume = {406},
pages = {110132},
year = {2024},
issn = {0165-0270},
doi = {https://doi.org/10.1016/j.jneumeth.2024.110132},
url = {https://www.sciencedirect.com/science/article/pii/S0165027024000773},
author = {Lei Liu and Jian Li and Rui Ouyang and Danya Zhou and Cunhang Fan and Wen Liang and Fan Li and Zhao Lv and Xiaopei Wu},
keywords = {Movement impairment, Rehabilitation, Brain–computer interface, Motor imagery, Steady-state visual evoked potential},
abstract = {Background:
Traditional therapist-based rehabilitation training for patients with movement impairment is laborious and expensive. In order to reduce the cost and improve the treatment effect of rehabilitation, many methods based on human–computer interaction (HCI) technology have been proposed, such as robot-assisted therapy and functional electrical stimulation (FES). However, due to the lack of active participation of brain, these methods have limited effects on the promotion of damaged nerve remodeling.
New method:
Based on the neurofeedback training provided by the combination of brain–computer interface (BCI) and exoskeleton, this paper proposes a multimodal brain-controlled active rehabilitation system to help improve limb function. The joint control mode of steady-state visual evoked potential (SSVEP) and motor imagery (MI) is adopted to achieve self-paced control and thus maximize the degree of brain involvement, and a requirement selection function based on SSVEP design is added to facilitate communication with aphasia patients.
Comparison with existing methods:
In addition, the Transformer is introduced as the MI decoder in the asynchronous online BCI to improve the global perception of electroencephalogram (EEG) signals and maintain the sensitivity and efficiency of the system.
Results:
In two multi-task online experiments for left hand, right hand, foot and idle states, subject achieves 91.25% and 92.50% best accuracy, respectively.
Conclusion:
Compared with previous studies, this paper aims to establish a high-performance and low-latency brain-controlled rehabilitation system, and provide an independent and autonomous control mode of the brain, so as to improve the effect of neural remodeling. The performance of the proposed method is evaluated through offline and online experiments.}
}
@incollection{ZANNI202057,
title = {Chapter 4 - Life cycle sustainability assessment: An ongoing journey},
editor = {Jingzheng Ren and Sara Toniolo},
booktitle = {Life Cycle Sustainability Assessment for Decision-Making},
publisher = {Elsevier},
pages = {57-93},
year = {2020},
isbn = {978-0-12-818355-7},
doi = {https://doi.org/10.1016/B978-0-12-818355-7.00004-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012818355700004X},
author = {Sara Zanni and Eric Awere and Alessandra Bonoli},
keywords = {Life cycle sustainability assessment, Integrated sustainability assessment, Sustainable development, Review, Standardization, Case studies},
abstract = {As the call for sustainable solutions at operational, industrial, and policy level increases, the need for a comprehensive assessment tool has been addressed by literature and practitioners. In particular, for the definition of a complete framework, the application of a life cycle thinking lens is required to explore the longitudinal dimension of the impacts and possible indirect effects triggered on environmental, social, and economic levels. The definition of an integrated life cycle sustainability assessment framework is currently an ongoing journey, which is summarized in the present chapters. The narrative follows a set of milestones, namely the definition of the concept and the preliminary scheme in the early years, the pathway towards the implementation of a standardized set of tools, an anthology of significant case studies in different sectors, and an overview of the challenges identified by literature and yet remaining open for future researches.}
}
@article{GALLISTEL2017498,
title = {The Coding Question},
journal = {Trends in Cognitive Sciences},
volume = {21},
number = {7},
pages = {498-508},
year = {2017},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2017.04.012},
url = {https://www.sciencedirect.com/science/article/pii/S1364661317300852},
author = {C.R. Gallistel},
abstract = {Recent electrophysiological results imply that the duration of the stimulus onset asynchrony in eyeblink conditioning is encoded by a mechanism intrinsic to the cerebellar Purkinje cell. This raises the general question – how is quantitative information (durations, distances, rates, probabilities, amounts, etc.) transmitted by spike trains and encoded into engrams? The usual assumption is that information is transmitted by firing rates. However, rate codes are energetically inefficient and computationally awkward. A combinatorial code is more plausible. If the engram consists of altered synaptic conductances (the usual assumption), then we must ask how numbers may be written to synapses. It is much easier to formulate a coding hypothesis if the engram is realized by a cell-intrinsic molecular mechanism.}
}
@article{BARADARANRAHIMI2025105859,
title = {Exploring the future of urban death: Speculative design and the concept of necropolis 4.0},
journal = {Cities},
volume = {161},
pages = {105859},
year = {2025},
issn = {0264-2751},
doi = {https://doi.org/10.1016/j.cities.2025.105859},
url = {https://www.sciencedirect.com/science/article/pii/S0264275125001593},
author = {Farzan {Baradaran Rahimi}},
keywords = {Urban future, Speculative design, TOPSIS, Necropolis, Emerging technologies, Design innovation},
abstract = {As urban areas rapidly expand, they grapple with multifaceted issues such as population growth, climate change, land shortage, resource constraints, and social inequalities. Proactive planning for the future is essential to foster the development of sustainable and resilient cities capable of adapting to evolving needs. One such needs is designing for death in the urban future, given the outdated, polluting, cumbersome, and unsustainable methods currently in use. Drawing inspiration from theories of social space, hybrid space, and the historical concept of the necropolis, while integrating technological advancements such as extended reality, super artificial intelligence, quantum computing, biodegradable materials, and Web 4.0, this study aims to reimagine the design for death in the urban future through three alternative scenarios. Integrating technique for order preference by similarity to ideal solution into speculative design approach, experts' evaluation identifies Necropolis 4.0 as the closest scenario to the ideal solution. Findings serve a dual purpose. First, focusing on Necropolis 4.0, establishes a nature-human-machine relationship and paves the way for designers, planners, and policymakers to envision a novel, green, and sustainable design for death in the urban future. Second, methodological contribution of this research enhances the way we use speculative design approach in urban planning.}
}
@article{KE2024111909,
title = {Improving the transferability of adversarial examples through neighborhood attribution},
journal = {Knowledge-Based Systems},
volume = {296},
pages = {111909},
year = {2024},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.111909},
url = {https://www.sciencedirect.com/science/article/pii/S0950705124005434},
author = {Wuping Ke and Desheng Zheng and Xiaoyu Li and Yuanhang He and Tianyu Li and Fan Min},
keywords = {Adversarial examples, Computer vision, Neural networks, AI security},
abstract = {Adversarial examples, which add carefully planned perturbations to images, pose a serious threat to neural network applications. Transferable adversarial attacks, in which adversarial examples generated on the source model can successfully attack the target model, provide a realistic and undetectable method. Existing transfer-based attacks tend to improve the transferability of adversarial examples by destroying their intrinsic features. They destabilized features differentially by assessing their importance, thus rendering the model incapable of inference. However, the existing methods generate feature-importance assessments that are overly dependent on the source model, leading to inaccurate importance guidance and insufficient feature destruction. In this paper, we propose neighborhood expectancy attribution attacks (NEAA) that accurately guide the destruction of deep features, leading to highly transferable adversarial examples. First, we design a highly versatile attribution tool called neighborhood attribution to represent the importance of features that attribute highly similar results to various source models. Specifically, we discard the imputation of a single baseline and adopt the imputed expectation of a baseline within the neighborhood of the image. Subsequently, we generalize the neighborhood attribution to the middle layer of the model and simplify the computation by assuming linear independence. Finally, the attribution result guides the attack to destroy the intrinsic features of the image and obtain highly transferable adversarial examples. Numerous experiments demonstrate the effectiveness of the proposed method. Code is available at Github: https://github.com/KWPCCC/NEAA.}
}
@article{MAXVILLE20111953,
title = {eScience: Building our Body of Knowledge},
journal = {Procedia Computer Science},
volume = {4},
pages = {1953-1963},
year = {2011},
note = {Proceedings of the International Conference on Computational Science, ICCS 2011},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2011.04.213},
url = {https://www.sciencedirect.com/science/article/pii/S1877050911002717},
author = {Valerie Maxville},
abstract = {This paper describes the need for an eScience BoK, particularly as a resource for educators. eScience is a term representing the computational technology and techniques utilised when undertaking research. As eScience matures, stakeholders, and particularly educators, can benefit from the clarity that a defined Body of Knowledge (BOK) can provide. The BOK would require domain-specific and technological aspects to be addressed. This paper describes a framework for a prototype BOK for eScience and discusses how the BOK can be used as a tool to drive education, outreach and infrastructure planning.}
}
@article{DIMARCO2020101290,
title = {(re)Producing mtEve},
journal = {Studies in History and Philosophy of Science Part C: Studies in History and Philosophy of Biological and Biomedical Sciences},
volume = {83},
pages = {101290},
year = {2020},
issn = {1369-8486},
doi = {https://doi.org/10.1016/j.shpsc.2020.101290},
url = {https://www.sciencedirect.com/science/article/pii/S1369848620300303},
author = {Marina DiMarco},
abstract = {In their 1987 Nature publication, “Mitochondrial DNA and Human Evolution,” Rebecca Cann, Mark Stoneking, and Allan C. Wilson gave a new reconstruction of human evolution on the basis of differences in mitochondrial DNA among contemporary human populations. This phylogeny included an African common ancestor for all human mitochondrial DNA (mtDNA) lineages, and Cann et al.’s reconstruction became known as the “Out of Africa” hypothesis. Since mtDNA is inherited exclusively through the maternal line, the common ancestor who was first branded African Eve later became known as Mitochondrial Eve (mtEve, for short). In this paper, I show that mtEve was not a single, successful, or purely scientific discovery. Instead, she was produced many times and in many ways, each of which informed the next. Importantly, though Wilson and colleagues heralded mitochondrial DNA as a source of certainty, objectivity, and consensus for evolutionary inference, their productions of Mitochondrial Eve depended as much on popular assumptions about the certainty of maternal inheritance as they did on new molecular and computational tools. This recognition lets us reevaluate the complex consequences of these productions, which, like mtEve herself, could not be confined to a purely social, material, or scientific dimension.}
}
@article{ROMANATO2000277,
title = {Computation of the strain field generated by dislocations with a position-dependent Burgers' vector distribution},
journal = {Micron},
volume = {31},
number = {3},
pages = {277-283},
year = {2000},
issn = {0968-4328},
doi = {https://doi.org/10.1016/S0968-4328(99)00094-3},
url = {https://www.sciencedirect.com/science/article/pii/S0968432899000943},
author = {F Romanato and M Natali and E Napolitani and A Drigo},
keywords = {Burgers' vector, Misfit dislocation, Reciprocal space maps},
abstract = {A new phenomenon of strain relaxation will be presented. In a series of InxGa1−xAs graded composition buffer layers grown on well cut (001) GaAs substrates, a curvature of the epilayer lattice has been found, i.e. a tilt of the epilayer lattice orientation with respect to the substrate which varies coherently along the sample surface on the scale of several mm. The most recent data analysis performed on a buffer layer compositionally graded with a six-step profile shows also a thickness functional dependence of the curvature. The epilayer lattice curvature has been attributed to a coherent lateral distribution of the Burgers’ vectors. An analytical model has been developed in the framework of the continuum elasticity theory to compute the related strain field. The results show small but unexpected contributions to the parallel strain.}
}
@article{HEMASPAANDRA202266,
title = {The complexity of online bribery in sequential elections},
journal = {Journal of Computer and System Sciences},
volume = {127},
pages = {66-90},
year = {2022},
issn = {0022-0000},
doi = {https://doi.org/10.1016/j.jcss.2022.01.006},
url = {https://www.sciencedirect.com/science/article/pii/S0022000022000071},
author = {Edith Hemaspaandra and Lane A. Hemaspaandra and Jörg Rothe},
keywords = {Bribery, Computational complexity, Computational social choice, Logic, Quantifier assignment, Sequential elections},
abstract = {Prior work on the complexity of bribery assumes that the bribery happens simultaneously, and that the briber has full knowledge of all votes. However, in many real-world settings votes come in sequentially, and the briber may have a use-it-or-lose-it moment to decide whether to alter a given vote, and when making that decision the briber may not know what votes remaining voters will cast. We introduce a model for, and initiate the study of, bribery in such an online, sequential setting. We show that even for election systems whose winner-determination problem is polynomial-time computable, an online, sequential setting may vastly increase the complexity of bribery, jumping the problem up to completeness for high levels of the polynomial hierarchy or even PSPACE. But we also show that for some natural, important election systems, such a dramatic complexity increase does not occur, and we pinpoint the complexity of their bribery problems.}
}
@article{SRIDHAR2022113207,
title = {Extraction techniques in food industry: Insights into process parameters and their optimization},
journal = {Food and Chemical Toxicology},
volume = {166},
pages = {113207},
year = {2022},
issn = {0278-6915},
doi = {https://doi.org/10.1016/j.fct.2022.113207},
url = {https://www.sciencedirect.com/science/article/pii/S0278691522004057},
author = {Adithya Sridhar and Vijay Vaishampayan and P. {Senthil Kumar} and Muthamilselvi Ponnuchamy and Ashish Kapoor},
keywords = {Extraction, Food, Modelling, Optimization, Sustainability},
abstract = {This review presents critical evaluation of the key parameters that affect the extraction of targeted components, giving due consideration to safety and environmental aspects. The crucial aspects of the extraction technologies along with protocols and process parameters for designing unit operations have been emphasized. The parameters like solvent usage, substrate type, concentration, particle size, temperature, quality and storage of extract as well as stability of extraction have been elaborately discussed. The process optimization using mathematical and computational modeling highlighting information and communication technologies have been given importance aiming for a green and sustainable industry level scaleup. The findings indicate that the extraction processes vary significantly depending on the category of food and its structure. There is no single extraction method or universal set of process conditions identified for extracting all value-added products from respective sources. A comprehensive understanding of process parameters and their optimization as well as synergistic combination of multiple extraction processes can aid in enhancement of the overall extraction efficiency. Future efforts must be directed toward the design of integrated unit operations that cause minimal harm to the environment along with investigations on economic feasibility to ensure sustainable extraction systems.}
}
@article{PALMER2024110848,
title = {Assessing between-individual variability in bioenergetics modelling: Opportunities, challenges, and potential applications},
journal = {Ecological Modelling},
volume = {498},
pages = {110848},
year = {2024},
issn = {0304-3800},
doi = {https://doi.org/10.1016/j.ecolmodel.2024.110848},
url = {https://www.sciencedirect.com/science/article/pii/S0304380024002369},
author = {Miquel Palmer and Irene Moro-Martínez and Joaquim Tomàs-Ferrer and Amalia Grau and María Dolores López-Belluga and Marine Herlin and Orestis Stavrakidis-Zachou and Andrea Campos-Candela},
keywords = {Between-individual differences, Dynamic Energy Budget, Bayesian},
abstract = {Population dynamics is influenced by between-individual variability. Dynamic Energy Budget (DEB) theory is an appealing framework for assessing such a variability, yet DEB parameters have rarely been estimated at the individual level. Bayesian hierarchical models show promise for inferring individual variability in DEB parameters, thought computational challenges have limited their use due to the need to solve differential equations. Timely, Stan has emerged as a general-purpose statistical tool for fitting dynamic models. This paper introduces an analytical strategy using Bayesian parametric inference and hierarchical modelling to estimate individual-specific DEB parameters. Two biologically relevant DEB parameters were successfully estimated for 69 Gilt-head breams (Sparus aurata) with up to 11 measures of length and wet weight each. The estimated between-individual variability in these two DEB parameters explained well the observed patterns in length and weight at between- and within-individual levels. Moreover, data-simulation experiments highlighted the potential and limitations of our approach, suggesting that improved data collection could enable to increase precision and the number of DEB parameters that can be estimated at the individual level. This strategy can better represent between-individual variability in DEB parameters, which ultimately may improve forecasting of population dynamics after integrating DEB into population models.}
}
@article{CHEN2023103837,
title = {Recursive reasoning-based training-time adversarial machine learning},
journal = {Artificial Intelligence},
volume = {315},
pages = {103837},
year = {2023},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2022.103837},
url = {https://www.sciencedirect.com/science/article/pii/S0004370222001771},
author = {Yizhou Chen and Zhongxiang Dai and Haibin Yu and Bryan Kian Hsiang Low and Teck-Hua Ho},
keywords = {Recursive reasoning, Adversarial machine learning, Game theory},
abstract = {The training process of a machine learning (ML) model may be subject to adversarial attacks from an attacker who attempts to undermine the test performance of the ML model by perturbing the training minibatches, and thus needs to be protected by a defender. Such a problem setting is referred to as training-time adversarial ML. We formulate it as a two-player game and propose a principled Recursive Reasoning-based Training-Time adversarial ML (R2T2) framework to model this game. R2T2 models the reasoning process between the attacker and the defender and captures their bounded reasoning capabilities (due to bounded computational resources) through the recursive reasoning formalism. In particular, we associate a deeper level of recursive reasoning with the use of a higher-order gradient to derive the attack (defense) strategy, which naturally improves its performance while requiring greater computational resources. Interestingly, our R2T2 framework encompasses a variety of existing adversarial ML methods which correspond to attackers (defenders) with different recursive reasoning capabilities. We show how an R2T2 attacker (defender) can utilize our proposed nested projected gradient descent-based method to approximate the optimal attack (defense) strategy at an arbitrary level of reasoning. R2T2 can empirically achieve state-of-the-art attack and defense performances on benchmark image datasets.}
}
@article{MIRZAEI2021102839,
title = {CFD modeling of micro and urban climates: Problems to be solved in the new decade},
journal = {Sustainable Cities and Society},
volume = {69},
pages = {102839},
year = {2021},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2021.102839},
url = {https://www.sciencedirect.com/science/article/pii/S2210670721001293},
author = {Parham A. Mirzaei},
keywords = {Computational fluid dynamics, Urban climate, Microclimate, Data acquisition, Building energy simulation, Sustainable, Resilient, Smart cities},
abstract = {Despite the popularity of the micro/urban climate CFD modeling as a powerful approach to simulate convective exchanges in urban areas, yet its application faces three profound limitations, including (1) computational barriers, (2) data acquisition, and (3) over-simplifications of underlying physics. Computational resources are not qualitatively studied to be allocated to their best of performance in urban climate models. Moreover, bigdata of city components and inhabitants are sometimes inaccessible or difficult to be effectively interpreted to be fed into CFD models. Furthermore, commonly adopted oversimplifications, and misinterpretation of underlying physics of urban climate can substantially render falsified results, no matter if they look otherwise followed by extravagant visual reports. This paper, hence, aims to explore the capabilities and limitations of urban climate CFD modeling. It further scrutinizes the common oversimplifications in the modeling techniques, potentially resulting in CFD capacities to be lost in the translation. The paper describes the extend to which CFD tools can be the favourable options and otherwise, while it underpins the areas in which further research is needed to conform urban climate CFD models as practical design and decision-making tools. It also offers a brief overview in the recent advancements in response to the mentioned challenges.}
}
@article{ALSAMHORI2024100133,
title = {Artificial intelligence for hearing loss prevention, diagnosis, and management},
journal = {Journal of Medicine, Surgery, and Public Health},
volume = {3},
pages = {100133},
year = {2024},
issn = {2949-916X},
doi = {https://doi.org/10.1016/j.glmedi.2024.100133},
url = {https://www.sciencedirect.com/science/article/pii/S2949916X24000860},
author = {Jehad Feras AlSamhori and Abdel Rahman Feras AlSamhori and Rama Mezyad Amourah and Yara AlQadi and Zina Wael Koro and Toleen Ramzi Abdallah Haddad and Ahmad Feras AlSamhori and Diala Kakish and Maya Jamal Kawwa and Margaret Zuriekat and Abdulqadir J. Nashwan},
keywords = {Artificial intelligence, Hearing loss, Machine learning, Computational audiology},
abstract = {This paper explores the transformative impact of artificial intelligence (AI), particularly machine learning (ML), on diagnosing and treating hearing loss, which affects over 5% of the global population across all ages and demographics. AI encompasses various applications, from natural language processing models like ChatGPT to image recognition systems; however, this paper focuses on ML, a subfield of AI that can revolutionize audiology by enhancing early detection, formulating personalized rehabilitation plans, and integrating electronic health records for streamlined patient care. The integration of ML into audiometry, termed "computational audiology," allows for automated, accurate hearing tests. AI algorithms can process vast data sets, provide detailed audiograms, and facilitate early detection of hearing impairments. Research shows ML's effectiveness in classifying audiograms, conducting automated audiometry, and predicting hearing loss based on noise exposure and genetics. These advancements suggest that AI can make audiological diagnostics and treatment more accessible and efficient. The future of audiology lies in the seamless integration of AI technologies. Collaborative efforts between audiologists, AI experts, and individuals with hearing loss are essential to overcome challenges and leverage AI's full potential. Continued research and development will enhance AI applications in audiology, improving patient outcomes and quality of life worldwide.}
}
@article{LUND2017556,
title = {Smart energy and smart energy systems},
journal = {Energy},
volume = {137},
pages = {556-565},
year = {2017},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2017.05.123},
url = {https://www.sciencedirect.com/science/article/pii/S0360544217308812},
author = {Henrik Lund and Poul Alberg Østergaard and David Connolly and Brian Vad Mathiesen},
keywords = {Renewable energy systems, Smart grid, Energy system modelling, Electro fuels, Power-to-Gas, Power-to-heat},
abstract = {In recent years, the terms “Smart Energy” and “Smart Energy Systems” have been used to express an approach that reaches broader than the term “Smart grid”. Where Smart Grids focus primarily on the electricity sector, Smart Energy Systems take an integrated holistic focus on the inclusion of more sectors (electricity, heating, cooling, industry, buildings and transportation) and allows for the identification of more achievable and affordable solutions to the transformation into future renewable and sustainable energy solutions. This paper first makes a review of the scientific literature within the field. Thereafter it discusses the term Smart Energy Systems with regard to the issues of definition, identification of solutions, modelling, and integration of storage. The conclusion is that the Smart Energy System concept represents a scientific shift in paradigms away from single-sector thinking to a coherent energy systems understanding on how to benefit from the integration of all sectors and infrastructures.}
}
@article{BANCHHOR2025100194,
title = {Integration of software-based cognitive approaches and brain-like computer machinery for efficient cognitive computing},
journal = {Neuroscience Informatics},
volume = {5},
number = {2},
pages = {100194},
year = {2025},
issn = {2772-5286},
doi = {https://doi.org/10.1016/j.neuri.2025.100194},
url = {https://www.sciencedirect.com/science/article/pii/S2772528625000093},
author = {Chitrakant Banchhor and Manoj Kumar Rawat and Rahul Joshi and Dharmesh Dhabliya and Omkaresh Kulkarni and Sandeep Dwarkanath Pande and Umesh Pawar},
keywords = {Multiple intelligences theory, English learning, Data mining algorithms, Legacy problems},
abstract = {The widespread adoption of the Internet has transformed various industries, driving significant systemic reforms across different sectors. This transformation has enhanced the Internet's role in information dissemination, resource sharing, and global connectivity, allowing for more efficient distribution of knowledge and services. The development of the Internet model and its research bring significant benefits from the network, enabling people to use and learn from it. However, the traditional education model provides only limited knowledge, restricting growth and progress. Moreover, there is a vast world of knowledge yet to be explored. Nowadays, with the help of network tools, people can understand the dynamics of the whole world and accept the culture and knowledge of different regions without going out. Throughout the study of English legacy problems in various countries, efficient learning methods and high levels of English skills are the goals pursued, while the traditional English model can't meet the students' learning needs in a short time. The model construction of data mining algorithm based on large open network courses is a model for solving legacy problems adopted both domestically and internationally. According to the survey data of universities in various countries, the use of data mining algorithm can fundamentally meet the student's desire and demand for English knowledge. This research, integrates the mining algorithm into English research, which will essentially improve the English legacy problems.}
}
@article{RAMFUL2014119,
title = {Reversible reasoning in fractional situations: Theorems-in-action and constraints},
journal = {The Journal of Mathematical Behavior},
volume = {33},
pages = {119-130},
year = {2014},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2013.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S0732312313000990},
author = {Ajay Ramful},
keywords = {Division, Fraction, Multiplicative reasoning, Reversibility, Units, Vergnaud's theory},
abstract = {The aim of this study was to investigate, at a fine-grained level of detail, the theorems-in-action deployed and the constraints encountered by middle-school students in reasoning reversibly in the multiplicative domain of fraction. A theorem-in-action (Vergnaud, 1988) is a conceptual construct to trace students’ reasoning in a problem solving situation. Two seventh grade students were interviewed in a rural middle-school in the southern part of the United States. The students’ strategies were examined with respect to the numerical features of the problem situations and the ways they viewed and operated on fractional units. The results show that reversible reasoning is sensitive to the numeric feature of problem parameters. Relatively prime numbers and fractional quantities acted as inhibitors preventing the cueing of the multiplication–division invariant, thereby constraining students from reasoning reversibly. Among others, two key resources were identified as being essential for reasoning reversibly in fractional contexts: firstly, interpreting fractions in terms of units, which enabled the students to access their whole number knowledge and secondly, the unit-rate theorem-in-action. Failure to conceptualize multiplicative relations in reverse constrained the students to use more primitive strategies, leading them to solve problems non-deterministically and at higher computational costs.}
}
@article{HERRERATAPIAS20251184,
title = {Legal Hallucinations and the Adoption of Artificial Intelligence in the Judiciary},
journal = {Procedia Computer Science},
volume = {257},
pages = {1184-1189},
year = {2025},
note = {The 16th International Conference on Ambient Systems, Networks and Technologies Networks (ANT)/ the 8th International Conference on Emerging Data and Industry 4.0 (EDI40)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2025.03.158},
url = {https://www.sciencedirect.com/science/article/pii/S1877050925008956},
author = {Beliña Annery Herrera-Tapias and Diego {Hernández Guzmán}},
keywords = {Artificial Intelligence, AI, Generative Pretrained Transformers, GPTs, Large Language Models, LLMs, Judiciary, Due Process},
abstract = {This article analyses the use of artificial intelligence in the judiciary, with a focus on Judgment T-343/24 of the Constitutional Court of Colombia. The judgment validates the use of artificial intelligence tools in judicial decision-making, provided they serve as supportive rather than substitutive instruments for judges. This paper highlights the potential of artificial intelligence in improving judicial efficiency and accuracy while also technically addressing the challenges posed by AI-generated "legal hallucinations," where large language models produce credible but incorrect outputs. Through qualitative legal analysis, the study explores the implications of integrating artificial intelligence in the judiciary in addressing those challenges while emphasizing the preservation of the right to a due process.}
}
@article{TIAN2015338,
title = {Safety assessment method of performance-based navigation airspace planning},
journal = {Journal of Traffic and Transportation Engineering (English Edition)},
volume = {2},
number = {5},
pages = {338-345},
year = {2015},
issn = {2095-7564},
doi = {https://doi.org/10.1016/j.jtte.2015.08.005},
url = {https://www.sciencedirect.com/science/article/pii/S2095756415000690},
author = {Yong Tian and Lili Wan and Chun-hung Chen and Ye Yang},
keywords = {Air traffic management, Safety assessment, Operational planning, Conflict risk, Adverse weather},
abstract = {The paper introduces a computational model of airspace conflict risk in the hierarchy of performance-based navigation (PBN) airspace operation and combines it with air traffic controller (ATC) workload to propose a method for safety assessment of PBN airspace operational planning. Capacity probability distribution is employed to depict airspace capacity in uncertain weather, errors of deviating from nominal PBN track are taken into consideration, and the stochastic process based on Gaussian distribution is used to depict random aircraft motion according to airspace PBN specification, so as to build an airspace conflict risk computational model in corresponding capacity scenario. Guangzhou No. 15 sector is chosen for simulation validation. The analysis results suggest that 60% of ATC workload is corresponding to sector traffic flow of 31 aircraft/h and airspace risk of 0.018 conflict/h, while 70% of ATC workload is corresponding to sector traffic flow of 35 aircraft/h and airspace risk of 0.03 conflict/h. As air traffic flow increases, both airspace conflict risk value and ATC workload will increase, resulting in reduction of airspace safety, though their increasing magnitudes differ with different capacity scenarios. The safety assessment method enables effective quantization of safety with regard to airspace operational planning strategy, and benefits the development of optimal operational scheme that balances risk with capacity demand.}
}
@article{BRODIN2009345,
title = {Univariate and bivariate GPD methods for predicting extreme wind storm losses},
journal = {Insurance: Mathematics and Economics},
volume = {44},
number = {3},
pages = {345-356},
year = {2009},
issn = {0167-6687},
doi = {https://doi.org/10.1016/j.insmatheco.2008.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S0167668708001455},
author = {Erik Brodin and Holger Rootzén},
keywords = {Extreme value statistics, Generalized Pareto distribution, Likelihood prediction intervals, Peaks over threshold, Trend analysis, Wind storm losses},
abstract = {Wind storm and hurricane risks are attracting increased attention as a result of recent catastrophic events. The aim of this paper is to select, tailor, and develop extreme value methods for use in wind storm insurance. The methods are applied to the 1982–2005 losses for the largest Swedish insurance company, the Länsförsäkringar group. Both a univariate and a new bivariate Generalized Pareto Distribution (GPD) gave models which fitted the data well. The bivariate model led to lower estimates of risk, except for extreme cases, but taking statistical uncertainty into account the two models lead to qualitatively similar results. We believe that the bivariate model provided the most realistic picture of the real uncertainties. It additionally made it possible to explore the effects of changes in the insurance portfolio, and showed that loss distributions are rather insensitive to portfolio changes. We found a small trend in the sizes of small individual claims, but no other trends. Finally, we believe that companies should develop systematic ways of thinking about “not yet seen” disasters.}
}
@article{BUSCEMA2022112439,
title = {A nonlinear, data-driven, ANNs-based approach to culture-led development policies in rural areas: The case of Gjakove and Peć districts, Western Kosovo},
journal = {Chaos, Solitons & Fractals},
volume = {162},
pages = {112439},
year = {2022},
issn = {0960-0779},
doi = {https://doi.org/10.1016/j.chaos.2022.112439},
url = {https://www.sciencedirect.com/science/article/pii/S096007792200649X},
author = {Massimo Buscema and Guido Ferilli and Christer Gustafsson and Giulia Massini and Pier Luigi Sacco},
keywords = {Theory of impossible worlds, Culture, Cultural policy, Topologically weighted centroid, AutoCM, Kosovo},
abstract = {We develop a computational approach to the analysis of cultural vibrancy and to the role of the cultural and creative sectors in the socio-economic organization of two districts of Western Kosovo, Gjakove and Peć. Our analysis is built on a geolocalized mapping of the cultural activities and facilities, and on the main socio-economic variables for the two districts, and makes use of innovative data analysis techniques: Theory of Impossible Words (TIW), the Topological Weighted Centroid (TWC), and the AutoCM ANN. We find that the dynamics of cultural vibrancy of the territory is mainly driven by the competing attraction pulls of the nearby countries of Serbia and Albania, that also form the region's main and often conflicting ethnicities, and that such dynamics are likely to further polarize in the future. We also find that the cultural system plays a marginal role in the territory's socio-economic organization. This situation makes a case for a more active role of cultural policy in shaping future local developmental models in rural areas and in acting as an agent of social cohesion.}
}
@article{LI2025112800,
title = {Optimizing mass transfer performance in triply periodic minimal surface porous scaffolds through isosurface offset},
journal = {Thin-Walled Structures},
volume = {208},
pages = {112800},
year = {2025},
issn = {0263-8231},
doi = {https://doi.org/10.1016/j.tws.2024.112800},
url = {https://www.sciencedirect.com/science/article/pii/S0263823124012394},
author = {Kun Li and Chunlin Zuo and Ruobing Liao and Haisong Liang and Xuan Liang and David Z. Zhang and Lawrence E. Murr and Huajun Cao},
keywords = {Triply periodic minimal surface, Bone scaffold, Permeability, Wall shear stress, Mass transfer performance},
abstract = {Efficient bone tissue regeneration remains a critical challenge in orthopedic medicine, with scaffold mass transfer capabilities playing a pivotal role. Triply periodic minimal surface (TPMS) scaffolds have emerged as promising candidates due to their unique structure characterized by smooth, continuous surfaces with zero mean curvature and high specific surface area. However, optimizing their mass transfer performance to meet the diverse needs of bone tissues at different anatomical sites has been a persistent challenge. This study addresses this gap by investigating the effects of isosurface offset on mass transfer performance in three TPMS scaffolds (Fisher-Koch S, Gyroid, and Split-P) using computational fluid dynamics (CFD). The results showed that isosurface offset significantly increased the effective scaffold permeability range (by 116.8 %, 5.3 %, and 64.3 % for F, G, and S scaffolds, respectively) and improved the wall shear stress (WSS) distribution, enhancing the area that effectively stimulates cell proliferation (by 25.2 %, 8.7 %, and 14.3 % increase, respectively). Additionally, it was found that porosity, specific surface area, the ratio of maximum pore size to tortuosity, and curvature significantly influenced the permeability and WSS distribution of the scaffolds. Finally, permeation experiments using porous scaffolds fabricated by laser powder bed fusion (LPBF) technology were performed to validate the simulation results. This study provides new insights into the design of TPMS porous scaffolds and customized bone implants, enhancing their application prospects in bone tissue engineering.}
}
@article{AHMED20201,
title = {A cognitive model to predict human interest in smart environments},
journal = {Computer Communications},
volume = {161},
pages = {1-9},
year = {2020},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2020.07.007},
url = {https://www.sciencedirect.com/science/article/pii/S0140366420306812},
author = {Tanveer Ahmed and Rishav Singh and Anil K. Pandey and Sanjay K. Singh},
keywords = {Cognition, Interest, Machine learning, Man Machine systems},
abstract = {Recently, the idea of smart cities has made several strides forward in literature. Work has hypothesize that the combination of Artificial Intelligence, Cloud Computing, and High powered computers will make technology more human-centric, even, the idea that smart cities will be able to understand the thought process of a human being seems very much likely today. This paper is along this line of thought. In particular, we try to present a method to model the cognitive state of human interest. This is done to take one more step towards the realization of a smart cognitive city. An approach which is Subjective–Objective in nature is presented to model the computation of activity inspired by interest. Based on activity, human latent state values are indirectly deduced. Inspiration is drawn from Physics and interest is modeled upon the Ornstein–Uhlenbeck (OU) process. Concepts of Adaptive filtering are used to formulate an evolving transformation function that automatically and adaptively models the conversion of interest into activity. Particle filter is employed to provide an elucidation which is computationally feasible. To validate the viability of the method, experimentation is performed with real datasets.}
}
@article{HAMALAINEN2013623,
title = {On the importance of behavioral operational research: The case of understanding and communicating about dynamic systems},
journal = {European Journal of Operational Research},
volume = {228},
number = {3},
pages = {623-634},
year = {2013},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2013.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S0377221713001197},
author = {Raimo P. Hämäläinen and Jukka Luoma and Esa Saarinen},
keywords = {Process of OR, Practice of OR, Systems dynamics, Behavioral OR, Systems thinking},
abstract = {We point out the need for Behavioral Operational Research (BOR) in advancing the practice of OR. So far, in OR behavioral phenomena have been acknowledged only in behavioral decision theory but behavioral issues are always present when supporting human problem solving by modeling. Behavioral effects can relate to the group interaction and communication when facilitating with OR models as well as to the possibility of procedural mistakes and cognitive biases. As an illustrative example we use well known system dynamics studies related to the understanding of accumulation. We show that one gets completely opposite results depending on the way the phenomenon is described and how the questions are phrased and graphs used. The results suggest that OR processes are highly sensitive to various behavioral effects. As a result, we need to pay attention to the way we communicate about models as they are being increasingly used in addressing important problems like climate change.}
}
@article{RINO2024102366,
title = {Timed alignments with mixed moves},
journal = {Data & Knowledge Engineering},
volume = {154},
pages = {102366},
year = {2024},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2024.102366},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X24000909},
author = {Neha Rino and Thomas Chatain},
keywords = {Conformance checking, Alignments, Timestamps, Time Petri nets},
abstract = {We study conformance checking for timed models, that is, process models that consider both the sequence of events that occur, as well as the timestamps at which each event is recorded. Time-aware process mining is a growing subfield of research, and as tools that seek to discover timing-related properties in processes develop, so does the need for conformance-checking techniques that can tackle time constraints and provide insightful quality measures for time-aware process models. One of the most useful conformance artefacts is the alignment, that is, finding the minimal changes necessary to correct a new observation to conform to a process model. In this paper, we extend the notion of timed distance from a previous work where an edit on an event’s timestamp came in two types, depending on whether or not it would propagate to its successors. Here, these different types of edits have a weighted cost each, and the ratio of their costs is denoted by α. We then solve the purely timed alignment problem in this setting for a large class of these weighted distances (corresponding to α∈{1}∪[2,∞)). For these distances, we provide linear time algorithms for both distance computation and alignment on models with sequential causal processes.}
}
@article{YUROVSKY201873,
title = {A communicative approach to early word learning},
journal = {New Ideas in Psychology},
volume = {50},
pages = {73-79},
year = {2018},
issn = {0732-118X},
doi = {https://doi.org/10.1016/j.newideapsych.2017.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S0732118X17300077},
author = {Daniel Yurovsky},
keywords = {Language acquisition, Learning, Cognitive development},
abstract = {Young children learn the meanings of thousands of words by the time they can run down the street. Many efforts to explain this rapid development begin by assuming that the computational-level problem being solved is acquisition. Consequently, work in this line has sought to understand how children infer the meanings of words from cues in the communicative signals of the speakers around them. I will argue, however, that this formulation of the problem is backwards: the computational problem is communication, and language acquisition provides cues about how to communicate successfully. Under this framing, the natural unit of analysis is not the child, but the parent-child dyad. A necessary consequence of this shift is the realization that the statistical structure of the input to the child is itself dependent on the child. This dependency radically simplifies the computational problem of learning and using language.}
}
@article{DUFVA201697,
title = {Metaphors of code—Structuring and broadening the discussion on teaching children to code},
journal = {Thinking Skills and Creativity},
volume = {22},
pages = {97-110},
year = {2016},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2016.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S1871187116301055},
author = {Tomi Dufva and Mikko Dufva},
keywords = {Code, Code literacy, Metaphors, Education, Programming, Teaching programming, Pedagogy, Media literacy},
abstract = {Digital technology has become embedded into our daily lives. Code is at the heart of this technology. The way code is perceived influences the way our everyday interaction with digital technologies is perceived: is it an objective exchange of ones and zeros, or a value- laden power struggle between white male programmers and those who think they are users, when they are, in fact, the product being sold. Understanding the nature of code thus enables the imagination and exploration of the present state and alternative future developments of digital technologies. A wider imagination is especially important for developing basic education so that it provides the capabilities for coping with these developments. Currently, the discussion has been mainly on the technical details of code. We study how to broaden this narrow view in order to support the design of more comprehensive and future-proof education around code and coding. We approach the concept of code through nine different metaphors from the existing literature on systems thinking and organisational studies. The metaphors we use are machine, organism, brain, flux and transformation, culture, political system, psychic prison, instrument of domination and carnival. We describe their epistemological backgrounds and give examples of how code is perceived through each of them. We then use the metaphors in order to suggest different complementary ways that ICT could be taught in schools. The metaphors illustrate different contexts and help to interpret the discussions related to developments in digital technologies such as free software movement, democratization of information and internet of things. They also help to identify the dominant views and the tensions between the views. We propose that the systematic use of metaphors described in this paper would be a useful tool for broadening and structuring the dialogue about teaching children to code.}
}
@article{JIANG2023102217,
title = {Cooperative localization for master–salve multi-AUVs based on range measurements},
journal = {Physical Communication},
volume = {61},
pages = {102217},
year = {2023},
issn = {1874-4907},
doi = {https://doi.org/10.1016/j.phycom.2023.102217},
url = {https://www.sciencedirect.com/science/article/pii/S1874490723002203},
author = {Ling Jiang and Wengen Gao and Yunfei Li and Mengxing Pan and Shaopeng Mu},
keywords = {AUV, Cooperative localization, Distance-dependent noise, Gaussian belief propagation},
abstract = {Underwater localization has consistently remained a prominent technical challenge for autonomous underwater vehicles (AUVs). The advent of cooperative localization techniques has emerged as a novel avenue for enhancing localization accuracy. The master–slave cooperative localization mode has gained widespread adoption due to its cost-effectiveness in implementation. In view of the complexity of underwater noise characteristics, in the multi-AUVs cooperative localization system, this paper addresses scenarios involving distance-dependent noise in a master–slave-based multi-AUVs cooperative localization system. To tackle the negative impact of distance-dependent noise and the non-linearity of the distance function, a two-step algorithm is proposed that combines maximum likelihood estimation and the Gaussian belief propagation algorithm (ML-GBP) to estimate the positions of AUVs. The maximum likelihood estimation is employed to cope with the interference caused by distance-dependent noise, and subsequently, the Gaussian belief propagation algorithm, based on range observations and reference information, is used to achieve accurate estimation of AUV positions and implement position correction. Simulation results demonstrate that the proposed ML-GBP algorithm outperforms traditional extended Kalman filter (EKF) and nonparametric belief propagation (NBP) methods by enhancing the localization accuracy of the system while exhibiting superior performance in terms of computational complexity and system communication overhead.}
}
@article{GIPPERT199015,
title = {Computational methods for determining protein structures from NMR data},
journal = {Biochemical Pharmacology},
volume = {40},
number = {1},
pages = {15-22},
year = {1990},
issn = {0006-2952},
doi = {https://doi.org/10.1016/0006-2952(90)90172-H},
url = {https://www.sciencedirect.com/science/article/pii/000629529090172H},
author = {Garry P. Gippert and Ping F. Yip and Peter E. Wright and David A. Case},
abstract = {The general procedures by which solution structures of proteins may be deduced from distance and angular constraints derived from NMR are reviewed, with an emphasis on practical aspects of the calculations. In addition, novel methods based on chemical shift calculations and on quantitative fits to nuclear Overhauser effect intensities are presented; these should provide improved understanding of the limits of our ability to simulate complex spectra, and may permit higher precision structures to be determined.}
}
@article{AHMAD20225041,
title = {Decision Level Fusion Using Hybrid Classifier for Mental Disease Classification},
journal = {Computers, Materials and Continua},
volume = {72},
number = {3},
pages = {5041-5058},
year = {2022},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2022.026077},
url = {https://www.sciencedirect.com/science/article/pii/S1546221822009067},
author = {Maqsood Ahmad and Noorhaniza Wahid and Rahayu A Hamid and Saima Sadiq and Arif Mehmood and Gyu Sang Choi},
keywords = {Mental health diagnosis, machine learning, depression, shrewd probing, diagnostic approach},
abstract = {Mental health signifies the emotional, social, and psychological well-being of a person. It also affects the way of thinking, feeling, and situation handling of a person. Stable mental health helps in working with full potential in all stages of life from childhood to adulthood therefore it is of significant importance to find out the onset of the mental disease in order to maintain balance in life. Mental health problems are rising globally and constituting a burden on healthcare systems. Early diagnosis can help the professionals in the treatment that may lead to complications if they remain untreated. The machine learning models are highly prevalent for medical data analysis, disease diagnosis, and psychiatric nosology. This research addresses the challenge of detecting six major psychological disorders, namely, Anxiety, Bipolar Disorder, Conversion Disorder, Depression, Mental Retardation and Schizophrenia. These challenges are mined by applying decision level fusion of supervised machine learning algorithms. A dataset was collected from a clinical psychologist consisting of 1771 observations that we used for training and testing the models. Furthermore, to reduce the impact of a conflicting decision, a voting scheme Shrewd Probing Prediction Model (SPPM) is introduced to get output from ensemble model of Random Forest and Gradient Boosting Machine (RF + GBM). This research provides an intuitive solution for mental disorder analysis among different target class labels or groups. A framework is proposed for determining the mental health problem of patients using observations of medical experts. The framework consists of an ensemble model based on RF and GBM with a novel SPPM technique. This proposed decision level fusion approach by combining RF + GBM with SPPM-MIN significantly improves the performance in terms of Accuracy, Precision, Recall, and F1-score with 71\%, 73\%, 71\% and 71\% respectively. This framework seems suitable in the case of huge and more diverse multi-class datasets. Furthermore, three vector spaces based on TF-IDF (unigram, bi-gram, and tri-gram) are also tested on the machine learning models and the proposed model.}
}
@article{JABEEN2023108475,
title = {Deep learning-based prediction of inhibitors interaction with Butyrylcholinesterase for the treatment of Alzheimer's disease},
journal = {Computers and Electrical Engineering},
volume = {105},
pages = {108475},
year = {2023},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2022.108475},
url = {https://www.sciencedirect.com/science/article/pii/S0045790622006905},
author = {Farah Jabeen and Zia Ur Rehman and Sajid Shah and Rima D. Alharthy and Saquib Jalil and Imtiaz Ali Khan and Jamshed Iqbal and Ahmed A. Abd El-Latif},
keywords = {Deep learning, Machine learning, Personalized and precision medicine, Alzheimer's disease, Computer-aided diagnosis and detection},
abstract = {Butyrylcholinesterase (BChE) is a significant pharmaceutical drug for treating Alzheimer's disease (AD) . Thanks to the computational methods as which decreases significantly the overhead for screening BChE inhibitors. However, some of them have used one-hot encoding which ignores the sequential information. In this study, Term Frequency-Inverse Document Frequency (TF-IDF) is used for encoding SMILES expressions and Long Short-Term Memory (LSTM) for classification to preserve sequential information. Apart from LSTM, different models were used to evaluate the discriminative power of TF-IDF and to show the significance of sequential information. The dataset used in this study con-sists of 4,515 records of BChE inhibitors and non-inhibitors in the form of SMILES. The results obtained by the machine learning models were tested through invitro activity assays as well. The molecular docking study further confirmed the binding modes inside the BChE. The LSTM model showed 98.20% testing ac-curacy for the prediction of BChE inhibitors.}
}
@article{ZHANG2024110034,
title = {Comprehensive reliability assessment method for distribution networks considering IIDG low voltage ride-through control},
journal = {International Journal of Electrical Power & Energy Systems},
volume = {159},
pages = {110034},
year = {2024},
issn = {0142-0615},
doi = {https://doi.org/10.1016/j.ijepes.2024.110034},
url = {https://www.sciencedirect.com/science/article/pii/S0142061524002552},
author = {Shuai Zhang and Wenxia Liu and Haiyang Wan and Tianlong Wang and Rui Cheng and Hanshen Li},
keywords = {Distribution networks, Reliability assessment, Low voltage ride through, Temporary fault, Permanent fault},
abstract = {Upon the large-scale integration of inverter-interfaced distributed generator (IIDG) into the grid, random faults in the distribution network can lead to momentary and sustained interruptions, significantly impacting system reliability. Although reliability methods have been widely used in distribution network adequacy assessment, using probabilistic method for reliability assessment including system dynamic security need to be investigated. To address this issue, a new method is designed to assess the reliability of distribution network using sequential Monte Carlo simulation. Firstly, the IIDG low-voltage ride-through (LVRT) control strategy is formulated, and the off-grid probability model for IIDG is developed based on the Gaussian distribution. Secondly, the component model is defined to consider the impact of random faults on power quality in security assessment. Following the fault tree analysis method, a protection action probability model was formulated to assess the failure probability of line current differential protection, IIDG anti-islanding protection, and reclosing protection. Finally, the method for momentary fault consequence analysis, based on depth-first search (DFS), and the method for sustained fault consequence analysis, based on the mixed-integer linear programming model, are developed. The study establishes a comprehensive probability reliability assessment framework. The validity of the method is demonstrated on the IEEE RBTS BUS6 F4 system, indicating good scalability.}
}
@article{AUCONI2020856,
title = {Computer-aided heuristics in orthodontics},
journal = {American Journal of Orthodontics and Dentofacial Orthopedics},
volume = {158},
number = {6},
pages = {856-867},
year = {2020},
issn = {0889-5406},
doi = {https://doi.org/10.1016/j.ajodo.2019.10.018},
url = {https://www.sciencedirect.com/science/article/pii/S0889540620304248},
author = {Pietro Auconi and James A. McNamara and Lorenzo Franchi},
abstract = {Introduction
During the decision-making process, physicians rely on heuristics that consist of simple, useful procedures for solving problems, intuitive shortcuts that produce reliable decisions based on limited information. In clinical situations characterized by a high degree of uncertainty such as those encountered in orthodontics, cognitive biases and judgment errors related to heuristics are not uncommon. This study aimed at promoting trust in the effective interface between the intuitive reasoning of the orthodontic practitioner and the computational heuristics emerging from simple statistical models.
Methods
We propose an integrative model based on the interaction between clinical reasoning and 2 computational tools, cluster analysis and fast-and-frugal trees, to extract a structured craniofacial representation of untreated subjects with Class III malocclusion and to forecast the worsening of the malocclusion over time.
Results
Cluster analysis of cephalometric values from 144 growing subjects with Class III malocclusion followed longitudinally (T1: mean age, 10.2 ± 1.9 years; T2: mean age, 13.8 ± 2.7 years) produced 3 morphologic subgroups with predominant sagittal, vertical, and slight maxillomandibular imbalances. Fast-and-frugal trees applied to different subgroups extracted heuristics that improved the prediction of key features associated with adverse craniofacial growth.
Conclusions
Provided that cephalometric values are placed in the appropriate framework, the matching between simple and fast computational approaches and clinical reasoning could help the intuitive logic, perception, and cognitive inferences of orthodontic practitioners on the outcome of patients affected by Class III disharmony, decreasing errors associated with flawed judgments and improving the accuracy of decision making.}
}
@article{SU2022100072,
title = {Artificial Intelligence (AI) in early childhood education: Curriculum design and future directions},
journal = {Computers and Education: Artificial Intelligence},
volume = {3},
pages = {100072},
year = {2022},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2022.100072},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X22000273},
author = {Jiahong Su and Yuchun Zhong},
keywords = {Artificial intelligence (AI), AI curriculum, Kindergarten, Early childhood education, AI literacy},
abstract = {With the rapid technological development of society brought on by Artificial Intelligence (AI), the demand for AI-literate workers will increase in the future. It is critical to develop the next generation's AI competencies and educate them about how to work with and use AI. Previous studies on AI were predominantly focused on secondary and university education; however, research on the Artificial Intelligence curriculum in early childhood education is scarce. Due to the lack of conformity on the standardisation of AI curriculum for early childhood education, this study examines the AI curriculum for kindergarten children using the framework which consists of four key components, including (1) aims, goals, objectives, or declarations of outcome, (2) subject matter, domains, or content, (3) methods or procedure, (4) evaluation and assessment. We recommend that AI literacy be achieved by three competencies: AI Knowledge, AI Skill, and AI Attitude. The employment of a social robot as a learning companion and programmable artifact was proven to be helpful in assisting young children in grasping AI principles. We also discovered which teaching methods had the most greatest influence on students' learning. We recommend problem-based learning for future AI education based on the findings.}
}
@article{CURTO201911,
title = {Relating network connectivity to dynamics: opportunities and challenges for theoretical neuroscience},
journal = {Current Opinion in Neurobiology},
volume = {58},
pages = {11-20},
year = {2019},
note = {Computational Neuroscience},
issn = {0959-4388},
doi = {https://doi.org/10.1016/j.conb.2019.06.003},
url = {https://www.sciencedirect.com/science/article/pii/S0959438819300443},
author = {Carina Curto and Katherine Morrison},
abstract = {We review recent work relating network connectivity to the dynamics of neural activity. While concepts stemming from network science provide a valuable starting point, the interpretation of graph-theoretic structures and measures can be highly dependent on the dynamics associated to the network. Properties that are quite meaningful for linear dynamics, such as random walk and network flow models, may be of limited relevance in the neuroscience setting. Theoretical and computational neuroscience are playing a vital role in understanding the relationship between network connectivity and the nonlinear dynamics associated to neural networks.}
}
@article{REN2024127126,
title = {FedBoosting: Federated learning with gradient protected boosting for text recognition},
journal = {Neurocomputing},
volume = {569},
pages = {127126},
year = {2024},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2023.127126},
url = {https://www.sciencedirect.com/science/article/pii/S0925231223012493},
author = {Hanchi Ren and Jingjing Deng and Xianghua Xie and Xiaoke Ma and Yichuan Wang},
keywords = {Deep learning, Federated learning, Privacy preserving},
abstract = {Conventional machine learning methodologies require the centralization of data for model training, which may be infeasible in situations where data sharing limitations are imposed due to concerns such as privacy and gradient protection. The Federated Learning (FL) framework enables the collaborative learning of a shared model without necessitating the centralization or sharing of data among the data proprietors. Nonetheless, in this paper, we demonstrate that the generalization capability of the joint model is suboptimal for Non-Independent and Non-Identically Distributed (Non-IID) data, particularly when employing the Federated Averaging (FedAvg) strategy as a result of the weight divergence phenomenon. Consequently, we present a novel boosting algorithm for FL to address both the generalization and gradient leakage challenges, as well as to facilitate accelerated convergence in gradient-based optimization. Furthermore, we introduce a secure gradient sharing protocol that incorporates Homomorphic Encryption (HE) and Differential Privacy (DP) to safeguard against gradient leakage attacks. Our empirical evaluation demonstrates that the proposed Federated Boosting (FedBoosting) technique yields significant enhancements in both prediction accuracy and computational efficiency in the visual text recognition task on publicly available benchmarks.}
}
@article{TAYLOR2022109098,
title = {Path integral radiative transfer via polyline representation allowing GPU implementation},
journal = {Annals of Nuclear Energy},
volume = {173},
pages = {109098},
year = {2022},
issn = {0306-4549},
doi = {https://doi.org/10.1016/j.anucene.2022.109098},
url = {https://www.sciencedirect.com/science/article/pii/S0306454922001335},
author = {Brennen Taylor and John Keyser and Jerry Tessendorf},
keywords = {Radiative transport, Feynman path integral, Monte Carlo, Particle transport, CUDA},
abstract = {Many research areas require simulating particle transport through scattering media. For instance, radiative transfer is useful for computer graphics and for neutron transport in nuclear physics. These transport simulations tend to be computationally expensive for problems involving large amounts of multiple scattering in generic geometries, requiring significant time to compute. Finding a fast solution for these types of problems remains an open area for research. Previous work shows that radiative transfer can be represented as a Feynman Path Integral over all paths between two points in a space. The path integral assigns a weight to each path based on the local curvature of the path, accumulating a transport kernel by summing the weights of all of the paths. Previous work demonstrated a Monte Carlo method for computing the radiative transfer Feynman path integral via repeatedly perturbing paths to generate new paths, using a discrete Frenet-Serret framework and an expensive root-solve calculation. The approach is highly parallelizable on the CPU, however computations require a supercomputer to complete in reasonable time. While a GPU implementation was considered for this previous approach, the root-solve and path representation mapped poorly to the GPU, hindering a reasonable implementation. In the present work, we propose a representation of paths as polylines, and a new curve perturbation method that guarantees production of a new unique path each execution while maintaining the boundary conditions of each path, but without the time-consuming root-solve. The new perturbation method’s structure maps well to the GPU, allowing implementation in CUDA which outperforms the CPU, and allows the utilization of GPU hardware.}
}
@article{DELEURAN201671,
title = {Exploratory Topology Modelling of Form-active Hybrid Structures},
journal = {Procedia Engineering},
volume = {155},
pages = {71-80},
year = {2016},
note = {TENSINET – COST TU1303 International Symposium 2016 "Novel structural skins - Improving sustainability and efficiency through new structural textile materials and designs"},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2016.08.008},
url = {https://www.sciencedirect.com/science/article/pii/S187770581632149X},
author = {Anders Holden Deleuran and Mark Pauly and Martin Tamke and Ida Friis Tinning and Mette Ramsgaard Thomsen},
keywords = {Form-Active, Hybrid Structures, Topology, Shaping, Form-Finding, Interactive Modelling, Design Space Search},
abstract = {The development of novel form-active hybrid structures (FAHS) is impeded by a lack of modelling tools that allow for exploratory topology modelling of shaped assemblies. We present a flexible and real-time computational design modelling pipeline developed for the exploratory modelling of FAHS that enables designers and engineers to iteratively construct and manipulate form-active hybrid assembly topology on the fly. The pipeline implements Kangaroo2's projection-based methods for modelling hybrid structures consisting of slender beams and cable networks. A selection of design modelling sketches is presented in which the developed modelling pipeline has been integrated to explore the design space delineated by FAHS.}
}
@article{MAMOLO2022101014,
title = {Coding and climate change: Investigating prospective teachers’ pathways of attention},
journal = {The Journal of Mathematical Behavior},
volume = {68},
pages = {101014},
year = {2022},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2022.101014},
url = {https://www.sciencedirect.com/science/article/pii/S0732312322000827},
author = {Ami Mamolo and Sheree Rodney and Diane Tepylo},
keywords = {Awareness, Curiosity, Climate change, Coding, Pathways of attention, Mathematics for social justice, Teacher education},
abstract = {This research is part of a broader research program that explores teacher educators’ mathematical knowledge. We examine the experiences, perceptions, and needs of prospective teachers as they navigate a complex set of new and interweaving ideas for how to teach mathematics with socially relevant and responsible connections. In doing so, we draw on Mason’s (1998) perspectives about the structure of attention and awareness for mathematics teaching, to investigate the pathways of attention of middle school prospective teachers in a technology-intensive undergraduate coding course. The research findings show that teachers face challenges when they try to navigate the interdisciplinary space of mathematics, technology and societal issues (climate change) and that curiosity acts as a potential stimulus for determining how each pathway is developed and sustained.}
}
@article{WANG2024119513,
title = {Real-sea validation of a model predictive controller's inherent robustness for medium-scale unmanned trimaran heading},
journal = {Ocean Engineering},
volume = {313},
pages = {119513},
year = {2024},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2024.119513},
url = {https://www.sciencedirect.com/science/article/pii/S0029801824028518},
author = {Jun Wang and Jian Wang and Xiaofeng Liang and Junjie Liu and Shunzhao Cheng and Hong Yi},
keywords = {Unmanned surface vehicle, Heading control, Path following, Trimaran, Model predictive control, Robustness},
abstract = {The development of medium-to large-scale and high-performance unmanned surface vehicles (USVs) is a burgeoning trend in intelligent marine systems. The heading controller is crucial for USVs to execute diverse missions, especially given their inherent underactuation characteristics and constraints. Recent efforts have focused on enhancing the robustness of controllers against external disturbances and employed two main strategies: one converges the control error to a bounded residual set through robust modifications, while the other eliminates the error by modelling disturbances. Notably, these enhancements have primarily catered to small USVs, where disturbances significantly impact their manoeuvrability, necessitating such robust control strategies. This focus has somewhat overshadowed the inherent robustness of closed-loop control systems. Compared with small USVs, medium-to large-scale USVs are less affected by external disturbances, despite undertaking more complex missions. Leveraging the intrinsic robustness of controllers presents an opportunity to simplify controller design, thereby reallocating computational resources towards enhancing mission capabilities. Model Predictive Control (MPC) has attracted significant attention recently, and its receding horizon and optimality theoretically provides a new level of inherent robustness, which remains under-explored in real sea. This paper focuses on the inherent robustness of MPC in managing the heading of a medium-scale unmanned trimaran subjected to the thrust angle and angular velocity constraints. A model predictive controller considering the constraints is designed based on the identified Nomoto model and the asymptotic stability is ensured with a terminal cost. Conducted real-sea experiments and comparative analyses with a Proportional-Integral-Derivative (PID) controller, the most widespread and dominant control algorithm in practical USV engineering, underscore the superiority of MPC in maintaining satisfactory closed-loop performance. Furthermore, the MPC controller is also successfully applied to real-sea path-following missions and demonstrated good tracking performance in various sea conditions ranging from level 3 to 5 and wind speeds spanning from level 6 to 8. This validation opens up new avenues for motion control strategies in the evolving landscape of larger-scale USVs.}
}
@article{NOORMOHAMMADIASL2025104821,
title = {Human leading or following preferences: Effects on human perception of the robot and the human–robot collaboration},
journal = {Robotics and Autonomous Systems},
volume = {183},
pages = {104821},
year = {2025},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2024.104821},
url = {https://www.sciencedirect.com/science/article/pii/S0921889024002057},
author = {Ali Noormohammadi-Asl and Kevin Fan and Stephen L. Smith and Kerstin Dautenhahn},
keywords = {Human–robot collaboration, Adaptive task planning, Leading/following preference, Team performance, Perception of the robot and collaboration},
abstract = {Achieving effective and seamless human–robot collaboration requires two key outcomes: enhanced team performance and fostering a positive human perception of both the robot and the collaboration. This paper investigates the capability of the proposed task planning framework to realize these objectives by integrating human leading/following preferences and performance into its task allocation and scheduling processes. We designed a collaborative scenario wherein the robot autonomously collaborates with participants. The outcomes of the user study indicate that the proactive task planning framework successfully attains the aforementioned goals. We also explore the impact of participants’ leadership and followership styles on their collaboration. The results reveal intriguing relationships between these factors which warrant further investigation in future studies.}
}
@article{TANG2025741,
title = {Remote sensing scene graph generation for improved retrieval based on spatial relationships},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {220},
pages = {741-752},
year = {2025},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2025.01.012},
url = {https://www.sciencedirect.com/science/article/pii/S0924271625000127},
author = {Jiayi Tang and Xiaochong Tong and Chunping Qiu and Yuekun Sun and Haoshuai Song and Yaxian Lei and Yi Lei and Congzhou Guo},
keywords = {Remote sensing (RS) scene graph generation, Spatial relationship computation, Spatial grid, Graph neural network, RS scene retrieval},
abstract = {RS scene graphs represent RS scenes as graphs with objects as nodes and their spatial relationships as edges, playing a crucial role in understanding and interpreting RS scenes at a higher level. However, existing RS scene graph generation methods, relying on deep learning models, face limitations due to their dependence on extensive relationship labels, restricted generation accuracy, and limited generalizability. To address these challenges, we proposed a spatial relationship computing model based on prior geographic information knowledge for RS scene graph generation. We refer to the RS scene graph generated using our method as SG-SSR for short. Furthermore, we investigated the application of SG-SSR in RS scene retrieval, demonstrating improved retrieval accuracy for spatial relationships between entities. The experiments show that our scene graph generation method does not rely on relationship labels, and has higher generation accuracy and greater universality. Moreover, the retrieval method based on SG-SSR outperformed other retrieval methods based on image feature vectors, with a retrieval accuracy index 0.098 higher than the alternatives(RemoteCLIP(mask)). The dataset and code are available at https://gitee.com/tangjiayitangjiayi/sg-ssr.}
}
@article{SISTLA20212464,
title = {Evaluating the performance of nature inspired algorithms using 52-bar steel truss subjected to dynamic load},
journal = {Materials Today: Proceedings},
volume = {38},
pages = {2464-2470},
year = {2021},
note = {International Conference & Exposition on Mechanical, Material and Manufacturing Technology (ICE3MT)},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2020.07.390},
url = {https://www.sciencedirect.com/science/article/pii/S2214785320355000},
author = {Saiteja Sistla and J.S. {Kalyana Rama}},
keywords = {Moth flame optimizer, Salp Swarm Algorithm, Whale optimization algorithm, MATLAB, Steel truss},
abstract = {Linear programing problem revolutionized during the world war since it has helped the army to minimize the costs and increase the efficiency in battlefield. Since then optimization techniques have gained popularity in various fields like science & technology, biology, mathematics etc. Nature inspired algorithms mimic the nature’s behavior in order to achieve certain optimization objectives which can achieve productive results to complex problems. Classical algorithms like Genetic algorithm, Particle swarm optimization etc. are widely known but the accuracy of the solution for complex engineering problems is less. Performance evaluation of latest nature inspired algorithms i.e. Moth flame optimizer, Salp Swarm optimizer and Whale optimizer is carried out in the present study. These algorithms are proposed recently and they possess salient features like better convergence rate, avoiding the local optimum and robustness, which is the motivation behind choosing these algorithms. A 52-bar steel truss has been chosen for the present study to assess the performance of the chosen optimization techniques. The behavior of steel truss subjected to two different ground motions is also assessed using the three optimization techniques. A comparative study is done to assess the performance of the chosen techniques. MATLAB is adopted for the simulation of chosen problem statement. Based on the results it is observed that Mouth Flame Optimizer has better performance in terms of accuracy, convergence rate and computational time and is suggested for various types of mechanical and structural problems involving 52-bar trusses.}
}
@article{ZHENG2019109539,
title = {Development of bridge influence line identification methods based on direct measurement data: A comprehensive review and comparison},
journal = {Engineering Structures},
volume = {198},
pages = {109539},
year = {2019},
issn = {0141-0296},
doi = {https://doi.org/10.1016/j.engstruct.2019.109539},
url = {https://www.sciencedirect.com/science/article/pii/S0141029619309538},
author = {Xu Zheng and Dong-Hui Yang and Ting-Hua Yi and Hong-Nan Li},
keywords = {Bridge influence line, Inverse problem, B-WIM, Bridge evaluation, Damage detection, Model correction},
abstract = {Bridge influence line, which is defined as the response curve of a certain point of the bridge under the moving unit concentrated load, contains tremendous structural information. However, the real bridge influence lines seldom correspond well with them calculated by bridge model. Accordingly, exact identification of the bridge influence line from the direct measurement data grows up to be an important issue for bridge weight-in-motion system, performance evaluation, model correction and bridge damage detection. This paper provides a comprehensive review of current research and development activities in bridge influence line identification method. At first, the development and applications of the bridge influence line are introduced. Following that, the mathematical models of bridge influence line identification and different bridge influence line identification methods are provided. Finally, four different indexes including noise immunity, peak reconstruction accuracy, computational complexity, and adaptability for axle weight change of different methods are compared and the features of these methods are summarized. The end of this paper provides a criterion for selecting suitable influence line identification method in different conditions and an outlook for further development of bridge influence line identification method.}
}
@article{MYERS2021100349,
title = {Mechanistic and data-driven models of cell signaling: Tools for fundamental discovery and rational design of therapy},
journal = {Current Opinion in Systems Biology},
volume = {28},
pages = {100349},
year = {2021},
issn = {2452-3100},
doi = {https://doi.org/10.1016/j.coisb.2021.05.010},
url = {https://www.sciencedirect.com/science/article/pii/S2452310021000342},
author = {Paul J. Myers and Sung Hyun Lee and Matthew J. Lazzara},
keywords = {Uncertainty, Sensitivity, Parameter sampling, Parameter estimation, Regression, Clustering, Classification, Cancer, Immunology},
abstract = {A full understanding of cell signaling processes requires knowledge of protein structure–function relationships, protein–protein interactions, and the abilities of pathways to control phenotypes. Computational models offer a valuable framework for integrating that knowledge to predict the effects of system perturbations and interventions in health and disease. Whereas mechanistic models are well suited for understanding the biophysical basis for signal transduction and principles of therapeutic design, data-driven models are particularly suited to distill complex signaling relationships among samples and between multivariate signaling changes and phenotypes. Both approaches have limitations and provide incomplete representations of signaling biology, but their careful implementation and integration can provide new understanding for how manipulating system variables impacts cellular decisions.}
}
@article{GEORGARA2025100388,
title = {Optimising team dynamics: The role of AI in enhancing challenge-based learning participation experience and outcomes},
journal = {Computers and Education: Artificial Intelligence},
volume = {8},
pages = {100388},
year = {2025},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2025.100388},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X25000281},
author = {Athina Georgara and Marc Santolini and Olga Kokshagina and Camila Justine {Jacinta Haux} and Desmé Jacobs and Gloria Biwott and Marcela Correa and Carles Sierra and Jose Luis Fernandez-Marquez and Juan A. Rodriguez-Aguilar},
keywords = {Artificial intelligence, Challenge-based learning, Participation experience, Teamwork, Relational well-being},
abstract = {The approach of engaging students with real-world challenges to enhance collaboration and problem-solving has attracted significant interest from scholars and practitioners across diverse disciplines. Often called Challenge-Based Learning (CBL), this educational approach emphasises developing collaborative and problem-solving skills, with significant learning occurring within team settings. Prior studies highlight the influence of team composition on the efficacy of learning outcomes, pointing out that factors such as gender diversity, personality trait diversity, and a wide range of skills affect team dynamics and performance. Despite these insights, the practical organisation of these teams remains a challenge, often reliant on ad-hoc methods driven primarily by the nature of the setting at hand. Importantly, CBL is typically assessed through the final product, neglecting the impact of CBL on how the participants experience the overall process. That is, CBL is usually considered effective if the outcome is of high quality, ignoring participants' experience and participation quality. This study investigates the potential of an Artificial Intelligence team composition algorithm to improve participation quality and outcomes in collaborative CBL environments.}
}
@article{BAYDOUN2021100026,
title = {Auto-contouring FDG-PET/MR images for cervical cancer radiation therapy: An intelligent sequential approach using focally trained, shallow U-Nets},
journal = {Intelligence-Based Medicine},
volume = {5},
pages = {100026},
year = {2021},
issn = {2666-5212},
doi = {https://doi.org/10.1016/j.ibmed.2021.100026},
url = {https://www.sciencedirect.com/science/article/pii/S2666521221000028},
author = {Atallah Baydoun and Ke Xu and Latoya A. Bethell and Feifei Zhou and Jin Uk Heo and Kaifa Zhao and Elisha T. Fredman and Rodney J. Ellis and Pengjiang Qian and Raymond F. Muzic and Bryan J. Traughber},
keywords = {Cervical cancer, Deep learning, Image segmentation, PET/MR Based radiation therapy, U-net},
abstract = {Background
Manual contouring for radiation therapy planning remains the most laborious and time consuming part in the radiation therapy workflow. Particularly for cervical cancer, this task is complicated by the complex female pelvic anatomy and the concomitant dependence on 18F-labeled Fluorodeoxyglucose (FDG) positron emission tomography (PET) and magnetic resonance (MR) images. Using deep learning, we propose a new auto-contouring method for FDG-PET/MR based cervical cancer radiation therapy by combining the high level anatomical topography and radiological properties, to the low-level pixel wise deep-learning based semantic segmentation.
Materials/methods
The proposed method: 1) takes advantage of PET data and left/right anatomical symmetry, creating sub-volumes that are centered on the structures to be contoured. 2) Uses a 3D shallow U-Net (sU-Net) model with an encoder depth of 2.3) Applies the successive training of 3 consecutive sU-Nets in a feed forward strategy. 4) Employs, instead of the usual generalized dice loss function (GDL), a patch dice loss function (PDL) that takes into account the Dice similarity index (DSI) at the level of each training patch. Experimental analysis was conducted on a set of 13 PET/MR images was using a leave-one-out strategy.
Results
Despite the limited data availability, 5 anatomical structures - the gross tumor volume, bladder, anorectum, and bilateral femurs - were accurately (DSI ​= ​0.78), rapidly (1.9 ​s/structure), and automatically delineated by our algorithm. Overall, PDL achieved a better performance than GDL and DSI was higher for organs at risk (OARs) with solid tissue (e.g. femurs) than for OARs with air-filled soft tissues (e.g. anorectum).
Conclusion
The presented workflow successfully addresses the challenge of auto-contouring in FDG-PET/MR based cervical cancer. It is expected to expedite the cervical cancer radiation therapy workflow in both, conventional and adaptive radiation therapy settings.}
}
@article{KERNER202154,
title = {Machine learning and big data provide crucial insight for future biomaterials discovery and research},
journal = {Acta Biomaterialia},
volume = {130},
pages = {54-65},
year = {2021},
issn = {1742-7061},
doi = {https://doi.org/10.1016/j.actbio.2021.05.053},
url = {https://www.sciencedirect.com/science/article/pii/S1742706121003639},
author = {Jacob Kerner and Alan Dogan and Horst {von Recum}},
keywords = {Machine learning, QSAR, QSPR, Material informatics},
abstract = {Machine learning have been widely adopted in a variety of fields including engineering, science, and medicine revolutionizing how data is collected, used, and stored. Their implementation has led to a drastic increase in the number of computational models for the prediction of various numerical, categorical, or association events given input variables. We aim to examine recent advances in the use of machine learning when applied to the biomaterial field. Specifically, quantitative structure properties relationships offer the unique ability to correlate microscale molecular descriptors to larger macroscale material properties. These new models can be broken down further into four categories: regression, classification, association, and clustering. We examine recent approaches and new uses of machine learning in the three major categories of biomaterials: metals, polymers, and ceramics for rapid property prediction and trend identification. While current research is promising, limitations in the form of lack of standardized reporting and available databases complicates the implementation of described models. Herein, we hope to provide a snapshot of the current state of the field and a beginner's guide to navigating the intersection of biomaterials research and machine learning.
Statement of significance
Machine learning and its methods have found a variety of uses beyond the field of computer science but have largely been neglected by those in realm of biomaterials. Through the use of more computational methods, biomaterials development can be expediated while reducing the need for standard trial and error methods. Within, we introduce four basic models that readers can potentially apply to their current research as well as current applications within the field. Furthermore, we hope that this article may act as a “call to action” for readers to realize and address the current lack of implementation within the biomaterials field.}
}
@article{MORA2020140,
title = {A collaborative working model for enhancing the learning process of science & engineering students},
journal = {Computers in Human Behavior},
volume = {103},
pages = {140-150},
year = {2020},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2019.09.008},
url = {https://www.sciencedirect.com/science/article/pii/S074756321930336X},
author = {Higinio Mora and María Teresa Signes-Pont and Andrés Fuster-Guilló and María L. Pertegal-Felices},
keywords = {Higher education, ICT learning technologies, Quality assessments, Computers in human behavior, Collaborative learning, Student experiences, Collective intelligence},
abstract = {Science and engineering education are mostly based on content assimilation and development of skills. However, to adequately prepare students for today's world, it is also necessary to stimulate critical thinking and make them reflect on how to improve current practices using new tools and technologies. In this line, the main motivation of this research consists in exploring ways supported by technology to enhance the learning process of students and to better prepare them to face the challenges of today's world. To this end, the purpose of this work is to design an innovative learning project based on collaborative work among students, and research its impact in achieving better learning outcomes, generating of collective intelligence and further motivation. The proposed collaborative working model is based on peer review assessment methodology implemented through a learning web-platform. Thus, students were encouraged to peer review their classmates' works. They had to make comments, suggest improvements, and assess final assignments. Teaching staff managed and supervised the whole process. Students were selected from computer science engineering at the University of Alicante (Spain). Results suggested greater content assimilation and enhanced learning in several scientific skills. The students' final grade exceeded what any student could produce individually, but we cannot conclude that real collective intelligence was generated. Learning methodologies based on the possibilities of Information and Communication Technologies (ICT) provide new ways to transmit and manage knowledge in higher education. Collaborating in peer assessment enhances the students' motivation and promotes the active learning. In addition, this method can be very helpful and time saving for instructors in the management of large groups.}
}
@article{SITTI2021101340,
title = {Physical intelligence as a new paradigm},
journal = {Extreme Mechanics Letters},
volume = {46},
pages = {101340},
year = {2021},
issn = {2352-4316},
doi = {https://doi.org/10.1016/j.eml.2021.101340},
url = {https://www.sciencedirect.com/science/article/pii/S2352431621001012},
author = {Metin Sitti},
keywords = {Physical Intelligence, mechanics, meta materials, multistability, mechanical memory, mechanical computation},
abstract = {Intelligence of physical agents, such as human-made (e.g., robots, autonomous cars) and biological (e.g., animals, plants) ones, is not only enabled by their computational intelligence (CI) in their brain, but also by their physical intelligence (PI) encoded in their body. Therefore, it is essential to advance the PI of human-made agents as much as possible, in addition to their CI, to operate them in unstructured and complex real-world environments like the biological agents. This article gives a perspective on what PI paradigm is, when PI can be more significant and dominant in physical and biological agents at different length scales and how bioinspired and abstract PI methods can be created in agent bodies. PI paradigm aims to synergize and merge many research fields, such as mechanics, materials science, robotics, mechanical design, fluidics, active matter, biology, self-assembly and collective systems, to enable advanced PI capabilities in human-made agent bodies, comparable to the ones observed in biological organisms. Such capabilities would progress the future robots and other machines beyond what can be realized using the current frameworks.}
}
@article{MONTANARO2024,
title = {Computable species descriptions and nanopublications: applying ontology-based technologies to dung beetles (Coleoptera, Scarabaeinae)},
journal = {Biodiversity Data Journal},
volume = {12},
year = {2024},
issn = {1314-2836},
doi = {https://doi.org/10.3897/BDJ.12.e121562},
url = {https://www.sciencedirect.com/science/article/pii/S1314283624001660},
author = {Giulio Montanaro and James P. Balhoff and Jennifer C. Girón and Max Söderholm and Sergei Tarasov},
keywords = {Phenoscript, taxonomy, semantic data, phenotypic traits, characters, morphology,   , microCT},
abstract = {Background
Taxonomy has long struggled with analysing vast amounts of phenotypic data due to computational and accessibility challenges. Ontology-based technologies provide a framework for modelling semantic phenotypes that are understandable by computers and compliant with FAIR principles. In this paper, we explore the use of Phenoscript, an emerging language designed for creating semantic phenotypes, to produce computable species descriptions. Our case study centers on the application of this approach to dung beetles (Coleoptera, Scarabaeinae).
New information
We illustrate the effectiveness of Phenoscript for creating semantic phenotypes. We also demonstrate the ability of the Phenospy python package to automatically translate Phenoscript descriptions into natural language (NL), which eliminates the need for writing traditional NL descriptions. We introduce a computational pipeline that streamlines the generation of semantic descriptions and their conversion to NL. To demonstrate the power of the semantic approach, we apply simple semantic queries to the generated phenotypic descriptions. This paper addresses the current challenges in crafting semantic species descriptions and outlines the path towards future improvements. Furthermore, we discuss the promising integration of semantic phenotypes and nanopublications, as emerging methods for sharing scientific information. Overall, our study highlights the pivotal role of ontology-based technologies in modernising taxonomy and aligning it with the evolving landscape of big data analysis and FAIR principles.}
}
@article{OU2023e15530,
title = {Investigation and analysis of the current situation of programming education in primary and secondary schools},
journal = {Heliyon},
volume = {9},
number = {4},
pages = {e15530},
year = {2023},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2023.e15530},
url = {https://www.sciencedirect.com/science/article/pii/S2405844023027378},
author = {Qizhong Ou and Weijie Liang and Zhenni He and Xiao Liu and Renxing Yang and Xiaojun Wu},
keywords = {Programming education in primary and secondary schools, Programming education for students, Programming learning, Investigation and current situation, Primary and secondary education},
abstract = {With the rapid development of the era of artificial intelligence, the application ability of programming is also highlighted. As one of the necessary abilities of social talents in the future, primary and secondary schools pay more and more attention to this, and programming education is also in full swing. Therefore, based on previous studies, this paper further clarifies the current situation when the current situation of programming education in primary and secondary schools is ambiguous. This paper is aimed at a wide range of primary and secondary school teachers. With 1500 teachers who participated in the online training class for programming teachers as the object in Chinese primary, middle and high school stages, mainly from the three levels of schools, teachers, and students. The questionnaire with good reliability and validity test was used as the research method, the survey data were statistically described and analyzed, and differences were analyzed using Microsoft Excel2019, SPSS26.0 and so on, it investigates and analyzes the current situation of programming education in primary and secondary schools. Results indicate that the overall quality of programming education offerings in elementary and secondary schools is subpar, and the construction of programming education curriculum in schools requires improvement. Nevertheless, schools prioritize improving students' comprehensive abilities, and teachers hold a positive attitude towards programming education and teaching. Although students demonstrate a strong interest in learning, their foundation is weak, resulting in poor learning outcomes. Consequently, the author provides specific recommendations regarding programming education's working mechanism, curriculum standard system, teacher training, and educational resources sharing to better develop programming education in primary and secondary schools.}
}
@incollection{BETZ202511,
title = {Chapter 2 - Conceptual and methodological issues in insect ecomorphology},
editor = {Oliver Betz},
booktitle = {Insect Ecomorphology},
publisher = {Academic Press},
pages = {11-55},
year = {2025},
isbn = {978-0-443-18544-1},
doi = {https://doi.org/10.1016/B978-0-443-18544-1.00011-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780443185441000119},
author = {Oliver Betz},
keywords = {Adaptation, Behaviour, Biomechanics, Ecological morphology, Evolutionary morphology, Functional morphology, Functional trait, Insect morphology, Macroevolution, Performance, Resource use, Trait},
abstract = {Ecomorphology can be defined as a research field that investigates the ecological and evolutionary consequences of animal construction by integrating the function (biomechanics) and form (morphology) of animals in their relationship to the environment. The conceptual framework of this research field is reviewed here, with a distinction being made between the ‘morphological-comparative concept’ (having emerged from the field of functional morphology) and the ‘ecological-correlative concept’ (representing a trait-based statistically correlative approach). Although the development of the field has mainly been driven by vertebrate morphologists, this review shows that, notably, insects represent a clade that deserves more ecomorphological research because of their tremendous species diversity connected to their enormous morphofunctional disparity. Based on the morphology–performance–ecology–fitness paradigm, research problems are examined regarding the benefits that accrue to morphologists who integrate ecological thinking into their research programmes and to ecologists who consider the functional mechanisms behind their research subjects.}
}
@incollection{GIOVANNONE202441,
title = {Chapter Two - Further steps towards a mechanistic functionalist framework for understanding individual differences in language and cognition},
editor = {Kara D. Federmeier},
series = {Psychology of Learning and Motivation},
publisher = {Academic Press},
volume = {81},
pages = {41-73},
year = {2024},
issn = {0079-7421},
doi = {https://doi.org/10.1016/bs.plm.2024.07.003},
url = {https://www.sciencedirect.com/science/article/pii/S0079742124000380},
author = {Nikole Giovannone and Joseph C. Toscano},
keywords = {Individual differences, Cognition, Language processing, Computational modeling, Mechanistic functionalism},
abstract = {Despite a growing focus on individual differences in cognitive psychology, research in this area is complicated by several issues related to how such differences are defined and measured. These challenges create a significant roadblock for the field. To combat this issue, we argue that the next critical step for language and cognitive science is careful and thorough investigation of the specific mechanisms that drive individual differences. In this chapter, our goal is to extend the process-based mechanistic functional normativist framework and to provide a test case for how researchers can leverage computational modeling to investigate individual differences in cognitive mechanisms (using pattern learning in the serial reaction time task as an example). By shifting our focus to characterizing the mechanisms that drive individual differences in language and cognition, the field stands to advance both theoretical frameworks and methodological approaches for studying these processes.}
}
@article{LI2023106688,
title = {Machine learning assisted advanced battery thermal management system: A state-of-the-art review},
journal = {Journal of Energy Storage},
volume = {60},
pages = {106688},
year = {2023},
issn = {2352-152X},
doi = {https://doi.org/10.1016/j.est.2023.106688},
url = {https://www.sciencedirect.com/science/article/pii/S2352152X23000853},
author = {Ao Li and Jingwen Weng and Anthony Chun Yin Yuen and Wei Wang and Hengrui Liu and Eric Wai Ming Lee and Jian Wang and Sanghoon Kook and Guan Heng Yeoh},
keywords = {Battery thermal management, Thermal runaway, Mitigation, Artificial neural networks, Machine learning},
abstract = {With an increasingly wider application of the lithium-ion battery (LIB), specifically the drastic increase of electric vehicles in cosmopolitan cities, improving the thermal and fire resilience of LIB systems is inevitable. Thus, in-depth analysis and performance-based study on battery thermal management system (BTMs) design have arisen as a popular research topic in energy storage systems. Among the LIB system parameters, such as battery temperature distribution, battery heat generation rate, cooling medium properties, electrical properties, physical dimension design, etc., multi-factor design optimisation is one of the most difficult experimental tasks. Computational simulations deliver a holistic solution to the BTMs design, yet it demands an immense amount of computational power and time, which is often not practical for the design optimisation process. Therefore, machine learning (ML) models play a non-substitute role in the safety management of battery systems. ML models aid in temperature prediction and safety diagnosis, thereby assisting in the early warning of battery fire and its mitigation. In this review article, we summarise extensive lists of literature on BTMs employing ML models and identify the current state-of-the-art research, which is expected to serve as a much-needed guideline and reference for future design optimisation. Following that, the application of various ML models in battery fire diagnosis and early warning is illustrated. Finally, the authors propose improved approaches to advanced battery safety management with ML. This review paper aims to bring new insights into the application of ML in the LIB thermal safety issue and BTMs design and anticipate boosting further advanced battery system design not limited to the thermal management system, as well as proposing potential digital twin modelling for BTMs.}
}
@article{BLACKMAN2022101661,
title = {Persistent mysteries of jet engines, formation, propagation, and particle acceleration: Have they been addressed experimentally?},
journal = {New Astronomy Reviews},
volume = {95},
pages = {101661},
year = {2022},
issn = {1387-6473},
doi = {https://doi.org/10.1016/j.newar.2022.101661},
url = {https://www.sciencedirect.com/science/article/pii/S1387647322000197},
author = {Eric G. Blackman and Sergey V. Lebedev},
keywords = {Jets, Laboratory astrophysics, Accretion, Magnetic fields, Young stellar objects, Active galactic nuclei, Microquasars, Particle acceleration, High energy density physics},
abstract = {The physics of astrophysical jets can be divided into three regimes: (i) engine and launch (ii) propagation and collimation, (iii) dissipation and particle acceleration. Since astrophysical jets comprise a huge range of scales and phenomena, practicality dictates that most studies of jets intentionally or inadvertently focus on one of these regimes, and even therein, one body of work may be simply boundary condition for another. We first discuss long standing persistent mysteries that pertain the physics of each of these regimes, independent of the method used to study them. This discussion makes contact with frontiers of plasma astrophysics more generally. While observations theory, and simulations, and have long been the main tools of the trade, what about laboratory experiments? Jet related experiments have offered controlled studies of specific principles, physical processes, and benchmarks for numerical and theoretical calculations. We discuss what has been accomplished on these fronts. Although experiments have indeed helped us to understand certain processes, proof of principle concepts, and benchmarked codes, they have yet to solved an astrophysical jet mystery on their own. A challenge is that experimental tools used for jet-related experiments so far, are typically not machines originally designed for that purpose, or designed with specific astrophysical mysteries in mind. This presents an opportunity for a different way of thinking about the development of future platforms: start with the astrophysical mystery and build an experiment to address it.}
}
@incollection{KADAR201425,
title = {2 - Developing a personalized and adapted curriculum for engineering education through an ambient intelligence environment},
editor = {J. {Paulo Davim}},
booktitle = {Engineering Education},
publisher = {Chandos Publishing},
address = {Oxford},
pages = {25-65},
year = {2014},
isbn = {978-1-84334-687-6},
doi = {https://doi.org/10.1533/9781780633589.25},
url = {https://www.sciencedirect.com/science/article/pii/B978184334687650002X},
author = {M. Kadar and M. Muntean and L. Marina},
keywords = {engineering education, ambient intelligence environment, brain lateralization system, adapted and personalized curriculum},
abstract = {Abstract:
This chapter describes a research model that enables students to become all that they are capable of becoming, and educators and decision makers to maximize their efforts in the field of engineering education through an ambient intelligence environment. This research proposes to translate conceptual ideas for the functionality of the environment and appliances into concrete designs. The core of the intelligent educational environment is an information system called the brain lateralization information system (BLIS). The BLIS can provide valuable information on users’ brain lateralization and students’ thinking style. Such information can be used by educators in designing new teaching methodologies that will finally lead to adapted, personalized study programs within the university curricula. The chapter shows how this approach, which has hitherto been applied to students, teaching staff and management staff from the departments of Computer Science, Applied Electronics, and Environmental Engineering of the University of Alba Iulia, was validated to allow future development of methodologies, strategies, and operational programs in the field of engineering education. In order to achieve this vision the chapter introduces a number of novel concepts and a model, in particular a new brain lateralization information system embedded into an ambient intelligent environment. Finally, the chapter reports on conclusions, recommendations and examples of adapted and personalized courseware designed for blended learning, and a user evaluation of this model, which demonstrates that users find the ambient intelligence environment useful for their career choice and easy and enjoyable to use for teachers and decision makers.}
}
@article{DSOUZA2021100949,
title = {What characterises creativity in narrative writing, and how do we assess it? Research findings from a systematic literature search},
journal = {Thinking Skills and Creativity},
volume = {42},
pages = {100949},
year = {2021},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2021.100949},
url = {https://www.sciencedirect.com/science/article/pii/S1871187121001644},
author = {Richard D'Souza},
keywords = {Creativity, Writing, Assessment, Narrative, Review},
abstract = {This paper reports findings from a systematic search of the empirical literature from 2000 to 2020 on the assessment of creativity in narrative writing. It seeks to synthesise the designs, methods and findings on how different disciplines have gathered relevant data to the question of how creativity in writing might be assessed, and feedback made more effective. It draws together the established knowledge base around two research questions. The methodology for the systematic involved searches on five academic databases for relevant keywords, producing 1796 papers. Initial screening of the abstracts identified 97 studies for further scrutiny, and for which full texts were accessed for secondary screening based on the inclusion criteria. The final 39 papers judged to satisfy the selection criteria were subject to in-depth analysis and synthesis. The findings of the review reveal that four main techniques have been utilised in efforts to assess creativity in writing, each with their own merits and limitations, and a paucity of research in several crucial areas. The review indicates that, while several disciplines have contributed to the knowledge base in this area, few interdisciplinary studies exist that draw together multiple techniques and provide clear answers for the research questions used in the study. Furthermore, there is little empirical evidence suggesting that assessment improves student creativity with regard to writing, and new research in the field would be advanced by addressing explicit definitions of creativity, the practices of writing ‘experts’, and writing considered within its social and cultural context.}
}
@article{ROUSE201472,
title = {Human interaction with policy flight simulators},
journal = {Applied Ergonomics},
volume = {45},
number = {1},
pages = {72-77},
year = {2014},
note = {Systems Ergonomics/Human Factors},
issn = {0003-6870},
doi = {https://doi.org/10.1016/j.apergo.2013.03.019},
url = {https://www.sciencedirect.com/science/article/pii/S0003687013000604},
author = {William B. Rouse},
keywords = {Computational modeling, Interactive visualization, Policy flight simulators},
abstract = {Policy flight simulators are designed for the purpose of exploring alternative management policies at levels ranging from individual organizations to national strategy. This article focuses on how such simulators are developed and on the nature of how people interact with these simulators. These interactions almost always involve groups of people rather than individuals, often with different stakeholders in conflict about priorities and courses of action. The ways in which these interactions are framed and conducted are discussed, as well as the nature of typical results.}
}
@article{PHAN2024,
title = {Precision synbiotics increase gut microbiome diversity and improve gastrointestinal symptoms in a pilot open-label study for autism spectrum disorder},
journal = {mSystems},
volume = {9},
number = {5},
year = {2024},
issn = {2379-5077},
doi = {https://doi.org/10.1128/msystems.00503-24},
url = {https://www.sciencedirect.com/science/article/pii/S2379507724001077},
author = {Joann Phan and Diana C. Calvo and Divya Nair and Suneer Jain and Thibaut Montagne and Summer Dietsche and Kelsey Blanchard and Shirin Treadwell and James Adams and Rosa Krajmalnik-Brown and Nicholas Chia},
keywords = {ASD, synbiotics, probiotics, prebiotics, gut microbiome, open-label, precision, supplements, metagenomics},
abstract = {ABSTRACT

The efficacy of prebiotics and probiotics (synbiotics when combined) to improve symptoms associated with autism spectrum disorder (ASD) has shown considerable inter-study variation, likely due to the complex, heterogeneous nature of the disorder and its associated behavioral, developmental, and gastrointestinal symptoms. Here, we present a precision synbiotic supplementation study in 296 children and adults diagnosed with ASD versus 123 age-matched neurotypical controls. One hundred seventy ASD participants completed the study. Baseline and post-synbiotic assessment of ASD and gastrointestinal (GI) symptoms and deep metagenomic sequencing were performed. Within the ASD cohort, there were significant differences in microbes between subpopulations based on the social responsiveness scale (SRS2) survey (Prevotella spp., Bacteroides, Fusicatenibacter, and others) and gluten and dairy-free diets (Bifidobacterium spp., Lactococcus, Streptococcus spp., and others). At the baseline, the ASD cohort maintained a lower taxonomic alpha diversity and significant differences in taxonomic composition, metabolic pathways, and gene families, with a greater proportion of potential pathogens, including Shigella, Klebsiella, and Clostridium, and lower proportions of beneficial microbes, including Faecalibacterium compared to controls. Following the 3-month synbiotic supplementation, the ASD cohort showed increased taxonomic alpha diversity, shifts in taxonomy and metabolic pathway potential, and improvements in some ASD-related symptoms, including a significant reduction in GI discomfort and overall improved language, comprehension, cognition, thinking, and speech. However, the open-label study design may include some placebo effects. In summary, we found that precision synbiotics modulated the gut microbiome and could be used as supplementation to improve gastrointestinal and ASD-related symptoms.
IMPORTANCE
Autism spectrum disorder (ASD) is prevalent in 1 out of 36 children in the United States and contributes to health, financial, and psychological burdens. Attempts to identify a gut microbiome signature of ASD have produced varied results. The limited pre-clinical and clinical population sizes have hampered the success of these trials. To understand the microbiome associated with ASD, we employed whole metagenomic shotgun sequencing to classify microbial composition and genetic functional potential. Despite being one of the most extensive ASD post-synbiotic assessment studies, the results highlight the complexity of performing such a case–control supplementation study in this population and the potential for a future therapeutic approach in ASD.
Autism spectrum disorder (ASD) is prevalent in 1 out of 36 children in the United States and contributes to health, financial, and psychological burdens. Attempts to identify a gut microbiome signature of ASD have produced varied results. The limited pre-clinical and clinical population sizes have hampered the success of these trials. To understand the microbiome associated with ASD, we employed whole metagenomic shotgun sequencing to classify microbial composition and genetic functional potential. Despite being one of the most extensive ASD post-synbiotic assessment studies, the results highlight the complexity of performing such a case–control supplementation study in this population and the potential for a future therapeutic approach in ASD.}
}
@article{VELICHKOVSKY2018227,
title = {Consciousness in a multilevel architecture: Evidence from the right side of the brain},
journal = {Consciousness and Cognition},
volume = {64},
pages = {227-239},
year = {2018},
note = {Visual Experience and Guidance of Action: A Tribute to Bruce Bridgeman},
issn = {1053-8100},
doi = {https://doi.org/10.1016/j.concog.2018.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S1053810018300412},
author = {Boris M. Velichkovsky and Olga A. Krotkova and Artemy A. Kotov and Vyacheslav A. Orlov and Vitaly M. Verkhlyutov and Vadim L. Ushakov and Maxim G. Sharaev},
keywords = {Consciousness, Dynamic Causal Modeling (DCM), Resting state, Lateralization, Frontopolar cortex, Hippocampal formation, Ventrolateral prefrontal-amygdala emotional pathway, Egocentric spatial representation, Self-referential cognition, Levels of cognitive organization},
abstract = {By taking into account Bruce Bridgeman's interest in an evolutionary framing of human cognition, we examine effective (cause-and-effect) connectivity among cortical structures related to different parts of the triune phylogenetic stratification: archicortex, paleocortex and neocortex. Using resting-state functional magnetic resonance imaging data from 25 healthy subjects and spectral Dynamic Causal Modeling, we report interactions among 10 symmetrical left and right brain areas. Our results testify to general rightward and top-down biases in excitatory interactions of these structures during resting state, when self-related contemplation prevails over more objectified conceptual thinking. The right hippocampus is the only structure that shows bottom-up excitatory influences extending to the frontopolar cortex. The right ventrolateral cortex also plays a prominent role as it interacts with the majority of nodes within and between evolutionary distinct brain subdivisions. These results suggest the existence of several levels of cognitive-affective organization in the human brain and their profound lateralization.}
}
@article{TANG2024101570,
title = {Disruptive content, cross agglomeration interaction, and agglomeration replacement: Does cohesion foster strength?},
journal = {Journal of Informetrics},
volume = {18},
number = {4},
pages = {101570},
year = {2024},
issn = {1751-1577},
doi = {https://doi.org/10.1016/j.joi.2024.101570},
url = {https://www.sciencedirect.com/science/article/pii/S1751157724000828},
author = {Kun Tang and Baiyang Li and Qiyu Zhu and Lecun Ma},
keywords = {Scholar agglomeration, Disruptive knowledge, Coauthor network, Agglomeration size},
abstract = {A trend in the academic field is agglomerations among scholars to generate knowledge with a disruptive influence on science and technology; however, the benefits have not been fully substantiated. This paper analyzes over 660,000 papers on artificial intelligence published from 1961 to 2023. We propose a method to calculate the innovative capacity of disruptive knowledge based on the similarity of historical, current, and future keywords, finding that scholars who commence their scientific endeavors earlier possess a heightened capability for disruptive knowledge innovation as Dkc index. The analysis reveals that multiagglomeration scholars have the highest average number of publications and citations, followed by agglomeration-flow scholars. Moreover, a larger agglomeration results in a lower ability to disrupt and consolidate knowledge innovation. Multiagglomeration and agglomeration-flow scholars harm disruptive/consolidative innovations. However, as the agglomeration effect intensifies, these two types of scholars from the disruptive perspective and multiagglomeration scholars from the consolidation perspective have a diminishing marginal effect on innovation capacity. The agglomeration size acts as a partial intermediary in the Multi→Size→Dkc index from the dual perspective and as a full mediator in the Flow→Size→Dkc index from the disruptive perspective, but only with a direct effect from the consolidative perspective.}
}
@incollection{MEY200651,
title = {Pragmatics: Overview},
editor = {Keith Brown},
booktitle = {Encyclopedia of Language & Linguistics (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {51-62},
year = {2006},
isbn = {978-0-08-044854-1},
doi = {https://doi.org/10.1016/B0-08-044854-2/00306-0},
url = {https://www.sciencedirect.com/science/article/pii/B0080448542003060},
author = {J.L. Mey},
keywords = {affordance, assimilation, bestrangement, common scene, communication, community of, context, conversational maxim, cooperative principle, cotext, deixis, felicity condition, flouting, ghettoization, illocutionary force, implicature, indexical, indexing, indirect speech act, interact, institutional, placement, mandate, misunderstanding, pragmatic acts, presuppositions, relevance, semantics, situated speech, situation, placement, situational condition, social, social environment, social practice, societal stance, speech act, syntax},
abstract = {Pragmatics is the study of human language use as it is exercised in a community of social practice. The exercise is, however, not limited to verbal signs: other communicative means are also included under the definition (e.g., as becomes clear when one studies pragmatic acts in addition to the traditional speech acts). In addition, emphasis is placed on the way communication is organized, first of all in the classical models put forward by Austin, Searle, and Grice, but extending also this approach to comprise more recent advances in pragmatic thinking, especially in relation to what used to be called the hyphenated disciplines: sociolinguistics, psycholinguistics, the study of humans in interaction with computers, the teaching of first and second languages, and a host of other practically and theoretically oriented fields of study.}
}
@article{LIU2024102118,
title = {Personalized fuzzy semantic model of PHFLTS: Application to linguistic group decision making},
journal = {Information Fusion},
volume = {103},
pages = {102118},
year = {2024},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2023.102118},
url = {https://www.sciencedirect.com/science/article/pii/S1566253523004347},
author = {Yaya Liu and Lina Zhu and Rosa M. Rodríguez and Luis Martínez},
keywords = {Computing with words, Linguistic group decision making, Proportional hesitant fuzzy linguistic term set, Personalized individual semantic},
abstract = {The proportional hesitant fuzzy linguistic term set (PHFLTS) has been effectively employed in analyzing the group’s hesitancy in linguistic group decision making (LGDM). The application of PHFLTS assists in capturing the individual’s hesitancy across diverse time periods. It is acknowledged that a single word could potentially convey various meanings to different decision makers, such differences can be proficiently managed by utilizing personalized individual semantic (PIS) models. Previous approaches for calculating PIS failed to incorporate the individual’s updating preference information over time, which increases the risk that the computation of PIS is affected by random factors in a specific moment. In our current research, individual linguistic preference gathered over a time period are leveraged to form the PHFLTS. Additionally, a consistency driven optimization model based on PHFLTS is formulated to obtain PIS of linguistic terms. Subsequently, a fuzzy representation model termed as the fuzzy envelope of PHFLTS is introduced to facilitate the computation with words processes, integrating PHFLTS in LGDM. The practicality and legitimacy of these proposed models are evaluated through a comparative analysis. Lastly, these proposed models are tested and applied in a dedicated case study to further prove their usefulness and efficacy.}
}
@article{ALMOJEL2000297,
title = {The implementation and performance evaluation of N-body gravitational simulation algorithm on high-performance computers},
journal = {Computers & Electrical Engineering},
volume = {26},
number = {3},
pages = {297-316},
year = {2000},
issn = {0045-7906},
doi = {https://doi.org/10.1016/S0045-7906(99)00048-8},
url = {https://www.sciencedirect.com/science/article/pii/S0045790699000488},
author = {Abdullah I. Almojel},
keywords = {Astrophysics simulations, Dynamic load balancing, Performance analysis, MIMD machines, Barnes–Hut algorithm},
abstract = {Gravity determines the dynamical evaluation of many astrophysical systems. Each of these systems can be described as N gravitationally interacting particles. Solutions of the resulting N-body problem by direct simulation entails the calculation of O(N2) forces at each time step. To increase the number of particles that can be simulated, approximate algorithms have been developed and implemented in so-called tree codes. In these algorithms the particles are stored into a spatial hierarchy that forms a tree data structure. For a fixed level of accuracy the complexity of this algorithm is O(Nlog(N)). In this paper, a “manager–worker” model for a parallel implementation of hierarchical N-body algorithm is introduced. We describe a load-balanced, efficient algorithm for solving the Astrophysics simulation of N-body problem using tree-based data structures and massively parallel computing architectures. This algorithm, based on the Barnes–Hut method, first assembles a tree data structure that represents the distribution of bodies, or particles, at all length-scales. An adaptive load balancing technique is used to assign bodies to processors as well as to insure that processors are assigned equal amounts of work. A number of performance measurements were carried out in order to reveal the behavior of the N-body application with respect to a partitioning technique and load imbalance overhead. We also show that, with using the introduced manager–worker model and the cost zones domain decomposition technique, the algorithm is load balanced and that the majority of the time of the algorithm is spent in performing on-processor functions and not in inter-processor communications. We have conducted our study on several high-performance MIMD supercomputer machines such as the 256-processor Cray T3D and the 64-processor Intel Paragon at NASA/JPL and on the 32-processor Thinking Machines CM-5 at UMC.}
}
@article{ADAMS2010324,
title = {Why we still need a mark of the cognitive},
journal = {Cognitive Systems Research},
volume = {11},
number = {4},
pages = {324-331},
year = {2010},
note = {Special Issue on Extended Mind},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2010.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S1389041710000331},
author = {Frederick Adams},
keywords = {Causal spread, Cognitive, Cognitive process, Computation, Constitutes, Content, Coupling, Extended mind, Information processing, Parity principle, Representation, Semantics, Think},
abstract = {What makes a process a cognitive process? I’m not just asking for a list of cognitive processes, but for what makes an item on that list a cognitive process. Why should it be on the list? This is a question that has been ignored far too long in the domain of research calling itself cognitive science. It is time to give an answer and that is what I propose in this paper. I contrast my answer with others that have been given and defend the need against some claims in the literature that a mark of the cognitive is not needed.}
}
@article{BARON2019319,
title = {Machine Learning and Other Emerging Decision Support Tools},
journal = {Clinics in Laboratory Medicine},
volume = {39},
number = {2},
pages = {319-331},
year = {2019},
note = {Clinical Decision Support: Tools, Strategies, and Emerging Technologies},
issn = {0272-2712},
doi = {https://doi.org/10.1016/j.cll.2019.01.010},
url = {https://www.sciencedirect.com/science/article/pii/S0272271219300101},
author = {Jason M. Baron and Danielle E. Kurant and Anand S. Dighe},
keywords = {Machine learning, Clinical decision support, Artificial intelligence, Knowledge discovery, Computational pathology}
}