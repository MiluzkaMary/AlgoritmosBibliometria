@article{LI2021512,
title = {Design and implementation of neural network computing framework on Zynq SoC embedded platform},
journal = {Procedia Computer Science},
volume = {183},
pages = {512-518},
year = {2021},
note = {Proceedings of the 10th International Conference of Information and Communication Technology},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.02.091},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921005676},
author = {Xingying Li and Zhenyu Yin and Fulong Xu and Feiqing Zhang and Guangyuan Xu},
keywords = {Neural network, embedded platform, Zynq SoC, darknet, depthwise separable convolution, MobileNetV2},
abstract = {Limited resources and low computing power of embedded platform make it difficult to apply neural network technology. To overcome this problem, a new neural network computing framework “Zynq-Darknet” was proposed. The framework is based on Darknet, which constructs depthwise separable convolution and a lightweight classiﬁcation model MobileNetV2 and was deployed to Xilinx Zynq-7000 System-on-Chip (SoC) with Linux operating system (OS). In order to verify the performance of the framework and model, experiments were conducted on imagenet-1k dataset using different network structures. The results show that the MobileNetV2 network model based on Zynq-Darknet can effectively perform image classification, and ensure a certain real-time and accuracy while reducing the computational complexity and storage overhead, assuming promising application prospects.}
}
@incollection{WANG20249,
title = {1.02 - Artificial Intelligence and Bioinformatics Applications in Precision Medicine and Future Implications},
editor = {Kenneth S. Ramos},
booktitle = {Comprehensive Precision Medicine (First Edition)},
publisher = {Elsevier},
edition = {First Edition},
address = {Oxford},
pages = {9-24},
year = {2024},
isbn = {978-0-12-824256-8},
doi = {https://doi.org/10.1016/B978-0-12-824010-6.00058-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128240106000587},
author = {Ni Wang and Qiang He},
keywords = {Artificial intelligence, Bioinformatics, Clinical trials, Disease risk assessment, Drug discovery, Early disease detection, Genome sequencing, Oncology, Personalized medicine, Pharmacogenomics, Precision medicine},
abstract = {Artificial intelligence (AI) and bioinformatics have emerged as key technologies for advancing precision medicine. Many tools of AI and bioinformatics have been applied in healthcare that would be of great value to advance the goals of precision medicine. AI is a branch of computer science that deals with the automation of intelligent behavior. Bioinformatics is a field of study that combines biology, computer science, and statistics to analyze and interpret biologic data. The latter involves the development and application of computational methods and tools for storing, organizing, analyzing, and interpreting biologic information, including genomic, proteomic, and metabolomic data. AI and bioinformatics are well-positioned to help tailor medical decisions and treatments at the individual and population levels. Some of those applications and the multidisciplinary implications are presented in this chapter.}
}
@article{FRANZ1994433,
title = {A critical framework for methodological research in architecture},
journal = {Design Studies},
volume = {15},
number = {4},
pages = {433-447},
year = {1994},
issn = {0142-694X},
doi = {https://doi.org/10.1016/0142-694X(94)90006-X},
url = {https://www.sciencedirect.com/science/article/pii/0142694X9490006X},
author = {Jill M. Franz},
keywords = {methodological research, critical framework, architecture, design},
abstract = {This paper reviews a cross-section of methodological studies undertaken in architecture since the Second World War. Despite a variety of orientations, technically, conceptually and philosophically, most studies reflect an understanding of people and objects as discrete entities interacting in an passive and unilateral manner. This dominant dualist understanding is concluded to be the essential cause of the ‘implementation gap' between architectural research and practice. For the gap to close, the development and institution of a critical framework is needed which encourages researchers to acknowledge explicitly the ontological and epistomological issues associated with architectural practice, education and research. Underlying this recommendation is a dialectic appreciation of person-world interaction; one which accepts as a holistic theme for inquiry, the experiential and interpretative quality of human thinking, feeling and action.}
}
@article{SHU2025121176,
title = {Altered brain network dynamics during rumination in remitted depression},
journal = {NeuroImage},
volume = {310},
pages = {121176},
year = {2025},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2025.121176},
url = {https://www.sciencedirect.com/science/article/pii/S1053811925001788},
author = {Su Shu and Wenwen Ou and Mohan Ma and Hairuo He and Qianqian Zhang and Mei Huang and Wentao Chen and Aoqian Deng and Kangning Li and Zhenman Xi and Fanyu Meng and Hui Liang and Sirui Gao and Yilin Peng and Mei Liao and Li Zhang and Mi Wang and Jin Liu and Bangshan Liu and Yumeng Ju and Yan Zhang},
keywords = {Rumination, Energy landscape, Brain state, Large-scale network, Depression},
abstract = {Rumination is a known risk factor for depression relapse. Understanding its neurobiological mechanisms during depression remission can inform strategies to prevent relapse, yet the temporal dynamics of brain networks during rumination in remitted depression remain unclear. Here, we collected rumination induction fMRI data from 42 patients with remitted depression and 41 healthy controls (HCs). Using an energy landscape approach, we investigated the temporal dynamics of brain networks during rumination. The appearance frequency (AF) and transition frequency (TF) metrics were defined to quantify the dynamic properties of brain states. Patients during remission showed higher levels of rumination than HCs. Both groups exhibited four brain states during rumination, which consisted of complementary network group activation (states 1 and 2, states 3 and 4). In patients, the AFs of and reciprocal TFs between states 1 and 2 during rumination were significantly increased, while AFs of states 3 and 4 and reciprocal TFs involving states 1–3, 1–4, 2–3, and 2–4 were decreased, both when compared to HCs and relative to patients themselves during distraction. Moreover, we found that for patients, the AF of state 1 was negatively correlated with rumination levels and marginally positively associated with attention, while the AF of state 2 was negatively associated with performance on attention tasks. Our study revealed altered dynamic characteristics of brain states composed of network groups during rumination in remitted depression. Additionally, the findings suggest that heightened self-focus linked to rumination may impair the brain's ability to efficiently allocate attentional resources.}
}
@article{BALE2015150,
title = {Energy and complexity: New ways forward},
journal = {Applied Energy},
volume = {138},
pages = {150-159},
year = {2015},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2014.10.057},
url = {https://www.sciencedirect.com/science/article/pii/S0306261914011076},
author = {Catherine S.E. Bale and Liz Varga and Timothy J. Foxon},
keywords = {Complexity science, Energy systems, Modelling, Complex adaptive systems, Agent-based modelling, Energy policy},
abstract = {The purpose of this paper is to review the application of complexity science methods in understanding energy systems and system change. The challenge of moving to sustainable energy systems which provide secure, affordable and low-carbon energy services requires the application of methods which recognise the complexity of energy systems in relation to social, technological, economic and environmental aspects. Energy systems consist of many actors, interacting through networks, leading to emergent properties and adaptive and learning processes. Insights on these type of phenomena have been investigated in other contexts by complex systems theory. However, these insights are only recently beginning to be applied to understanding energy systems and systems transitions. The paper discusses the aspects of energy systems (in terms of technologies, ecosystems, users, institutions, business models) that lend themselves to the application of complexity science and its characteristics of emergence and coevolution. Complex-systems modelling differs from standard (e.g. economic) modelling and offers capabilities beyond those of conventional models, yet these methods are only beginning to realize anything like their full potential to address the most critical energy challenges. In particular there is significant potential for progress in understanding those challenges that reside at the interface of technology and behaviour. Some of the computational methods that are currently available are reviewed: agent-based and network modelling. The advantages and limitations of these modelling techniques are discussed. Finally, the paper considers the emerging themes of transport, energy behaviour and physical infrastructure systems in recent research from complex-systems energy modelling. Although complexity science is not well understood by practitioners in the energy domain (and is often difficult to communicate), models can be used to aid decision-making at multiple levels e.g. national and local, and to aid understanding and allow decision making. The techniques and tools of complexity science, therefore, offer a powerful means of understanding the complex decision-making processes that are needed to realise a low-carbon energy system. We conclude with recommendations for future areas of research and application.}
}
@article{KEE19891479,
title = {A computational model of the structure and extinction of strained, opposed flow, premixed methane-air flames},
journal = {Symposium (International) on Combustion},
volume = {22},
number = {1},
pages = {1479-1494},
year = {1989},
issn = {0082-0784},
doi = {https://doi.org/10.1016/S0082-0784(89)80158-4},
url = {https://www.sciencedirect.com/science/article/pii/S0082078489801584},
author = {Robert J. Kee and James A. Miller and Gregory H. Evans and Graham Dixon-Lewis},
abstract = {The application of laminar flamelet concepts to turbulent flame propagation requires a detailed understanding of strained laminar flames. Here we use numerical methods, including are-length continuation, to simulate the complex chemical kinetic behavior in premixed methane-air flames that are stabilized between two opposed-flow burners. We predict both the detailed structure and the extinction limits for these flames over a range of fuel-air mixtures. In addition to discussing the flame structure, a sensitivity analysis provides further insight on the chemical behavior near extinction. Finally, we discuss the comparison of the predictions with Law's experimental extinction data. An especially important aspect of this comparison is the recognition that fluid mechanical aspects of the traditional strained-flame analysis are deficient in representing experiments such as Law's. We develop and solve a new system of equations that is able to describe the experiments much more accurately.}
}
@article{DESAFERREIRA2013135,
title = {Promoting integrative medicine by computerization of traditional Chinese medicine for scientific research and clinical practice: The SuiteTCM Project},
journal = {Journal of Integrative Medicine},
volume = {11},
number = {2},
pages = {135-139},
year = {2013},
issn = {2095-4964},
doi = {https://doi.org/10.3736/jintegrmed2013013},
url = {https://www.sciencedirect.com/science/article/pii/S2095496414601096},
author = {Arthur {de Sá Ferreira}},
keywords = {traditional Chinese medicine, evidence-based practice, computer-assisted decision making},
abstract = {Background
Chinese and contemporary Western medical practices evolved on different cultures and historical contexts and, therefore, their medical knowledge represents this cultural divergence. Computerization of traditional Chinese medicine (TCM) is being used to promote the integrative medicine to manage, process and integrate the knowledge related to TCM anatomy, physiology, semiology, pathophysiology, and therapy.
Methods
We proposed the development of the SuiteTCM software, a collection of integrated computational models mainly derived from epidemiology and statistical sciences for computerization of Chinese medicine scientific research and clinical practice in all levels of prevention. The software includes components for data management (DataTCM), simulation of cases (SimTCM), analyses and validation of datasets (SciTCM), clinical examination and pattern differentiation (DiagTCM, TongueTCM, and PulseTCM), intervention selection (AcuTCM, HerbsTCM, and DietTCM), management of medical records (ProntTCM), epidemiologic investigation of sampled data (ResearchTCM), and medical education, training, and assessment (StudentTCM).
Discussion
The SuiteTCM project is expected to contribute to the ongoing development of integrative medicine and the applicability of TCM in worldwide scientific research and health care. The SuiteTCM 1.0 runs on Windows XP or later and is freely available for download as an executable application.}
}
@article{MEHREGAN2012426,
title = {An application of Soft System Methodology},
journal = {Procedia - Social and Behavioral Sciences},
volume = {41},
pages = {426-433},
year = {2012},
note = {The First International Conference on Leadership, Technology and Innovation Management},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2012.04.051},
url = {https://www.sciencedirect.com/science/article/pii/S1877042812009317},
author = {M. Reza Mehregan and Mahnaz Hosseinzadeh and Aliyeh Kazemi},
keywords = {Soft System Methodology (SSM), University course timetabling, rich picture, conceptual model},
abstract = {The typical course timetabling problem is assigning Classes of students to appropriate faculty members, suitable classrooms and available timeslots. Hence, it involves a large number of stakeholders including students, teachers and institutional administrators. Different kinds of Hard Operational Research techniques have been employed over the years to address such problems. Due to the computational difficulties of this NP complete problem as well as the size and the complexity of the real world instances, an efficient optimal solution cannot be found easily.As an alternative strategy, this paper investigates the application of Checkland‘s Soft System Methodology (SSM) to the course timetabling problem. Besides giving an ideal course timetable, even to large and complex real problems, application of SSM, generates debate, learning, and understanding; enables key changes; facilitates negotiating the actions to be taken and makes possible the meaningful collaboration among concerned stakeholders. This paper also provides an appropriate course timetable for the management faculty at University of Tehran to show the potential of this application to real problems.}
}
@article{GERO2001283,
title = {The differences between retrospective and concurrent protocols in revealing the process-oriented aspects of the design process},
journal = {Design Studies},
volume = {22},
number = {3},
pages = {283-295},
year = {2001},
issn = {0142-694X},
doi = {https://doi.org/10.1016/S0142-694X(00)00030-2},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X00000302},
author = {John S. Gero and Hsien-Hui Tang},
keywords = {design cognition, protocol studies, case study},
abstract = {This paper presents the results of studying a single designer using protocol analyses and examines the implications of the results on studies of design thinking. It contrasts two types of protocols: concurrent protocols and retrospective protocols. The results indicate that concurrent and retrospective protocols both produce very similar outcomes in terms of exploring the process-oriented aspects of designing. As a result, it is argued there is no associated interference with the ongoing design process when using concurrent protocols.}
}
@article{HURST2024105918,
title = {Continuous and discrete proportion elicit different cognitive strategies},
journal = {Cognition},
volume = {252},
pages = {105918},
year = {2024},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2024.105918},
url = {https://www.sciencedirect.com/science/article/pii/S001002772400204X},
author = {Michelle A. Hurst and Steven T. Piantadosi},
keywords = {Proportion, Strategy, Bayesian analysis, Model comparison},
abstract = {Despite proportional information being ubiquitous, there is not a standard account of proportional reasoning. Part of the difficulty is that there are several apparent contradictions: in some contexts, proportion is easy and privileged, while in others it is difficult and ignored. One possibility is that although we see similarities across tasks requiring proportional reasoning, people approach them with different strategies. We test this hypothesis by implementing strategies computationally and quantitatively comparing them with Bayesian tools, using data from continuous (e.g., pie chart) and discrete (e.g., dots) stimuli and preschoolers, 2nd and 5th graders, and adults. Overall, people's comparisons of highly regular and continuous proportion are better fit by proportion strategy models, but comparisons of discrete proportion are better fit by a numerator comparison model. These systematic differences in strategies suggest that there is not a single, simple explanation for behavior in terms of success or failure, but rather a variety of possible strategies that may be chosen in different contexts.}
}
@article{MEJIA20113964,
title = {On the complexity of sandpile critical avalanches},
journal = {Theoretical Computer Science},
volume = {412},
number = {30},
pages = {3964-3974},
year = {2011},
note = {Cellular Automata and Discrete Complex Systems},
issn = {0304-3975},
doi = {https://doi.org/10.1016/j.tcs.2011.02.029},
url = {https://www.sciencedirect.com/science/article/pii/S0304397511001496},
author = {Carolina Mejia and J. {Andres Montoya}},
keywords = {Abelian sandpile model, Self-organized criticality,  complete problems, Parallel algorithms},
abstract = {In this work, we study The Abelian Sandpile Model from the point of view of computational complexity. We begin by studying the length distribution of sandpile avalanches triggered by the addition of two critical configurations: we prove that those avalanches are long on average, their length is bounded below by a constant fraction of the length of the longest critical avalanche which is, in most of the cases, superlinear. At the end of the paper we take the point of view of computational complexity, we analyze the algorithmic hardness of the problem consisting in computing the addition of two critical configurations, we prove that this problem is P complete, and we prove that most algorithmic problems related to The Abelian Sandpile Model are NC reducible to it.}
}
@article{HOFFMAN202472,
title = {AI’s impact on war’s enduring nature},
journal = {Orbis},
volume = {68},
number = {1},
pages = {72-91},
year = {2024},
issn = {0030-4387},
doi = {https://doi.org/10.1016/j.orbis.2023.11.006},
url = {https://www.sciencedirect.com/science/article/pii/S0030438723000583},
author = {Frank Hoffman and Axel D'Amelio},
abstract = {This article reassesses the impact of Artificial Intelligence on war and revisits an article published in 2018 by one of the authors in Orbis. Despite the remarkable progress in generative AI, the authors contend that war’s essential nature will be impacted to a degree but will not be substantially altered.}
}
@article{MERIVAARA2021480,
title = {Preservation of biomaterials and cells by freeze-drying: Change of paradigm},
journal = {Journal of Controlled Release},
volume = {336},
pages = {480-498},
year = {2021},
issn = {0168-3659},
doi = {https://doi.org/10.1016/j.jconrel.2021.06.042},
url = {https://www.sciencedirect.com/science/article/pii/S0168365921003400},
author = {Arto Merivaara and Jacopo Zini and Elle Koivunotko and Sami Valkonen and Ossi Korhonen and Francisco M. Fernandes and Marjo Yliperttula},
keywords = {Freeze-drying, Cells, Extracellular vesicles, Process analytical technology, Quality-by-design, Biophotonics},
abstract = {Freeze-drying is the most widespread method to preserve protein drugs and vaccines in a dry form facilitating their storage and transportation without the laborious and expensive cold chain. Extending this method for the preservation of natural biomaterials and cells in a dry form would provide similar benefits, but most results in the domain are still below expectations. In this review, rather than consider freeze-drying as a traditional black box we “break it” through a detailed process thinking approach. We discuss freeze-drying from process thinking aspects, introduce the chemical, physical, and mechanical environments important in this process, and present advanced biophotonic process analytical technology. In the end, we review the state of the art in the freeze-drying of the biomaterials, extracellular vesicles, and cells. We suggest that the rational design of the experiment and implementation of advanced biophotonic tools are required to successfully preserve the natural biomaterials and cells by freeze-drying. We discuss this change of paradigm with existing literature and elaborate on our perspective based on our new unpublished results.}
}
@article{PIHLAJAMAKI2020101103,
title = {Subjective cognitive complaints and sickness absence: A prospective cohort study of 7059 employees in primarily knowledge-intensive occupations},
journal = {Preventive Medicine Reports},
volume = {19},
pages = {101103},
year = {2020},
issn = {2211-3355},
doi = {https://doi.org/10.1016/j.pmedr.2020.101103},
url = {https://www.sciencedirect.com/science/article/pii/S2211335520300632},
author = {Minna Pihlajamäki and Heikki Arola and Heini Ahveninen and Jyrki Ollikainen and Mikko Korhonen and Tapio Nummi and Jukka Uitti and Simo Taimela},
keywords = {Subjective cognitive complaints, Screening questionnaire, Occupational healthcare, Self-reported data, Sickness allowance, Register data},
abstract = {Knowledge-intensive work requires capabilities like monitoring multiple sources of information, prioritizing between competing tasks, switching between tasks, and resisting distraction from the primary task(s). We assessed whether subjective cognitive complaints (SCC), presenting as self-rated problems with difficulties of concentration, memory, clear thinking and decision making predict sickness absence (SA) in knowledge-intensive occupations. We combined SCC questionnaire results with reliable registry data on SA of 7743 professional/managerial employees (47% female). We excluded employees who were not active in working life, on long-term SA, and those on a work disability benefit at baseline. The exposure variable was the presence of SCC. Age and SA before the questionnaire as a proxy measure of general health were treated as confounders and the analyses were conducted by gender. The outcome measure was the accumulated SA days during a 12-month follow-up. We used a hurdle model to analyse the SA data. SCC predicted the number of SA days during the 12-month follow-up. The ratio of the means of SA days was higher than 2.8 as compared to the reference group, irrespective of gender, with the lowest limit of 95% confidence interval 2.2. In the Hurdle model, SCC, SA days prior to the questionnaire, and age were additive predictors of the likelihood of SA and accumulated SA days, if any. Subjective cognitive complaints predict sickness absence in knowledge-intensive occupations, irrespective of gender, age, or general health. This finding has implications for supporting work ability (productivity) among employees with cognitively demanding tasks.}
}
@incollection{MAXWELL19961,
title = {Driving forces for innovation in applied catalysis},
editor = {Joe W. Hightower and W. {Nicholas Delgass} and Enrique Iglesia and Alexis T. Bell},
series = {Studies in Surface Science and Catalysis},
publisher = {Elsevier},
volume = {101},
pages = {1-9},
year = {1996},
booktitle = {11th International Congress On Catalysis - 40th Anniversary},
issn = {0167-2991},
doi = {https://doi.org/10.1016/S0167-2991(96)80210-2},
url = {https://www.sciencedirect.com/science/article/pii/S0167299196802102},
author = {Ian E. Maxwell},
abstract = {Publisher Summary
Catalytic environmental technologies such as automobile exhaust catalysts and the selective catalytic reduction (SCR) DeNOx systems in power plants have significantly contributed to the reduction of environmentally harmful emissions into the lower atmosphere. Some studies have identified catalysis as not only being pervasive but also offering significant scope for the innovative development of new and improved technologies for environmentally acceptable processes and products in the future. The spectrum of process industries that are directly impacted by catalysis include, for example, oil refining, natural gas conversion, petrochemicals, fine chemicals, and pharmaceuticals. Environmental catalytic technologies also play an important role in emission control systems for power generation, fossil fuel driven transportation, oil refining, and chemical industries. Catalytic technologies typically embrace a wide range of disciplines, such as heterogeneous and homogeneous catalysis, materials science, process technology, reactor engineering, separation technology, surface science, computational chemistry, and analytic chemistry. Innovation in this field is, therefore, often achieved by lateral thinking across these different disciplines. This chapter attempts to develop this theme by means of examples from recent commercial successes and from this platform provides some guidelines for multi-disciplinary approaches at the academic and industrial interface to enhanced innovation in catalytic technologies in the future.}
}
@article{FESSAKIS201387,
title = {Problem solving by 5–6 years old kindergarten children in a computer programming environment: A case study},
journal = {Computers & Education},
volume = {63},
pages = {87-97},
year = {2013},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2012.11.016},
url = {https://www.sciencedirect.com/science/article/pii/S0360131512002813},
author = {G. Fessakis and E. Gouli and E. Mavroudi},
keywords = {Programming and programming languages, Kindergarten, Improving classroom teaching, Teaching/learning strategies},
abstract = {Computer programming is considered an important competence for the development of higher-order thinking in addition to algorithmic problem solving skills. Its horizontal integration throughout all educational levels is considered worthwhile and attracts the attention of researchers. Towards this direction, an exploratory case study is presented concerning dimensions of problem solving using computer programming by 5–6 years old kindergarten children. After a short introductory experiential game the children were involved in solving a series of analogous computer programming problems, using a Logo-based environment on an Interactive White Board. The intervention was designed as a part of the structured learning activities of the kindergarten which are teacher-guided and are conducted in a whole-class social mode. The observation of the video recording of the intervention along with the analysis of teacher's interview and the researcher's notes allow for a realistic evaluation of the feasibility, the appropriateness and the learning value of integrating computer programming in such a context. The research evidence supports the view that children enjoyed the engaging learning activities and had opportunities to develop mathematical concepts, problem solving and social skills. Interesting results about children learning, difficulties, interactions, problem solving strategies and the teacher's role are reported. The study also provides proposals for the design of future research.}
}
@article{KARUNATHILAKE2019558,
title = {Renewable energy selection for net-zero energy communities: Life cycle based decision making under uncertainty},
journal = {Renewable Energy},
volume = {130},
pages = {558-573},
year = {2019},
issn = {0960-1481},
doi = {https://doi.org/10.1016/j.renene.2018.06.086},
url = {https://www.sciencedirect.com/science/article/pii/S0960148118307389},
author = {Hirushie Karunathilake and Kasun Hewage and Walter Mérida and Rehan Sadiq},
keywords = {Multi-criteria decision making, Life cycle thinking, Fuzzy techniques, Renewable energy, Community energy system planning},
abstract = {Developing net-zero energy communities powered by renewable energy (RE) resources has become a popular concept. To make the best choices for community-level net-zero energy systems, it is necessary to identify the best energy technologies at local level. Evaluation of RE technologies has to be extended from technical and economic aspects to include environmental and social wellbeing. It is possible to identify the true costs and benefits of energy use by taking a cradle-to-grave life cycle perspective. In this study, a RE screening and multi-stage energy selection framework was developed. A fuzzy multi-criteria decision making approach was used in ranking the technologies to incorporate the conflicting requirements, stakeholder priorities, and uncertainties. Different scenarios were investigated to reflect different decision maker priorities. Under a pro-environment scenario, small hydro, onshore wind, and biomass combustion technologies perform best. Under a pro-economic decision scenario, biomass combustion, small hydro, and landfill gas have the best rankings. Triple bottom line sustainability was combined with technical feasibility through a ruled-based approach to avoid the theoretical pitfalls inherent in energy-related decision making. This assessment is geared towards providing decision makers with flexible tools, and is expected to aid in the pre-project planning stage of RE projects.}
}
@article{LUO2011384,
title = {Application of Improved EAHP on Stability Evaluation of Coal Seam Roof},
journal = {Procedia Earth and Planetary Science},
volume = {3},
pages = {384-393},
year = {2011},
note = {2011 Xi'an International Conference on Fine Exploration and Control of Water & Gas in Coal Mines},
issn = {1878-5220},
doi = {https://doi.org/10.1016/j.proeps.2011.09.110},
url = {https://www.sciencedirect.com/science/article/pii/S1878522011001111},
author = {Donghai Luo and Shunxin Sun and Dunhu Zhang and Yuqing Wan and Guangchao Zhang and Junqiang Niu},
keywords = {Coal Seam Roof, Stability Evaluation, EAHP, Model},
abstract = {Stability of coal seam roof is one of the important factors to ensure safe and efficient coal production. Stability result is the complex interaction subjected to a larger number of geological factors. Only taking comprehensive consideration into evaluation can the result be in line with the actual complex geological environment. Main factors of coal seam roof stability are divided into four major factors and eight secondary factors. Major factors are sedimentary environment, structural feature, rock mechanics property and so on. Secondary factors are the combination of roof rock, lithology difference, bedding changes, and so on. Stability rank is divided into four grades: super stability, stability, basically stability and instability. EAHP model of stability evaluation of coal seam roof and the extension comparison matrix are established by means of the improved EAHP (Extension Analytical Hierarchy Process) method. Using the method based on judgment by possibility degree matrix can get the Sorting order. Evaluation results show that: the stability grade of main coal seam 5# roof of the mine is stable. It is true and credible. The method not only has the merits of “Extension to consider fuzziness of human thinking to judge”, but also eliminates a lot of spreadsheet work in traditional AHP. These studies are useful experiment and explore to study on comprehensive evaluation of coal seam roof stability.}
}
@article{DIRKSEN2022102994,
title = {From agent to action: The use of ethnographic social simulation for crime research},
journal = {Futures},
volume = {142},
pages = {102994},
year = {2022},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2022.102994},
url = {https://www.sciencedirect.com/science/article/pii/S0016328722000945},
author = {Vanessa Dirksen and Martin Neumann and Ulf Lotzmann},
keywords = {Agent-based modelling, Complementarity, Computational social science, Simulation, Ethnography, Policing},
abstract = {This paper proposes a methodology for grounding agent-based social simulation in ethnographic data, using the example of crime research. The application of computational tools in crime research typically entails a removal of the “intelligible frame” of criminal behaviour and, hence, of meaningful evidence. Ethnography is a microscopic research tradition geared towards the preservation of contextualized meaning deemed essential for the exploration of the variety of prospective alternative scenarios and, hence, of plausible futures. On the basis of exemplary empirical material from a qualitative study on the transit trade of cocaine in the Netherlands, this paper looks into the complementarity and potential integration of the research traditions of ethnography and agent-based modelling. That is to say, it explores the compatibility of the formal languages of both these domains and the mutual benefit of “stitching together” these at first sight very different methods. The ethnographic approach to social simulation specifies the what-if relations of traditional/conventional ABM modelling into condition-action sequences. As we contend, it is exactly this more microscopic level of condition-action sequences that is needed to facilitate ”thick description” and, in turn, enable the grounding of ABM in meaningful evidence.}
}
@article{FALL2010140,
title = {Artificial states? On the enduring geographical myth of natural borders},
journal = {Political Geography},
volume = {29},
number = {3},
pages = {140-147},
year = {2010},
issn = {0962-6298},
doi = {https://doi.org/10.1016/j.polgeo.2010.02.007},
url = {https://www.sciencedirect.com/science/article/pii/S0962629810000533},
author = {Juliet J. Fall},
keywords = {Artificial states, Boundaries, Ethnic homogeneity, Failed states, Nationalism, Natural boundaries, Territorial trap},
abstract = {Alberto Alesina, William Easterly and Janina Matuszeski's paper Artificial States, published as a National Bureau of Economic Research Working Paper in June 2006, suggests a theory linking the nature of country borders to the economic success of countries (Alesina, Easterly, & Matuszeski, 2006). This paper critically examines this suggestion that natural boundaries and ethnic homogeneity are desirable for economic reasons. It takes issue with the understanding of artificial and natural boundaries that they develop, arguing that this ignores two centuries of critical and quantitative geographical scholarship that has mapped, documented and critiqued the obsession of a link between topography and the appropriate shape of states and boundaries. It explores how their argument is linked to a defence of ethnically homogeneous states. The focus is on their teleological and paradoxically ahistorical vision that naturalizes politics by appealing to spatial myths of homogeneity and geometric destiny, grounded in a reactionary understanding of space as container. In so doing, I am mindful of the strong links between such proposals and calls for post-conflict partition, and the corresponding discourses of ethnic and cultural homogenization on which they rely. Instead of thinking of boundaries as geometric objects, squiggly or not, I consider boundaries through the simultaneous processes of reification, naturalization, and fetishization.}
}
@article{WARING2015254,
title = {Managerial and non-technical factors in the development of human-created disasters: A review and research agenda},
journal = {Safety Science},
volume = {79},
pages = {254-267},
year = {2015},
issn = {0925-7535},
doi = {https://doi.org/10.1016/j.ssci.2015.06.015},
url = {https://www.sciencedirect.com/science/article/pii/S0925753515001575},
author = {Alan Waring},
keywords = {Major hazards, Disasters, Safety management, Safety culture, Risk decisions},
abstract = {A number of common underlying factors in the development of human-created disasters, as cited in numerous official inquiry reports, encompass in particular, safety management system defects and weaknesses in an organization’s safety culture. Human factors such as faulty risk cognition, bounded rationality, groupthink, failure of foresight and organizational learning, suspect motivations, reactive attitudes, and inappropriate risk decision-making, are commonly associated characteristics of such shortcomings. This article summarizes and discusses underlying managerial and non-technical factors in human-created major hazard accidents in the light of theories of accident causation, findings from disaster inquiries and published research, and the systemic holism-versus-reductionism debate. Ideally, all site operators would know and understand disaster aetiology and preventive requirements and be motivated to enact them. However, there is sufficient empirical evidence from inquiry reports into major hazard incidents and disasters that idealized enactment rarely occurs and in many cases safety policy and strategy as enacted is distant from espoused safety policy and strategy. Research questions relating to board level thinking and actions on major hazard risks are posited and a proposal for a more holistic and potentially more effective major hazard safety research framework is put forward.}
}
@article{RONI2022100796,
title = {Integrated water-power system resiliency quantification, challenge and opportunity},
journal = {Energy Strategy Reviews},
volume = {39},
pages = {100796},
year = {2022},
issn = {2211-467X},
doi = {https://doi.org/10.1016/j.esr.2021.100796},
url = {https://www.sciencedirect.com/science/article/pii/S2211467X21001796},
author = {Mohammad S. Roni and Thomas Mosier and Tzvi D. Feinberg and Timothy McJunkin and Ange-Lionel Toba and Liam D. Boire and Luis Rodriguez-Garcia and Majid Majidi and Masood Parvania},
keywords = {Resiliency, Irrigation, Integrated water-power system, Optimization},
abstract = {Resiliency has been studied in the power and water systems separately. Often the resiliency study is not so comprehensive as to understand interdependent, integrated water and power systems. This research outlines the relevant factors necessary to understand and advance quantification of such integrated systems. It also presents a review of integrated water-power systems resiliency. Based on literature survey and identification of challenges, the authors present quantification and computational steps needed to understand integrated water-power systems resiliency. A conceptual framework is proposed to quantify integrated water-power system resiliency. Finally, the authors presented an opportunity for improved water and power system resilience.}
}
@incollection{REYESGARCIA2025151,
title = {Chapter 7 - Decoding imagined speech for EEG-based BCI},
editor = {Ayman S. El-Baz and Jasjit S. Suri},
booktitle = {Brain-Computer Interfaces},
publisher = {Academic Press},
pages = {151-175},
year = {2025},
series = {Advances in Neural Engineering},
isbn = {978-0-323-95439-6},
doi = {https://doi.org/10.1016/B978-0-323-95439-6.00004-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780323954396000041},
author = {Carlos A. Reyes-García and Alejandro A. Torres-García and Tonatiuh Hernández-del-Toro and Jesús S. García-Salinas and Luis Villaseñor-Pineda},
keywords = {Brain–computer interfaces (BCI), Classification, Electroencephalograms (EEG), Imagined speech, Silent speech interfaces (SSI)},
abstract = {Brain–computer interfaces (BCIs) are systems that transform the brain's electrical activity into commands to control a device. To create a BCI, it is necessary to establish the relationship between a certain stimulus, internal or external, and the brain activity it provokes. A common approach in BCIs is motor imagery, which involves imagining limb movement. Unfortunately, this approach allows few commands. As an alternative, this chapter presents another approach, an internal language-related stimulus known as imagined speech, which is the action of imagining the diction of a word without emitting any sound or articulating any movement. This neuroparadigm is more intuitive, less subjective, and ambiguous, which are very relevant advantages; however, the cost to properly process the brain signal is not trivial. This chapter describes the main components of an EEG-based imagined speech BCI, along with key works, emerging trends, and challenges in this research area. Regarding the challenges, we present four of them in the pursuit of decoding imagined speech. The first challenge involves accurately recognizing isolated words. The second one is the automatic selection of a subset of EEG channels aiming to reduce computational cost and provide evidence of promising locations for studying imagined speech. The third challenge introduces an innovative approach to addressing scenarios where a new word needs to be added to the vocabulary after the computational model has been trained. Lastly, the fourth challenge concerns the online recognition of words from continuous EEG signals. Despite advances in the area, there is still much work to be done. Important initial steps have been taken in terms of the application of novel techniques for preprocessing, artifact removal, feature extraction, and classification which are the stages to be taken to process the collected signal. Additionally, the community has shared datasets and organized evaluation forums to accelerate the search for solutions.}
}
@article{JAYAWARDENA2025118097,
title = {Marine specialized metabolites: Unveiling Nature's chemical treasures from the deep blue},
journal = {TrAC Trends in Analytical Chemistry},
volume = {183},
pages = {118097},
year = {2025},
issn = {0165-9936},
doi = {https://doi.org/10.1016/j.trac.2024.118097},
url = {https://www.sciencedirect.com/science/article/pii/S0165993624005806},
author = {Thilina U. Jayawardena and Natacha Merindol and Nuwan Sameera Liyanage and Fatima Awwad and Isabel Desgagné-Penix},
keywords = {Natural products, Conservation, Bioprospecting, metabolomics, Biotechnology, Biosynthesis, Isolation and spectroscopic characterization},
abstract = {Marine specialized metabolites (MSM) represent a fascinating realm of chemical diversity with multifaceted functions across the spectrum of life on Earth. These metabolites serve as weapons, metal transporters, regulatory agents, and more. The conservation of genes responsible for their production over extensive evolutionary timescales underscores their selective advantage. Recent decades have witnessed an upsurge in MSM studies, driven by advancements in analytical techniques and the ever-growing accessibility of the aquatic environment. Marine macro and microorganisms offer a rich tapestry of specialized metabolites, some exhibiting potent activities in diverse domains, including medicine. The study of MSM presents several challenges, reflecting the need to separate complex mixtures into individual bioactive metabolites and utilize state-of-the-art extraction methods. Comprehensive structural analysis relies on advanced spectroscopic approaches, including nuclear magnetic resonance and mass spectrometry. These tools are instrumental in unravelling the chemical diversity of MSM and understanding their potential applications. While bioprospecting offers enormous potential, it raises critical challenges concerning sustainability, conservation, and equitable benefit-sharing. International protocols like the Nagoya Protocol seeks to regulate access to and share benefits from genetic resources, with considerable implications for marine bioprospecting. The convergence of advanced metabolomics, metagenomics, and synthetic biology offers promising avenues for accelerating the discovery and sustainable production of MSM, shaping the future of this field. This comprehensive review provides a deep dive into the challenges, methodologies, and emerging trends in studying marine-derived natural products, underscoring the immense potential of MSM for advancing chemical sciences and their transformative applications in diverse areas such as food, medicine, biotechnology, and environmental conservation. By bridging multiple disciplines, the continued exploration and sustainable utilization of these metabolites hold the promise of unlocking new innovations for society's benefit.}
}
@article{KEAN2025109125,
title = {Intuitive physical reasoning is not mediated by linguistic nor exclusively domain-general abstract representations},
journal = {Neuropsychologia},
volume = {213},
pages = {109125},
year = {2025},
issn = {0028-3932},
doi = {https://doi.org/10.1016/j.neuropsychologia.2025.109125},
url = {https://www.sciencedirect.com/science/article/pii/S0028393225000600},
author = {Hope H. Kean and Alexander Fung and R.T. Pramod and Jessica Chomik-Morales and Nancy Kanwisher and Evelina Fedorenko},
abstract = {The ability to reason about the physical world is a critical tool in the human cognitive toolbox, but the nature of the representations that mediate physical reasoning remains debated. Here, we use fMRI to illuminate this question by investigating the relationship between the physical-reasoning system and two well-characterized systems: a) the domain-general Multiple Demand (MD) system, which supports abstract reasoning, including mathematical and logical reasoning, and b) the language system, which supports linguistic computations and has been hypothesized to mediate some forms of thought. We replicate prior findings of a network of frontal and parietal areas that are robustly engaged by physical reasoning and identify an additional physical-reasoning area in the left frontal cortex, which also houses components of the MD and language systems. Critically, direct comparisons with tasks that target the MD and the language systems reveal that the physical-reasoning system overlaps with the MD system, but is dissociable from it in fine-grained activation patterns, which replicates prior work. Moreover, the physical-reasoning system does not overlap with the language system. These results suggest that physical reasoning does not rely on linguistic representations, nor exclusively on the domain-general abstract reasoning that the MD system supports.}
}
@article{WALHA20252959,
title = {Deep Learning and Machine Learning Architectures for Dementia Detection from Speech in Women},
journal = {CMES - Computer Modeling in Engineering and Sciences},
volume = {142},
number = {3},
pages = {2959-3001},
year = {2025},
issn = {1526-1492},
doi = {https://doi.org/10.32604/cmes.2025.060545},
url = {https://www.sciencedirect.com/science/article/pii/S1526149225000396},
author = {Ahlem Walha and Amel Ksibi and Mohammed Zakariah and Manel Ayadi and Tagrid Alshalali and Oumaima Saidani and Leila Jamel and Nouf Abdullah Almujally},
keywords = {Dementia detection in women, Alzheimer’s disease, deep learning, machine learning, support vector machine, voting classifier},
abstract = {Dementia is a neurological disorder that affects the brain and its functioning, and women experience its effects more than men do. Preventive care often requires non-invasive and rapid tests, yet conventional diagnostic techniques are time-consuming and invasive. One of the most effective ways to diagnose dementia is by analyzing a patient’s speech, which is cheap and does not require surgery. This research aims to determine the effectiveness of deep learning (DL) and machine learning (ML) structures in diagnosing dementia based on women’s speech patterns. The study analyzes data drawn from the Pitt Corpus, which contains 298 dementia files and 238 control files from the Dementia Bank database. Deep learning models and SVM classifiers were used to analyze the available audio samples in the dataset. Our methodology used two methods: a DL-ML model and a single DL model for the classification of diabetics and a single DL model. The deep learning model achieved an astronomic level of accuracy of 99.99% with an F1 score of 0.9998, Precision of 0.9997, and recall of 0.9998. The proposed DL-ML fusion model was equally impressive, with an accuracy of 99.99%, F1 score of 0.9995, Precision of 0.9998, and recall of 0.9997. Also, the study reveals how to apply deep learning and machine learning models for dementia detection from speech with high accuracy and low computational complexity. This research work, therefore, concludes by showing the possibility of using speech-based dementia detection as a possibly helpful early diagnosis mode. For even further enhanced model performance and better generalization, future studies may explore real-time applications and the inclusion of other components of speech.}
}
@article{HUDLICKA201498,
title = {Affective BICA: Challenges and open questions},
journal = {Biologically Inspired Cognitive Architectures},
volume = {7},
pages = {98-125},
year = {2014},
issn = {2212-683X},
doi = {https://doi.org/10.1016/j.bica.2013.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S2212683X13000947},
author = {Eva Hudlicka},
keywords = {Emotion modeling, Emotion theories, Cognitive–affective architectures},
abstract = {In spite of the progress in emotion research over the past 20years, emotions remain an elusive phenomenon. While some underlying circuitry has been identified for some aspects of affective processing (e.g., amygdala-mediated processing of threatening stimuli, the role of orbitofrontal cortex in emotion regulation), much remains unknown about the mechanisms of emotions. Computational models of cognitive and affective processes provide a unique and powerful means of refining psychological theories, and can help elucidate the mechanisms that mediate affective phenomena. This paper outlines a number of open questions and challenges associated with developing computational models of emotion, and with their integration within biologically-inspired cognitive architectures. These include the following: the extent to which mechanisms in biological affective agents should be simulated or emulated in affective BICAs; importance of more precise, design-based terminology; identification of fundamental affective processes, and the computational tasks necessary for their implementation; improved understanding of affective dynamics and development of more accurate models of these phenomena; and understanding the alternative means of integrating emotions within agent architectures. The challenges associated with data availability and model validation are also discussed.}
}
@article{PASMAN201880,
title = {How can we improve process hazard identification? What can accident investigation methods contribute and what other recent developments? A brief historical survey and a sketch of how to advance},
journal = {Journal of Loss Prevention in the Process Industries},
volume = {55},
pages = {80-106},
year = {2018},
issn = {0950-4230},
doi = {https://doi.org/10.1016/j.jlp.2018.05.018},
url = {https://www.sciencedirect.com/science/article/pii/S0950423018300329},
author = {Hans J. Pasman and William J. Rogers and M. Sam Mannan},
keywords = {Accident-incident investigation, Hazard identification, Causation, System approach},
abstract = {Risk assessment is essential for various purposes such as facility siting, safeguarding, and licensing. Hazard identification (HAZID), which suffers greatly from incompleteness, is still the weakest link in risk assessment. Of course, this recognition is not new and many efforts have been spent to improve the situation, of which some have been rather successful. To find out what can go wrong, creative divergent thinking is required. Hazard identification should result in scenario definition. In that respect, applying the present tools as HAZOP and FMEA there is still a great emphasis on the material and equipment aspects. In contrast, underlying management and leadership failure in its many forms reflecting in organizational and human failure, due to complexity, attracts much less attention. Unlike in HAZID, in accident investigation the occurrence of an event with nasty consequences is no doubt a fact, so there must be one or more causes and the traces will lead to them. Over the years, methods for accident and incident investigation have gone through a significant evolution. From the early-on simplistic domino stone model and the human operator always at fault, via models of latent failure due to failing management involvement and via extensive root cause analysis (RCA) to a system approach. Hence, in accident investigation, management failure appearing in the many possible forms of human and organizational factors, obtained already 30 years ago with the RCA technique much attention, while it nowadays culminates in the socio-technical system approach. So, the question arises whether for improved HAZID we can learn from the accident investigation experience. In addition, safer design and advances from static risk assessment towards more accurate predictive operational dynamic risk assessment and management, will also be enabled by possibilities offered by big data and analytics. Digitization, automation and simulation, hence computerization, will be of great help in improving the identification of hazards and tracing the corresponding scenarios. The paper reviews the developmental history of both accident investigation and hazard identification methodology; incidentally it will identify commonality and differences. On the basis of the comparison and of recent advances in computerization, the paper will investigate to what extent beneficial modifications and additions can be made to obtain a higher degree of completeness in HAZID.}
}
@article{ABDULLAH2024100212,
title = {Recent development of combined heat transfer performance for engine systems: A comprehensive review},
journal = {Results in Surfaces and Interfaces},
volume = {15},
pages = {100212},
year = {2024},
issn = {2666-8459},
doi = {https://doi.org/10.1016/j.rsurfi.2024.100212},
url = {https://www.sciencedirect.com/science/article/pii/S2666845924000321},
author = {Md. Abdullah and Mohammad {Zoynal Abedin}},
keywords = {Combined heat transfer, Performance, Engine system, Enhancement},
abstract = {Heat transfer regulation between engine components has a direct impact on engine efficiency and performance. Improving engine system efficiency, reducing emissions, and prolonging component life all depend on efficient heat management. This review looks at recent advancements in integrated heat transfer optimization to boost efficiency, reduce emissions, and improve engine system performance. This effort also investigates producing intricate heat transfer components with improved geometry using additive manufacturing techniques. With additive printing, designers may more easily construct complex structures and optimize heat transfer surfaces for better performance. The development of nanofluids and nanocoatings now allows for improving heat transfer qualities. Nanotechnology advancements have made this possible, and nanostructured materials' enhanced surface properties as well as thermal conductivity contribute to better heat dissipation in engine systems. The modern automotive and aerospace sectors have high standards, which are met through novel designs, materials, computational tools, and integrated cooling systems. Additionally, these improvements pave the way for more efficient and environmentally friendly engine operation. Because of the integration of computational technologies like numerical modeling and CFD, engineers can now see complex heat flow patterns and construct more efficient cooling solutions. Additive manufacturing has changed component manufacturing by enabling sophisticated designs that maximize heat transfer surfaces.}
}
@article{XIONG2023105811,
title = {Neural vortex method: From finite Lagrangian particles to infinite dimensional Eulerian dynamics},
journal = {Computers & Fluids},
volume = {258},
pages = {105811},
year = {2023},
issn = {0045-7930},
doi = {https://doi.org/10.1016/j.compfluid.2023.105811},
url = {https://www.sciencedirect.com/science/article/pii/S0045793023000361},
author = {Shiying Xiong and Xingzhe He and Yunjin Tong and Yitong Deng and Bo Zhu},
keywords = {Vortex method, Neural network, Lagrangian dynamics, Eulerian dynamics},
abstract = {In fluid analysis, there has been a long-standing problem: lacking a rigorous mathematical tool to map from a continuous flow field to finite discrete particles, hurdling the Lagrangian particles from inheriting the high resolution of a large-scale Eulerian solver. To tackle this challenge, we propose a novel learning-based framework, the neural vortex method (NVM). NVM builds a neural-network description of the Lagrangian vortex structures and their interaction dynamics to reconstruct the high-resolution Eulerian flow field in a physically-precise manner. The key components of our infrastructure consist of two networks: a vortex detection network to identify the Lagrangian vortices from a grid-based velocity field and a vortex dynamics network to learn the underlying governing interactions of these finite structures. By embedding these two networks with a vorticity-to-velocity Poisson solver and training its parameters using the fluid data obtained from grid-based numerical simulation, we can predict the accurate fluid dynamics on a precision level that was infeasible for all the previous conventional vortex methods. We demonstrate the efficacy of our method in generating highly accurate prediction results with low computational cost by predicting the evolution of the leapfrogging vortex rings system, the turbulence system, and the systems governed by Navier–Stokes (NS) equations with different external forces. We compare the prediction results made by NVM and the Lagrangian vortex method (LVM) for solving the NS equation in the periodic box and find that the relative error of the predicted velocity using NVM is more than 10 times lower than that of the LVM. Moreover, our method only requires data collected from a very short training window, more than 100 times smaller than the prediction period, which potentially facilitates data acquisition in real systems.}
}
@article{FIELDS2023104927,
title = {Regulative development as a model for origin of life and artificial life studies},
journal = {Biosystems},
volume = {229},
pages = {104927},
year = {2023},
issn = {0303-2647},
doi = {https://doi.org/10.1016/j.biosystems.2023.104927},
url = {https://www.sciencedirect.com/science/article/pii/S0303264723001028},
author = {Chris Fields and Michael Levin},
keywords = {Free energy principle, Kinematic replication, Learning, Multicellularity, Multiscale competency architecture, Target morphology},
abstract = {Using the formal framework of the Free Energy Principle, we show how generic thermodynamic requirements on bidirectional information exchange between a system and its environment can generate complexity. This leads to the emergence of hierarchical computational architectures in systems that operate sufficiently far from thermal equilibrium. In this setting, the environment of any system increases its ability to predict system behavior by “engineering” the system towards increased morphological complexity and hence larger-scale, more macroscopic behaviors. When seen in this light, regulative development becomes an environmentally-driven process in which “parts” are assembled to produce a system with predictable behavior. We suggest on this basis that life is thermodynamically favorable and that, when designing artificial living systems, human engineers are acting like a generic “environment”.}
}
@incollection{RAPAPORT1994225,
title = {CHAPTER 10 - Syntactic Semantics: Foundations of Computational Natural-Language Understanding},
editor = {Eric Dietrich},
booktitle = {Thinking Computers and Virtual Persons},
publisher = {Academic Press},
pages = {225-273},
year = {1994},
isbn = {978-0-12-215495-9},
doi = {https://doi.org/10.1016/B978-0-12-215495-9.50015-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780122154959500156},
author = {William J. Rapaport},
abstract = {Publisher Summary
This chapter discusses the way by which it is possible to understand natural language and whether a computer could do so. It presents the argument that although a certain kind of semantic interpretation is needed for understanding natural language, it is a kind that only involves syntactic symbol manipulation of precisely the sort of which computers are capable, so that it is possible, in principle, for computers to understand natural language. The chapter highlights the recent arguments by John R. Searle and by Fred Dretske to the effect that computers cannot understand natural language. A program is like the script of a play. The computer is like the actors, sets, and a process is like an actual production of the play—the play in the process of being performed. The computer must be able to do things like: to convince someone, to imitate a human, that is, it must not merely be a cognitive agent, but also an acting one. In particular, to imitate a human, the computer needs to be able to reason about what another cognitive agent, such as a human, believes.}
}
@article{HONG2025134554,
title = {Energy-saving optimal control of secondary district cooling system based on tribal intelligent evolution optimization algorithm},
journal = {Energy},
volume = {316},
pages = {134554},
year = {2025},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2025.134554},
url = {https://www.sciencedirect.com/science/article/pii/S0360544225001963},
author = {Xiaoxi Hong and Ye Yao and Kui Wang and Jianzhong Yang and Qimei Liu},
keywords = {Secondary district cooling system, Energy-saving optimization control, Energy consumption calculation model, Tribal intelligent evolution optimization algorithm},
abstract = {With the significant increase in energy consumption for large central air conditioning systems, optimal control of district cooling systems is crucial for energy conservation and CO2 emission reduction. This study focuses on the energy-saving optimal control of secondary district cooling systems (SDCSs). Firstly, energy models for SDCSs utilizing distribution manifolds or plate heat exchangers are presented, with the goal of establishing global optimal control models for energy conservation. Next, a novel metaheuristic algorithm called Tribal Intelligent Evolution Optimization (TIEO) is proposed, which innovatively introduces human intelligent behavioral characteristics into the tribal evolution process. The TIEO and seven other optimization algorithms have been tested for optimizing the SDCSs. The test results demonstrated that the TIEO surpassed other algorithms in terms of optimization effectiveness, stability, and computational efficiency. Additionally, this study has conducted engineering validation of the TIEO algorithm on the SDCSs with distribution manifolds or plate heat exchangers. Compared to the traditional control strategies, the TIEO algorithm improved the energy efficiency ratio of the system by 13.56 % and 11.40 %, respectively. Therefore, the TIEO algorithm has the potential to serve as an optimization tool for contributing to energy conservation and promoting the sustainable development of large-scale district cooling systems.}
}
@article{GONG2025113594,
title = {A survey of Video Action Recognition based on Deep Learning},
journal = {Knowledge-Based Systems},
pages = {113594},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.113594},
url = {https://www.sciencedirect.com/science/article/pii/S0950705125006409},
author = {Ping Gong and Xudong Luo},
keywords = {Video action recognition, Deep learning, Multi-modal learning, AI-powered human behaviour analysis, Action recognition benchmark dataset},
abstract = {Video Action Recognition (VAR) involves identifying and classifying human actions from video data. Deep Learning (DL) has revolutionised VAR, significantly enhancing its accuracy and efficiency. However, large-scale practical applications of VAR using DL remain limited, underscoring the need for further research and innovation. Thus, this survey provides a comprehensive overview of recent advancements in DL-based VAR. Specifically, we summarise the key DL architectures for VAR, including two-stream networks, 3D-CNNs, RNNs, LSTMs, and Attention Mechanisms, and analyse their strengths, limitations, and benchmark performances. The survey also explores the diverse applications of DL-based VAR, such as surveillance, human–computer interaction, sports analytics, healthcare, and education, while presenting a detailed summary of commonly used datasets and evaluation metrics. Moreover, critical challenges, such as computational demands and the need for robust temporal modelling, are identified, along with potential future directions. This paper is a valuable resource for researchers and practitioners striving to advance VAR using DL techniques by systematically presenting concepts, methodologies, and trends.}
}
@article{AMMAR2018116,
title = {MPEG-4 AVC stream-based saliency detection. Application to robust watermarking},
journal = {Signal Processing: Image Communication},
volume = {60},
pages = {116-130},
year = {2018},
issn = {0923-5965},
doi = {https://doi.org/10.1016/j.image.2017.09.007},
url = {https://www.sciencedirect.com/science/article/pii/S0923596517301674},
author = {Marwa Ammar and Mihai Mitrea and Marwen Hasnaoui and Patrick {Le Callet}},
keywords = {Saliency map, MPEG-4 AVC stream, Density fixation map, Saccade locations, Robust watermarking},
abstract = {By bridging uncompressed-domain saliency detection and MPEG-4 AVC compression principles, the present paper advances a methodological framework for extracting the saliency maps directly from the stream syntax elements. In this respect, inside each GOP, the intensity, color, orientation and motion elementary saliency maps are related to the energy of the luma coefficients, to the energy of chroma coefficients, to the gradient of the prediction modes and to the amplitude of the motion vectors, respectively. The three spatial saliency maps are pooled according to an average formula, while the static-temporal fusion is achieved by six different formulas. The experiments consider both ground-truth and applicative evaluations. The ground-truth benchmarking investigates the relation between the predicted MPEG-4 AVC saliency map and the actual human saliency, captured by eye-tracking devices. It is based on two corpora (representing density fixation maps and saccade locations), two objective criteria (related to the closeness between the predicted and the real saliency maps and to the difference between the behavior of the predicted saliency map in fixation and random locations), two objective measures (KLD – the Kullback Leibler Divergence and AUC – the Area Under the ROC Curve) and 5 state-of-the-art saliency models (3 acting in spatial domain and 2 acting in compressed domain). The applicative validation is carried out by integrating the MPEG-4 AVC saliency map into a robust watermarking application. As an overall conclusion, the paper demonstrates that although the MPEG-4 AVC standard does not explicitly relies on any visual saliency principle, its stream syntax elements preserve this property. Four main benefits for the MPEG-4 AVC based saliency extraction are thus brought to light: (1) it outperforms (or, at least, is as good as) state-of-the-art uncompressed domain methods, (2) it allows significant gains to be obtained in watermarking transparency (for prescribed data payload and robustness), (3) it is less sensitive to the randomness in the processed visual content, and (4) it has a linear computational complexity. For instance, the ground truth results exhibit absolute relative gains between 60% and 164% in KLD, between 17% and 21% in AUC, and relative gains in KLD sensitivity between 1.18 and 6.12 and in AUC sensitivity between 1.06 and 33.7; the applicative validation brings to light transparency gains up to 10 dB in PSNR.}
}
@article{JACOB2022470,
title = {Algorithmic Approaches to Classify Autism Spectrum Disorders: A Research Perspective},
journal = {Procedia Computer Science},
volume = {201},
pages = {470-477},
year = {2022},
note = {The 13th International Conference on Ambient Systems, Networks and Technologies (ANT) / The 5th International Conference on Emerging Data and Industry 4.0 (EDI40)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.03.061},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922004744},
author = {Shomona Gracia Jacob and Majdi Mohammed {Bait Ali Sulaiman} and Bensujin Bennet},
keywords = {Machine learning, Supervised learning, Pattern discovery, Autism disorder, Data Mining},
abstract = {Autism Spectrum Disorder (ASD) is a neurodevelopmental disability that exhibits sluggish progress in vocal development, restricted interest in normal activity and repetitive disoriented behavior. This syndrome, has gained a lot of attention due to its prevalence among children across all countries and from different economic backgrounds. However, ASD detection and treatment yet remains in its infancy due to the lack of awareness among parents, limited screening of proper developmental milestones and a dearth of diagnostic tools to classify this syndrome with convincing accuracy. Recent studies report that scalable biomarkers for early detection have made little progress in research due to the erraticism of this disorder. Moreover, the study on developing tools or applications for parents, teachers, and healthcare workers to identify children who exhibit any form of autism is still a work in progress. The research work undertaken in this paper presents an analysis of supervised machine learning algorithms on mining interesting details that link the diverse nature of ASD and the possibility of computationally detecting markers for the syndrome. The preliminary findings on the performance of traditional machine learning algorithms in ASD classification is reported with the possibility of integrating deep learning architectures for ASD detection and therapy.}
}
@incollection{CAMERER2014479,
title = {Chapter 25 - The Neural Basis of Strategic Choice},
editor = {Paul W. Glimcher and Ernst Fehr},
booktitle = {Neuroeconomics (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {San Diego},
pages = {479-492},
year = {2014},
isbn = {978-0-12-416008-8},
doi = {https://doi.org/10.1016/B978-0-12-416008-8.00025-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780124160088000255},
author = {Colin F. Camerer and Todd A. Hare},
keywords = {Game theory, Learning, Strategic choice},
abstract = {In this chapter, we present a set of concepts and tools for defining and examining strategic choice that are drawn from behavioral economics and discuss how they can be applied to and tested with neuroscience techniques. The standard language for studying strategic choice in economics is game theory. Game theory provides concrete mathematical formulas for linking strategic actions to rewarding payoffs. After outlining the four components necessary to make predictions about strategic social behavior, we present recent evidence that the computations predicted by game theory in specific strategic choice contexts are reflected in the brain. In addition, we discuss links between strategic decision making and the psychological concept of theory of mind. We conclude by suggesting that developing mathematical models of social and strategic actions may aid in the understanding of how the brain implements typical choice behavior as well as categorizing dysfunctions that lead to aberrant behavior in psychiatric disorders.}
}
@article{JANSEN2022100020,
title = {The illusion of data validity: Why numbers about people are likely wrong},
journal = {Data and Information Management},
volume = {6},
number = {4},
pages = {100020},
year = {2022},
issn = {2543-9251},
doi = {https://doi.org/10.1016/j.dim.2022.100020},
url = {https://www.sciencedirect.com/science/article/pii/S2543925122001188},
author = {Bernard J. Jansen and Joni Salminen and Soon-gyo Jung and Hind Almerekhi},
keywords = {People data, Measurement, Quantitative paradigm, Statistics},
abstract = {This reflection article addresses a difficulty faced by scholars and practitioners working with numbers about people, which is that those who study people want numerical data about these people. Unfortunately, time and time again, this numerical data about people is wrong. Addressing the potential causes of this wrongness, we present examples of analyzing people numbers, i.e., numbers derived from digital data by or about people, and discuss the comforting illusion of data validity. We first lay a foundation by highlighting potential inaccuracies in collecting people data, such as selection bias. Then, we discuss inaccuracies in analyzing people data, such as the flaw of averages, followed by a discussion of errors that are made when trying to make sense of people data through techniques such as posterior labeling. Finally, we discuss a root cause of people data often being wrong – the conceptual conundrum of thinking the numbers are counts when they are actually measures. Practical solutions to address this illusion of data validity are proposed. The implications for theories derived from people data are also highlighted, namely that these people theories are generally wrong as they are often derived from people numbers that are wrong.}
}
@article{KARSTEN2020104512,
title = {Closing the gap: Merging engineering and anthropology in holistic fire safety assessments in the maritime and offshore industries},
journal = {Safety Science},
volume = {122},
pages = {104512},
year = {2020},
issn = {0925-7535},
doi = {https://doi.org/10.1016/j.ssci.2019.104512},
url = {https://www.sciencedirect.com/science/article/pii/S0925753518316175},
author = {Mette Marie Vad Karsten and Aqqalu Thorbjørn Ruge and Thomas Hulin},
keywords = {Anthropology, Fire safety engineering, Transdisciplinary, Risk, Maritime, Offshore},
abstract = {This article reports on the endeavor to merge the fields of anthropology and fire safety engineering in holistic fire safety assessments within the maritime and offshore industries. The article suggests a combination of the two disciplines to transition from an interdisciplinary approach towards transdisciplinarity. The approach has been developed and adjusted during three cases of risk analyses and prevention strategies on fire safety. The article presents two methodological insights illustrating the necessary attitude of interdisciplinarity as a foundation towards transdisciplinarity. It advocates for the need of willingness in organizations and project teams to consider both disciplines as equally valid, integrate them in research definition, and create a base for common understanding. Subsequently, it is proposed that transdisciplinary work requires the creation of a group of core members acting as guarantors of transdisciplinarity, thus becoming themselves transdisciplinary humans working in a joined framework of thinking and methods. The article also presents two operational findings integrating the two disciplines within the area of fire safety. The first finding concerns including ‘daily operations’ in fire safety design, as daily practices and perceptions among crew can have a high impact on fire safety. The second finding concerns ‘reclassification of space and place’. It highlights mixing and shifting between work- and leisure-related practices within the same physical space, leading to the identification of new fire scenarios. It also explores the shifts between work, leisure, and emergency places, and their link to the shifts in professional roles of crew.}
}
@article{DURSTEWITZ2008739,
title = {The Dual-State Theory of Prefrontal Cortex Dopamine Function with Relevance to Catechol-O-Methyltransferase Genotypes and Schizophrenia},
journal = {Biological Psychiatry},
volume = {64},
number = {9},
pages = {739-749},
year = {2008},
note = {Neurodevelopment and the Transition from Schizophrenia Prodrome to Schizophrenia},
issn = {0006-3223},
doi = {https://doi.org/10.1016/j.biopsych.2008.05.015},
url = {https://www.sciencedirect.com/science/article/pii/S000632230800646X},
author = {Daniel Durstewitz and Jeremy K. Seamans},
keywords = {Attractor dynamics, computational model, dopamine, GABA currents, NMDA currents, prefrontal cortex, schizophrenia},
abstract = {There is now general consensus that at least some of the cognitive deficits in schizophrenia are related to dysfunctions in the prefrontal cortex (PFC) dopamine (DA) system. At the cellular and synaptic level, the effects of DA in PFC via D1- and D2-class receptors are highly complex, often apparently opposing, and hence difficult to understand with regard to their functional implications. Biophysically realistic computational models have provided valuable insights into how the effects of DA on PFC neurons and synaptic currents as measured in vitro link up to the neural network and cognitive levels. They suggest the existence of two discrete dynamical regimes, a D1-dominated state characterized by a high energy barrier among different network patterns that favors robust online maintenance of information and a D2-dominated state characterized by a low energy barrier that is beneficial for flexible and fast switching among representational states. These predictions are consistent with a variety of electrophysiological, neuroimaging, and behavioral results in humans and nonhuman species. Moreover, these biophysically based models predict that imbalanced D1:D2 receptor activation causing extremely low or extremely high energy barriers among activity states could lead to the emergence of cognitive, positive, and negative symptoms observed in schizophrenia. Thus, combined experimental and computational approaches hold the promise of allowing a detailed mechanistic understanding of how DA alters information processing in normal and pathological conditions, thereby potentially providing new routes for the development of pharmacological treatments for schizophrenia.}
}
@incollection{BAKER2023209,
title = {From capacity to ability to automation: “Western” conceptions of the figure of man and ableist subjectivities},
editor = {Robert J Tierney and Fazal Rizvi and Kadriye Ercikan},
booktitle = {International Encyclopedia of Education (Fourth Edition)},
publisher = {Elsevier},
edition = {Fourth Edition},
address = {Oxford},
pages = {209-218},
year = {2023},
isbn = {978-0-12-818629-9},
doi = {https://doi.org/10.1016/B978-0-12-818630-5.12005-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128186305120056},
author = {Bernadette Baker},
keywords = {Ableist subjectivities, Wynter, Colonialism, Humanism, Locke, Gall, Artificial intelligence (AI)},
abstract = {This chapter analyzes the shifting construct of ableist subjectivities that lie at the heart of a modernity-colonialism-racialization vortex and that were entangled with humanism's rise. Drawing on and building on the work of Sylvia Wynter, it examines how a “figure of Man” in three different but related iterations helped shape ontological hierarchies and violent inclusions/exclusions. It illustrates these moves via whitestream scholarship that came to dominate certain geopolitical locales from the 1600s, 1800s, and 2000s and that subsequently spread. From discourses of capacity (tutoring children), to ability (compulsory schooling), to automation (systems theory and computational superintelligence), the figure of Man not only overrepresents for “the human” but also puts in jeopardy all lives and ways of being beyond its exclusive borders. The questions that remain for education exceed standard policy debates about school reform, inclusive schooling, or evaluation, pointing instead toward a wider planetary context, existential issues, and power relations that sit within the tensions between “Man's” exclusivity, “the human's” idiosyncracies and primacy, and the potential rewriting/erasure of both via new technologies.}
}
@article{QI2021338821,
title = {Accurate diagnosis of lung tissues for 2D Raman spectrogram by deep learning based on short-time Fourier transform},
journal = {Analytica Chimica Acta},
volume = {1179},
pages = {338821},
year = {2021},
issn = {0003-2670},
doi = {https://doi.org/10.1016/j.aca.2021.338821},
url = {https://www.sciencedirect.com/science/article/pii/S0003267021006474},
author = {Yafeng Qi and Lin Yang and Bangxu Liu and Li Liu and Yuhong Liu and Qingfeng Zheng and Dameng Liu and Jianbin Luo},
keywords = {Raman spectrogram, Lung cancer, Short-time Fourier transform, Deep learning},
abstract = {Multivariate statistical analysis methods have an important role in spectrochemical analyses to rapidly identify and diagnose cancer and the subtype. However, utilizing these methods to analyze lager amount spectral data is challenging, and poses a major bottleneck toward achieving high accuracy. Here, a new convolutional neural networks (CNN) method based on short-time Fourier transform (STFT) to diagnose lung tissues via Raman spectra readily is proposed. The models yield that the accuracies of the new method are higher than the conventional methods (principal components analysis -linear discriminant analysis and support vector machine) for validation group (95.2% vs 85.5%, 94.4%) and test group (96.5% vs 90.4%, 93.9%) after cross-validation. The results illustrate that the new method which converts one-dimensional Raman data into two-dimensional Raman spectrograms improve the discriminatory ability of lung tissues and can achieve automatically accurate diagnosis of lung tissues.}
}
@article{NAKHLE2024100411,
title = {Shrinking the giants: Paving the way for TinyAI},
journal = {Device},
volume = {2},
number = {8},
pages = {100411},
year = {2024},
issn = {2666-9986},
doi = {https://doi.org/10.1016/j.device.2024.100411},
url = {https://www.sciencedirect.com/science/article/pii/S2666998624002473},
author = {Farid Nakhle},
keywords = {accelerated models, compressed models, miniaturized intelligence, tiny artificial intelligence, tiny machine learning},
abstract = {Summary
In the current era of technological advancement, the quest for more efficient and accessible artificial intelligence (AI) is driving the investigation of the predictive potential of small architecture-based, compressed, and accelerated AI models (TinyAI) and the benefits of running those on small-scale digital edge computing devices. This perspective delves into the expanding world of TinyAI, envisioning a future in which powerful machine intelligence can be encapsulated within pocket-sized devices, and discusses the technological challenges and opportunities associated with it. In addition, some of the myriad applications and benefits that can arise from their deployment will be discussed.}
}
@article{TUPPURAINEN2024108835,
title = {Conceptual design of furfural extraction, oxidative upgrading and product recovery: COSMO-RS-based process-level solvent screening},
journal = {Computers & Chemical Engineering},
volume = {191},
pages = {108835},
year = {2024},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2024.108835},
url = {https://www.sciencedirect.com/science/article/pii/S0098135424002539},
author = {Ville Tuppurainen and Lorenz Fleitmann and Jani Kangas and Kai Leonhard and Juha Tanskanen},
keywords = {Furfural oxidation, Hydrogen peroxide, Conceptual process design, COSMO-RS predictive thermodynamics, Solvent screening},
abstract = {Liquid phase oxidation of furfural using hydrogen peroxide offers a promising route for bio-based C4 furanones and diacids; however, only dilute water-based process designs have been previously suggested that have limited techno-economic potential. In this study, a conceptual process design is presented, where aqueous furfural is extracted using an organic solvent, coupled with peroxide oxidation and product recovery in the presence of the solvent. To address the problem of solvent selection, the COSMO-RS-based solvent screening framework is applied, where quantum mechanics-based thermodynamics are utilized in pinch-based process models. About 2500 solvent candidates were identified as feasible. Focusing on a set of 400 solvent candidates revealed energy consumption values (Qreb,tot/ṁprod recov) between approximately 2 MWh/tonne and 33 MWh/tonne, signifying the potential of the solvent-based process in outperforming the reference aqueous process (49.4 MWh/tonne). The study provides potential solvent candidates and future directions to consider in more costly computational and experimental efforts.}
}
@article{YUTING2023e15851,
title = {Current status of digital humanities research in Taiwan},
journal = {Heliyon},
volume = {9},
number = {5},
pages = {e15851},
year = {2023},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2023.e15851},
url = {https://www.sciencedirect.com/science/article/pii/S240584402303058X},
author = {Pan Yuting and Jiang Yinfeng and Zhang Jingli},
keywords = {Digital humanities, Text mining, Social network analysis, GIS},
abstract = {Purpose
Review the current research status of the theory, techniques, and practice of digital humanities in Taiwan.
Methods
Select the 8 issues of the Journal of Digital Archives and Digital Humanities from its inception in 2018–2021, and the papers of the 5-year International Conference of Digital Archives and Digital Humanities from 2017 to 2021 as the research data, and conduct text analysis of the collected 252 articles.
Results
From the statistical analysis results, the number of practical articles is the largest, followed by tools and techniques, and the least number of theoretical articles. Text tools and literature research are the most concentrated aspects of digital humanities research in Taiwan.
Limitations
It still needs to be further compared with the current research status of digital humanities in Mainland China.
Conclusions
Digital humanities in Taiwan focuses on the development of tools and techniques, and practical applications of literature and history, and focuses on Taiwan's native culture to form its own digital humanities research characteristics.}
}
@incollection{SCHILLER2018246,
title = {Some Thermodynamics and Electrostatics With a View to Electrochemistry},
editor = {Klaus Wandelt},
booktitle = {Encyclopedia of Interfacial Chemistry},
publisher = {Elsevier},
address = {Oxford},
pages = {246-257},
year = {2018},
isbn = {978-0-12-809894-3},
doi = {https://doi.org/10.1016/B978-0-12-409547-2.13605-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780124095472136054},
author = {R. Schiller},
keywords = {Activity coefficient, Chemical potential, Cole–Cole plot, Conservation laws, Dielectric relaxation time, Dipole moment, Electric dipole, Entropy of mixing, Gauss law of electrostatics, Kramers–Kronig relations, Osmotic coefficient, Poisson equation, Polarization, Relative permittivity, Solvation energy},
abstract = {This article tries to offer an overview of some basic laws of thermodynamics and electrostatics which are considered to be part of the foundations of electrochemical thinking. Equilibrium thermodynamics is introduced in terms of conservation laws paying particular attention to the notion of chemical potential. After discussing the forces, potentials, and energetics of charges in vacuum the same problems are dealt with in continuous dielectric media. Here polarization, formation and role of dipole moments, their relation to relative permittivity (dielectric constant) are discussed both in macroscopic and atomic/molecular terms. Finally the kinetics of the response of relative permittivity to the variation of polarizing fields and charges are described. Solvation processes are referred to in connection with both thermodynamic and electrostatic considerations.}
}
@article{BEJINES2023108405,
title = {Counting semicopulas on finite structures},
journal = {Fuzzy Sets and Systems},
volume = {462},
pages = {108405},
year = {2023},
note = {Aggregation operations (186 p.)},
issn = {0165-0114},
doi = {https://doi.org/10.1016/j.fss.2022.09.011},
url = {https://www.sciencedirect.com/science/article/pii/S0165011422004122},
author = {C. Bejines and M. Ojeda-Hernández},
keywords = {Semicopula, Fuzzy Logic, Finite plane partition, Discrete Mathematics},
abstract = {Semicopulas are the operators chosen to model conjunction in the fuzzy/many-valued logics. In fact, a special kind of semicopula, called t-norm, is widely used in many applications of logic to engineering, computer science and fuzzy systems. The main result of this paper is the computation of the exact number of semicopulas that can be defined on a finite chain in terms of its length. The final formula is achieved via relating semicopulas with finite plane partitions.}
}
@article{YANG2024234071,
title = {Application and development of the Lattice Boltzmann modeling in pore-scale electrodes of solid oxide fuel cells},
journal = {Journal of Power Sources},
volume = {599},
pages = {234071},
year = {2024},
issn = {0378-7753},
doi = {https://doi.org/10.1016/j.jpowsour.2024.234071},
url = {https://www.sciencedirect.com/science/article/pii/S0378775324000223},
author = {Xiaoxing Yang and Guogang Yang and Shian Li and Qiuwan Shen and He Miao and Jinliang Yuan},
keywords = {Lattice Boltzmann method modeling, Pore-scale simulation, Reactive transport, Solid oxide fuel cells simulation},
abstract = {The lattice Boltzmann method (LBM) plays an important role in the study of the internal flow behavior at the pore-scale inside the electrodes of solid oxide fuel cells (SOFCs). Porosity, tortuosity, and particle size have a remarkable effect on gas transport and electrocatalytic processes, determining the performance of cells when SOFCs are applied in electric power generation, energy storage systems, and industrial production in recent years. However, these pore-scale transport progresses are not well characterized in the numerical studies of conventional computational fluid dynamics (CFD), thus modeling with LBM at the pore-scale is an effective tool for simulating gas transport and electrochemical reactions in electrodes. It overcomes the drawbacks of experimental techniques that do not characterize these processes accurately enough and detail the distribution of important variables. In this review, the methodology and process of electrode pore-scale modeling are presented, along with the application and current studies of LBM for diffusion, electrochemical reactions, and ion migration in SOFC porous electrodes. Important results are discussed. Finally, future perspectives on pore-scale studies of porous electrodes are given. This in-depth review intends to provide ideas for the development and further application of LBM in porous SOFC electrodes.}
}
@article{YANG2023103712,
title = {Study of the water entry and exit problems by coupling the APR and PST within SPH},
journal = {Applied Ocean Research},
volume = {139},
pages = {103712},
year = {2023},
issn = {0141-1187},
doi = {https://doi.org/10.1016/j.apor.2023.103712},
url = {https://www.sciencedirect.com/science/article/pii/S0141118723002535},
author = {Xi Yang and Song Feng and Jinxin Wu and Guiyong Zhang and Guangqi Liang and Zhifan Zhang},
keywords = {Water entry, Water exit, Particle shifting technique, Adaptive particle refinement, Coupled scheme},
abstract = {In this manuscript, the adaptive particle refinement (APR) and particle shifting technique (PST) are coupled and applied to investigate the entire process of water entry and exit. The PST implementation for various types of particles is quantitatively discussed in detail in the coupled APR-PST approach. A comprehensive analysis of different APR-PST coupled schemes is conducted with crucial variables compared and analyzed for the first time. The stability, accuracy and robustness of the developed numerical model are verified by three benchmark tests: 2D wedge water entry, 2D cylinder water exit and 2D cylinder water entry and exit. The obtained results show that the current model can ensure the overall computational precision in the situation of local refinement, which indicates it is a viable way to solve the problems of water entry and exit.}
}
@article{POTTS2025104968,
title = {Explaining institutional technology},
journal = {European Economic Review},
volume = {173},
pages = {104968},
year = {2025},
issn = {0014-2921},
doi = {https://doi.org/10.1016/j.euroecorev.2025.104968},
url = {https://www.sciencedirect.com/science/article/pii/S0014292125000182},
author = {Jason Potts and Kurt Dopfer and Bill Tulloh},
keywords = {Technology, Economic evolution, Knowledge, Information theory, User innovation},
abstract = {This paper offers a review and several refinements and extensions of Explaining Technology, by Koppl et al. (2023), which develops a combinatorial theory of the evolution of technology. First, we suggest that the mechanism of tinkering can be formulated in the theory of user innovation. Second, we propose that a useful refinement is to focus on institutional technologies. This offers a better explanation of emergent levels of evolutionary selection, or major transitions, and also adapts their framework to better explain the nature of a digital economy. Third, we propose a more ambitious line of generalisation from explaining technology to explaining knowledge. We suggest this is possible from the rubric of the new ‘physics of information’ in constructor theory, assembly theory and Bayesian mechanics.}
}
@article{OXMAN2000337,
title = {Design media for the cognitive designer1This paper is based on the keynote speech on `The Challenge of Design Computation' given by the author at ECAADE '97 in Vienna.1},
journal = {Automation in Construction},
volume = {9},
number = {4},
pages = {337-346},
year = {2000},
issn = {0926-5805},
doi = {https://doi.org/10.1016/S0926-5805(99)00017-5},
url = {https://www.sciencedirect.com/science/article/pii/S0926580599000175},
author = {Rivka Oxman},
keywords = {Generic design knowledge, Design collaboration, Re-representation, Typology, },
abstract = {Work on media for design which are responsive to the cognitive processes of the human designer are introduced as a paradigm for research and development. Design media are intended to support the cognitive nature of design and, particularly, the exploitation of design knowledge in computational environments. Basic theoretical assumptions are presented which underlie the development of design media. A central assumption is that designers share common forms of design knowledge which can be formalized, represented, and employed in computational environments. Generic knowledge is proposed as one such seminal form of design knowledge. We then develop a cognitive model which relates to the internal mental representations, strategies and mechanisms of generic design. The paper emphasizes the theoretical foundations of design media. This theoretical discussion is then exemplified through case studies presenting current research for the support of visual cognition in design. We introduce an approach to design schema as a visual form of generic design knowledge. Secondly we present a conceptual framework for the support of schema emergence in visual reasoning in design media. Finally, some implications of schema emergence in design collaboration are presented and discussed.}
}
@incollection{MANIKTALA2008247,
title = {Chapter 12 - Discussion Forums, Datasheets, and Other Real-World Issues},
editor = {Sanjaya Maniktala},
booktitle = {Troubleshooting Switching Power Converters},
publisher = {Newnes},
address = {Burlington},
pages = {247-289},
year = {2008},
isbn = {978-0-7506-8421-7},
doi = {https://doi.org/10.1016/B978-075068421-7.50014-X},
url = {https://www.sciencedirect.com/science/article/pii/B978075068421750014X},
author = {Sanjaya Maniktala},
abstract = {Publisher Summary
This chapter explains a few concepts such as thinking is the key, one needs to cross check everything, and product liability concerns. The chapter describes that the company's online tools can be used to discover design problems and correct them as long as thinking is applied as well. But what about “errors” in the online tools themselves? The chapter deals in and highlights that if the thinking process is done assiduously, sometimes one might arrive at the opposite conclusion that one initially foresaw. Anyone can even suddenly realize that he/she can be a part of the very problem that they are trying to fix; it could be in his/her own backyard. The chapter thinks about the customers and highlights that the very idea of a company starting a forum such as this one is essentially brilliant and thoroughly laudable. It also imparts a perception of transparency to their operations from the get-go. One should not outrightly believe anything and put in front of everyone, even if it is on semiglossy paper or in high-definition video or Flash HTML format. As engineers, one is required to put pen to paper, and at least do a sanity check.}
}
@incollection{HU2025629,
title = {Chapter 28 - The lung exposome: Accelerating precision respiratory health},
editor = {Kent E. Pinkerton and Richard Harding and Elizabeth Georgian},
booktitle = {The Lung (Third Edition)},
publisher = {Academic Press},
edition = {Third Edition},
pages = {629-645},
year = {2025},
isbn = {978-0-323-91824-4},
doi = {https://doi.org/10.1016/B978-0-323-91824-4.00017-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780323918244000174},
author = {Xin Hu},
keywords = {Environmental exposure, Exposome, High-resolution mass spectrometry, Mixed chemical exposure, Multi-omics, Respiratory health, Systems biology},
abstract = {The exposome concept, first proposed as a complement to the genome for understanding non-genetic causes of disease, has rapidly undergone significant evolution to recognize the totality of environmental exposures experienced across an individual's lifespan and to embrace the complexity of biological responses to those exposures. Over almost twodecades of work, the landscape of lung exposome research has expanded to include extensive characterization of external and internal exposure levels, molecular responses, and health parameters. The exposome approach has identified new environmental factors and profiles that impact the variation of lung function trajectories and the development of chronic respiratory diseases over the life course. Aligned with the goal of precision medicine and leveraging systems biology, exposome research directly addresses the heterogeneity in disease origins and management. Recent advancements in omics technologies, particularly omics-scale chemical quantification using high-resolution mass spectrometry, have revolutionized exposure assessment and biological discovery. These analytic platforms, now widely available for large population studies, have propelled advanced statistical models and computational pipelines, which together contribute to an affordable, automatable, and standardizable framework that can be integrated into health care and respiratory epidemiology for studies with greater precision, breadth, and depth.}
}
@article{GOLDSCHMIDT2006549,
title = {Variances in the impact of visual stimuli on design problem solving performance},
journal = {Design Studies},
volume = {27},
number = {5},
pages = {549-569},
year = {2006},
issn = {0142-694X},
doi = {https://doi.org/10.1016/j.destud.2006.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X06000172},
author = {Gabriela Goldschmidt and Maria Smolkov},
keywords = {creativity, design problems, problem solving, visual stimuli},
abstract = {Research in cognitive psychology and in design thinking has shown that the generation of inner representations in imagery and external representations via sketching are instrumental in design problem solving. In this paper we focus on another facet of visual representation in design: the ‘consumption’ of external visual representations, regarded as stimuli, when those are present in the designer's work environment. An empirical study revealed that the presence of visual stimuli of different kinds can affect performance, measured in terms of practicality, originality and creativity scores attained by designs developed by subjects under different conditions. The findings suggest that the effect of stimuli is contingent on the type of the design problem that is being solved.}
}
@article{CAFFERATA2023106879,
title = {Financial fragility and credit risk: A simulation model},
journal = {Communications in Nonlinear Science and Numerical Simulation},
volume = {116},
pages = {106879},
year = {2023},
issn = {1007-5704},
doi = {https://doi.org/10.1016/j.cnsns.2022.106879},
url = {https://www.sciencedirect.com/science/article/pii/S1007570422003665},
author = {Alessia Cafferata and Simone Casellina and Simone Landini and Mariacristina Uberti},
keywords = {Credit risk, Financial instability, Minsky, Agent-based model},
abstract = {Financial and economic crises are not always the same. It is important to understand why some episodes of crisis generate prolonged and systemic recessions. Developing the Financial Instability Hypothesis, Hyman Minsky introduced the idea that in periods of stability, financial actors tend to increase their risk exposure, moving from a stable hedge-dominated structure to an unstable one, characterized by speculative and ultra-speculative (Ponzi) financial positions: hence, stability turns out being destabilizing. Starting from the three different relationships introduced by Minsky (income–debt–hedge, speculative and Ponzi) for financial units, we involve a simple partial equilibrium agent-based model in which firms, the banking sector, the real and financial sides of the economy, interact. This theoretic framework is used as computational laboratory to extend the migration rates open system modelling based on the E(ntry)–S(tay)–L(eave) processes by considering the economic system, the business cycle and with attention to so-called zombie-firms.}
}
@article{LORENZODUS202015,
title = {The communicative modus operandi of online child sexual groomers: Recurring patterns in their language use},
journal = {Journal of Pragmatics},
volume = {155},
pages = {15-27},
year = {2020},
issn = {0378-2166},
doi = {https://doi.org/10.1016/j.pragma.2019.09.010},
url = {https://www.sciencedirect.com/science/article/pii/S0378216619306162},
author = {Nuria Lorenzo-Dus and Anina Kinzel and Matteo {Di Cristofaro}},
keywords = {Child sexual abuse, Online grooming, Linguistic patterns, Corpus assisted discourse studies},
abstract = {Online child sexual groomers manipulate their targets into partaking in sexual activity online and, in some cases, offline. To do so they use language (and other semiotic means, such as images) strategically. This study uses a Corpus-Assisted Discourse Studies methodology to identify recurring patterns in online groomers' language use, mapping them to the specific grooming goal that their use in context fulfils. The analysis of the groomers' language (c. 3.3 million words) within 622 conversations from the Perverted Justice website newly identifies 70 such recurring linguistic patterns (three-word collocations), as well as their relative strength of association to one or more grooming goals. The results can be used to inform computational models for detecting online child sexual grooming language. They can also support the development of training resources that raise awareness of typical language structures that characterise online sexual groomers’ communicative modus operandi.}
}
@article{ANAZKHAN2023,
title = {Metal additive manufacturing of alloy structures in architecture: A review on achievements and challenges},
journal = {Materials Today: Proceedings},
year = {2023},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2023.05.192},
url = {https://www.sciencedirect.com/science/article/pii/S2214785323028183},
author = {Muhammed {Anaz Khan} and Aysha Latheef},
keywords = {Architecture, Additive manufacturing, Facades, Construction industry, Structural engineering},
abstract = {There is a growing trend in modern architecture towards asymmetrical building layouts. This development can be attributed to the proliferation of cutting-edge production methods and structurally novel approaches. With the advent of computational design and digital manufacturing techniques, formerly static designs may now be modified to create one-of-a-kind, high-performance prototypes. Metal additive manufacturing (MAM) is a cutting-edge production method that paves the way for new architectural designs, construction processes, and materials. Interesting possibilities for optimising structural elements are made possible by MAM because of its ability to deposit material just where it is structurally necessary. The construction sector places a premium on directed energy deposition additive manufacturing (DED AM) and wire arc additive manufacturing (WAAM). The adaptability of these two technologies in the construction industry and their possible future applications are explored in this literature review.}
}
@article{WANG2024119836,
title = {Novel score function and standard coefficient-based single-valued neutrosophic MCDM for live streaming sales},
journal = {Information Sciences},
volume = {654},
pages = {119836},
year = {2024},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2023.119836},
url = {https://www.sciencedirect.com/science/article/pii/S0020025523014214},
author = {Fei Wang},
keywords = {Single-valued neutrosophic set, Score function, Standard coefficient, Multi-criteria decision making, Live streaming sales},
abstract = {Single-valued neutrosophic sets (SVNS) provide a comprehensive approach to express uncertainty in decision scenarios, surpassing the utility of fuzzy sets (FS) and intuitionistic fuzzy sets (IFS). Yet, current SVNS score function concepts stem from FS and IFS construction methods, showing inconsistencies. Thus, we introduce a novel SVNS score function based on inherent uncertainty essence. Additionally, we devise a standard coefficient to gauge SVNS standardization akin to fuzzy sets. Addressing SVNS researchability and limitations in fundamental concepts, especially the score function, we propose an SVNS-based multi-criteria decision-making (MCDM) model. This leverages the new score function and standard coefficient. We demonstrate its effectiveness on two decisions: “software engineer recruitment” with known weights and “investment selection” with unknown weights. Ultimately, we successfully applied the model to the field of live streaming sales to solve the actual MCDM problem. By comparing with existing methods, we affirm the model's validity and practicality. Compared to prior approaches, the new method exhibits: (1) Enhanced stability and credibility in result values and rankings, promoting robust optimal solutions. (2) Reduced computational steps and workload, enhancing usability and practicality.}
}
@article{FOLLEY2003467,
title = {Psychoses and creativity: is the missing link a biological mechanism related to phospholipids turnover?},
journal = {Prostaglandins, Leukotrienes and Essential Fatty Acids},
volume = {69},
number = {6},
pages = {467-476},
year = {2003},
note = {Recent Advances of Membran e Pathology in Schizophrenia},
issn = {0952-3278},
doi = {https://doi.org/10.1016/j.plefa.2003.08.019},
url = {https://www.sciencedirect.com/science/article/pii/S0952327803001716},
author = {Bradley S Folley and Mikisha L Doop and Sohee Park},
keywords = {Creativity, Norepinephrine, Fatty acids, Schizophrenia, Psychoses},
abstract = {Recent evidence suggests that genetic and biochemical factors associated with psychoses may also provide an increased propensity to think creatively. The evolutionary theories linking brain growth and diet to the appearance of creative endeavors have been made recently, but they lack a direct link to research on the biological correlates of divergent and creative thought. Expanding upon Horrobin's theory that changes in brain size and in neural microconnectivity came about as a result of changes in dietary fat and phospholipid incorporation of highly unsaturated fatty acids, we propose a theory relating phospholipase A2 (PLA2) activity to the neuromodulatory effects of the noradrenergic system. This theory offers probable links between attention, divergent thinking, and arousal through a mechanism that emphasizes optimal individual functioning of the PLA2 and NE systems as they interact with structural and biochemical states of the brain. We hope that this theory will stimulate new research in the neural basis of creativity and its connection to psychoses.}
}
@article{BROWN2022110672,
title = {“Deep reinforcement learning for engineering design through topology optimization of elementally discretized design domains”},
journal = {Materials & Design},
volume = {218},
pages = {110672},
year = {2022},
issn = {0264-1275},
doi = {https://doi.org/10.1016/j.matdes.2022.110672},
url = {https://www.sciencedirect.com/science/article/pii/S0264127522002933},
author = {Nathan K. Brown and Anthony P. Garland and Georges M. Fadel and Gang Li},
keywords = {Reinforcement learning, Topology optimization, Deep learning, Engineering design, Structural design, Data-driven},
abstract = {Advances in machine learning algorithms and increased computational efficiencies give engineers new capabilities and tools to apply to engineering design. Machine learning models can approximate complex functions and, therefore, can be useful for various tasks in the engineering design workflow. This paper investigates using reinforcement learning (RL), a subset of machine learning that teaches an agent to complete a task through accumulating experiences in an interactive environment, to automate the designing of 2D discretized topologies. RL agents use past experiences to learn sequential sets of actions to best achieve some objective. In the proposed environment, an RL agent can make sequential decisions to design a topology by removing elements to best satisfy compliance minimization objectives. After each action, the agent receives feedback by evaluating how well the current topology satisfies the design objectives. After training, the agent was tasked with designing optimal topologies under various load cases. The agent's proposed designs had similar or better compliance minimization performance to those produced by traditional gradient-based topology optimization methods. These results show that a deep RL agent can learn generalized design strategies to satisfy multi-objective design tasks and, therefore, shows promise as a tool for arbitrarily complex design problems across many domains.}
}
@article{FABOLUDE2025100246,
title = {Smart cities, smart systems: A comprehensive review of system dynamics model applications in urban studies in the big data era},
journal = {Geography and Sustainability},
volume = {6},
number = {1},
pages = {100246},
year = {2025},
issn = {2666-6839},
doi = {https://doi.org/10.1016/j.geosus.2024.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S2666683924000993},
author = {Gift Fabolude and Charles Knoble and Anvy Vu and Danlin Yu},
keywords = {Urban sustainability, Smart cities, System dynamics models, Big data analytics, Urban system complexity, Data-driven urbanism},
abstract = {This paper addresses urban sustainability challenges amid global urbanization, emphasizing the need for innovative approaches aligned with the Sustainable Development Goals. While traditional tools and linear models offer insights, they fall short in presenting a holistic view of complex urban challenges. System dynamics (SD) models that are often utilized to provide holistic, systematic understanding of a research subject, like the urban system, emerge as valuable tools, but data scarcity and theoretical inadequacy pose challenges. The research reviews relevant papers on recent SD model applications in urban sustainability since 2018, categorizing them based on nine key indicators. Among the reviewed papers, data limitations and model assumptions were identified as major challenges in applying SD models to urban sustainability. This led to exploring the transformative potential of big data analytics, a rare approach in this field as identified by this study, to enhance SD models’ empirical foundation. Integrating big data could provide data-driven calibration, potentially improving predictive accuracy and reducing reliance on simplified assumptions. The paper concludes by advocating for new approaches that reduce assumptions and promote real-time applicable models, contributing to a comprehensive understanding of urban sustainability through the synergy of big data and SD models.}
}
@article{JOGO2025106357,
journal = {Environmental Modelling & Software},
volume = {186},
pages = {106357},
year = {2025},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2025.106357},
url = {https://www.sciencedirect.com/science/article/pii/S1364815225000416},
author = {Fransiskus Serfian Jogo and Hanum Khairana Fatmah and Aufaclav Zatu {Kusuma Frisky}}
}
@article{PITKOW2017943,
title = {Inference in the Brain: Statistics Flowing in Redundant Population Codes},
journal = {Neuron},
volume = {94},
number = {5},
pages = {943-953},
year = {2017},
issn = {0896-6273},
doi = {https://doi.org/10.1016/j.neuron.2017.05.028},
url = {https://www.sciencedirect.com/science/article/pii/S089662731730466X},
author = {Xaq Pitkow and Dora E. Angelaki},
keywords = {brain, inference, theory, population code, message-passing, redundant, coding, nuisance, nonlinear},
abstract = {It is widely believed that the brain performs approximate probabilistic inference to estimate causal variables in the world from ambiguous sensory data. To understand these computations, we need to analyze how information is represented and transformed by the actions of nonlinear recurrent neural networks. We propose that these probabilistic computations function by a message-passing algorithm operating at the level of redundant neural populations. To explain this framework, we review its underlying concepts, including graphical models, sufficient statistics, and message-passing, and then describe how these concepts could be implemented by recurrently connected probabilistic population codes. The relevant information flow in these networks will be most interpretable at the population level, particularly for redundant neural codes. We therefore outline a general approach to identify the essential features of a neural message-passing algorithm. Finally, we argue that to reveal the most important aspects of these neural computations, we must study large-scale activity patterns during moderately complex, naturalistic behaviors.}
}
@article{TEY2021153,
title = {The Impact of Concession Patterns on Negotiations: When and Why Decreasing Concessions Lead to a Distributive Disadvantage},
journal = {Organizational Behavior and Human Decision Processes},
volume = {165},
pages = {153-166},
year = {2021},
issn = {0749-5978},
doi = {https://doi.org/10.1016/j.obhdp.2021.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S0749597821000613},
author = {Kian Siong Tey and Michael Schaerer and Nikhil Madan and Roderick I. Swaab},
keywords = {Negotiations, Concessions, Reservation price, Offers, Signaling, Distributive},
abstract = {We propose that making a series of decreasing concessions (e.g., $1,500–1,210–1,180–1,170) signals that negotiators are reaching their limit and that this results in a negotiation disadvantage for offer recipients. Although we find that most negotiators do not use this strategy naturally, seven studies (N = 2,311) demonstrate that decreasing concessions causes recipients to make less ambitious counteroffers (Studies 1–5) and reach worse deals (Study 2) in distributive negotiations. We find that this disadvantage occurs because decreasing concessions shape recipients’ expectations of the subsequent offers that will be made, which results in inflated perceptions of the counterparts’ reservation price relative to the other concession strategies (Study 3). In addition, we find that this disadvantage is particularly large when concessions decrease at a moderate rate (Study 4a) and when decreasing concessions takes place over more (vs. fewer) rounds (Study 4b). Finally, we find that recipients can protect themselves against the deleterious effects of decreasing concession by thinking of a target before they enter the negotiation (Study 5).}
}
@article{LI2013262,
title = {Improved Particle Filter for Target Tracing Application based on ChinaGrid},
journal = {AASRI Procedia},
volume = {5},
pages = {262-267},
year = {2013},
note = {2013 AASRI Conference on Parallel and Distributed Computing and Systems},
issn = {2212-6716},
doi = {https://doi.org/10.1016/j.aasri.2013.10.087},
url = {https://www.sciencedirect.com/science/article/pii/S2212671613000887},
author = {Yuqiang Li and Xixu He and Haitao Jia},
keywords = {Target tracing, Particle filter, ChinaGrid},
abstract = {Most practical target tracking are usually maneuvering, while most target tracking algorithm are linear filter. More estimation error is introduced from linear filter. Nowadays more and more researchers pay their attention in Maneuvering Target Tracking algorithm. Particle filter has been developed for estimation of nonlinear system states. This paper presents an improved particle filter, which can apply the maneuvering target tracking problem. In practice, the particle filter would take abundant computation for estimate the maneuvering target tracking. The ChinaGrid system use the agile and distributed federations to reduce the computing time, which achieve to fast resolution for particle filter computation of target tracing application. Lastly the simulation proves it.}
}
@article{NEUDERT2024100092,
journal = {Journal of Responsible Technology},
volume = {20},
pages = {100092},
year = {2024},
issn = {2666-6596},
doi = {https://doi.org/10.1016/j.jrt.2024.100092},
url = {https://www.sciencedirect.com/science/article/pii/S2666659624000180},
author = {Philipp Neudert and Mareike Smolka and Britta Acksel and Yana Boeva}
}
@article{HARTMAN2025100398,
title = {Recommendations for the Development of Artificial Intelligence Applications for the Retail Level},
journal = {Journal of Food Protection},
volume = {88},
number = {1},
pages = {100398},
year = {2025},
issn = {0362-028X},
doi = {https://doi.org/10.1016/j.jfp.2024.100398},
url = {https://www.sciencedirect.com/science/article/pii/S0362028X24001820},
author = {Jim Hartman},
keywords = {AI applications based on cognitive models, Development of HACCP plans, Food safety root cause analyses, Foodborne illness outbreak investigations},
abstract = {Some of the early applications of artificial intelligence (AI) for food safety appear to be intended for use at the level of manufacturing and distribution. Artificial intelligence applications to facilitate foodborne illness outbreak investigations, development of HACCP plans, and food safety root cause analyses at the retail level are needed. For example, the interview form in the International Association for Food Protection booklet, Procedures to Investigate Foodborne Illness, could be filled out by humans, but much of the rest of the forms could be completed by artificial intelligence applications. Humans would still have to do the environmental assessments. Most AI applications to date have consisted of pattern identification. Pattern recognition applications may not be capable of assisting in all the proposed retail applications, but it would not be helpful to propose these retail applications without offering a possible path forward. Progress in the proposed directions may require the development of more robust artificial intelligence based on cognitive models. Because this paradigm shift is less familiar to food safety professionals, a comparison between pattern recognition algorithms and cognitive models is offered. An explanation of cognitive models is included to raise awareness of this approach.}
}
@incollection{CORTELLNICOLAU20241090,
title = {Agent-Based Modeling},
editor = {Efthymia Nikita and Thilo Rehren},
booktitle = {Encyclopedia of Archaeology (Second Edition) (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {Oxford},
pages = {1090-1098},
year = {2024},
isbn = {978-0-323-91856-5},
doi = {https://doi.org/10.1016/B978-0-323-90799-6.00094-X},
url = {https://www.sciencedirect.com/science/article/pii/B978032390799600094X},
author = {Alfredo Cortell-Nicolau},
keywords = {Agency, Agent-based models, Coding, Complexity, Heuristic modeling, Hypothesis testing, Netlogo, Python, R, Reproducibility, Tactical modeling},
abstract = {This work constitutes a very brief overview of Agent-Based Modeling applied to archaeology. The aim is to provide a synthetic overview of the most fundamental concepts, so that researchers interested in starting to explore this methodological tool have a first contact with it. The text covers basic concepts, as well as offers an example of a simple simulation so that interested readers gain initial insight to this topic.}
}
@article{MENG2023116227,
title = {Efficient path planning for AUVs in unmapped marine environments using a hybrid local–global strategy},
journal = {Ocean Engineering},
volume = {288},
pages = {116227},
year = {2023},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2023.116227},
url = {https://www.sciencedirect.com/science/article/pii/S0029801823026112},
author = {Wenlong Meng and Ya Gong and Fan Xu and Pingping Tao and Pengbo Bo and Shiqing Xin},
keywords = {Path planning, Autonomous undersea vehicle, Unmapped obstacle environment, Rapidly exploring random tree, Dynamic step},
abstract = {The ability of autonomous undersea vehicles (AUVs) to plan paths in unknown marine environments is the precondition for executing complicated missions. However, existing path planning algorithms based on underwater sensing equipment often struggle to achieve efficient exploration and generate high-quality trajectories. In this paper, we introduce a novel approach to efficiently handle the challenge of AUV navigation under limited information. Our solution combines global and local planning techniques to generate optimized paths that guarantee collision-free and efficient operations. In global path planning, we incrementally use the rolling windows to make decisions on high-level path branching while utilizing waypoints from selected branches to refine the calculation of local paths for enhanced accuracy. We employ an efficient small-scale path search strategy at the local path computation level by leveraging sensor-detected environments. In this stage, we propose an advanced rapidly exploring random tree (RRT) algorithm called circle-RRT. By combining adaptive circle sampling with dynamic step sizes, this algorithm can significantly reduce the generation of redundant sampling points and improve the efficiency of local path planning. We evaluated the efficiency of our algorithm in unknown environments through simulations and compared it with previous leading methods.}
}
@article{TIAN2016363,
title = {Identifying informative energy data in Bayesian calibration of building energy models},
journal = {Energy and Buildings},
volume = {119},
pages = {363-376},
year = {2016},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2016.03.042},
url = {https://www.sciencedirect.com/science/article/pii/S0378778816301967},
author = {Wei Tian and Song Yang and Zhanyong Li and Shen Wei and Wei Pan and Yunliang Liu},
keywords = {Bayesian computation, Cluster analysis, Model calibration, Building energy},
abstract = {Bayesian computation has received increasing attention in calibrating building energy models due to its flexibility and accuracy. However, there has been little research on how to determine informative energy data in Bayesian calibration in building energy models. Therefore, this study aims to determine and choose informative energy data using correlation analysis and hierarchical clustering method. A case study of retail building is used to demonstrate the proposed methods to infer four unknown input parameters using EnergyPlus program. The results indicate that the different combinations of energy data can provide various levels of accuracy in estimating unknown input variables in model calibration. This suggests that Bayesian computation is suitable for inferring the parameters when there are missing energy data that can be treated as uninformative output data. The proposed method can be also used to find the redundant information on energy data in order to improve computational efficiency in Bayesian calibration.}
}
@article{BRINK201339,
title = {Computing with networks of spiking neurons on a biophysically motivated floating-gate based neuromorphic integrated circuit},
journal = {Neural Networks},
volume = {45},
pages = {39-49},
year = {2013},
note = {Neuromorphic Engineering: From Neural Systems to Brain-Like Engineered Systems},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2013.02.011},
url = {https://www.sciencedirect.com/science/article/pii/S089360801300052X},
author = {S. Brink and S. Nease and P. Hasler},
keywords = {Neuromorphic VLSI, Floating-gate transistor, Single transistor learning synapse, Spiking winner-take-all, Synfire chain},
abstract = {Results are presented from several spiking network experiments performed on a novel neuromorphic integrated circuit. The networks are discussed in terms of their computational significance, which includes applications such as arbitrary spatiotemporal pattern generation and recognition, winner-take-all competition, stable generation of rhythmic outputs, and volatile memory. Analogies to the behavior of real biological neural systems are also noted. The alternatives for implementing the same computations are discussed and compared from a computational efficiency standpoint, with the conclusion that implementing neural networks on neuromorphic hardware is significantly more power efficient than numerical integration of model equations on traditional digital hardware.}
}
@article{WANG201715,
title = {An overview on the roles of fuzzy set techniques in big data processing: Trends, challenges and opportunities},
journal = {Knowledge-Based Systems},
volume = {118},
pages = {15-30},
year = {2017},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2016.11.008},
url = {https://www.sciencedirect.com/science/article/pii/S0950705116304452},
author = {Hai Wang and Zeshui Xu and Witold Pedrycz},
keywords = {Big data, Data-intensive science, Fuzzy sets, Fuzzy logic, Granular computing},
abstract = {In the era of big data, we are facing with an immense volume and high velocity of data with complex structures. Data can be produced by online and offline transactions, social networks, sensors and through our daily life activities. A proper processing of big data can result in informative, intelligent and relevant decision making completed in various areas, such as medical and healthcare, business, management and government. To handle big data more efficiently, new research paradigm has been engaged but the ways of thinking about big data call for further long-term innovative pursuits. Fuzzy sets have been employed for big data processing due to their abilities to represent and quantify aspects of uncertainty. Several innovative approaches within the framework of Granular Computing have been proposed. To summarize the current contributions and present an outlook of further developments, this overview addresses three aspects: (1) We review the recent studies from two distinct views. The first point of view focuses on what types of fuzzy set techniques have been adopted. It identifies clear trends as to the usage of fuzzy sets in big data processing. Another viewpoint focuses on the explanation of the benefits of fuzzy sets in big data problems. We analyze when and why fuzzy sets work in these problems. (2) We present a critical review of the existing problems and discuss the current challenges of big data, which could be potentially and partially solved in the framework of fuzzy sets. (3) Based on some principles, we infer the possible trends of using fuzzy sets in big data processing. We stress that some more sophisticated augmentations of fuzzy sets and their integrations with other tools could offer a novel promising processing environment.}
}
@article{YUE2023940,
title = {A guidebook of spatial transcriptomic technologies, data resources and analysis approaches},
journal = {Computational and Structural Biotechnology Journal},
volume = {21},
pages = {940-955},
year = {2023},
issn = {2001-0370},
doi = {https://doi.org/10.1016/j.csbj.2023.01.016},
url = {https://www.sciencedirect.com/science/article/pii/S2001037023000156},
author = {Liangchen Yue and Feng Liu and Jiongsong Hu and Pin Yang and Yuxiang Wang and Junguo Dong and Wenjie Shu and Xingxu Huang and Shengqi Wang},
keywords = {Spatial transcriptomic technologies},
abstract = {Advances in transcriptomic technologies have deepened our understanding of the cellular gene expression programs of multicellular organisms and provided a theoretical basis for disease diagnosis and therapy. However, both bulk and single-cell RNA sequencing approaches lose the spatial context of cells within the tissue microenvironment, and the development of spatial transcriptomics has made overall bias-free access to both transcriptional information and spatial information possible. Here, we elaborate development of spatial transcriptomic technologies to help researchers select the best-suited technology for their goals and integrate the vast amounts of data to facilitate data accessibility and availability. Then, we marshal various computational approaches to analyze spatial transcriptomic data for various purposes and describe the spatial multimodal omics and its potential for application in tumor tissue. Finally, we provide a detailed discussion and outlook of the spatial transcriptomic technologies, data resources and analysis approaches to guide current and future research on spatial transcriptomics.}
}
@article{CLARKE2009460,
title = {The mediating effects of coping strategies in the relationship between automatic negative thoughts and depression in a clinical sample of diabetes patients},
journal = {Personality and Individual Differences},
volume = {46},
number = {4},
pages = {460-464},
year = {2009},
issn = {0191-8869},
doi = {https://doi.org/10.1016/j.paid.2008.11.014},
url = {https://www.sciencedirect.com/science/article/pii/S0191886908004285},
author = {Dave Clarke and Tanya Goosen},
keywords = {Automatic thoughts, Cognitive behaviour therapy, Coping, Depression, Diabetes},
abstract = {High levels of depression have been found among diabetes patients, but few studies have examined the influence of coping strategies on the relationship between diabetics’ negative thoughts and their depression. The purpose of the study was to investigate the effects of coping strategies as mediators in the path from automatic negative thoughts to depression. A questionnaire containing the Automatic Thoughts Questionnaire, the Ways of Coping Checklist, a depression inventory and demographic questions was completed by 57 male and 57 female New Zealand diabetic patients, aged 28–88 years (median=60.5, mean=59.3, SD=14.6). Automatic negative thoughts, emotion-focused coping and depression, but not problem-focused coping, were significantly correlated, after controlling for relevant demographic and diabetes variables. Hierarchical linear regression analysis of data showed that emotion-focused coping functioned as a partial mediator between negative thoughts and depression. Cognitive therapy was suggested to control both automatic negative thoughts and emotion-focused coping behaviours of self-blame, wishful thinking and avoidance.}
}
@article{CRESPO201916,
title = {General solution procedures to compute the stored energy density of conservative solids directly from experimental data},
journal = {International Journal of Engineering Science},
volume = {141},
pages = {16-34},
year = {2019},
issn = {0020-7225},
doi = {https://doi.org/10.1016/j.ijengsci.2019.05.013},
url = {https://www.sciencedirect.com/science/article/pii/S0020722517327635},
author = {José Crespo and Francisco J. Montáns},
keywords = {Hyperelasticity, Soft materials, Classical invariants, Data-driven constitutive modelling},
abstract = {Energy-conservative, hyperelastic solids assume the existence of a stored energy density which relates stresses and strains for any deformation state. The usual approach to model such materials is to impose an analytical expression of the stored energy function as a function of some invariants and material parameters. These material parameters are best-fitted to available experimental data. This approach is good for analytical derivations but less optimal for data-driven computational approaches and for accurate and efficient finite element analyses. We show in this paper that the stored energy solution of a solid may be accurately obtained in a general case from suitable numerical procedures, regardless of the invariants being use, without using material parameters nor fitting any assumed analytical form. We explain two general, simple, computational procedures to solve the problem. The numerically computed stored energies may be used in general-purpose finite element programs, yielding more general procedures that have an efficiency equivalent to that of the classical approach, which uses pre-defined analytical “models” and fitting parameters.}
}
@article{SOLEIMANIJAVID2024113958,
title = {Challenges and opportunities of occupant-centric building controls in real-world implementation: A critical review},
journal = {Energy and Buildings},
volume = {308},
pages = {113958},
year = {2024},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2024.113958},
url = {https://www.sciencedirect.com/science/article/pii/S0378778824000744},
author = {Atiye Soleimanijavid and Iason Konstantzos and Xiaoqi Liu},
keywords = {Occupant-centric control, Comfort, Sensing, Controls, Learning, Energy efficiency, Smart buildings},
abstract = {Over the past few decades, attention in buildings’ design and operation has gradually shifted from promoting only energy efficiency objectives to also addressing human comfort and well-being. Researchers have developed a wide range of control algorithms ranging from rule-based controls to complex learning approaches that can fully capture occupants’ personalized preferences in smart buildings. This direction of occupant-centric building controls can bridge the gap between occupants’ satisfaction and sustainability objectives. However, most of these promising technologies have not yet found their way into real-world applications. This study will perform a critical review on occupant-centric thermal and lighting control studies aiming to (i) analyze the strengths and weaknesses of different approaches; (ii) identify the requirements for these techniques to be implemented in real-world systems; and (iii) propose new research directions that will promote the usability of such controls and will be a catalyst towards their wide adoption. Computational complexity, integration with Building Automation Systems (BAS), data availability and data quality, scalability, and the lack of more research featuring actual building implementation emerge as critical barriers. Addressing these challenges is imperative for the successful deployment of occupant-centric controls in real-world applications.}
}
@article{AUGSBURGER2009270,
title = {Methodologies to assess blood flow in cerebral aneurysms: Current state of research and perspectives},
journal = {Journal of Neuroradiology},
volume = {36},
number = {5},
pages = {270-277},
year = {2009},
issn = {0150-9861},
doi = {https://doi.org/10.1016/j.neurad.2009.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S0150986109000479},
author = {L. Augsburger and P. Reymond and E. Fonck and Z. Kulcsar and M. Farhat and M. Ohta and N. Stergiopulos and D. {A. Rüfenacht}},
keywords = {Cerebral aneurysms, Blood flow assessment, Particle image velocimetry, Computational fluid dynamics},
abstract = {Summary
With intracranial aneurysms disease bringing a weakened arterial wall segment to initiate, grow and potentially rupture an aneurysm, current understanding of vessel wall biology perceives the disease to follow the path of a dynamic evolution and increasingly recognizes blood flow as being one of the main stakeholders driving the process. Although currently mostly morphological information is used to decide on whether or not to treat a yet unruptured aneurysm, among other factors, knowledge of blood flow parameters may provide an advanced understanding of the mechanisms leading to further aneurismal growth and potential rupture. Flow patterns, velocities, pressure and their derived quantifications, such as shear and vorticity, are today accessible by direct measurements or can be calculated through computation. This paper reviews and puts into perspective current experimental methodologies and numerical approaches available for such purposes. In our view, the combination of current medical imaging standards, numerical simulation methods and endovascular treatment methods allow for thinking that flow conditions govern more than any other factor fate and treatment in cerebral aneurysms. Approaching aneurysms from this perspective improves understanding, and while requiring a personalized aneurysm management by flow assessment and flow correction, if indicated.}
}
@article{ZHANG2025100978,
title = {The paradox of self-efficacy and technological dependence: Unraveling generative AI's impact on university students' task completion},
journal = {The Internet and Higher Education},
volume = {65},
pages = {100978},
year = {2025},
issn = {1096-7516},
doi = {https://doi.org/10.1016/j.iheduc.2024.100978},
url = {https://www.sciencedirect.com/science/article/pii/S109675162400040X},
author = {Ling Zhang and Junzhou Xu},
keywords = {Generative AI, Technological dependence, Self-efficacy, Learning efficiency, Educational application, Paradoxical relationship},
abstract = {In the era of proliferating artificial intelligence (AI) technology, generative AI is reshaping educational landscapes, prompting a critical examination of its influence on students' learning processes and their self-efficacy amid concerns over growing technological dependence. This study investigates the nuanced relationship between generative AI use and university students' self-efficacy and technological dependence, illuminating the underlying paradoxes and implications for inclusive education practices. Through a survey of 348 university students, with 200 valid responses analyzed, we uncover the direct and indirect impacts of generative AI usage frequency on AI dependence. Our findings reveal a paradoxical effect: enhanced AI usage significantly amplifies students' confidence and efficiency in learning, yet simultaneously intensifies their dependence on AI. This dual impact both supports and complicates the incorporation of AI technologies into educational settings, underscoring the need for a balanced approach to leveraging AI in teaching and learning. Our study underscores the critical importance of a nuanced understanding of AI's role in education. It highlights the necessity of crafting an educational landscape where technology augments learning processes without compromising independent learning capabilities. By navigating the complex interplay between technological advancement and educational inclusivity, our findings guide the development of AI-assisted learning environments that are not only effective but also equitable and accessible.}
}
@article{BRIERLEY2021107870,
title = {The dark art of interpretation in geomorphology},
journal = {Geomorphology},
volume = {390},
pages = {107870},
year = {2021},
issn = {0169-555X},
doi = {https://doi.org/10.1016/j.geomorph.2021.107870},
url = {https://www.sciencedirect.com/science/article/pii/S0169555X21002786},
author = {Gary Brierley and Kirstie Fryirs and Helen Reid and Richard Williams},
keywords = {Landform, Landscape, Explanation, Prediction, Big Data, Fieldwork, Modelling},
abstract = {The process of interpretation, and the ways in which knowledge builds upon interpretations, has profound implications in scientific and managerial terms. Despite the significance of these issues, geomorphologists typically give scant regard to such deliberations. Geomorphology is not a linear, cause-and-effect science. Inherent complexities and uncertainties prompt perceptions of the process of interpretation in geomorphology as a frustrating form of witchcraft or wizardry — a dark art. Alternatively, acknowledging such challenges recognises the fun to be had in puzzle-solving encounters that apply abductive reasoning to make sense of physical landscapes, seeking to generate knowledge with a reliable evidence base. Carefully crafted approaches to interpretation relate generalised understandings derived from analysis of remotely sensed data with field observations/measurements and local knowledge to support appropriately contextualised place-based applications. In this paper we develop a cognitive approach (Describe-Explain-Predict) to interpret landscapes. Explanation builds upon meaningful description, thereby supporting reliable predictions, in a multiple lines of evidence approach. Interpretation transforms data into knowledge to provide evidence that supports a particular argument. Examples from fluvial geomorphology demonstrate the data-interpretation-knowledge sequence used to analyse river character, behaviour and evolution. Although Big Data and machine learning applications present enormous potential to transform geomorphology into a data-rich, increasingly predictive science, we outline inherent dangers in allowing prescriptive and synthetic tools to do the thinking, as interpreting local differences is an important element of geomorphic enquiry.}
}
@article{CHEN1998541,
title = {Toward a better understanding of idea processors},
journal = {Information and Software Technology},
volume = {40},
number = {10},
pages = {541-553},
year = {1998},
issn = {0950-5849},
doi = {https://doi.org/10.1016/S0950-5849(98)00080-9},
url = {https://www.sciencedirect.com/science/article/pii/S0950584998000809},
author = {Z. Chen},
keywords = {Artificial intelligence, Brainstorming, Computational creativity, Idea processors, Creativity support systems},
abstract = {Idea processors, as a kind of software widely used in the business world, have not received much attention from academia. In this paper we provide an overview of the current status of idea processors. We start from the foundations of idea processors, pointing out their roots in brainstorming techniques. By examining several experimental systems and commercial products, we further discuss how idea processors work, their nature, and their typical architecture. We also summarize some research work related to idea processors, as well as relationships between idea processors and studies of computational creativity in artificial intelligence. Other related issues, such as group decision support systems and evaluation methods, are also briefly examined.}
}
@article{COWLEY2021101364,
title = {Reading: skilled linguistic action},
journal = {Language Sciences},
volume = {84},
pages = {101364},
year = {2021},
note = {A Dialogue between Distributed Language and Reading Disciplines},
issn = {0388-0001},
doi = {https://doi.org/10.1016/j.langsci.2021.101364},
url = {https://www.sciencedirect.com/science/article/pii/S0388000121000103},
author = {Stephen J. Cowley},
keywords = {Reading, Distributed language, Embodied cognitive science, Languaging, Literacy, Radical embodied cognitive science},
abstract = {The paper links critique of ‘inner process’ to a perspective that treats language as activity that is accomplished by living beings. The view traces reading to human ways of coordinating with ‘the seen’. Having contrasted this distributed view with organism-first alternatives, I use a case study of reports to sketch how readers engage with written materials to both select details and project an imagined ‘source’ (e.g. a meaning, author or intention). Far from using inner process (‘decoding’) readers coordinate with a field of patternings. Where skilled, they use recollecting to link looking, silent thinking, expectations and strategic moves. Using judgements, they transform what they observe by setting off experience. I thus build on Wittgenstein's critique of inner process while also endorsing Trybulec’s (2019) radicalization of his view. To avoid treating the sense of ‘written words’ as subjective, the material aspect of patternings is taken to index outward criteria (roughly, standards of judgement). In seeking to replace theories that presuppose ‘text’, I stress how patternings invite directed sensorimotor activity by an intelligent person. Indeed, since persons learn to see wordings (or take a language stance) arrangements of patternings act as marks, ‘symbols’ and aggregations that set off recollection, judgements and iterated action. Skilled readers can use re-reading, the already read etc. to modulate ways of attending. Readers link the said, hints, recollections and ways of actualizing movements to grant reading experience a specific sense. By considering how outer criteria are evoked, reading is traced back to skilled linguistic action.}
}
@article{SUZEN2020726,
title = {Automatic short answer grading and feedback using text mining methods},
journal = {Procedia Computer Science},
volume = {169},
pages = {726-743},
year = {2020},
note = {Postproceedings of the 10th Annual International Conference on Biologically Inspired Cognitive Architectures, BICA 2019 (Tenth Annual Meeting of the BICA Society), held August 15-19, 2019 in Seattle, Washington, USA},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.02.171},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920302945},
author = {Neslihan Süzen and Alexander N. Gorban and Jeremy Levesley and Evgeny M. Mirkes},
keywords = {Natural Language Processing, Information Extraction, Automatic Grading, Machine Learning, Text Mining, Similarity, Clustering, k-means},
abstract = {Automatic grading is not a new approach but the need to adapt the latest technology to automatic grading has become very important. As the technology has rapidly became more powerful on scoring exams and essays, especially from the 1990s onwards, partially or wholly automated grading systems using computational methods have evolved and have become a major area of research. In particular, the demand of scoring of natural language responses has created a need for tools that can be applied to automatically grade these responses. In this paper, we focus on the concept of automatic grading of short answer questions such as are typical in the UK GCSE system, and providing useful feedback on their answers to students. We present experimental results on a dataset provided from the introductory computer science class in the University of North Texas. We first apply standard data mining techniques to the corpus of student answers for the purpose of measuring similarity between the student answers and the model answer. This is based on the number of common words. We then evaluate the relation between these similarities and marks awarded by scorers. We consider an approach that groups student answers into clusters. Each cluster would be awarded the same mark, and the same feedback given to each answer in a cluster. In this manner, we demonstrate that clusters indicate the groups of students who are awarded the same or the similar scores. Words in each cluster are compared to show that clusters are constructed based on how many and which words of the model answer have been used. The main novelty in this paper is that we design a model to predict marks based on the similarities between the student answers and the model answer. We argue that computational methods be used to enhance the reliability of human scoring, and not replace it. Humans are required to calibrate the system, and to deal with situations that are challenging. Computational methods can provide insight into which student answers will be found challenging and thus be a place human judgement is required.}
}
@article{WANG2023170277,
title = {MLKCA-Unet: Multiscale large-kernel convolution and attention in Unet for spine MRI segmentation},
journal = {Optik},
volume = {272},
pages = {170277},
year = {2023},
issn = {0030-4026},
doi = {https://doi.org/10.1016/j.ijleo.2022.170277},
url = {https://www.sciencedirect.com/science/article/pii/S0030402622015352},
author = {Biao Wang and Juan Qin and Lianrong Lv and Mengdan Cheng and Lei Li and Dan Xia and Shike Wang},
keywords = {Deep learning, Spine segmentation, Receptive fields, Multiscale large-kernel convolution, Attention},
abstract = {Medical image segmentation plays a key role in the diagnosis of spinal diseases. Unet has become a universal structure for image segmentation because of its unique skip connection structure in recent years. However, since Unet uses small-kernel convolution, the relationship between remote features is difficult to obtain due to the small receptive fields, and the key information cannot be highlighted, resulting in insufficient edge information. To overcome these problems, this paper proposes multiscale large-kernel convolution Unet (MLKCA-Unet), which develops MLKC block for effective feature extraction. Large-kernel convolution with different convolution kernels is used according to the feature map. For large feature maps, smaller large- kernel convolution is used, and for small feature maps, larger large-kernel convolution is used. All large-kernel convolution can be reduced the dimension by 1 × 1 convolution kernel. This method has a significant reduction in computation. By paralleling each large kernel convolution branch with the 3 × 3 convolution branch, it helps to capture detailed information. At the same time, an attention mechanism is added to the network to emphasize rich feature areas and enhance useful information. Finally, various indicators are employed to evaluate the network’s accuracy, similarity and speed, including IOU, DSC, TPR, PPV, and ET. The published spinesagt2wdataset3 spinal MRI image dataset is adopted in the experiment. The IOU, DSC, TPR, PPV, and ET on the test set are 0.8302, 0.9017, 0.9000, 0.9051 and 70 s/epoch respectively. The experimental result shows that MLKCA-Unet demonstrates superior segmentation performance and robustness, which can be well extended to other medical image segmentation.}
}
@article{BAKHTAVAR2020122886,
title = {Assessment of renewable energy-based strategies for net-zero energy communities: A planning model using multi-objective goal programming},
journal = {Journal of Cleaner Production},
volume = {272},
pages = {122886},
year = {2020},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2020.122886},
url = {https://www.sciencedirect.com/science/article/pii/S0959652620329310},
author = {Ezzeddin Bakhtavar and Tharindu Prabatha and Hirushie Karunathilake and Rehan Sadiq and Kasun Hewage},
keywords = {Community energy planning, Renewable energy, Life cycle assessment, Multi-objective optimi, z, ation, Grey numbers, Goal programming},
abstract = {Planning decentralised community-level hybrid energy systems has emerged as a solution to the various environmental and economic issues associated with conventional centralised energy supply systems. However, the optimal planning of community energy systems is a challenging issue due to the complexities, uncertainties, conflicting objectives, and high computational times in analysis. This study introduces a new multi-objective model based on weighted goal programming and grey pairwise comparison to assess renewable energy-based strategies in the case of net-zero energy communities. The problem was formulated to determine the optimal energy mix based on minimization of life cycle impacts and costs and maximization of renewable contributions and operational energy savings. To this end, binary integer and continues variables were applied on the code developed in a CPLEX environment. A pairwise comparison based on grey numbers was used to find the impacts of the goals in the objective function of the model under uncertainty. In addition to the grey-based weighting scenario, different weighting scenarios were employed to consider the importance of all goals on the system. These weighting scenarios were used to investigate the effects of changing decision priorities on the outcomes and on stakeholder interests at different levels. The developed goal-programming model was applied to the data of a case example and solved based on the weighting scenarios. Results indicated that the model is capable of finding the best possible strategies with the lowest total undesirable deviations from the desired levels of the goals compared to the literature of the decision-making techniques. The integration of maximum renewable energy (RE) supply in the energy mix with the locally available energy resources can deliver considerable benefits in terms of energy supply cost reduction as well as in mitigating life cycle environmental impacts. When environmental goals are prioritized, integrating low emissions RE as much as possible and excluding waste-to-energy technologies makes best sense, while under a pro-economic perspective, solar integration is comparatively discouraged. The findings of the study are expected to assist community developers and other decision makers involved in regional energy planning. The developed method will also be of use for those who are interested in the use of goal programming to solve complex planning issues involving numerous uncertain parameters.}
}
@article{BLEMKER2023111745,
title = {In vivo imaging of skeletal muscle form and function: 50 years of insight},
journal = {Journal of Biomechanics},
volume = {158},
pages = {111745},
year = {2023},
issn = {0021-9290},
doi = {https://doi.org/10.1016/j.jbiomech.2023.111745},
url = {https://www.sciencedirect.com/science/article/pii/S0021929023003159},
author = {Silvia S. Blemker},
keywords = {Skeletal muscle, Imaging, In vivo},
abstract = {Skeletal muscle form and function has fascinated scientists for centuries. Our understanding of muscle function has long been driven by advancements in imaging techniques. For example, the sliding filament theory of muscle, which is now widely leveraged in biomechanics research, stemmed from observations made possible by scanning electron microscopy. Over the last 50 years, advancing in medical imaging, combined with ingenuity and creativity of biomechanists, have provide a wealth of new and important insights into in vivo human muscle function. Incorporation of in vivo imaging has also advanced computational modeling and allowed our research to have an impact in many clinical populations. While this review does not provide a comprehensive or meta-analysis of the all the in vivo muscle imaging work over the last five decades, it provides a narrative about the past, present, and future of in vivo muscle imaging.}
}
@article{MILLER200021,
title = {Representational Tools and Conceptual Change: The Young Scientist's Tool Kit},
journal = {Journal of Applied Developmental Psychology},
volume = {21},
number = {1},
pages = {21-25},
year = {2000},
issn = {0193-3973},
doi = {https://doi.org/10.1016/S0193-3973(99)00047-7},
url = {https://www.sciencedirect.com/science/article/pii/S0193397399000477},
author = {Kevin F Miller},
abstract = {We interpret the world and its regularities through representations and procedures that are a complex mélange of formal experience, rules of thumb, and naive concepts that precede formal education. These representational tools give us the language in which we can think about science. Three propositions are argued: (a) that such tools are fundamental to scientific reasoning and science education; (b) that cognitive science has a great deal to say about how cognitive tools affect thinking and conceptual change, particularly how the representations intrinsic to ordinary language relate to the symbol systems of formal science and mathematics; and finally, (c) that cognitive science may play a role in developing representational tools that make scientific information more accessible.}
}
@article{YENI2025111157,
title = {Revealing risk mitigation strategies for supply chain resilience in aquaculture industry through a methodology equipped with lean tools and stochastic programming},
journal = {Computers & Industrial Engineering},
volume = {205},
pages = {111157},
year = {2025},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2025.111157},
url = {https://www.sciencedirect.com/science/article/pii/S0360835225003031},
author = {Fatma Betül Yeni and Beren Gürsoy Yılmaz and Gökhan Özçelik and Ömer Faruk Yılmaz and Ozan Kalaycıoğlu},
keywords = {Lean tools, Two-stage stochastic programming model, Aquaculture industry, Ripple effect, Disruption},
abstract = {With a notable focus on seafood, especially salmon, which is crucial for global food security, substantial investments have fueled the growth of Turkish salmon farming in the Black Sea region. Despite being profitable, the industry faces challenges, including supply chain vulnerabilities to internal and external disruptions, leading to a ripple effect. One of the most significant challenges in the sector is fish escapes, which directly impact lead time and customer satisfaction levels. By specifically focusing on undercurrent and storm factors, this study proposes a comprehensive methodology, adopting a continuous improvement cycle to manage this challenge. This methodology involves a novel scenario-based two-stage stochastic programming model with the objective of minimizing the expected overall cost. The model is formulated for the transportation problem, addressing fish escapes within the aquaculture industry to establish a resilient supply chain structure. It also explores the comprehensiveness of lean implementation associated with various reliability levels, representing the decision-maker’s risk propensity. Moreover, a design of experiment (DoE) setting is established to scrutinize the impact of controlled factors and their interactions on the outcomes. After conducting computational analysis, considering the impact of factors, lean maturity levels, and the comprehensiveness of lean implementation on the results, valuable managerial insights are provided within the context of risk mitigation strategies. The findings indicate that in the event of severe disruptions, significant improvements in risk mitigation can be achieved through the continuous improvement-based methodology, primarily driven by lean tools.}
}
@article{ABRAMSKY200637,
title = {What are the Fundamental Structures of Concurrency?: We still don't know!},
journal = {Electronic Notes in Theoretical Computer Science},
volume = {162},
pages = {37-41},
year = {2006},
note = {Proceedings of the Workshop "Essays on Algebraic Process Calculi" (APC 25)},
issn = {1571-0661},
doi = {https://doi.org/10.1016/j.entcs.2005.12.075},
url = {https://www.sciencedirect.com/science/article/pii/S1571066106004105},
author = {Samson Abramsky},
keywords = {Concurrency, process algebra, Petri nets, geometry, quantum information and computation},
abstract = {Process algebra has been successful in many ways; but we don't yet see the lineaments of a fundamental theory. Some fleeting glimpses are sought from Petri Nets, physics and geometry.}
}
@article{JEREMIAH2025103529,
title = {The human-AI dyad: Navigating the new frontier of entrepreneurial discourse},
journal = {Futures},
volume = {166},
pages = {103529},
year = {2025},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2024.103529},
url = {https://www.sciencedirect.com/science/article/pii/S001632872400212X},
author = {Faith Jeremiah},
keywords = {AI, Entrepreneurship, Human, New technologies, Self-identity},
abstract = {The rapid progression of Artificial Intelligence (AI) is further solidifying the importance of redefining entrepreneurship and reshaping how ventures and innovations are conceived, developed, and managed. This paper investigates how AI is transforming entrepreneurship by reshaping traditional business paradigms and entrepreneurs’ roles. It examines AI's influence on entrepreneurial decision-making, innovation, and identity. Through a comprehensive literature review and thematic analysis of previously published data, the research identifies emergent themes in AI's integration into entrepreneurial discourse. The findings indicate that AI enhances operational efficiency and decision-making but challenges traditional notions of entrepreneurial identity and creativity. Furthermore, entrepreneurs increasingly depend on AI for data-driven insights, strategic foresight, and personalised customer interactions, reshaping business strategies and competitive landscapes. This article emphasises the importance of AI literacy and adaptive strategies for entrepreneurs to leverage the human-AI dyad effectively while maintaining values and ethics. It also highlights the significance of concentrating research efforts on entrepreneurs as they are the very cohort to shape new norms and pioneer new business models and innovations. Future research should explore AI's long-term impacts on entrepreneurial ecosystems, including psychological, ethical, and socio-cultural dimensions, with comparative studies across industries and regions providing further insights.}
}
@article{KNUUTTILA201476,
title = {Varieties of noise: Analogical reasoning in synthetic biology},
journal = {Studies in History and Philosophy of Science Part A},
volume = {48},
pages = {76-88},
year = {2014},
issn = {0039-3681},
doi = {https://doi.org/10.1016/j.shpsa.2014.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S0039368114000612},
author = {Tarja Knuuttila and Andrea Loettgers},
keywords = {Synthetic biology, Interdisciplinarity, Analogical reasoning, Engineering sciences, Complex systems, Noise},
abstract = {The picture of synthetic biology as a kind of engineering science has largely created the public understanding of this novel field, covering both its promises and risks. In this paper, we will argue that the actual situation is more nuanced and complex. Synthetic biology is a highly interdisciplinary field of research located at the interface of physics, chemistry, biology, and computational science. All of these fields provide concepts, metaphors, mathematical tools, and models, which are typically utilized by synthetic biologists by drawing analogies between the different fields of inquiry. We will study analogical reasoning in synthetic biology through the emergence of the functional meaning of noise, which marks an important shift in how engineering concepts are employed in this field. The notion of noise serves also to highlight the differences between the two branches of synthetic biology: the basic science-oriented branch and the engineering-oriented branch, which differ from each other in the way they draw analogies to various other fields of study. Moreover, we show that fixing the mapping between a source domain and the target domain seems not to be the goal of analogical reasoning in actual scientific practice.}
}
@article{FURTADO2024100086,
title = {A task-oriented framework for generative AI in design},
journal = {Journal of Creativity},
volume = {34},
number = {2},
pages = {100086},
year = {2024},
issn = {2713-3745},
doi = {https://doi.org/10.1016/j.yjoc.2024.100086},
url = {https://www.sciencedirect.com/science/article/pii/S2713374524000128},
author = {Lara Sucupira Furtado and Jorge Barbosa Soares and Vasco Furtado},
keywords = {Generative artificial intelligence, Product, Creative computing, Transformational Creativity},
abstract = {The intersection of Artificial Intelligence and Design disciplines such as Architecture, Urban Planning, Engineering and Product Design has been a longstanding pursuit, with Generative AI (GAI) ushering in a new era of possibilities. The research presented here explores how GAI can enhance creativity and assist Design practitioners with tasks to create products such as, but not limited to, renderings, concepts, construction techniques, materials, data analytics or maps. We apply a framework of combinational, exploratory and transformational creativity to organize how recent advancements in GAI can support each creative category. We propose a conceptual framework of GAI towards transformational creativity, and identify real-world examples to demonstrate GAI's impact, such as transforming sketches into detailed renders, facilitating real-time 3D model generation, predicting trends through analytics and creating images or reports via text prompts. Our work envisions a future where GAI becomes a real-time collaborator to complete certain automated tasks while liberating Designers to focus on transformational innovation.}
}
@article{CARAMIA2022100040,
title = {Sustainable two stage supply chain management: A quadratic optimization approach with a quadratic constraint},
journal = {EURO Journal on Computational Optimization},
volume = {10},
pages = {100040},
year = {2022},
issn = {2192-4406},
doi = {https://doi.org/10.1016/j.ejco.2022.100040},
url = {https://www.sciencedirect.com/science/article/pii/S2192440622000168},
author = {Massimiliano Caramia and Giuseppe Stecca},
keywords = {Supply chain optimization, Green management, Successive linear approximations},
abstract = {Designing a supply chain to comply with environmental policy requires awareness of how work and/or production methods impact the environment and what needs to be done to reduce those environmental impacts and make the company more sustainable. This is a dynamic process that occurs at both the strategic and operational levels. However, being environmentally friendly does not necessarily mean improving the efficiency of the system at the same time. Therefore, when allocating a production budget in a supply chain that implements the green paradigm, it is necessary to figure out how to properly recover costs in order to improve both sustainability and routine operations, offsetting the negative environmental impact of logistics and production without compromising the efficiency of the processes to be executed. In this paper, we study the latter problem in detail, focusing on the CO2 emissions generated by the transportation from suppliers to production sites, and by the production activities carried out in each plant. We do this using a novel mathematical model that has a quadratic objective function and all linear constraints except one, which is also quadratic, and models the constraint on the budget that can be used for green investments caused by the increasing internal complexity created by large production flows in the production nodes of the supply network. To solve this model, we propose a multistart algorithm based on successive linear approximations. Computational results show the effectiveness of our proposal.}
}
@article{GAO2022112486,
title = {Regarding the shallow water in an ocean via a Whitham-Broer-Kaup-like system: hetero-Bäcklund transformations, bilinear forms and M solitons},
journal = {Chaos, Solitons & Fractals},
volume = {162},
pages = {112486},
year = {2022},
issn = {0960-0779},
doi = {https://doi.org/10.1016/j.chaos.2022.112486},
url = {https://www.sciencedirect.com/science/article/pii/S0960077922006944},
author = {Xin-Yi Gao and Yong-Jiang Guo and Wen-Rui Shan},
keywords = {Ocean, Shallow water, Whitham-Broer-Kaup-like system, Hetero-Bäcklund transformations, Bilinear forms,  solitons, Symbolic computation},
abstract = {Considering the water waves, people have investigated many systems. In this paper, what we study is a Whitham-Broer-Kaup-like system for the dispersive long waves in the shallow water in an ocean. With respect to the water-wave horizontal velocity and deviation height from the equilibrium of the water, we construct (A) two branches of the hetero-Bäcklund transformations, from that system to a known constant-coefficient nonlinear dispersive-wave system, (B) two branches of the bilinear forms and (C) two branches of the M-soliton solutions, with M as a positive integer. Results rely upon the oceanic shallow-water coefficients in that system.}
}
@article{SHAHRYARI2021101395,
title = {Energy and task completion time trade-off for task offloading in fog-enabled IoT networks},
journal = {Pervasive and Mobile Computing},
volume = {74},
pages = {101395},
year = {2021},
issn = {1574-1192},
doi = {https://doi.org/10.1016/j.pmcj.2021.101395},
url = {https://www.sciencedirect.com/science/article/pii/S1574119221000535},
author = {Om-Kolsoom Shahryari and Hossein Pedram and Vahid Khajehvand and Mehdi Dehghan TakhtFooladi},
keywords = {Internet of Things, Fog computing, Task offloading, Genetic algorithm, Particle swarm optimization, Resource allocation},
abstract = {In order to improve the quality of experience in executing computation-intensive tasks of real-time IoT applications in a fog-enabled IoT network, resource-constrained IoT devices can offload the tasks to resource-rich nearby fog nodes. It causes a reduction in energy consumption compared with local processing, although it extends task completion time due to communication latency. In this paper, we propose a task offloading scheme that optimizes task offloading decision, fog node selection, and computation resource allocation, investigating the trade-off between task completion time and energy consumption. Weighting coefficients of time and energy consumption are determined based on specific demands of the user and residual energy of devices’ battery. Accordingly, we formulate the task offloading problem as a mixed-integer nonlinear program (MINLP), which is NP-hard. A sub-optimal algorithm based on the hybrid of genetic algorithm and particle swarm optimization is designed to solve the formulated problem. Extensive simulations prove the convergence of the proposed algorithm and its superior performance in comparison with baseline schemes.}
}
@article{MORIN1992371,
title = {From the concept of system to the paradigm of complexity},
journal = {Journal of Social and Evolutionary Systems},
volume = {15},
number = {4},
pages = {371-385},
year = {1992},
issn = {1061-7361},
doi = {https://doi.org/10.1016/1061-7361(92)90024-8},
url = {https://www.sciencedirect.com/science/article/pii/1061736192900248},
author = {Edgar Morin},
abstract = {This paper is an overview of the author's ongoing reflections on the need for a new paradigm of complexity capable of informing all theories, whatever their field of application or the phenomena in question. Beginning with a critique of General System Theory and the principle of holism with which it is associated, the author suggests that contemporary advances in our knowledge of organization call for a radical reformation in our organization of knowledge. This reformation involves the mobilization of recursive thinking, which is to say a manner of thinking capable of establishing a dynamic and generative feedback loop between terms or concepts (such as whole and part, order and disorder, observer and observed, system and ecosystem, etc.) that remain both complementary and antagonistic. The paradigm of complexity thus stands as a bold challenge to the fragmentary and reductionistic spirit that continues to dominate the scientific enterprise.}
}
@article{PRADEEP2025101194,
title = {Sustainable solutions in the era of water scarcity: Multi-basin solar still innovations},
journal = {Desalination and Water Treatment},
volume = {322},
pages = {101194},
year = {2025},
issn = {1944-3986},
doi = {https://doi.org/10.1016/j.dwt.2025.101194},
url = {https://www.sciencedirect.com/science/article/pii/S1944398625002103},
author = {B Pradeep and S Joe Patrick Gnanaraj and R Samuel Sanjay Raja},
keywords = {Solar desalination, Stepped basin configuration, Thermal efficiency, Compartmental basin, Corrugated basin},
abstract = {This study addresses the urgent challenge of clean water scarcity in the era of sustainability by investigating advanced multi-basin solar stills with stepped configurations designed to enhance water yield and thermal efficiency. Traditional single basin designs often fall short in productivity and thermal performance, which this research overcomes through geometric innovations such as fins, corrugated surfaces, and compartmental structures. Six basin configurations were tested, revealing that the compartmental basin with fins achieved the highest productivity of 27.8 liters over 12 hours, a 124 % improvement compared to the plain single basin and 26 % higher yield than the corrugated basin with fins. Adding fins to corrugated and compartmental basins further increased productivity by 40 % and 41 %, respectively. These advanced designs align with the sustainability goals of this era, demonstrating significant cost-effectiveness and environmental benefits, including a 70 % energy utilization efficiency and carbon emissions reduction of up to 320 kg CO₂/year. The findings emphasize the transformative potential of innovative basin configurations in revolutionizing solar desalination systems to meet the pressing water needs of a growing global population while adhering to sustainable development objectives.}
}
@article{ANTONIETTI20082172,
title = {Undergraduates’ metacognitive knowledge about the psychological effects of different kinds of computer-supported instructional tools},
journal = {Computers in Human Behavior},
volume = {24},
number = {5},
pages = {2172-2198},
year = {2008},
note = {Including the Special Issue: Internet Empowerment},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2007.10.004},
url = {https://www.sciencedirect.com/science/article/pii/S0747563207001598},
author = {Alessandro Antonietti and Barbara Colombo and Yuri Lozotsev},
keywords = {Metacognition, Belief, Learning, Education, Computer},
abstract = {Literature about metacognition suggests that learners develop personal beliefs about the educational technologies that they are asked to employ and that such beliefs can influence learning outcomes. In this perspective, opinions about the psychological effects of computer-supported instructional tools were analysed by means of a questionnaire which included items about the motivational and emotional aspects of learning, the behaviour to have during the learning process, the mental abilities and the style of thinking required, and the cognitive benefits. Items were presented five times: each time they made reference to a different kind of tool (online courses, hypertexts, Web forums, multimedia presentations, and virtual simulations). The questionnaire was filled out by 99 undergraduates attending engineering courses. Results showed that students ranked the psychological effects of the computer-supported tools in a relative different order according to the kind of tool and attributed distinctive effects to each tool. Gender and expertise played a minor role in modulating undergraduates’ beliefs. Implications for instruction were discussed.}
}
@article{AUBIN2014204,
title = {“Principles of Mechanics that are Susceptible of Application to Society”: An unpublished notebook of Adolphe Quetelet at the root of his social physics},
journal = {Historia Mathematica},
volume = {41},
number = {2},
pages = {204-223},
year = {2014},
issn = {0315-0860},
doi = {https://doi.org/10.1016/j.hm.2014.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S0315086014000020},
author = {David Aubin},
keywords = {Mechanics, Sociology, Adolphe Quetelet, Astronomy, Social physics, Average man, Applications of mathematics, Analogical thinking},
abstract = {Founder of the Brussels Observatory, Adolphe Quetelet (1796–1874) is especially well known for his theory of the average man. Like the average position of a star obtained through a large quantity of observed data, the average man was, according to Quetelet, subject to fixed causal laws. Published in 1835, his book On Man: Essay of Social Physics is one of the founding works of sociology and mathematical statistics. The sources of the analogy between astronomy and social physics have been debated by historians. To shed light on this question and the conditions of application of mathematics in the 19th century, we publish for the first time a manuscript that is kept in Quetelet's papers at the Royal Academy of Belgium, and give an English translation of it.}
}
@article{GOSWAMI2024111921,
title = {Real-time evaluation of object detection models across open world scenarios},
journal = {Applied Soft Computing},
volume = {163},
pages = {111921},
year = {2024},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2024.111921},
url = {https://www.sciencedirect.com/science/article/pii/S1568494624006951},
author = {Puneet Goswami and Lakshita Aggarwal and Arun Kumar and Rahul Kanwar and Urvi Vasisht},
keywords = {Computer vision, Faster R-CNN, DETR, YOLO, Resnet 50, Resnet 101, Object detection, Model comparison, Evaluation metrices},
abstract = {Object detection models have been experiencing significant improvements over the years due to advancements in deep learning techniques, increased availability of large-scale annotated datasets, and computational resources. Different object detection models have varying levels of accuracy, speed, and robustness. With the increasing complexity and diversity of object detection models, it becomes a problem for researchers and practitioners to choose the most suitable model for their specific needs. This research paper outlines the escalating demand for robust comparison of object detection models in response to rapidly advancing technology. This evaluation helps in identifying the strengths and weaknesses of these models and selecting the most suitable one for a specific task. This highlights a significant challenge stemming from the lack of recent comparative studies on object detection models across various image qualities, object sizes, and training data sizes. The above challenges are tackled by a meticulous evaluation of three state-of-the-art object detection models: YOLO-v8, Faster R-CNN with ResNet 50 and 101 backbones, and End-to-End Object Detection Transformers (DETR) utilizing ResNet 50 and 101 backbones by employing a rigorous assessment framework encompassing mean Average Precision (mAP), accuracy, and inference speed. This study focuses on thoroughly examining how well the models perform across three different datasets: TACO, PlastOPol, and TACO 4.5. These datasets consist of open-world images captured in real-time from various locations. They include 1500, 2500, and 6500 images respectively, depicting real-world environments with varying lighting conditions and complex backgrounds. The results identify YOLOv8 as the superior model for high and medium-quality images, while Faster R-CNN performs better for low-quality images. However, DETR's accuracy falls short compared to other models. The paper fills a crucial gap in understanding model performance across varying image qualities and object sizes and helps in taking informed decisions in object detection systems.}
}
@article{CASTILLOFELISOLA2023108748,
title = {Cadabra and Python algorithms in general relativity and cosmology II: Gravitational waves},
journal = {Computer Physics Communications},
volume = {289},
pages = {108748},
year = {2023},
issn = {0010-4655},
doi = {https://doi.org/10.1016/j.cpc.2023.108748},
url = {https://www.sciencedirect.com/science/article/pii/S0010465523000930},
author = {Oscar Castillo-Felisola and Dominic T. Price and Mattia Scomparin},
keywords = {Computer algebra system, , Gravitation, Gravitational waves, Perturbative field theory, Python, Cosmology, General relativity},
abstract = {Computer Algebra Systems (CASs) like Cadabra Software play a prominent role in a wide range of research activities in physics and related fields. We show how Cadabra language is easily implemented in the well established Python programming framework, gaining excellent flexibility and customization to address the issue of tensor perturbations in General Relativity. We obtain a performing algorithm to decompose tensorial quantities up to any perturbative order of the metric. The features of our code are tested by discussing some concrete computational issues in research activities related to first/higher-order gravitational waves.}
}