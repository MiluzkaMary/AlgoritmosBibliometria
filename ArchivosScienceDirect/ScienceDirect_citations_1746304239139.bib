@incollection{BHATE2024165,
title = {Chapter 7 - Tissue schematics: Representing tissues as assemblies of neighborhoods},
editor = {Wendy J. Fantl},
booktitle = {Revealing Uncharted Biology With Single Cell Multiplex Proteomic Technologies},
publisher = {Academic Press},
pages = {165-189},
year = {2024},
isbn = {978-0-12-822209-6},
doi = {https://doi.org/10.1016/B978-0-12-822209-6.00005-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128222096000059},
author = {Salil S. Bhate and Graham L. Barlow and Garry P. Nolan},
keywords = {Cellular neighborhoods, Spatial biology, Tissue schematics, Tumor microenvironment},
abstract = {The technologies discussed in previous chapters provide an unprecedented lens on single cells, phenotypes, and their interactions. In this chapter, we discuss tissue schematics (as introduced in Bhate et al., 2021): a computational and conceptual framework for using highly multiplexed imaging data to infer and visually represent how complex, tissue-level behaviors are generated through the composition of simpler, interpretable biological processes. We focus on the practical application of tissue schematics for generating biological understanding of tissues from imaging data.}
}
@article{ZHENG2022104214,
title = {Evaluation of an automated phenotyping algorithm for rheumatoid arthritis},
journal = {Journal of Biomedical Informatics},
volume = {135},
pages = {104214},
year = {2022},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2022.104214},
url = {https://www.sciencedirect.com/science/article/pii/S1532046422002192},
author = {Henry W. Zheng and Veena K. Ranganath and Lucas C. Perry and David A. Chetrit and Karla M. Criner and Angela Q. Pham and Richard Seto and Sitaram Vangala and David A. Elashoff and Alex A.T. Bui},
keywords = {Phenotyping algorithm, Computational phenotyping, Rheumatoid arthritis, PheKB},
abstract = {To better understand the challenges of generally implementing and adapting computational phenotyping approaches, the performance of a Phenotype KnowledgeBase (PheKB) algorithm for rheumatoid arthritis (RA) was evaluated on a University of California, Los Angeles (UCLA) patient population, focusing on examining its performance on ambiguous cases. The algorithm was evaluated on a cohort of 4,766 patients, along with a chart review of 300 patients by rheumatologists against accepted diagnostic guidelines. The performance revealed low sensitivity towards specific subtypes of positive RA cases, which suggests revisions in features used for phenotyping. A close examination of select cases also indicated a significant portion of patients with missing data, drawing attention to the need to consider data integrity as an integral part of phenotyping pipelines, as well as issues around the usability of various codes for distinguishing cases. We use patterns in the PheKB algorithm’s errors to further demonstrate important considerations when designing a phenotyping algorithm.}
}
@article{CHELLAM2018928,
title = {Intrusion Detection in Computer Networks using Lazy Learning Algorithm},
journal = {Procedia Computer Science},
volume = {132},
pages = {928-936},
year = {2018},
note = {International Conference on Computational Intelligence and Data Science},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.05.108},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918308408},
author = {Aditya Chellam and Ramanathan L and Ramani S},
keywords = {Lazy Learning, Intrusion Detection System, Machine Learning, IBk, kNN},
abstract = {Intrusion Detection Systems (IDS) are used in computer networks to safeguard the integrity and confidentiality of sensitive data. In recent years, network traffic has become sizeable enough to be considered under the big data domain. Current machine learning based techniques used in IDS are largely defined on eager learning paradigms which lose performance efficiency by trying to generalize training data before receiving queries thereby incurring overheads for trivial computations. This paper, proposes the use of lazy learning methodologies to improve overall performance of IDS. A novel heuristic weight based indexing technique has been used to overcome the drawback of high search complexity inherent in lazy learning. IBk and LWL, two popular lazy learning algorithms have been compared and applied on the NSL-KDD dataset for simulating a real-world like scenario and comparing their relative performances with hw-IBk. The results of this paper clearly indicate lazy algorithms as a viable solution for real-world network intrusion detection.}
}
@article{KRISHNANUNNI2025117938,
title = {An adaptive and stability-promoting layerwise training approach for sparse deep neural network architecture},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {441},
pages = {117938},
year = {2025},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2025.117938},
url = {https://www.sciencedirect.com/science/article/pii/S0045782525002105},
author = {C.G. Krishnanunni and Tan Bui-Thanh},
keywords = {Architecture adaptation, Manifold regularization, Stability-promoting algorithm, Sequential learning},
abstract = {This work presents a two-stage adaptive framework for progressively developing deep neural network (DNN) architectures that generalize well for a given training data set. In the first stage, a layerwise training approach is adopted where a new layer is added each time and trained independently by freezing parameters in the previous layers. We impose desirable structures on the DNN by employing manifold regularization, sparsity regularization, and physics-informed terms. We introduce a ɛ−δ− stability-promoting concept as a desirable property for a learning algorithm and show that employing manifold regularization yields a ɛ−δ stability-promoting algorithm. Further, we also derive the necessary conditions for the trainability of a newly added layer and investigate the training saturation problem. In the second stage of the algorithm (post-processing), a sequence of shallow networks is employed to extract information from the residual produced in the first stage, thereby improving the prediction accuracy. Numerical investigations on prototype regression and classification problems demonstrate that the proposed approach can outperform fully connected DNNs of the same size. Moreover, by equipping the physics-informed neural network (PINN) with the proposed adaptive architecture strategy to solve partial differential equations, we numerically show that adaptive PINNs not only are superior to standard PINNs but also produce interpretable hidden layers with provable stability. We also apply our architecture design strategy to solve inverse problems governed by elliptic partial differential equations.}
}
@article{LIU2018772,
title = {Ecosystem services in life cycle assessment - Part 2: Adaptations to regional and serviceshed information},
journal = {Journal of Cleaner Production},
volume = {197},
pages = {772-780},
year = {2018},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2018.05.283},
url = {https://www.sciencedirect.com/science/article/pii/S0959652618316421},
author = {Xinyu Liu and Guy Ziv and Bhavik R. Bakshi},
keywords = {Computational structure, Life cycle assessment, Ecosystem service, Environmental sustainability},
abstract = {Regionalized life cycle assessment (LCA) is receiving more attention among LCA practitioners due to spatial variation in process efficiency of technological systems and current status of ecological systems. However, the role of ecosystem services (ES) in supporting technological activities is still ignored. Techno-Ecological Synergies in Life Cycle Assessment (TES-LCA) is a methodology that captures the interactions between and within technological and ecological systems, along a product's life cycle. It accounts for local and absolute environmental sustainability by comparing the demand and supply of ES at multiple spatial scales. To facilitate its wider adoption, the basic computational framework has been proposed in Part 1, which includes technologies and ecosystems in an integrated manner. To handle the complications induced by explicitly considering ES, which operate within servicesheds at multiple spatial scales, the TES-LCA computational structure is modified to account for spatial variation in technological and ecological systems in Part 2. The regionalized TES-LCA framework is demonstrated through an expanded case study to show its capability to capture different scenarios of regionalization, including variation in process efficiency, ecological carrying capacity (CC), characterization factors (CF), and the scales at which ES operate. The approach is then proved to be general and able to subsume existing approaches, such as regionalized LCA, GIS-LCA and recent extensions based on normalizing CF by ecological CC to calculate absolute sustainability metrics. It is recommended that the developed computational structure should be implemented in LCA software with the functionality for handling geographical information. Also, the regionalized information about ES demand and supply needs to be made available in future versions of life cycle inventory databases.}
}
@article{HADJTAIEB2014238,
title = {Ontology-based approach for measuring semantic similarity},
journal = {Engineering Applications of Artificial Intelligence},
volume = {36},
pages = {238-261},
year = {2014},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2014.07.015},
url = {https://www.sciencedirect.com/science/article/pii/S0952197614001833},
author = {Mohamed Ali {Hadj Taieb} and Mohamed {Ben Aouicha} and Abdelmajid {Ben Hamadou}},
keywords = {Semantic similarity, WordNet ontology, Taxonomic knowledge, Taxonomical parameters},
abstract = {The challenge of measuring semantic similarity between words is to find a method that can simulate the thinking process of human. The use of computers to quantify and compare semantic similarities has become an important area of research in various fields, including artificial intelligence, knowledge management, information retrieval and natural language processing. The development of efficient measures for the computation of concept similarity is fundamental for computational semantics. Several computational measures rely on knowledge resources to quantify semantic similarity, such as the WordNet « is a » taxonomy. Several of these measures are based on taxonomical parameters to achieve the best expression possible for the semantics of content. This paper presents a new measure for quantifying the degree of the semantic similarity between concepts and words based on the WordNet hierarchy and using a number of topological parameters related to the “is a” taxonomy. Our proposal combines, in a complementary way, the hyponyms and depth parameters. This measure takes the problem of fine granularity into account. It is argued, however, that WordNet sense distinctions are highly fine-grained even for humans. We, therefore, propose a new method to quantify the hyponyms subgraph of a given concept based on depth distribution. Common nouns datasets (RG65, MC30 and AG203), medical terms dataset (MED38) and verbs dataset (YP130) formed by word pairs are used in the assessment. We start by calculating semantic similarities and then compute the correlation coefficient between human judgement and computational measures. The results demonstrate that, compared to other currently available computational methods, the measure presented in this study yields into better levels of performance. Compared to several measures, it shows good accuracy covering all the pairwises of the verbs dataset YP130.}
}
@article{CLARO20121042,
title = {Assessment of 21st century ICT skills in Chile: Test design and results from high school level students},
journal = {Computers & Education},
volume = {59},
number = {3},
pages = {1042-1053},
year = {2012},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2012.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S0360131512000887},
author = {Magdalena Claro and David D. Preiss and Ernesto {San Martín} and Ignacio Jara and J. Enrique Hinostroza and Susana Valenzuela and Flavio Cortes and Miguel Nussbaum},
keywords = {ICT skills, 21st century skills, Information literacy, Computer literacy, Higher-order thinking skills, Digital divide, Rasch model},
abstract = {This paper describes a study that evaluates fifteen-year-old Chilean students Information and Communication Technology (ICT) skills. The paper presents an operational definition of ICT skills, an instrument measuring these skills as well as the students' results in the test. The definition of ICT skills used considers Chile's curricular framework, functional and cognitive skills. Specifically, ICT skills were defined as the capacity to solve problems of information, communication and knowledge in digital environments. A performance-based assessment was designed in a virtual environment to measure these skills. The analysis of the results showed that the majority of students were able to solve tasks related to the use of information as consumers, i.e., approximately three quarters of the students were able to search for information and half of them were also able to organize and manage digital information. Additionally, they show that very few students were able to succeed in tasks related to the use of information as producers, i.e., only one third of the students were able to develop their own ideas in a digital environment and less than one fifth were able to refine digital information and create a representation in a digital environment. Socioeconomic group, access, daily use and confidence in doing ICT-related activities were all positively associated with higher scores, showing the need to implement strategies to compensate this inequality, possibly by explicitly defining these aims in the national curriculum.}
}
@article{SERNAM2015647,
title = {Maturity model of transdisciplinary knowledge management},
journal = {International Journal of Information Management},
volume = {35},
number = {6},
pages = {647-654},
year = {2015},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2015.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S026840121500064X},
author = {Edgar {Serna M.}},
keywords = {Management, Complexity, Interdisciplinary, Multidisciplinary, Complex thinking},
abstract = {In this article a maturity model for the management of transdisciplinary knowledge is presented, although research nowadays is transdisciplinary the different maturity models proposed in the literature are oriented towards interdisciplinary knowledge management, and, at most, they are oriented toward multidisciplinary knowledge management. The objective is proposing an evolutionary model which accepts knowledge as intensely active and dynamic and evolving in maturity from the early stages of research. But this is possible only if the research team adopt a clear, clean and joint process of disciplinary integration and transdisciplinary integration of the produced and discovered knowledge. In this way, the results of research will have a greater influence on society and they also will be adopted by society.}
}
@article{ARS2021102805,
title = {Underground ancient mine work ventilation modeling},
journal = {Journal of Archaeological Science: Reports},
volume = {37},
pages = {102805},
year = {2021},
issn = {2352-409X},
doi = {https://doi.org/10.1016/j.jasrep.2021.102805},
url = {https://www.sciencedirect.com/science/article/pii/S2352409X21000171},
author = {Christophe Ars and Joseph Gauthier and Nicolas Florsch},
keywords = {Mining archaeology, Ventilation system, Numerical modeling, Computational fluid dynamics, Air quality, Sainte-Marie-aux-Mines},
abstract = {Excavations at the Giro mine located in the commune of Sainte-Marie-aux-Mines (France) have revealed a metallic and cylindrical artifact that resembles a connecting element for wooden duct sections. Early modern literature, and especially De Re Metallica, mention such technologies intended in particular to force the ventilation of underground mines in which air quality was harmful for miners. The connecting element was found in a gallery leading to a stope of which ventilation seems problematic. The numerical simulation of the air flow in the tunnel makes it possible to test the ventilation hypotheses formulated from archaeological data. These simulations are performed with OpenFOAM, a free and open source software for computational fluid dynamics (CFD). Simulating not only the air flow, but also the heat and CO2 production of five miners at work highlights the need to force ventilation in the underground with a ventilation system. It also appears that the construction of scaffolding in the stope can fulfill the double function of facilitating circulation and improving ventilation. This first quantitative approach to one of the main obstacles to mining offers a new method for testing the solutions implemented by miners of the past.}
}
@article{KIM2024105050,
title = {G-TRACE: Grouped temporal recalibration for video object segmentation},
journal = {Image and Vision Computing},
volume = {147},
pages = {105050},
year = {2024},
issn = {0262-8856},
doi = {https://doi.org/10.1016/j.imavis.2024.105050},
url = {https://www.sciencedirect.com/science/article/pii/S0262885624001549},
author = {Jiyun Kim and JooHo Kim and Sungeun Hong},
keywords = {Semi-supervised video object segmentation, Memory attention, Hierarchical grouping},
abstract = {In Semi-supervised Video Object Segmentation (SVOS), there is a critical emphasis on enhancing the memory and readout mechanisms for frame matching, especially in relation to temporal dynamics. Current methods predominantly use 2D CNNs for encoding video frames, which unfortunately neglects the crucial aspect of addressing temporal variations in individual frames and their associated masks during the encoding process. One potential solution would be to implement temporal models such as 3D CNNs instead of 2D CNNs, but this significantly increases computational requirements, making it impractical for real-world SVOS applications. In this paper, we introduce the Grouped Temporal Recalibration with Attention for Convolutional Encoders (G-TRACE), a novel plug-and-play module that is compatible with various existing SVOS frameworks. G-TRACE uses hierarchical memory-centric attention and integrates effortlessly with 2D CNNs, offering a novel approach to temporal modeling that operates orthogonally to traditional frame matching methods. Extensive evaluations on four widely-used benchmarks demonstrate that our method consistently delivers significant performance improvements over various baseline models.}
}
@incollection{STRUBE20012158,
title = {Cognitive Science: Overview},
editor = {Neil J. Smelser and Paul B. Baltes},
booktitle = {International Encyclopedia of the Social & Behavioral Sciences},
publisher = {Pergamon},
address = {Oxford},
pages = {2158-2166},
year = {2001},
isbn = {978-0-08-043076-8},
doi = {https://doi.org/10.1016/B0-08-043076-7/01441-8},
url = {https://www.sciencedirect.com/science/article/pii/B0080430767014418},
author = {G. Strube},
abstract = {Cognitive science (CS) emerged in 1975 as a field spanning parts of psychology, artificial intelligence, linguistics, philosophy, anthropology, and the neurosciences. CS is unique in its basic tenet that cognitive processes are computations, a perspective which allows for direct comparison of natural and artificial intelligence and emphasizes a methodology that integrates formal and empirical analyses with computational synthesis. Computer simulations have therefore become the hallmark of CS. Today, CS is an internationally established discipline. Its dominant tradition, close to symbol-processing architectures, has been enriched by neural networks and by the recognition that human cognition rests on both biological and cultural foundations. CS studies cognitive systems: organisms, machines, or any combination of these, acting in dynamically changing environments. Cognition in CS denotes advanced control mechanisms that allow for sophisticated adaptation through computations operating on mental representations. CS recognizes that cognition in biological systems is implemented in brain processes, but emphasizes analyses at the functional level, with cognitive neuroscience relating both domains. Applications of CS may be found in the design of software, in human factors engineering, health care, and education.}
}
@article{GAO2022109390,
title = {A kernel-free fuzzy reduced quadratic surface ν-support vector machine with applications},
journal = {Applied Soft Computing},
volume = {127},
pages = {109390},
year = {2022},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2022.109390},
url = {https://www.sciencedirect.com/science/article/pii/S1568494622005336},
author = {Zheming Gao and Yiwen Wang and Min Huang and Jian Luo and Shanshan Tang},
keywords = {Data mining, -SVM, Kernel-free SVM, Fuzzy SVM, Binary classification},
abstract = {The kernel-free support vector machine (SVM) models are recently developed and studied to overcome some drawbacks induced by the kernel-based SVM models. To further improve the classification accuracy and computational efficiency of existing kernel-free quadratic surface support vector machine (QSSVM) models, a novel kernel-free ν-fuzzy reduced QSSVM model is proposed. The proposed model utilizes a reduced quadratic surface for nonlinear binary classification as well as reducing the effect of outliers in the data set. Some theoretical properties are rigorously studied, especially, the effects of the parameter ν on the dual feasibility and the number of support vectors. Computational experiments are conducted on some public benchmark data sets to indicate the superior performance of the proposed model over some well-known binary classification models. The numerical results also favors the higher training efficiency of the proposed model over those of other kernel-free SVM models. Moreover, the proposed model is successfully applied to the prodromal detection of Alzheimer’s Disease with good performance, by using the data from the Alzheimer’s Disease Neuroimaging Initiative (ADNI) database.}
}
@article{GROOS2024105420,
title = {Combining user-centered design and behavioral theory to enhance health technologies: A personas-based approach for a primary-care based multifactorial falls risk assessment tool},
journal = {International Journal of Medical Informatics},
volume = {186},
pages = {105420},
year = {2024},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2024.105420},
url = {https://www.sciencedirect.com/science/article/pii/S1386505624000832},
author = {Sara S. Groos and Annemiek J. Linn and Judith I. Kuiper and Natasja M. {van Schoor} and Nathalie {van der Velde} and Julia C.M. {van Weert}},
keywords = {Personas, User-centered design, Behavioral theory, Multifactorial falls risk assessment tools, Implementation},
abstract = {Introduction
Multifactorial falls risk assessment tools (FRATs) can be an effective falls prevention method for older adults, but are often underutilized by health care professionals (HCPs). This study aims to enhance the use and implementation of multifactorial FRATs by combining behavioral theory with the user-centered design (UCD) method of personas construction. Specifically, the study aimed to (1) construct personas that are based on external (i.e., needs, preferences) and intrinsic user characteristics (i.e., behavioral determinants); and (2) use these insights to inform requirements for optimizing an existing Dutch multifactorial FRAT (i.e., the ‘Valanalyse’).
Methods
Survey data from HCPs (n = 31) was used to construct personas of the ‘Valanalyse.’ To examine differences between clusters on 68 clustering variables, a multivariate cluster analysis technique with non-parametric analyses and computational methods was used. The aggregated external and intrinsic user characteristics of personas were used to inform key design and implementation requirements for the ‘Valanalyse,’ respectively, whereby intrinsic user characteristics were matched with appropriate behavior change techniques to guide implementation.
Results
Significant differences between clusters were observed in 20 clustering variables (e.g., behavioral beliefs, situations for use). These variables were used to construct six personas representing users of each cluster. Together, the six personas helped operationalize four key design requirements (e.g., guide treatment-related decision making) and 14 implementation strategies (e.g., planning coping responses) for optimizing the ‘Valanalyse’ in Dutch geriatric, primary care settings.
Conclusion
The findings suggest that theory- and evidence-based personas that encompass both external and intrinsic user characteristics are a useful method for understanding how the use and implementation of multifactorial FRATs can be optimized with and for HCPs, providing important implications for developers and eHealth interventions with regards to encouraging technology adoption.}
}
@article{KADANE1985256,
title = {Parallel and sequential computation: a statistician's view},
journal = {Journal of Complexity},
volume = {1},
number = {2},
pages = {256-263},
year = {1985},
issn = {0885-064X},
doi = {https://doi.org/10.1016/0885-064X(85)90014-7},
url = {https://www.sciencedirect.com/science/article/pii/0885064X85900147},
author = {Joseph B Kadane},
abstract = {I borrow themes from statistics—epsecially the Bayesian ideas underlying average-case analysis and ideas of sequential design of experiments—to discuss when parallel computation is likely to be an attractive technique.}
}
@article{WATANOBE2014417,
title = {Hybrid intelligence aspects of programming in *AIDA algorithmic pictures},
journal = {Future Generation Computer Systems},
volume = {37},
pages = {417-428},
year = {2014},
note = {Special Section: Innovative Methods and Algorithms for Advanced Data-Intensive Computing Special Section: Semantics, Intelligent processing and services for big data Special Section: Advances in Data-Intensive Modelling and Simulation Special Section: Hybrid Intelligence for Growing Internet and its Applications},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2013.12.031},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X13002951},
author = {Yutaka Watanobe and Nikolay Mirenkov},
keywords = {Programming in pictures, *AIDA, Algorithmic CyberFilm},
abstract = {Programming in algorithmic pictures (a-pictures) is an approach where pictures and moving pictures are used as super-characters for representing features of computational algorithms and data structures. Within this approach some “data space structures” are traversed by “fronts of computation” and/or some “units of activity” are traversed by flows of data. There are compound a-pictures to define algorithmic steps (called Algorithmic CyberFrames) and generic a-pictures to define the contents of compound pictures. Compound a-pictures are assembled into special series to represent some algorithmic features. The series are assembled into an Algorithmic CyberFilm. The generic/compound a-pictures and their series are developed and acquired in special galleries of an open type where supportive pictures of embedded clarity annotations are also included. In this paper, *AIDA (Star-AIDA) modeling/programming language (AIDA stands for Animation and Images to Develop Algorithms) and its Filmification modeling (F-modeling) environment are briefly considered and examples of programs in a-pictures are provided. A special attention is paid to *AIDA programs as special information resources which perception, comprehension and cognition depend on interaction with, at least, a few different but mutually supplementing features of a-pictures. A scheme of data/knowledge acquisition based on clusters of different views and how this acquisition is oriented to enhancing user’s ability within works on developing application models, corresponding algorithms and programs, are presented.}
}
@article{PANDEY2018141,
title = {Understanding the mechanics of creep deformation to develop a surrogate model for contact assessment in CANDU® fuel channels},
journal = {Nuclear Engineering and Design},
volume = {330},
pages = {141-156},
year = {2018},
issn = {0029-5493},
doi = {https://doi.org/10.1016/j.nucengdes.2018.01.032},
url = {https://www.sciencedirect.com/science/article/pii/S0029549318300323},
author = {M.D. Pandey and F.J. Tallavo and N.C. Christodoulou and B. Leitch and G.A. Bickel},
keywords = {Fuel channel, Creep deformation, Finite element method, Surrogate model, Pressure tube, Calandria tube, Contact assessment, Zirconium alloy, Probabilistic assessments},
abstract = {A key element of the fuel channel life cycle management in CANDU® reactors is to prevent contact between the pressure tube (PT) and the calandria tube (CT) and to avoid the development of hydride blisters that lead to delayed hydride cracking of the PT. The PT-CT contact is the result of in-reactor deformation due to irradiation induced creep of the fuel channel assembly, which in turn is affected by uncertainties associated with various parameters like material properties, dimensional changes in the channel and boundary conditions (e.g., end slopes) of the channel. To account for these uncertainties, probabilistic assessment methods are developed to evaluate the risk of PT-CT contact and demonstrate compliance with provisions of the CSA Standard N285.8. Currently, a simulation is based on probabilistic assessments in which input parameters to a finite element model (FEM) of creep deformation are randomly sampled from their respective distributions. A simulation model involves numerous repetitive solutions of the FEM model to determine the probability distribution of PT-CT gap and the time to contact. Since the creep deformation analysis using FEM is computationally involved, this brute force Monte Carlo simulation method is not an efficient way to carry out the probabilistic assessment of the reactor core. This paper proposes a new line of thinking for probabilistic assessments of PT-CT contact in fuel channels, which is based on replacing a full FEM model by a surrogate model of a much simpler analytical form. The surrogate model not only simplifies the creep deformation analysis, but also provides a more logical basis for probabilistic assessments. This paper presents an insightful analysis of creep deformation and shows that a simple surrogate model can be developed to predict the PT-CT gap as a linear function of two primary random variables, namely, a creep factor and end slopes. This simplified representation has a far reaching effect on the probabilistic assessment of fuel channels.}
}
@article{LAVIGNE2007630,
title = {Statistical reasoning of middle school children engaged in survey inquiry},
journal = {Contemporary Educational Psychology},
volume = {32},
number = {4},
pages = {630-666},
year = {2007},
issn = {0361-476X},
doi = {https://doi.org/10.1016/j.cedpsych.2006.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S0361476X06000488},
author = {Nancy C. Lavigne and Susanne P. Lajoie},
keywords = {Statistical reasoning, Inquiry, Mathematics education, Middle school, Thinking, Cognition},
abstract = {The case study examined two groups of grade 7 students as they engaged in four inquiry phases: posing a question and collecting, analyzing, and representing data. Previous studies reported analyses of statistical reasoning on a single inquiry phase. Our goal was to identify the modes of statistical reasoning displayed during group discussions in all phases as children designed and conducted their own inquiry. A content analysis of audio and video recorded discussions yielded 10 statistical reasoning modes: six relate to Garfield and Gal’s [Garfield, J., Gal, I. (1999). Teaching and assessing statistical reasoning. In L. V. Stiff, & F. R. Curcio (Eds.), Developing mathematical reasoning in grades K-12. 1999 Yearbook (pp. 207–219). Reston, VA: National Council of Teachers of Mathematics] statistical reasoning types involved in the collection, analysis, and representation of data and four modes deal with an aspect of inquiry not exclusively focused upon in the literature on statistical reasoning—i.e., the problem-posing phase. Although students’ reasoning reflected an incomplete understanding of statistics they serve as building blocks for instruction.}
}
@article{WEERASINGHE2025270,
title = {ABC-based forecasting in misspecified state space models},
journal = {International Journal of Forecasting},
volume = {41},
number = {1},
pages = {270-289},
year = {2025},
issn = {0169-2070},
doi = {https://doi.org/10.1016/j.ijforecast.2024.05.005},
url = {https://www.sciencedirect.com/science/article/pii/S016920702400044X},
author = {Chaya Weerasinghe and Rubén Loaiza-Maya and Gael M. Martin and David T. Frazier},
keywords = {Approximate Bayesian computation, Auxiliary model, Loss-based prediction, Focused Bayesian prediction, Proper scoring rules, Stochastic volatility model},
abstract = {Approximate Bayesian Computation (ABC) has gained popularity as a method for conducting inference and forecasting in complex models, most notably those which are intractable in some sense. In this paper, we use ABC to produce probabilistic forecasts in state space models (SSMs). Whilst ABC-based forecasting in correctly-specified SSMs has been studied, the misspecified case has not been investigated. It is this case that we emphasize. We invoke recent principles of ‘focused’ Bayesian prediction, whereby Bayesian updates are driven by a scoring rule that rewards predictive accuracy; the aim being to produce predictives that perform well in that rule, despite misspecification. Two methods are investigated for producing the focused predictions. In a simulation setting, ‘coherent’ predictions are in evidence for both methods. That is, the predictive constructed using a particular scoring rule often predicts best according to that rule. Importantly, both focused methods typically produce more accurate forecasts than an exact but misspecified predictive, in particular when the degree of misspecification is marked. An empirical application to a truly intractable SSM completes the paper.}
}
@article{HORVATH2015161,
title = {Ubiquitous computer aided design: A broken promise or a Sleeping Beauty?},
journal = {Computer-Aided Design},
volume = {59},
pages = {161-175},
year = {2015},
issn = {0010-4485},
doi = {https://doi.org/10.1016/j.cad.2014.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S0010448514002358},
author = {Imre Horváth and Regine W. Vroom},
keywords = {Ubiquitous computing, Computer aided design, Ubiquitous design enablers, Competing technology exploitation, Ubiquitous CAD applications},
abstract = {As a novel computational approach, ubiquitous computing was emerging at the beginning of the 1980s and has reached a rather mature level by now. It assumes that computing can be available anywhere, anytime and in any context due to technological developments, social demands and calm implementations. Over the years, the opportunities of this computing paradigm have been explored and the benefits have been exploited successfully in many application fields. This survey paper addresses ubiquitous computing from the perspective of enabling computer aided design. The specific objectives of the reported survey are to: (i) give an overall account of the current status of ubiquitous computing and technologies, (ii) cast light on how ubiquitous computing has influenced the development of CAD systems, tools, and methods, and (iii) critically investigate future development opportunities of ubiquitous computing enabled computer aided design. First, the paper discusses the principles and typical technologies of ubiquitous computing. Then, the development and spectrum of the so-called standard computer aided design tasks are analyzed from a computational point of view. Afterwards, the already implemented design enabling functionalities are discussed and some additional functional possibilities are considered. The literature provides evidence that ubiquitous computing has not managed to revolutionize the methodologies or the systems of computer aided design so far, though many researchers intensively studied the affordances and the application possibilities of ubiquitous technologies. One reason is that ubiquitous computing technologies had in the last two decades to compete with other kinds of computational technologies, such as high-capacity computing, high-speed networking, immersive virtual reality, knowledge ontologies, smart software agents, mobile communication, etc., which had a much stronger influence on the development of computer aided design methods and systems. In combination with the rather conservative and conventionalist industrial practice of CAD system development and application, this may explain why the ubiquitous computing revolution remained weak in computer aided design. The literature clearly indicates that application of ubiquitous technologies did not lead to radically new functionalities that could have been exploited by the concerned industries. Consequently, it seems to be possible that computer aided design simply steps over the paradigm of ubiquitous computing and expects new functionalities from the emerging new computing paradigms, such as brain–computer interfacing, cyber–physical computing, biological computing, or quantum computing.}
}
@article{RUFFO2023100531,
title = {Studying fake news spreading, polarisation dynamics, and manipulation by bots: A tale of networks and language},
journal = {Computer Science Review},
volume = {47},
pages = {100531},
year = {2023},
issn = {1574-0137},
doi = {https://doi.org/10.1016/j.cosrev.2022.100531},
url = {https://www.sciencedirect.com/science/article/pii/S157401372200065X},
author = {Giancarlo Ruffo and Alfonso Semeraro and Anastasia Giachanou and Paolo Rosso},
keywords = {Disinformation, Network analysis, Natural language processing, Opinion dynamics, Fake news spreading, Social bots},
abstract = {With the explosive growth of online social media, the ancient problem of information disorders interfering with news diffusion has surfaced with a renewed intensity threatening our democracies, public health, and news outlets’ credibility. Therefore, thousands of scientific papers have been published in a relatively short period, making researchers of different disciplines struggle with an information overload problem. The aim of this survey is threefold: (1) we present the results of a network-based analysis of the existing multidisciplinary literature to support the search for relevant trends and central publications; (2) we describe the main results and necessary background to attack the problem under a computational perspective; (3) we review selected contributions using network science as a unifying framework and computational linguistics as the tool to make sense of the shared content. Despite scholars working on computational linguistics and networks traditionally belong to different scientific communities, we expect that those interested in the area of fake news should be aware of crucial aspects of both disciplines.}
}
@article{ROLLS2024102636,
title = {A theory of hippocampal function: New developments},
journal = {Progress in Neurobiology},
volume = {238},
pages = {102636},
year = {2024},
issn = {0301-0082},
doi = {https://doi.org/10.1016/j.pneurobio.2024.102636},
url = {https://www.sciencedirect.com/science/article/pii/S0301008224000728},
author = {Edmund T. Rolls and Alessandro Treves},
keywords = {Hippocampus, Episodic memory, Attractor network, Memory recall, Neocortical memory, Consolidation},
abstract = {We develop further here the only quantitative theory of the storage of information in the hippocampal episodic memory system and its recall back to the neocortex. The theory is upgraded to account for a revolution in understanding of spatial representations in the primate, including human, hippocampus, that go beyond the place where the individual is located, to the location being viewed in a scene. This is fundamental to much primate episodic memory and navigation: functions supported in humans by pathways that build ‘where’ spatial view representations by feature combinations in a ventromedial visual cortical stream, separate from those for ‘what’ object and face information to the inferior temporal visual cortex, and for reward information from the orbitofrontal cortex. Key new computational developments include the capacity of the CA3 attractor network for storing whole charts of space; how the correlations inherent in self-organizing continuous spatial representations impact the storage capacity; how the CA3 network can combine continuous spatial and discrete object and reward representations; the roles of the rewards that reach the hippocampus in the later consolidation into long-term memory in part via cholinergic pathways from the orbitofrontal cortex; and new ways of analysing neocortical information storage using Potts networks.}
}
@article{CAIRNS201964,
title = {Future design of accessibility in games: A design vocabulary},
journal = {International Journal of Human-Computer Studies},
volume = {131},
pages = {64-71},
year = {2019},
note = {50 years of the International Journal of Human-Computer Studies. Reflections on the past, present and future of human-centred technologies},
issn = {1071-5819},
doi = {https://doi.org/10.1016/j.ijhcs.2019.06.010},
url = {https://www.sciencedirect.com/science/article/pii/S1071581919300801},
author = {Paul Cairns and Christopher Power and Mark Barlet and Greg Haynes},
keywords = {Digital games, Accessibility, Guidelines, Design vocabulary, Accessible player experiences},
abstract = {Games represent one of the most significant cultural artefacts of this century. They are a massive force in economies around the world and are enjoyed by millions of players worldwide. With their cultural significance firmly in place, it is important to ensure that all people can participate in and play games in order to feel included in our wider society. For people with disabilities, games in particular provide a cultural outlet where they can be included with everyone else, and enabled to do things on an even footing with their non-disabled peers. However, this only happens if we create the necessary design environments that provide inclusive opportunities to game alongside the rest of the player base. Guidelines have been successful in raising awareness of accessibility in games and still function well for evaluating finished games. However, they are not the generative design thinking tools that developers need. Further in being divided to address specific disabilities, they are not capturing the diversity of needs of players with disabilities and the personalised and idiosyncratic adaptations that they make in order to play. We therefore propose developing a vocabulary and language of game accessibility which is no longer about whether someone can perceive or operate an interactive technology, but instead as to whether they can have the experience they want to have. We propose the structure for such a vocabulary showing that it needs to distinguish between access to controls, enablement to meet the challenges of the game and the player experience itself. We show how the intermediate-level knowledge embodied in guidelines can be reformulated in this way to be more generative and so support designers to develop games that deliver accessible player experiences.}
}
@article{LEE2024100211,
title = {A systematic review of AI education in K-12 classrooms from 2018 to 2023: Topics, strategies, and learning outcomes},
journal = {Computers and Education: Artificial Intelligence},
volume = {6},
pages = {100211},
year = {2024},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2024.100211},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X24000122},
author = {Sang Joon Lee and Kyungbin Kwon},
keywords = {Artificial intelligence, AI education, Systematic review, K-12},
abstract = {AI education aims to teach AI concepts, essential knowledge, and skills related to the fundamental ideas in AI. As AI becomes increasingly prevalent in our daily lives, schools and educators have started to recognize the importance of AI education in K-12 schools. However, there have been a limited number of studies reporting on the implementation of AI education in classrooms. This systematic review aimed to provide an overview of the current state of AI education in K-12 schools, exploring topics, instructional approaches, and learning outcomes. Twenty-five peer-reviewed journal articles published between 2018 and 2023 were selected for this systematic review. The findings highlighted that various topics were covered in K-12 AI education, including fundamental AI concepts, different types of AI, AI applications, and ethical considerations related to AI. To facilitate meaningful learning experiences, educators frequently integrated hands-on activities and project-based learning. The findings supported the benefits of AI education in enhancing students' AI literacy, problem-solving skills, and ethical reflections on AI's societal impact. Furthermore, it fostered motivation, positive attitudes toward AI, and an interest in technology while inspiring career aspirations. It is recommended to develop tailored AI curricula, instructional strategies, and appropriate tools and resources that seamlessly integrate into various subjects within the standard school curriculum.}
}
@article{SINHA2008955,
title = {Thermal pressure of ionic solids at high temperatures},
journal = {Solid State Sciences},
volume = {10},
number = {7},
pages = {955-959},
year = {2008},
issn = {1293-2558},
doi = {https://doi.org/10.1016/j.solidstatesciences.2007.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S1293255807003366},
author = {Pallavi Sinha},
keywords = {Thermal pressure, Volume expansion},
abstract = {In the present study, a relationship between thermal pressure and volume expansion ratio is disclosed for ionic solids at 1bar pressure. The ionic solids NaCl, KCl, MgO and CaO are considered for the analytic thinking. The analysis is based on the experimental data tabulated by Anderson and generalized data reported by Srivastava. A close agreement between the present study and the experiment reveals the validity of the present work. The extrapolated values of thermal pressure at higher temperatures are useful to understand the thermoelastic behaviour of solids.}
}
@article{CLEMENTI198713,
title = {Large-scale computations on a scalar, vector and parallel ‘supercomputer’},
journal = {Parallel Computing},
volume = {5},
number = {1},
pages = {13-44},
year = {1987},
note = {Proceedings of the International Conference on Vector and Parallel Computing-Issues in Applied Research and Development},
issn = {0167-8191},
doi = {https://doi.org/10.1016/0167-8191(87)90004-4},
url = {https://www.sciencedirect.com/science/article/pii/0167819187900044},
author = {E Clementi and J Detrich and S Chin and G Corongiu and D Folsom and D Logan and R Caltabiano and A Carnevali and J Helin and M Russo and A Gnudi and P Palamidese},
keywords = {Parallel computer systems, 1CAP-1, 1CAP-2, 1CAP-3, programming strategy, migration of code from sequential to parallel systems, performance analysis},
abstract = {We discuss two experimental parallel computer systems 1CAP-1 and 1CAP-2 which can be applied to the entire spectrum of scientific and engineering applications. These systems achieve ‘supercomputer’ levels of performance by spreading large scale computations across multiple cooperating processors—several with vector capabilities. We outline system hardware and software, and discuss our programming strategy for migrating codes from a conventional sequential system to a parallel one. The performance of a variety of applications programs is analyzed to demonstrate the merits of this approach. Finally, we discuss 1CAP-3, an extension to this computing system, which has been recently assembled.}
}
@article{BURR2020R907,
title = {Horace Barlow (1921–2020)},
journal = {Current Biology},
volume = {30},
number = {16},
pages = {R907-R910},
year = {2020},
issn = {0960-9822},
doi = {https://doi.org/10.1016/j.cub.2020.07.060},
url = {https://www.sciencedirect.com/science/article/pii/S096098222031085X},
author = {David Burr and Simon Laughlin}
}
@article{ZAGREBAEV2018568,
title = {About using of AI to choosing a refueling channel and manipulating control rods in RBMK-type reactor},
journal = {Procedia Computer Science},
volume = {123},
pages = {568-572},
year = {2018},
note = {8th Annual International Conference on Biologically Inspired Cognitive Architectures, BICA 2017 (Eighth Annual Meeting of the BICA Society), held August 1-6, 2017 in Moscow, Russia},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.01.086},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918300875},
author = {A.M. Zagrebaev and R.N. Ramazanov and A.V. Trifonenkov},
keywords = {AI, refueling channel, manipulating control rods, RBMK-type reactor},
abstract = {Nuclear reactor control is usually the composition of automatic and manual control types. This article deals with manual parts of power control and refueling systems of RBMK-type reactor. There are always such aspects of the reactor operation, which automatic systems do not control. The aim of this research is to determine the set of actual choices made by the operator and create mathematical model of decision-making operator based on a neural network. The further research may include estimations of the automatic control system imperfection, estimations of the quality of decisions made and performance tests for the composite computational and AI decision-making software for nuclear reactors.}
}
@article{KLASIOS2016103,
title = {Evolutionizing human nature},
journal = {New Ideas in Psychology},
volume = {40},
pages = {103-114},
year = {2016},
issn = {0732-118X},
doi = {https://doi.org/10.1016/j.newideapsych.2015.08.004},
url = {https://www.sciencedirect.com/science/article/pii/S0732118X15000501},
author = {John Klasios},
keywords = {Human nature, Evolutionary psychology, Adaptationism, Darwin, Homeostatic property clusters, Developmental systems},
abstract = {Many have argued that the very notion of human nature is untenable given the facts of evolution and should accordingly be discarded. This paper, by contrast, argues that the notion can be retained in a coherent and modern way. The present account expounds on the view of human nature as a collection of species-typical psychological adaptations, and outlines how it can be understood in formally modeled computational terms. The view defended is also heavily developmental and connects directly with contemporary evolutionary developmental biology. Furthermore, the notion of human nature developed here allows us to abstract away from the obfuscating variability that manifests not only between individuals across ontogeny, but also cross-culturally and throughout time.}
}
@article{WANG2024100257,
title = {A resource prediction method for air traffic cyber-physical-social system},
journal = {Transportation Engineering},
volume = {17},
pages = {100257},
year = {2024},
issn = {2666-691X},
doi = {https://doi.org/10.1016/j.treng.2024.100257},
url = {https://www.sciencedirect.com/science/article/pii/S2666691X24000320},
author = {Jintao Wang and Huaiqi Chen and Yulong Yin and Zijian Jiang and Meili Chen},
keywords = {Air traffic system, Complex network, Resource prediction, Cyber-physical-social system, Neural network},
abstract = {Air traffic is exhibiting the characteristics of large flow, strong coupling, and high time variation. Therefore, the complex network of air traffic is more vulnerable to disturbances. When it is disturbed, the failure of some nodes spreads through dependency relationships in the network, resulting in cascade failure. In the event of a cascade failure, the network may quickly collapse until it is paralyzed, with widespread delays and flight cancellations. The current flow management and deployment methods still remain in the control-oriented stage, which is mainly completed by air traffic controls (ATCs), and lack of accurate flow adjustment and effective utilization of capacity. The whole air traffic system and its peripheral factors are intricate, so human and social factors must be integrated into the control and decision-making of the system. Considering engineering and social factors such as operation environment, social environment, personnel, rules, equipment, and information processing, we analyse the air traffic in a cyber-physical-social system (CPSS). To reflect the actual system behaviour rules, dynamic response, limit state, and so on, the corresponding computational experiment and comprehensive evaluation system are established. Based on neural networks and other technologies, a resource prediction scheme based on task demand is proposed for multi-dimensional resources such as airports, air routes, and ATC, to reduce the cost of system resource scheduling and improve resource utilization through resource prediction and adjustment. Finally, the accuracy of the proposed resource prediction algorithm is verified by theoretical analysis and simulation.}
}
@incollection{SULLIVAN2008XIX,
title = {Preface},
series = {Methods in Cell Biology},
publisher = {Academic Press},
volume = {85},
pages = {XIX-XX},
year = {2008},
booktitle = {Fluorescent Proteins},
issn = {0091-679X},
doi = {https://doi.org/10.1016/S0091-679X(08)85026-1},
url = {https://www.sciencedirect.com/science/article/pii/S0091679X08850261},
author = {Kevin F. Sullivan},
abstract = {Publisher Summary
The chapter highlights the content of the book “Fluorescent proteins 2nd edition.” The book discusses the rich palette of autofluorescent proteins that now spans the spectral range from blue to deep red. From presentation of the ideas and concepts that provide the foundation for methods through discussing the factual knowledge and sources required to design experiments to the detailed exposition of actual experimental protocols, the chapters in this book combine to provide an essential tool for thinking about using genetically encoded fluorescent molecules. The experimental goals and systems discussed in the chapter present range from biophysical interrogation of individual molecules to the analysis of the behavior of cell populations in whole animals. The range of autofluorescent proteins available is presented and discussed in several chapters of the book. The construction of FP fusions is also discussed in several contexts, from developing biosensors and optimizing FRET to constructing intramolecular fusions and hemi-FP chimeras used for detecting protien–protein interactions.}
}
@article{HAGHGOO2024108332,
title = {The percolation inception of the CNT-polymer nanocomposites with the magneto-electric field effects on the CNT subbands},
journal = {Composites Part A: Applied Science and Manufacturing},
volume = {185},
pages = {108332},
year = {2024},
issn = {1359-835X},
doi = {https://doi.org/10.1016/j.compositesa.2024.108332},
url = {https://www.sciencedirect.com/science/article/pii/S1359835X24003294},
author = {Mojtaba Haghgoo and Reza Ansari and Mohammad {Kazem Hassanzadeh-Aghdam} and Jaehwan Kim},
keywords = {A. Carbon nanotubes and nanofibers, A. Multifunctional composites, B. Electrical properties, C. Analytical modelling},
abstract = {The percolation inception of CNT-polymer nanocomposites is studied considering the magneto-electric field effects on CNT subbands. The analytical model predicts the electrical conductivity where CNTs are modeled as slender rods with their geometric orientations as randomly distributed or aligned to transfer electrons at tunneling distance range. The tunneling effect takes into account the electron transmission between every linked pair of CNTs when evaluating electrical resistance. The subsequent CNT displacement computation and the resistance change comprise the other phase of the modeling approach. Piezoresistivity results of the analyses agree well with the experimental data when considering tunneling behavior in the percolation transition zone. The magnetic field enhances the field affected subbands and increases the electrical conductivity by enhancing the mobility of the charges. The results reveal that the efficiency of CNT network in transmitting charges is increased with higher aspect ratio CNTs that scaled the sensitivity to lower values.}
}
@incollection{BRANDT2006136,
title = {Grammatology},
editor = {Keith Brown},
booktitle = {Encyclopedia of Language & Linguistics (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {136-140},
year = {2006},
isbn = {978-0-08-044854-1},
doi = {https://doi.org/10.1016/B0-08-044854-2/00355-2},
url = {https://www.sciencedirect.com/science/article/pii/B0080448542003552},
author = {P.A. Brandt},
keywords = {Barthes, deconstruction, Derrida, differance, grammatology, Lacan, logocentrism, McLuhan, phenomenology, Saussure, sign, symbolization, writing},
abstract = {Grammatology, the study of writing systems and processes, is presented through the French philosopher Jacques Derrida's critique of the concept of meaning, following from semiology and the notion of sign as such in Western thinking: ‘logocentrism.’ This critical view of writing was the starting point of deconstruction in literary criticism. It is argued in this article that written texts are different from oral utterances as concerns their enunciation, or speaker role, and that this circumstance deeply affects their interpretation. The difference could explain the sacralization of texts and text volumes, their cultural status and importance. The computer age is about to transform the relation between oral and written communication, so that we now can have written dialogue in addition to oral dialogue.}
}
@article{HAMMOND19951593,
title = {Implementation and performance issues of a massively parallel atmospheric model},
journal = {Parallel Computing},
volume = {21},
number = {10},
pages = {1593-1619},
year = {1995},
note = {Climate and weather modeling},
issn = {0167-8191},
doi = {https://doi.org/10.1016/0167-8191(95)01017-9},
url = {https://www.sciencedirect.com/science/article/pii/0167819195010179},
author = {Steven W. Hammond and Richard D. Loft and John M. Dennis and Richard K. Sato},
keywords = {Atmospheric general circulation modeling, Climate modeling, Data parallelism, Spectral transform, Semi-Lagrangian transport},
abstract = {We present implementation and performance issues of a data parallel version of the National Center for Atmospheric Research (NCAR) Community Climate Model (CCM2). We describe automatic conversion tools used to aid in converting a production code written for a traditional vector architecture to data parallel code suitable for the Thinking Machines Corporation CM-5. Also, we describe the 3-D transposition method used to parallelize the spherical harmonic transforms in CCM2. This method employs dynamic data mapping techniques to improve data locality and parallel efficiency of these computations. We present performance data for the 3-D transposition method on the CM-5 for machine size up to 512 processors. We conclude that the parallel performance of the 3-D transposition method is adversely affected on the CM-5 by short vector lengths and array padding. We also find that the CM-5 spherical harmonic transforms spend about 70% of their execution time in communication. We detail a transposition-based data parallel implementation of the semi-Lagrangian Transport (SLT) algorithm used in CCM2. We analyze two approaches to parallelizing the SLT, called the departure point and arrival point based methods. We develop a performance model for choosing between these methods. We present SLT performance data which shows that the localized horizontal interpolation in the SLT takes 70% of the time, while the data remapping itself only require approximately 16%. We discuss the importance of scalable I/O to CCM2, and present the I/O rates measured on the CM-5. We compare the performance of the data parallel version of CCM2 on a 32-processor CM-5 with the optimized vector code running on a single processor Cray Y-MP. We show that the CM-5 code is 75% faster. We also give the overall performance of CCM2 running at higher resolutions on different numbers of CM-5 processors. We conclude by discussing the significance of these results and their implications for data parallel climate models.}
}
@article{HAMEDUH20203494,
title = {Homology modeling in the time of collective and artificial intelligence},
journal = {Computational and Structural Biotechnology Journal},
volume = {18},
pages = {3494-3506},
year = {2020},
issn = {2001-0370},
doi = {https://doi.org/10.1016/j.csbj.2020.11.007},
url = {https://www.sciencedirect.com/science/article/pii/S2001037020304748},
author = {Tareq Hameduh and Yazan Haddad and Vojtech Adam and Zbynek Heger},
keywords = {Homology modeling, Machine learning, Protein 3D structure, Structural bioinformatics, Collective intelligence, Artificial intelligence},
abstract = {Homology modeling is a method for building protein 3D structures using protein primary sequence and utilizing prior knowledge gained from structural similarities with other proteins. The homology modeling process is done in sequential steps where sequence/structure alignment is optimized, then a backbone is built and later, side-chains are added. Once the low-homology loops are modeled, the whole 3D structure is optimized and validated. In the past three decades, a few collective and collaborative initiatives allowed for continuous progress in both homology and ab initio modeling. Critical Assessment of protein Structure Prediction (CASP) is a worldwide community experiment that has historically recorded the progress in this field. Folding@Home and Rosetta@Home are examples of crowd-sourcing initiatives where the community is sharing computational resources, whereas RosettaCommons is an example of an initiative where a community is sharing a codebase for the development of computational algorithms. Foldit is another initiative where participants compete with each other in a protein folding video game to predict 3D structure. In the past few years, contact maps deep machine learning was introduced to the 3D structure prediction process, adding more information and increasing the accuracy of models significantly. In this review, we will take the reader in a journey of exploration from the beginnings to the most recent turnabouts, which have revolutionized the field of homology modeling. Moreover, we discuss the new trends emerging in this rapidly growing field.}
}
@article{CHI2024117852,
title = {Artificial intelligence in metabolomics: a current review},
journal = {TrAC Trends in Analytical Chemistry},
volume = {178},
pages = {117852},
year = {2024},
issn = {0165-9936},
doi = {https://doi.org/10.1016/j.trac.2024.117852},
url = {https://www.sciencedirect.com/science/article/pii/S0165993624003352},
author = {Jinhua Chi and Jingmin Shu and Ming Li and Rekha Mudappathi and Yan Jin and Freeman Lewis and Alexandria Boon and Xiaoyan Qin and Li Liu and Haiwei Gu},
keywords = {Artificial intelligence, Metabolomics, Machine learning, Deep learning, Systems biology, Disease diagnosis, Precision medicine, Drug discovery},
abstract = {Metabolomics and artificial intelligence (AI) form a synergistic partnership. Metabolomics generates large datasets comprising hundreds to thousands of metabolites with complex relationships. AI, aiming to mimic human intelligence through computational modeling, possesses extraordinary capabilities for big data analysis. In this review, we provide a recent overview of the methodologies and applications of AI in metabolomics studies in the context of systems biology and human health. We first introduce the AI concept, history, and key algorithms for machine learning and deep learning, summarizing their strengths and weaknesses. We then discuss studies that have successfully used AI across different aspects of metabolomic analysis, including analytical detection, data preprocessing, biomarker discovery, predictive modeling, and multi-omics data integration. Lastly, we discuss the existing challenges and future perspectives in this rapidly evolving field. Despite limitations and challenges, the combination of metabolomics and AI holds great promises for revolutionary advancements in enhancing human health.}
}
@article{KULIHA2024161,
title = {Secure internet of medical things based electronic health records scheme in trust decentralized loop federated learning consensus blockchain},
journal = {International Journal of Intelligent Networks},
volume = {5},
pages = {161-174},
year = {2024},
issn = {2666-6030},
doi = {https://doi.org/10.1016/j.ijin.2024.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S2666603024000162},
author = {Megha Kuliha and Sunita Verma},
keywords = {Blockchain, Electronic health records, Federated learning, Healthcare monitoring, Internet of medical things, Security, Privacy, Health monitoring systems, Normalization, MIMIC-III},
abstract = {Electronic Health Records (EHRs) have become an increasingly significant source of information for healthcare professionals and researchers. Two technical challenges are addressed: motivating federated learning members to contribute their time and effort, and ensuring accurate aggregation of the global model by the centralized federated learning server. To overcome these issues and establish a decentralized solution, the integration of blockchain and federated learning proves effective, offering enhanced security and privacy for smart healthcare. The proposed approach includes a gamified element to incentivize and recognize contributions from federated learning members. This research work offers a solution involving resource management within the Internet of Medical Things (IoMT) using a newly proposed trust decentralized loop federated learning consensus blockchain. The obtained raw data is pre-processed by using handling missing values and adaptive min-max normalization. The appropriate features are selected with the aid of hybrid weighted-leader exponential distribution optimization algorithm. Because, data with multiple features exhibits varying levels of variation across each feature. The selected features are then forwarded to the training phase through the proposed pyramid squeeze attention generative adversarial networks to classify the EHR as positive and negative. The proposed classification model demonstrates high flexibility and scalability, making it applicable to a wide range of network architectures for various computer vision tasks. The introduced model provides better outcomes in terms of 98.5% in the training accuracy and 99% in the validation accuracy over Medical Information Mart for Intensive Care III (MIMIC-III) dataset, which is more efficient than the other traditional methods.}
}
@incollection{NIE20181939,
title = {Land use modeling and optimization based on food-energy-water nexus: a case study on crop-livestock systems},
editor = {Mario R. Eden and Marianthi G. Ierapetritou and Gavin P. Towler},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {44},
pages = {1939-1944},
year = {2018},
booktitle = {13th International Symposium on Process Systems Engineering (PSE 2018)},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-444-64241-7.50318-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780444642417503189},
author = {Yaling Nie and Styliani Avraamidou and Jie Li and Xin Xiao and Efstratios N. Pistikopoulos},
keywords = {land use, nexus, data-driven modeling, global optimization},
abstract = {Efficient land use in agricultural systems is a complicated decision-making problem with resource competitions and conflicting objectives. Systematic thinking based on food-energy-water (FEW) nexus is a necessity for modeling and optimization of the systems. However, challenges arise in making decisions while encountering conflicting objectives, limited data and coupling components. To address these challenges, we developed a global optimization-based land allocation framework, which provides an adaptive data-driven modeling method based on limited realistic data to predict yields for production components, a FEW index to help solve the multi-objective optimization problem and carry out assessments. Computational results indicate that the framework can provide valuable production models and a comprehensive FEW index to select strategies for optimal land allocation and limit stresses in the FEW nexus.}
}
@article{BAMMER2008875,
title = {Enhancing research collaborations: Three key management challenges},
journal = {Research Policy},
volume = {37},
number = {5},
pages = {875-887},
year = {2008},
issn = {0048-7333},
doi = {https://doi.org/10.1016/j.respol.2008.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S0048733308000528},
author = {Gabriele Bammer},
keywords = {Collaboration, Integration, Boundary, Authorization, Evaluation},
abstract = {This conceptual paper explores three areas of research collaboration: (a) effectively harnessing differences, (b) setting defensible boundaries and (c) gaining legitimate authorization. The focus is on their potential lessons for individuals leading and managing research collaborations, evaluation of research partnerships and areas for further investigation. Examples from three partnerships – building the atomic bomb, the Human Genome Project and the World Commission on Dams – are used to highlight key elements of the ideas presented. The paper provides a framework for systematically thinking about integration of different perspectives and other elements essential to any particular collaboration. It also sketches out ideas for (1) managing differences which may destroy partnerships, (2) deciding what the collaboration should encompass, (3) understanding and accommodating forces which may distort what the collaboration is able to achieve, and (4) enlisting necessary supporters while preserving research independence.}
}
@article{REIHLEN2013706,
title = {Uncertainty, pluralism, and the knowledge-based theory of the firm: From J.-C. Spender’s contribution to a socio-cognitive approach},
journal = {European Management Journal},
volume = {31},
number = {6},
pages = {706-716},
year = {2013},
issn = {0263-2373},
doi = {https://doi.org/10.1016/j.emj.2013.07.006},
url = {https://www.sciencedirect.com/science/article/pii/S0263237313001011},
author = {Markus Reihlen and Torsten Ringberg},
keywords = {Knowledge-based approach, Theory of the firm, Knowledge transfer, Social constructionism, Tacit knowledge, Socio-cognitive theory, Intuition, Mental models},
abstract = {J.-C. Spender’s award-winning, knowledge-based theory of the firm is based on four premises: (1) The firm can be sufficiently understood as a system of knowledge, (2) explicit and implicit knowing can be clearly dissociated, (3) organizations are conceived as cognizing entities, and (4) intuition shaped by shared cultural practices is a superior source of managerial knowledge. This line of reasoning represents a social constructionist view of the enactment, transfer, and storage of knowledge according to which managerial knowledge is largely tacitly shaped by industry recipes and the firm’s socio-cultural conventions and other social processes. Although comprehensive in scope, we argue that a knowledge-based theory of the firm needs to integrate a cognitivist approach that includes the synergetic production of tacit and explicit knowledge, the role of reflective thinking in resolving strategic uncertainties, and the interaction between the individual and the social. This socio-cognitive theory of the firm posits that sustained competitive advantage of a firm is founded on the ability to align knowledge internally within the firm as well as externally with its stakeholders through the individual sense-making of feedback from other individuals.}
}
@article{DOUGLAS2025106178,
title = {Classification Schemes of Altered States of Consciousness},
journal = {Neuroscience & Biobehavioral Reviews},
pages = {106178},
year = {2025},
issn = {0149-7634},
doi = {https://doi.org/10.1016/j.neubiorev.2025.106178},
url = {https://www.sciencedirect.com/science/article/pii/S0149763425001782},
author = {Fort Larry Douglas and Costines Cyril and Wittmann Marc and Demertzi Athena and Schmidt Timo Torsten},
keywords = {altered states of consciousness, Psychedelics, Phenomenology, classification},
abstract = {In recent years, there has been a renewed interest in the conceptual and empirical study of altered states of consciousness (ASCs) induced pharmacologically or otherwise, driven by their potential clinical applications. To draw attention to the rich history of research in this domain, we review prominent classification schemes that have been proposed to introduce systematicity in the scientific study of ASCs. The reviewed ASC classification schemes fall into three groups according to the criteria they use for categorization: (1) based on the nature, variety, and intensity of subjective experiences (state-based), including conceptual descriptions and psychometric assessments, (2) based on the technique of induction (method-based), and (3) descriptions of neurophysiological mechanisms of ASCs (neuro/physio-based). By comparing and extending existing classification schemes, we can enhance efforts to identify neural correlates of consciousness, particularly when examining mechanisms of ASC induction and the resulting subjective experience. Furthermore, an overview of what defining ASC characteristics different authors have proposed can inform future research in the conceptualization and quantification of ASC subjective effects, including the identification of those that might be relevant in clinical research. This review concludes by clustering the concepts from the state-based schemes, which are suggested for classifying ASC experiences. The resulting clusters can inspire future approaches to formulate and quantify the core phenomenology of ASC experiences to assist in basic and clinical research.}
}
@article{ABDELHAMID2022101673,
title = {Discovering epistasis interactions in Alzheimer's disease using deep learning model},
journal = {Gene Reports},
volume = {29},
pages = {101673},
year = {2022},
issn = {2452-0144},
doi = {https://doi.org/10.1016/j.genrep.2022.101673},
url = {https://www.sciencedirect.com/science/article/pii/S2452014422001819},
author = {Marwa M. {Abd El Hamid} and Yasser M.K. Omar and Mohamed Shaheen and Mai S. Mabrouk},
keywords = {Alzheimer's disease, Epistasis interactions, Personalized medicine, Deep learning model, SHAP},
abstract = {Alzheimer's disease (AD) is the most common form of dementia. Single Nucleotide Polymorphisms (SNPs) are single nucleotide alterations that can be used as genomic markers disclosing susceptibility to complex diseases like AD. Epistasis has long been significant for recognizing the function of genetic pathways and the evolutionary dynamics of difficult genetic systems. Discovering epistasis interactions holds a vital key to personalized medicine (PM). PM needs a better understanding of the relationship between human genetic data and complex diseases. In this proposed work, a deep neural network (DNN) is applied using SHapley Additive exPlanations (SHAP) to get top 20, 100, 300, and 500 ranking SNPs responsible for AD risk through epistasis interactions. Multi-locus interaction analysis is performed on these identified SNPs using Multifactor Dimensionality Reduction (MDR). This constructive induction algorithm is integrated with DNN for discovering epistasis interactions in a computationally effective method. The proposed framework is applied to Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset. The best accuracies are achieved using the top 500 SNPs, and the classification accuracies varied between 0.860 and 0.874 in the five-way interaction model. However, the classification accuracies of 2-way, 3-way, 4-way models varied between 0.663 and 0.670, 0.718 and 0.727, and 0.793 and 0.803, respectively. The results revealed that the reported accuracy scores of the proposed framework outperform the referenced literature work. The proposed framework presents high-ranked risk genes and promising epistasis interactions that may help in explaining the risk of AD.}
}
@article{WILKINS20258,
title = {Does DeepSeek herald AI's future?},
journal = {New Scientist},
volume = {265},
number = {3529},
pages = {8-9},
year = {2025},
issn = {0262-4079},
doi = {https://doi.org/10.1016/S0262-4079(25)00207-6},
url = {https://www.sciencedirect.com/science/article/pii/S0262407925002076},
author = {Alex Wilkins},
abstract = {The success of Chinese firm DeepSeek suggests tech companies can train and run powerful AIs without consuming vast amounts of power, finds Alex Wilkins}
}
@article{BUSE2022396,
title = {Asynchronous Background Processing for accelerated simulation of wireless communication on multi-core systems},
journal = {Computer Communications},
volume = {193},
pages = {396-409},
year = {2022},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2022.07.032},
url = {https://www.sciencedirect.com/science/article/pii/S0140366422002791},
author = {Dominik S. Buse and Georg Echterling and Falko Dressler},
keywords = {Parallel simulation, Wireless network simulation, Asynchronous parallelization, Vehicular networking},
abstract = {Discrete event simulation (DES) is an important tool for the development and analysis of wireless networks. However, with increasing network size and complexity, the computational effort and simulation time increases significantly, often exponentially. This increase in response time may be critical if DES is interfacing real-time systems like Hardware in the Loop (HIL) or network emulation. It also slows down development cycles of users designing or debugging simulation models. Most popular DES software packages run single-threaded. Thus, they achieve only limited performance improvements from more modern multi-core CPUs. At the same time, existing approaches for parallel simulation of networks do not perform well on wireless systems or require complex paradigm shifts in simulation models. In this paper, we propose Asynchronous Background Processing (ABP) to accelerate the simulation of wireless communication on multi-core systems. By moving expensive computation from the main thread into asynchronous tasks computed by background threads, it accelerates the progression of events and thus reduces response time. Tasks are started as early as possible to exploit the time the main thread spends processing other events, ideally providing results before they are needed in the simulation. We showcase the application of ABP using Veins, a popular vehicular network simulator, demonstrating speedups of up to 3.5 on typical desktop platforms. We further perform an in-depth analysis using advanced profiling techniques to investigate the effectiveness of the parallelization and guide further optimizations.}
}
@article{DIX201013,
title = {Human–computer interaction: A stable discipline, a nascent science, and the growth of the long tail},
journal = {Interacting with Computers},
volume = {22},
number = {1},
pages = {13-27},
year = {2010},
note = {Special Issue: Festschrift for John Long},
issn = {0953-5438},
doi = {https://doi.org/10.1016/j.intcom.2009.11.007},
url = {https://www.sciencedirect.com/science/article/pii/S0953543809000952},
author = {Alan Dix},
keywords = {HCI discipline, Methodology, Theory, Peak experience, Single person study},
abstract = {This paper represents a personal view of the state of HCI as a design discipline and as a scientific discipline, and how this is changing in the face of new technological and social situations. Going back 20years a frequent topic of discussion was whether HCI was a ‘discipline’. It is unclear whether this was ever a fruitful topic, but academic disciplines are effectively about academic communities and there is ample evidence of the long-term stability of the international HCI/CHI community. However, as in computer ‘science’, the central scientific core of HCI is perhaps still unclear; for example, a strength of HCI is the closeness between theory and practice, but the corresponding danger is that the two are often confused. The paper focuses particularly on the challenge of methodological thinking in HCI, especially as the technological and social context of HCI rapidly changes. This is set alongside two other challenges: the development of reliable knowledge in HCI and the clear understanding of interlinked human roles within the discipline. As a case study of the need for methodological thinking, the paper considers the use of single person studies in research and design. These are likely to be particularly valuable as we move from a small number of applications used by many people to a ‘long tail’ where large numbers of applications are used by small numbers of people. This change calls for different practical design strategies; focusing on the peak experience of a few rather than acceptable performance for many. Moving back to the broader picture, as we see more diversity both in terms of types of systems and kinds of concerns, this may also be an opportunity to reflect on what is core across these; potential fragmentation becoming a locus to understand more clearly what defines HCI, not just for the things we see now, but for the future that we cannot see.}
}
@article{RIDDERINKHOF20143,
title = {Neurocognitive mechanisms of perception–action coordination: A review and theoretical integration},
journal = {Neuroscience & Biobehavioral Reviews},
volume = {46},
pages = {3-29},
year = {2014},
note = {Micro- and Macro-Perspectives on Cognitive Conflict Control},
issn = {0149-7634},
doi = {https://doi.org/10.1016/j.neubiorev.2014.05.008},
url = {https://www.sciencedirect.com/science/article/pii/S0149763414001250},
author = {K. Richard Ridderinkhof},
keywords = {Perception–action coordination, Impetus, Motivation, Prediction, Appraisal, Valuation, Impulsive action, Intuitive action, Dual systems, Conation},
abstract = {The present analysis aims at a theoretical integration of, and a systems-neuroscience perspective on, a variety of historical and contemporary views on perception–action coordination (PAC). We set out to determine the common principles or lawful linkages between sensory and motor systems that explain how perception is action-oriented and how action is perceptually guided. To this end, we analyze the key ingredients to such an integrated framework, examine the architecture of dual-system conjectures of PAC, and endeavor in an historical analysis of the key characteristics, mechanisms, and phenomena of PACs. This analysis will reveal that dual-systems views are in need of fundamental re-thinking, and its elements will be amalgamated with current views on action-oriented predictive processing into a novel integrative theoretical framework (IMPPACT: Impetus, Motivation, and Prediction in Perception–Action Coordination theory). From this framework and its neurocognitive architecture we derive a number of non-trivial predictions regarding conative, motive-driven PAC. We end by presenting a brief outlook on how IMPPACT might present novel insights into certain pathologies and into action expertise.}
}
@article{FRERICHS2018135,
title = {Mind maps and network analysis to evaluate conceptualization of complex issues: A case example evaluating systems science workshops for childhood obesity prevention},
journal = {Evaluation and Program Planning},
volume = {68},
pages = {135-147},
year = {2018},
issn = {0149-7189},
doi = {https://doi.org/10.1016/j.evalprogplan.2018.03.003},
url = {https://www.sciencedirect.com/science/article/pii/S0149718917300113},
author = {Leah Frerichs and Tiffany L. Young and Gaurav Dave and Doris Stith and Giselle Corbie-Smith and Kristen {Hassmiller Lich}},
keywords = {Concept mapping, Mental models, Network analysis, Systems science},
abstract = {Across disciplines, it is common practice to bring together groups to solve complex problems. Facilitators are often asked to help groups organize information about and better understand the problem in order to develop and prioritize solutions. However, despite existence of several methods to elicit and characterize how individuals and groups think about and conceptualize an issue, many are difficult to implement in practice-based settings where resources such as technology and participant time are limited and research questions shift over time. This paper describes an easy-to-implement diagramming technique for eliciting conceptualization and a flexible network analysis method for characterizing changes in both individual and group conceptualization. We use a case example to illustrate how we used the methods to evaluate African American adolescent’s conceptual understanding of obesity before and after participating in a series of four systems thinking workshops. The methods produced results that were sensitive to changes in conceptualization that were likely driven by the specific activities employed during the workshop sessions. The methods appear strong for capturing salient levels of conceptualization at both individual and collective levels. The paper concludes with a critical examination of strengths and weaknesses of the methods and implications for future practice and research.}
}
@article{HALKOS2017140,
title = {Climate change effects and their interactions: An analysis aiming at policy implications},
journal = {Economic Analysis and Policy},
volume = {53},
pages = {140-146},
year = {2017},
issn = {0313-5926},
doi = {https://doi.org/10.1016/j.eap.2017.01.005},
url = {https://www.sciencedirect.com/science/article/pii/S031359261630217X},
author = {George E. Halkos and Kyriaki D. Tsilika},
keywords = {Graph theory, Node centrality, Mathematica, Climate related factors, Environmental economics computation},
abstract = {In this study we provide a computerized graph structure for synthesizing and displaying the data on a region’s ecosystem-economic system. By applying Mathematica-based graph modeling we create a causal network of the synergistic impact mechanism among certain climate related factors. Our computational approach identifies a climate factor that affects most immediately or most strongly the others. Important factors are indicated through the use of graph theoretical tools. Our graph-based approach and its computational aspects allow for factor ranking(s) according to their importance to the network both numerically and visually, for certain settlement types. Our contribution provides quantitative estimates of impacts and adaptation potentials of five potential effects of climate change (migration, flooding-landslides-fire, air and water pollution, human health and energy-water-other resources) which play a substantial role at the synergistic impact mechanism. By using graph visualization techniques, the structure of the synergistic impact mechanism is self-evident. Specifically, graph layouts are created to detect i) the causal relationships of the synergistic mechanism under study ii) the most influential factor(s) in the synergistic mechanism and iii) classify the factor’s roles (based on the degree of their impact) within the coping mechanism. Highlighting graph elements let information for policy implications stand out.}
}
@article{KOLERS1984289,
title = {Symbol manipulation: Alternatives to the computational view of mind},
journal = {Journal of Verbal Learning and Verbal Behavior},
volume = {23},
number = {3},
pages = {289-314},
year = {1984},
issn = {0022-5371},
doi = {https://doi.org/10.1016/S0022-5371(84)90182-8},
url = {https://www.sciencedirect.com/science/article/pii/S0022537184901828},
author = {Paul A. Kolers and William E. Smythe},
abstract = {Acquisition and manipulation of symbols are the fundamental constituents of cognitive activity, but modern information processing theory has not explored their basis sufficiently. Computationalism is the one modern approach that takes an explicit position in regard to symbol manipulation. We here explore some of the virtues of that approach and show its principal deficiency, which is to construe symbolization too narrowly, thereby blocking more adequate treatments of learning and acquisition of skills. Symbols come in many kinds; the different kinds allow for different representational capabilities. A proper sorting of symbols and an understanding of their different capabilities is prerequisite, and should be of the greatest benefit, to an account of cognitive processes.}
}
@article{VIDES202258,
title = {A Subspace Method for Time Series Anomaly Detection in Cyber-Physical Systems},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {41},
pages = {58-63},
year = {2022},
note = {4th IFAC Workshop on Cyber-Physical and Human Systems CPHS 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2023.01.103},
url = {https://www.sciencedirect.com/science/article/pii/S2405896323001106},
author = {Fredy Vides and Esteban Segura and Carlos Vargas-Agüero},
keywords = {Anomaly detection, Hankel matrix, time series analysis, sensors, signals},
abstract = {Time series anomaly detection is an important process for system monitoring and model switching, among other applications in cyber-physical systems. In this document we present a fast subspace method for time series anomaly detection, with a relatively low computational cost, that has been designed for anomaly detection in real sensor signals corresponding to dynamical systems. We also present some general results corresponding to the theoretical foundations of our method, together with a prototypical algorithm for time series anomaly detection. Some numerical examples corresponding to applications of the prototypical algorithm are presented, and some computational tools based on the theory and algorithms presented in this paper, are provided.}
}
@article{IGAMBERDIEV2024105346,
title = {Reflexive neural circuits and the origin of language and music codes},
journal = {BioSystems},
volume = {246},
pages = {105346},
year = {2024},
issn = {0303-2647},
doi = {https://doi.org/10.1016/j.biosystems.2024.105346},
url = {https://www.sciencedirect.com/science/article/pii/S0303264724002314},
author = {Abir U. Igamberdiev},
keywords = {Aristotle, Language, Consciousness, Musical code, Reflexive psychology, Self-awareness},
abstract = {Conscious activity is grounded in the reflexive self-awareness in sense perception, through which the codes signifying sensual perceptive events operate and constrain human behavior. These codes grow via the creative generation of hypertextual statements. We apply the model of Vladimir Lefebvre (Lefebvre, V.A., 1987, J. Soc. Biol. Struct. 10, 129–175) to reveal the underlying structures on which the perception and creative development of language and music codes are based. According to this model, the reflexive structure of conscious subject is grounded in three thermodynamic cycles united by the control of the basic functional cycle by the second one, and resulting in the internal action that it turn is perceived by the third cycle evaluating this action. In this arrangement, the generative language structures are formed and the frequencies of sounds that form musical phrases and patterns are selected. We discuss the participation of certain neural brain structures and the establishment of reflexive neural circuits in the ad hoc transformation of perceptive signals, and show the similarities between the processes of perception and of biological self-maintenance and morphogenesis. We trace the peculiarities of the temporal encoding of emotions in music and musical creativity, as well as the principles of sharing musical information between the performing and the perceiving individuals.}
}
@article{MAHAJAN2022103942,
title = {Participatory resilience: Surviving, recovering and improving together},
journal = {Sustainable Cities and Society},
volume = {83},
pages = {103942},
year = {2022},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2022.103942},
url = {https://www.sciencedirect.com/science/article/pii/S2210670722002633},
author = {Sachit Mahajan and Carina I. Hausladen and Javier {Argota Sánchez-Vaquerizo} and Marcin Korecki and Dirk Helbing},
keywords = {Risk, Resilience, Participation, Collective intelligence, Connective action, Sustainability},
abstract = {In the context of urbanization and a growing population, cities and citizens are becoming more exposed and vulnerable to social and environmental changes, ranging from natural disasters like earthquakes and floods to uncertainties caused by issues related to climate change and complex social dynamics or even pandemics. There have been many debates about implementing resilience thinking that allow cities and communities to prepare for possible stresses and shocks. Although there are sets of frameworks aimed at building inclusive resilience strategies fostering participation and engagement, there is limited resilience-related literature on how to conceptualize participation. Through an extensive review of various kinds of publications on resilience, policy documents, and case studies, which emphasize the concepts of participation, coordination, and co-creation, this review explores and investigates how citizen participation is discussed and applied in the context of participatory resilience. We conclude that participatory approaches possess a great potential to enhance multi-stakeholder cooperation, social innovation, and capacity building for resilience. Realization of the potential of participatory resilience will remain limited, however, unless participation strategies and frameworks are made more transparent, inclusive, and context-sensitive.}
}
@article{HU2019202,
title = {Effective Connectivity of the Fronto-Parietal Network during the Tangram Task in a Natural Environment},
journal = {Neuroscience},
volume = {422},
pages = {202-211},
year = {2019},
issn = {0306-4522},
doi = {https://doi.org/10.1016/j.neuroscience.2019.09.021},
url = {https://www.sciencedirect.com/science/article/pii/S0306452219306669},
author = {Zhishan Hu and Keng-Fong Lam and Zhen Yuan},
keywords = {Tangram, Visuospatial reasoning, Granger causality, fNIRS, Brain networks},
abstract = {Although the neural basis underlying visuospatial reasoning has been widely explored by neuroimaging techniques, the brain activation patterns during naturalistic visuospatial reasoning such as tangram remains unclear. In this study, the directional functional connectivity of fronto-parietal networks during the tangram task was carefully inspected by using combined functional near-infrared spectroscopy (fNIRS) and conditional Granger causality analysis (GCA). Meanwhile, the causal networks during the traditional spatial reasoning task were also characterized to exhibit the differences with those during the tangram task. We discovered that the tangram task in a natural environment showed enhanced activation in the fronto-parietal regions, particularly the frontal cortex. In addition, a strong directional connectivity from the right prefrontal cortex to left angular gyrus was detected for the complex spatial reasoning condition of spatial reasoning task, whereas no effective connectivity was identified between the frontal and parietal cortices during the tangram task. Further correlation analyses showed that the behavioral performance in the spatial reasoning rather than the tangram task manifested the relationship with the connectivity between the frontal and parietal cortex. Our findings demonstrate that the tangram task measures a different aspect of the visuospatial reasoning ability which requires more trial-and-error strategies and creative thinking rather than inductive reasoning. In particular, the frontal cortex is mostly involved in tangram puzzle-solving, whereas the interaction between frontal and parietal cortices is regulated by the hands-on experience during the tangram task.}
}
@article{LAURENT2025,
title = {HUMAn, a Real-Time Evolutive Patient Model for Major Incident Simulation: Development and Validation Study},
journal = {JMIR Formative Research},
volume = {9},
year = {2025},
issn = {2561-326X},
doi = {https://doi.org/10.2196/66201},
url = {https://www.sciencedirect.com/science/article/pii/S2561326X25001945},
author = {Maxence Laurent and Arnaud Jaccard and Laurent Suppan and Elio Erriquez and Xavier Good and Eric Golay and Dominique Jaccard and Mélanie Suppan},
keywords = {physiological model, mathematical model, computer simulation, major incident management, emergency medicine, mass casualties, healthcare professional education, professional education, continuing education},
abstract = {Background
Major incidents correspond to any situation where the location, number, severity, or type of casualties requires extraordinary resources. Major incident management must be efficient to save as many lives as possible. As any paramedic or emergency medical technician may unexpectedly have to respond to major incidents, regular training is mandatory. Those trainings usually include simulations. The vast majority of major incident simulations are limited by the fact that simulated patients do not evolve during the simulation, regardless of the time elapsed and treatment decisions. Therefore, most simulations fail to incorporate the critical temporal effect of decision-making.
Objective
This study aimed to develop and validate a simplified mathematical model of physiology, capable of plausibly simulating the real-time evolution of several injuries.
Methods
A modified version of the user-centered design framework, including a relevance, development, and validation phase, was used to define the development process of the physiological model. A 12-member design and development team was established, including prehospital physicians, paramedics, and computer scientists. To determine whether the developed model was clinically realistic, 15 experienced professionals working in the prehospital field participated in the validation phase. They were asked to rate clinical and physiological parameters according to a 5-point Likert scale ranging from 1 (impossible) to 5 (absolutely realistic).
Results
The design and development team led to the development of the HUMAn model (Human is an Uncomplicated Model of Anatomy). During the relevance phase, the team defined the needed features of the model: clinically realistic, able to compute the evolution of prehospital vital signs, yet simple enough to allow real-time computation for several simulated patients on regular computers or tablets. During the development phase, iterations led to the development of a heart-lung-brain interaction model coupled to functional blocks representing the main anatomical body parts. During the validation phase, the evolution of nine simulated patients presenting pathologies devised to test the different systems and their interactions was assessed. Overall, clinical parameters of all patients had a median rating of 5 (absolutely realistic; IQR 4-5). Most (n=52, 96%) individual clinical parameters had a median rating of 5, the remainder (n=2, 4%) being rated 4. Overall physiological parameters of all patients had a median rating of 5 (absolutely realistic; IQR 3-5). The majority of individual physiological parameters (n=43, 79%) had a median rating of 5, with (n=9, 17%) rated 4, and only (n=2 ,4%) rated 3.
Conclusions
A simplified model of trauma patient evolution was successfully created and deemed clinically realistic by experienced clinicians. This model should now be included in computer-based simulations and its impact on the teaching of major incident management assessed through randomized trials.}
}
@article{GONZALEZ2024446,
title = {BOIS: Bayesian Optimization of Interconnected Systems},
journal = {IFAC-PapersOnLine},
volume = {58},
number = {14},
pages = {446-451},
year = {2024},
note = {12th IFAC Symposium on Advanced Control of Chemical Processes ADCHEM 2024},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2024.08.377},
url = {https://www.sciencedirect.com/science/article/pii/S2405896324011364},
author = {Leonardo D. González and Victor M. Zavala},
keywords = {Bayesian optimization, grey-box modeling, composite functions},
abstract = {Bayesian optimization (BO) has proven to be an effective paradigm for the global optimization of expensive-to-sample systems. One of the main advantages of BO is its use of Gaussian processes (GPs) to characterize model uncertainty which can be leveraged to guide the learning and search processes. However, BO typically treats systems as black-boxes and this limits the ability to exploit structural knowledge (e.g., physics and sparse interconnections). Composite functions of the form f(x,y(x)), wherein GP modeling is shifted from the performance function f to an intermediate function y, offer an avenue for exploiting structural knowledge. However, the use of composite functions in a BO framework is complicated by the need to generate a probability density for f from the Gaussian density of y calculated by the GP (e.g., when f is nonlinear it is not possible to obtain a closed-form expression). Previous work has handled this issue using sampling techniques; these are easy to implement and flexible but are computationally intensive. In this work, we introduce a new paradigm which allows for the efficient use of composite functions in BO; this uses adaptive linearizations of f to obtain closed-form expressions for the statistical moments of the composite function. We show that this simple approach (which we call BOIS) enables the exploitation of structural knowledge, such as that arising in interconnected systems as well as systems that embed multiple GP models and combinations of physics and GP models. Using a chemical process optimization case study, we benchmark the effectiveness of BOIS against standard BO and sampling approaches. Our results indicate that BOIS achieves performance gains and accurately captures the statistics of composite functions.}
}
@article{SANDERS2023107790,
title = {Methodological innovations to strengthen evidence-based serious illness communication},
journal = {Patient Education and Counseling},
volume = {114},
pages = {107790},
year = {2023},
issn = {0738-3991},
doi = {https://doi.org/10.1016/j.pec.2023.107790},
url = {https://www.sciencedirect.com/science/article/pii/S0738399123001702},
author = {Justin J. Sanders and Danielle Blanch-Hartigan and Jonathan Ericson and Elise Tarbi and Donna Rizzo and Robert Gramling and Liesbeth {van Vliet}},
keywords = {Communication, Palliative care, Methodology},
abstract = {Background/Objective
A growing population of those affected by serious illness, prognostic uncertainty, patient diversity, and healthcare digitalization pose challenges for the future of serious illness communication. Yet, there is paucity of evidence to support serious illness communication behaviors among clinicians. Herein, we propose three methodological innovations to advance the basic science of serious illness communication.
Results
First, advanced computation techniques – e.g. machine-learning techniques and natural language processing – offer the possibility to measure the characteristics and complex patterns of audible serious illness communication in large datasets. Second, immersive technologies – e.g., virtual- and augmented reality – allow for experimentally manipulating and testing the effects of specific communication strategies, and interactional and environmental aspects of serious illness communication. Third, digital-health technologies – e.g., shared notes and videoconferences – can be used to unobtrusively observe and manipulate communication, and compare in-person to digitally-mediated communication elements and effects. Immersive and digital health technologies allow integration of physiological measurement (e.g. synchrony or gaze) that may advance our understanding of patient experience.
Conclusion/practice implications
New technologies and measurement approaches, while imperfect, will help advance our understanding of the epidemiology and quality of serious illness communication in an evolving healthcare environment.}
}
@article{MAASDORP2025102610,
title = {Non-tuberculosis mycobacteria identified by line probe assays in respiratory and non-respiratory samples in South Africa between 2015 and 2019},
journal = {Tuberculosis},
volume = {151},
pages = {102610},
year = {2025},
issn = {1472-9792},
doi = {https://doi.org/10.1016/j.tube.2025.102610},
url = {https://www.sciencedirect.com/science/article/pii/S1472979225000058},
author = {Elizna Maasdorp and Yonas Ghebrekristos and Amanda Khumalo and Lynthia Paul and Monique J. Williams},
keywords = {Non-tuberculosis mycobacteria, Pulmonary disease, Line probe assay},
abstract = {In recent years, a rise in non-tuberculosis mycobacteria pulmonary disease (NTM-PD) has been reported in several countries. However, data for high-burden tuberculosis settings, including South Africa, is currently limited. In this study, we conducted a retrospective analysis of routine diagnostic data obtained from one diagnostic laboratory in South Africa between 2015 and 2019. During this period, samples from 275 individuals with suspected mycobacterial infection were tested using the GenoType Mycobacterium CM (Common mycobacteria) or AS (Additional species) line probe assay (LPA) (Brucker-Hain Life science, Nehren, Germany), yielding an NTM-positive result for 163 of these individuals. Interestingly, the positivity rate in respiratory samples declined from 93 % in 2015 to 79 % in 2019. Just over half of the positive samples were of respiratory origin, and the most common species identified in respiratory samples was Mycobacterium intracellulare/Mycobacteium avium complex (28.9 %), followed by M. avium (17.4 %). Where the mycobacterial species was not identified by the LPA, a higher proportion of the subsequent cultures were negative, suggestive of colonisation rather than infection. More than half of patients with a positive NTM-LPA were HIV positive (55.9 %), and this association declined slightly during the study period (62.5 %–50 %).}
}
@article{CHANG2023109277,
title = {Location and timestamp-based chip contour detection using LWMG-YOLOv5},
journal = {Computers & Industrial Engineering},
volume = {180},
pages = {109277},
year = {2023},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2023.109277},
url = {https://www.sciencedirect.com/science/article/pii/S0360835223003017},
author = {Bao Rong Chang and Hsiu-Fen Tsai and Chia-Wei Hsieh},
keywords = {Chip contour detection, Real-time image recognition, LWMG-YOLOv5 model, Ghost convolution, Attention mechanism, and Minimizing production costs},
abstract = {In the fab, semiconductor manufacturers often use deep learning approaches for chip contour detection to shorten automated optical inspection to minimize the loss of production costs and lower power consumption in chip contour detection for realizing energy-efficient computing. However, YOLOv5 and GSEH-YOLOv5 models have sacrificed their accuracy to improve the operational speed. MobileNetv3-YOLOv5 model can enhance the accuracy but lacks high-speed operation. Therefore, this study presents a light version of MobileNetv3-YOLOv5 model with ghost convolution, abbreviated LWMG-YOLOv5, to speed up chip contour detection because this architecture can reduce the number of model parameters and computational burden at the same time. As a result, the proposed approach can outperform the other methods by getting a 3.62% speed-up in chip contour detection to gain a better manufacturing advantage in increasing the chip yields by 1.7% and reducing the loss of production costs by 1.83% significantly.}
}
@article{MEZINSKA2024100429,
title = {Design-driven innovation in STEM disciplines in higher education: The role and impact of transversal competences},
journal = {Journal of Open Innovation: Technology, Market, and Complexity},
volume = {10},
number = {4},
pages = {100429},
year = {2024},
issn = {2199-8531},
doi = {https://doi.org/10.1016/j.joitmc.2024.100429},
url = {https://www.sciencedirect.com/science/article/pii/S2199853124002233},
author = {Silvija Mezinska and Anda Abolina and Velta Lubkina},
keywords = {Design-driven innovations, Transversal competences, STEM disciplines, Higher education},
abstract = {The purpose of this study is to explore students’ and academic staff’ perspectives on the importance of transversal competences in creating design-driven innovations within STEM disciplines in higher education, aiming to find solutions for the enhancement of transversal competences. Accordingly, the following research questions are explored in this study: 1) What is the connection between design-driven innovation and transversal competences in STEM disciplines in higher education? 2) How can students' and academic staff’s self-assessment of transversal competences influence the creation of design-driven innovations? Throughout this study, the authors analyze the promotion of design-driven innovation by enhancing the transversal competences of academic staff of STEM disciplines and students. The exploration of qualitative components of transversal competences enable the identification of conditions that may contribute design-driven innovations in higher education. By examining the link between design-driven innovations and transversal competences, added value in competence, design and values-based higher education, based on socio-cultural environment and technological development. The results of the study, based on contemporary knowledge including survey data and focus group discussions on this topic, will be utilized as development conditions, an assessment system to promote progress towards design-driven innovations in higher education facilitated by the enhancement of transversal competences.}
}
@article{SHAFFER202041,
title = {Artificial intelligence products reshape accounting: time to re-train},
journal = {Development and Learning in Organizations: An International Journal},
volume = {34},
number = {6},
pages = {41-43},
year = {2020},
issn = {1477-7282},
doi = {https://doi.org/10.1108/DLO-10-2019-0242},
url = {https://www.sciencedirect.com/science/article/pii/S1477728220001288},
author = {Kathie J. Shaffer and Carol J. Gaumer and Kiersten P. Bradley},
keywords = {Organizational change, Accounting, Artificial intelligence},
abstract = {Purpose
Managers are expected to increase productivity in the most cost-efficient manner, using all available resources and, “work smarter.” As technology improves, there is greater incentive for managers to invest in options where automation becomes less expensive than the high cost of human capital. When repetitive tasks can be accurately duplicated through automation, the decision becomes a fait accompli. Advances in artificial intelligence (AI) or synthetic intelligence that simulates human intellectual function has significant impact potential in the service sector. This paper examines productivity efficiencies sought through artificial intelligence and the need for re-training, specifically in the accounting profession.
Design/methodology/approach
This is a conceptual paper for practitioners without research methodology.
Findings
The accounting profession 10 years from now will look noticeably different than it does now. The accountants, who embrace the new technologies, like artificial intelligence, will survive and even thrive by becoming more specialized. This will require training and, in some instances, re-training. Organizations must be willing to absorb those development costs. I hope that new graduates will enter the profession with updated skills providing added value for organizations and employers who started into the profession many years ago. The biggest challenge may lie in the re-training of accountants who have been in practice for many years and managing the resistance to change. Employers must first set the example by accepting the inevitable and then encourage and support employees to improve and update their skills. Additionally, they will have to coach employees through the changes with reassurance that those who embrace the change will experience less chance of job elimination. Embracing the available technology will enable firms to serve clients more efficiently and effectively by providing up to date business solutions regardless of the services being offered.
Research limitations/implications
There is no empirical research in this paper. It is a conceptual piece looking at the changing organization in accounting, specifically due to artificial intelligence.
Practical implications
Accounting firms that focus on basic accounting functions should find new services to offer. The same clients can be served, but at a higher-level. Accountants will offer more value to clients by detecting patterns and trends when more time can be devoted to analysis. Helping clients beyond the preparation of documents requires that accountants understand the current market conditions and potential effects of inflation and, engage in more critical thinking while at the same time be able to teach clients and help them understand at the higher level. Just as accountants’ responsibilities and duties will be transformed through the integration of AI, accounting education must be altered.
Social implications
Implications related to the workplace are only discussed in this paper.
Originality/value
It is not completely original. It is a compilation of research that is out there as a means to address critical workforce training needs in accounting as technology moves forward.}
}
@article{STROJNY2024,
title = {Use of 4 Open-Ended Text Responses to Help Identify People at Risk of Gaming Disorder: Preregistered Development and Usability Study Using Natural Language Processing},
journal = {JMIR Serious Games},
volume = {12},
year = {2024},
issn = {2291-9279},
doi = {https://doi.org/10.2196/56663},
url = {https://www.sciencedirect.com/science/article/pii/S2291927924001053},
author = {Paweł Strojny and Ksawery Kapela and Natalia Lipp and Sverker Sikström},
keywords = {gaming disorder, natural language processing, machine learning, mental health, NLP, text, open-ended, response, risk, psychological, Question-based Computational Language Assessment, QCLA, transformers-based, language model analysis, Polish, Pearson, correlation, Python},
abstract = {Background
Words are a natural way to describe mental states in humans, while numerical values are a convenient and effective way to carry out quantitative psychological research. With the growing interest of researchers in gaming disorder, the number of screening tools is growing. However, they all require self-quantification of mental states. The rapid development of natural language processing creates an opportunity to supplement traditional rating scales with a question-based computational language assessment approach that gives a deeper understanding of the studied phenomenon without losing the rigor of quantitative data analysis.
Objective
The aim of the study was to investigate whether transformer-based language model analysis of text responses from active gamers is a potential supplement to traditional rating scales. We compared a tool consisting of 4 open-ended questions formulated based on the clinician's intuition (not directly related to any existing rating scales for measuring gaming disorders) with the results of one of the commonly used rating scales.
Methods
Participants recruited using an online panel were asked to answer the Word-Based Gaming Disorder Test, consisting of 4 open-ended questions about gaming. Subsequently, they completed a closed-ended Gaming Disorders Test based on a numerical scale. Of the initial 522 responses collected, we removed a total of 105 due to 1 of 3 criteria (suspiciously low survey completion time, providing nonrelevant or incomplete responses). Final analyses were conducted on the responses of 417 participants. The responses to the open-ended questions were vectorized using HerBERT, a large language model based on Google's Bidirectional Encoder Representations from Transformers (BERT). Last, a machine learning model, specifically ridge regression, was used to predict the scores of the Gaming Disorder Test based on the features of the vectorized open-ended responses.
Results
The Pearson correlation between the observable scores from the Gaming Disorder test and the predictions made by the model was 0.476 when using the answers of the 4 respondents as features. When using only 1 of the 4 text responses, the correlation ranged from 0.274 to 0.406.
Conclusions
Short open responses analyzed using natural language processing can contribute to a deeper understanding of gaming disorder at no additional cost in time. The obtained results confirmed 2 of 3 preregistered hypotheses. The written statements analyzed using the results of the model correlated with the rating scale. Furthermore, the inclusion in the model of data from more responses that take into account different perspectives on gaming improved the performance of the model. However, there is room for improvement, especially in terms of supplementing the questions with content that corresponds more directly to the definition of gaming disorder.
Trial Registration
OSF Registries osf.io/957nz; https://osf.io/957nz}
}
@article{ASEMI2025106795,
title = {Improving EEG signal-based emotion recognition using a hybrid GWO-XGBoost feature selection method},
journal = {Biomedical Signal Processing and Control},
volume = {99},
pages = {106795},
year = {2025},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2024.106795},
url = {https://www.sciencedirect.com/science/article/pii/S174680942400853X},
author = {Hanie Asemi and Nacer Farajzadeh},
keywords = {Emotion recognition, Brain signals, EEG, Prediction, Feature selection, Machine learning},
abstract = {Emotion plays a crucial role in daily life, influencing cognitive functions such as language comprehension, decision-making, attention, and concentration. With the growing integration of computer systems into our everyday activities, it is essential to understand and detect emotional states accurately. Emotion detection through EEG signals allows direct assessment of the human’s internal state and is considered an important factor in the interaction between humans and external devices. In this paper, we introduce a novel feature selection algorithm proposed to improve the accuracy of emotion classification using EEG signals, aligned with decreasing the input dimension to reduce computations, making it more suitable for real-time applications. We performed two experiments utilizing the DEAP and the MAHNOB-HCI datasets. Various features were extracted and employed for emotion classification using SVM, KNN, and XGBoost classifiers. Initially, the highest accuracy for binary emotion classification in the DEAP dataset was achieved with statistical features and the XGBoost model, reaching 78.85% for arousal and 79.02% for valence. In the MAHNOB-HCI dataset, the highest accuracy with statistical features and the XGBoost model was 67.08% for arousal and 62.24% for valence. Subsequently, we applied the grey wolf optimization algorithm as a feature selection method, optimizing the cost function based on XGBoost accuracy. This approach significantly enhanced the classification performance. For the DEAP dataset, accuracy increased to 89.63% for arousal and 89.08% for valence using statistical features. For the MAHNOB-HCI dataset, accuracy improved to 84.94% for arousal and 82.29% for valence using statistical features.}
}
@article{KARIM2024742,
title = {Repetitive Negative Thoughts and the Brain as a Resource-Limited Machine},
journal = {Biological Psychiatry: Cognitive Neuroscience and Neuroimaging},
volume = {9},
number = {8},
pages = {742-743},
year = {2024},
issn = {2451-9022},
doi = {https://doi.org/10.1016/j.bpsc.2024.06.011},
url = {https://www.sciencedirect.com/science/article/pii/S2451902224001691},
author = {Helmet T. Karim}
}
@article{COCHRAN20201237,
title = {Sustainable Enterprise Design 4.0: Addressing Industry 4.0 Technologies from the Perspective of Sustainability},
journal = {Procedia Manufacturing},
volume = {51},
pages = {1237-1244},
year = {2020},
note = {30th International Conference on Flexible Automation and Intelligent Manufacturing (FAIM2021)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.10.173},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920320308},
author = {David S. Cochran and Erwin Rauch},
keywords = {Industry 4.0, sustainability, triple bottom line, sustainable enterprise design, industrial revolution},
abstract = {The introduction of Industry 4.0 and sustainability in production is currently on everyone’s mind. However, companies face difficulties to address these trends in their long-term enterprise strategy and design. Industry 4.0 promises strategic advantages for companies in many respects, but there is a lack of instruments and concepts for integrating emerging technologies in an overall enterprise system design. Similarly, the multiple perspectives regarding economic, environmental and social sustainability provide a framework for thinking about a strategy for sustainable enterprise design. Based on the three principles presented in this paper for Sustainable Enterprise Design, this article aims to present an approach to better address sustainability as well as Industry 4.0 in terms of a long-term strategic, enterprise design that is sustainable. As a result, a list of needs, functional requirements as well as possible Industry 4.0 physical solutions is proposed to achieve a long-term sustainable enterprise design. The consequence of the perspective of an enterprise as a system that can be designed provides a rigorous approach that takes advantage of Industry 4.0 technologies and the multiple perspectives and candidate physical solutions that the research community offers.}
}
@article{PETIT2018135,
title = {Combining eco-social and environmental indicators to assess the sustainability performance of a food value chain: A case study},
journal = {Journal of Cleaner Production},
volume = {191},
pages = {135-143},
year = {2018},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2018.04.156},
url = {https://www.sciencedirect.com/science/article/pii/S0959652618311831},
author = {Gaëlle Petit and Caroline Sablayrolles and Gwenola {Yannou-Le Bris}},
keywords = {Life cycle assessment, Pork value chain, Sustainability, Metrics, Indicator, Framework},
abstract = {Stakeholders are increasingly demanding transparency on food value chain sustainability performance. Today there is no standard framework to meet this demand and support defining indicators to be used to conduct an overall sustainable performance assessment. This paper mobilizes existing frameworks and indicators to build new sustainable performance metrics for actors willing to work together for their value chain sustainability. Popular methods or tools for assessing dimensions of agrifood products or activities are selected and analyzed to determine how they could contribute to this metric. The analysis aims to distinguish the sustainable development pillars addressed (economic, environmental and/or social), the frames concerned (life cycle thinking or not; multi-actor or not), and the focus of performance measured (drivers, pressures, states, impacts, responses). This categorization is then used to develop a proposal for specifications adapted to food value chain sustainability performance assessment. The applicability of the framework is demonstrated through a case study in a pork agrifood value chain.}
}
@article{GU2024110161,
title = {A Bayesian decision network–based pre-disaster mitigation model for earthquake-induced cascading events to balance costs and benefits on a limited budget},
journal = {Computers & Industrial Engineering},
volume = {191},
pages = {110161},
year = {2024},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2024.110161},
url = {https://www.sciencedirect.com/science/article/pii/S0360835224002821},
author = {Wenjing Gu and Jiangnan Qiu and Jilei Hu and Xiaowei Tang},
keywords = {Earthquake-induced cascading disasters, Pre-disaster mitigation, Bayesian decision network, Cost–benefit analyses, Limited budgets},
abstract = {Cascading disasters induced by earthquakes amplify the severity of the initial impact on environment. Although decisionmakers may face uncertainty, an effective mitigation strategy is critical in environmental management. We propose an earthquake-induced cascading disaster mitigation–Bayesian decision network (ECDM-BDN) model to assess pre-disaster mitigating strategies under limited budgets from the perspective of systematic thinking. This model graphically represents the complex relationship among various variables in a seismic hazard system and resistance system based on disaster system theory. It can predict the triggering of cascading events through probabilistic reasoning and identify the key variable accountable for the range of outputs observed through sensitivity analysis. In addition, cost–benefit analyses are carried out by combining Bayesian decision network utility nodes and dynamic programming to obtain a balance between costs and benefits in the context of limited budgets. An earthquake-induced liquefaction served as a case study to demonstrate the proposed model’s effectiveness. Experimental results indicate that the ECDM-BDN model can balance the costs and effects of each pre-disaster mitigating strategy as well as select the optimal one according to the utility value. The proposed model can perform a “white-box” decision-making process, which is expected to guide earthquake-induced cascading event pre-disaster mitigation in cases of limited budgets.}
}
@article{DOWLING201949,
title = {Interactive Visual Analytics for Sensemaking with Big Text},
journal = {Big Data Research},
volume = {16},
pages = {49-58},
year = {2019},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2019.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S2214579618302995},
author = {Michelle Dowling and Nathan Wycoff and Brian Mayer and John Wenskovitch and Scotland Leman and Leanna House and Nicholas Polys and Chris North and Peter Hauck},
keywords = {Text analytics, Big data, Visualization, Interactive visual analytics, Semantic interaction, Topic modeling},
abstract = {Analysts face many steep challenges when performing sensemaking tasks on collections of textual information larger than can be reasonably analyzed without computational assistance. To scale up such sensemaking tasks, new methods are needed to interactively integrate human cognitive sensemaking activity with machine learning. Towards that goal, we offer a human-in-the-loop computational model that mirrors the human sensemaking process, and consists of foraging and synthesis sub-processes. We model the synthesis loop as an interactive spatial projection and the foraging loop as an interactive relevance ranking combined with topic modeling. We combine these two components of the sensemaking process using semantic interaction such that the human's spatial synthesis actions are transformed into automated foraging and synthesis of new relevant information. Ultimately, the model's ability to forage as a result of the analyst's synthesis activities makes interacting with big text data easier and more efficient, thereby facilitating analysts' sensemaking ability. We discuss the interaction design and theory behind our interactive sensemaking model. The model is embodied in a novel visual analytics prototype called Cosmos in which analysts synthesize structure within the larger corpus by directly interacting with a reduced-dimensionality space to express relationships on a subset of data. We then demonstrate how Cosmos supports sensemaking tasks with a realistic scenario that investigates the affect of natural disasters in Adelaide, Australia in September 2016 using a database of over 30,000 news articles.}
}
@article{ALBABA20191,
title = {Modeling cyber-physical human systems via an interplay between reinforcement learning and game theory},
journal = {Annual Reviews in Control},
volume = {48},
pages = {1-21},
year = {2019},
issn = {1367-5788},
doi = {https://doi.org/10.1016/j.arcontrol.2019.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S1367578819301026},
author = {Berat Mert Albaba and Yildiray Yildiz},
keywords = {Cyber-physical human systems, Game theory, Reinforcement learning, Model validation},
abstract = {Predicting the outcomes of cyber-physical systems with multiple human interactions is a challenging problem. This article reviews a game theoretical approach to address this issue, where reinforcement learning is employed to predict the time-extended interaction dynamics. We explain that the most attractive feature of the method is proposing a computationally feasible approach to simultaneously model multiple humans as decision makers, instead of determining the decision dynamics of the intelligent agent of interest and forcing the others to obey certain kinematic and dynamic constraints imposed by the environment. We present two recent exploitations of the method to model (1) unmanned aircraft integration into the National Airspace System and (2) highway traffic. We conclude the article by providing ongoing and future work about employing, improving and validating the method. We also provide related open problems and research opportunities.}
}
@incollection{GOLDSCHMIDT202050,
title = {Architecture☆},
editor = {Mark Runco and Steven Pritzker},
booktitle = {Encyclopedia of Creativity (Third Edition)},
publisher = {Academic Press},
edition = {Third Edition},
address = {Oxford},
pages = {50-56},
year = {2020},
isbn = {978-0-12-815615-5},
doi = {https://doi.org/10.1016/B978-0-12-809324-5.23520-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128093245235207},
author = {Gabriela Goldschmidt},
keywords = {Architectural design, Architectural education, Culture, Digital design, Form, Function, Ideas, Performance, Starchitect, Style},
abstract = {Architecture is a cultural arena based on ideas, which jointly produce styles and individually, at their best, generate outstanding buildings. In our era architecture is expected to innovate in its forms, while ensuring perfect performance and adaptation to the environment. Form and function handling are rough correlates of originality and practicality, by which we measure design creativity. Architecture is also a product of the technological state of its time. At present we experience computational advances that fundamentally change buildings and the way they are designed. Architectural education is groping to adjust to the changes.}
}
@article{WANG2024104116,
title = {Revealing association rules within intricate ecosystems: A spatial co-location mining method based on Geo-Eco knowledge graph},
journal = {International Journal of Applied Earth Observation and Geoinformation},
volume = {133},
pages = {104116},
year = {2024},
issn = {1569-8432},
doi = {https://doi.org/10.1016/j.jag.2024.104116},
url = {https://www.sciencedirect.com/science/article/pii/S1569843224004709},
author = {Jinghan Wang and Guangyue Li and Tinghua Ai},
keywords = {Geo-eco knowledge graph, Spatial co-location pattern, Environmental similarity, Neo4j graph database},
abstract = {The analysis of association rules within ecosystems is crucial for monitoring, managing, and conserving natural resources. As widely adopted approaches for this task, geospatial methods involving spatial co-location pattern mining can reveal distribution rules and inherent associations among diverse geographical elements. Rooted in Tobler’s first law of geography, these methods focus on the impact of spatial proximity. However, apart from proximity, heterogeneity of environmental attributes such as elevation, temperature and precipitation are also essential for the formation of associations. For environmental co-location (Eco-location) pattern detection, we propose a method based on the Geo-Eco Knowledge Graph (GEKG) to mine multi-impact association rules. Firstly, we introduce the Adaptive Threshold (AT) to constrain the Delaunay triangular network, dynamically regulating adjacency relationships to generate geo-eco knowledge graph’s skeleton. For comprehensive ecosystem representation, various environmental attributes are integrated as semantic information into GEKG. In the reasoning of Eco-location patterns, we innovate beyond the traditional co-location paradigm by considering both spatial proximity and semantic similarity. Under the impact of various environmental information, sub-sets of geographically proximate entities are extracted to detect Eco-location patterns. For effective management and efficient computation, we utilize the Neo4j graph database to manage large-scale GEKG and mine Eco-location patterns with its graph search function. Experiments conducted on simulated and real-world ecological datasets show that, compared to existing techniques, our GEKG-based method can detect Eco-location patterns with greater accuracy and efficiency.}
}
@incollection{VOIT20211,
title = {Networks and Dynamic Models in Systems Medicine: Overview},
editor = {Olaf Wolkenhauer},
booktitle = {Systems Medicine},
publisher = {Academic Press},
address = {Oxford},
pages = {1-7},
year = {2021},
isbn = {978-0-12-816078-7},
doi = {https://doi.org/10.1016/B978-0-12-801238-3.11661-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128012383116617},
author = {Eberhard O. Voit},
keywords = {Complexity, Dynamic system, Emergent properties, Exposome, Graph, Network, Personalized medicine, Precision medicine, Predictive health, Reductionism, Systems biology},
abstract = {Systems medicine approaches health and disease as a holistic response of the human body. Its main challenge is the complexity of the web of uncounted processes that govern normal physiology, as well as deviations toward disease. Expanding the vast knowledge amassed by traditional medicine, systems medicine addresses this challenge by adopting and adapting concepts and methods from systems biology and by making use of powerful new datasets stemming, for instance, from wearable devices and electronic health records. Systems medicine shares concepts and goals with exposome research and can be considered the foundation for personalized or precision medicine and predictive health. Systems medicine is in its early infancy, and its character, concepts, and methodologies will probably change over time. Nevertheless, many of its computational approaches will continue to benefit from the representation of health and disease processes as networks and dynamic systems. This overview describes the basic concepts of computational systems medicine and briefly summarizes the subsequent chapters, which discuss methods, challenges, and applications of networks and dynamic models in systems medicine.}
}
@article{ACAR2010405,
title = {Designing insightful inquiring systems for sustainable organizational foresight},
journal = {Futures},
volume = {42},
number = {4},
pages = {405-416},
year = {2010},
note = {Learning the Future Faster},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2009.11.025},
url = {https://www.sciencedirect.com/science/article/pii/S0016328709001992},
author = {William Acar and Douglas A. Druckenmiller},
abstract = {This paper contributes to the theory of collaborative problem solving and strategy design by reviewing the state of the art in the application of problem solving and dialectical methods, and then linking up with analytical and computer-aided approaches. Churchman's concept of dialectical inquiry (DI) is presented, and some major derivatives of DI are reviewed, as well as an integrative method for sustainable insightful foresight developed by the authors called comprehensive situation mapping (CSM). In addition to its dialectical process side, CSM offers computational capabilities for devising and figuring out change scenarios. The theory for the manual application of CSM is summarized; in addition, its recent computerized version is presented and likely future improvements are sketched out in light of a current spate of case studies investigating the user-friendliness of its computerization.}
}
@article{VASEY2025113719,
title = {Influence of initial conditions on data-driven model identification and information entropy for ideal mhd problems},
journal = {Journal of Computational Physics},
volume = {524},
pages = {113719},
year = {2025},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2025.113719},
url = {https://www.sciencedirect.com/science/article/pii/S0021999125000026},
author = {Gina Vasey and Daniel Messenger and David Bortz and Andrew Christlieb and Brian O'Shea},
keywords = {Weak sparse identification of nonlinear dynamics, Data-driven, Ideal MHD, Shannon information entropy},
abstract = {Data-driven methods of model identification are able to discern governing dynamics of a system from data. Such methods are well suited to help us learn about systems with unpredictable evolution or systems with ambiguous governing dynamics given our current understanding. Many plasma problems of interest fall into these categories as there are a wide range of models that exist, however each model is only useful in a certain regime and often limited by computational complexity. To ensure data-driven methods align with theory, they must be consistent and predictable when acting on data whose governing dynamics are known. Weak Sparse Identification of Nonlinear Dynamics (WSINDy) is a recently developed data-driven method that has shown promise in learning governing dynamics from data with high noise levels [1]. This work examines how WSINDy acts on ideal MHD test problems as the initial conditions are varied and specifies limiting requirements for successful equation identification. It is hard to recover the governing dynamics from data that emphasize a single dominant behavior. In these low information cases, Shannon information entropy is able to pick up on the redundancies in the data that affect recoverability.}
}
@article{ASHTIANI201549,
title = {A survey of quantum-like approaches to decision making and cognition},
journal = {Mathematical Social Sciences},
volume = {75},
pages = {49-80},
year = {2015},
issn = {0165-4896},
doi = {https://doi.org/10.1016/j.mathsocsci.2015.02.004},
url = {https://www.sciencedirect.com/science/article/pii/S0165489615000165},
author = {Mehrdad Ashtiani and Mohammad Abdollahi Azgomi},
abstract = {There has always been a steady interest in how humans make decisions amongst researchers from various fields. Based on this interest, many approaches such as rational choice theory or expected utility hypothesis have been proposed. Although these approaches provide a suitable ground for modeling the decision making process of humans, they are unable to explain the corresponding irrationalities and existing paradoxes and fallacies. Recently, a new formulation of decision theory that can correctly describe these paradoxes and possibly provide a unified and general theory of decision making has been proposed. This new formulation is founded based on the application of the mathematical structure of quantum theory to the fields of human decision making and cognition. It is shown that by applying these quantum-like models, one can better describe the uncertainty, ambiguity, emotions and risks involved in the human decision making process. Even in computational environments, an agent that follows the correct patterns of human decision making will have a better functionality in performing its role as a proxy for a real user. In this paper, we present a comprehensive survey of the researches and the corresponding recent developments. Finally, the benefits of leveraging the quantum-like modeling approaches in computational domains and the existing challenges and limitations currently facing the field are discussed.}
}
@article{MEYER202013,
title = {Changing Design Education for the 21st Century},
journal = {She Ji: The Journal of Design, Economics, and Innovation},
volume = {6},
number = {1},
pages = {13-49},
year = {2020},
note = {Design Education. Part I},
issn = {2405-8726},
doi = {https://doi.org/10.1016/j.sheji.2019.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S2405872620300046},
author = {Michael W. Meyer and Don Norman},
keywords = {Design education, Design-driven transformation, Design thinking, Design doing, Major societal challenges, Complex sociotechnical systems, DesignX},
abstract = {Designers are entrusted with increasingly complex and impactful challenges. However, the current system of design education does not always prepare students for these challenges. When we examine what and how our system teaches young designers, we discover that the most valuable elements of the designer’s perspective and process are seldom taught. Instead, some designers grow beyond their education through their experience working in industry, essentially learning by accident. Many design programs still maintain an insular perspective and an inefficient mechanism of tacit knowledge transfer. Meanwhile, skills for developing creative solutions to complex problems are increasingly essential. Organizations are starting to recognize that designers bring something special to this type of work, a rational belief based upon numerous studies that link commercial success to a design-driven approach. So, what are we to do? Other learned professions such as medicine, law, and business provide excellent advice and guidance embedded within their own histories of professionalization. In this article, we borrow from their experiences to recommend a course of action for design. It will not be easy: it will require a study group to make recommendations for a roster of design and educational practices that schools can use to build a curriculum that matches their goals and abilities. And then it will require a conscious effort to bootstrap the design profession toward both a robust practitioner community and an effective professoriate, capable together of fully realizing the value of design in the 21st century. In this article, we lay out that path.}
}
@article{BEESON1988297,
title = {Towards a computation system based on set theory},
journal = {Theoretical Computer Science},
volume = {60},
number = {3},
pages = {297-340},
year = {1988},
issn = {0304-3975},
doi = {https://doi.org/10.1016/0304-3975(88)90115-6},
url = {https://www.sciencedirect.com/science/article/pii/0304397588901156},
author = {Michael J. Beeson},
abstract = {An axiomatic theory of sets and rules is formulated, which permits the use of sets as data structures and allows rules to operate on rules, numbers, or sets. We might call it a “polymorphic set theory”. Our theory combines the λ-calculus with traditional set theories. A natural set-theoretic model of the theory is constructed, establishing the consistency of the theory and bounding its proof-theoretic strength, and giving in a sense its denotational semantics. Another model, a natural recursion-theoretic model, is constructed, in which only recursive operations from integers to integers are represented, even though the logic can be classical. Some related philosophical considerations on the notions of set, type, and data structure are given in an appendix.}
}
@incollection{HARTSON2019293,
title = {Chapter 14 - Generative Design: Ideation, Sketching, and Critiquing},
editor = {Rex Hartson and Pardha Pyla},
booktitle = {The UX Book (Second Edition)},
publisher = {Morgan Kaufmann},
edition = {Second Edition},
address = {Boston},
pages = {293-325},
year = {2019},
isbn = {978-0-12-805342-3},
doi = {https://doi.org/10.1016/B978-0-12-805342-3.00014-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012805342300014X},
author = {Rex Hartson and Pardha Pyla},
keywords = {Design thinking, Immersion, Synthesis, Ideas, Ideation, Sketching, Critiquing, Ideation informers, Ideation catalysts, Ideation techniques, Rules of engagement},
abstract = {In this chapter we get into the UX design process, starting with generative design or design creation. The overarching objective of design creation is to formulate a plan for how the system will be structured to satisfy the ecological, interaction, and emotional needs of users. Compared to usage research (a study of things as they currently are), design (about lateral thinking and generating new ideas to make things better) is less procedural and more creative. Generative design is an intertwining of ideation (brainstorming), sketching (capturing and exploring design ideas visually), and critiquing (evaluation, review, and judgment). Ideation is collaborative, iterative, and exploratory, being informed via usage research data and models and design catalysts and is supported by a number of ideation techniques. Sketching is an essential embodied partner of ideation; you are not doing design if you are not sketching. A sketch is a conversation about design. Much of the work in design occurs within the ecological perspective, the interaction perspective, and/or the emotional perspective. The process works best if the team follows certain “rules of engagement.”}
}
@article{DERBYSHIRE201777,
title = {Potential surprise theory as a theoretical foundation for scenario planning},
journal = {Technological Forecasting and Social Change},
volume = {124},
pages = {77-87},
year = {2017},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2016.05.008},
url = {https://www.sciencedirect.com/science/article/pii/S0040162516300671},
author = {James Derbyshire},
keywords = {G. L. S. Shackle, Scenario planning, Plausibility, Intuitive Logics, Crucial decisions, Uncertainty},
abstract = {Despite some recent progress, scenario planning's development as an academic discipline remains constrained by the perception it is solely a practical tool for thinking about the future, with limited theoretical foundations. The paper addresses this issue by showing that G. L. S. Shackle's ‘Potential Surprise Theory’ (PST) contains much that can lend theoretical support to scenario planning - especially its use of plausibility rather than probability, and its focus on potential extreme outcomes. Moreover, PST and scenario planning share the same ontology, viewing the future as constructed by the imagination of individuals. Yet, under PST, while the future is imagined and, therefore, subjective, individuals nevertheless seek to identify the ‘best’ option through a deductive process of elimination. PST therefore assists in overcoming the divide between the constructivist and deductivist perspectives in scenario planning as it employs both. Finally, the paper shows that theoretically underpinning scenario planning with PST would place it at the heart of contemporary debates on decision making under uncertainty taking place in economics and other fields, enhancing its status and profile as a discipline.}
}
@article{JIN2024105113,
title = {An enhanced approach for few-shot segmentation via smooth downsampling mask and label smoothing loss},
journal = {Image and Vision Computing},
volume = {148},
pages = {105113},
year = {2024},
issn = {0262-8856},
doi = {https://doi.org/10.1016/j.imavis.2024.105113},
url = {https://www.sciencedirect.com/science/article/pii/S0262885624002178},
author = {Hailong Jin and Huiying Li},
keywords = {Few-shot segmentation, Segmentation, Few-shot learning, Deep learning},
abstract = {Few-shot semantic segmentation aims to segment new categories with only a small number of annotated images. Previous methods mainly focused on exploiting the pixel-level correlation between the support image and the query image, combined with attention-based methods, resulting in significant advancements. In this paper, we introduce a new perspective to enhance few-shot segmentation. We identify that utilizing the bilinear interpolation method to downsample the mask leads to the loss of fine-grained information from the target features. To address this issue, we propose a Smooth Downsampling Mask (SDM) method. The SDM method is designed to retain more effective target semantic features by employing a cascaded downsampling approach with a smooth kernel for mask processing. Additionally, we propose a label smoothing loss to further enhance the performance, which provides direct guidance for low-resolution feature map optimization. Both methods can be used as plug-and-play modules for existing methods. Notably, our proposed method does not involve additional learnable parameters and is computationally efficient, thus achieving painless gains. To validate the effectiveness of our method, we take three publicly available models as baselines and conduct extensive experiments on three public benchmarks PASCAL-5i, COCO-20i and FSS-1000, and achieve considerable improvement.}
}
@article{KAJIC201935,
title = {The semantic pointer theory of emotion: Integrating physiology, appraisal, and construction},
journal = {Cognitive Systems Research},
volume = {58},
pages = {35-53},
year = {2019},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2019.04.007},
url = {https://www.sciencedirect.com/science/article/pii/S1389041718303838},
author = {Ivana Kajić and Tobias Schröder and Terrence C. Stewart and Paul Thagard},
keywords = {Emotion, Language, Appraisal, Construction, Multi-level mechanisms, Affective computing, Neural engineering framework, Semantic pointers},
abstract = {Emotion theory needs to explain the relationship of language and emotions, and the embodiment of emotions, by specifying the computational mechanisms underlying emotion generation in the brain. We used Chris Eliasmith’s Semantic Pointer Architecture to develop POEM, a computational model that explains numerous important phenomena concerning emotions, including how some stimuli generate immediate emotional reactions, how some emotional reactions depend on cognitive evaluations, how bodily states influence the generation of emotions, how some emotions depend on interactions between physiological inputs and cognitive appraisals, and how some emotional reactions concern syntactically complex representations. We contrast our theory with current alternatives, and discuss some possible applications to individual and social emotions.}
}
@article{FACQUE2022281,
title = {Present bias in economic choice demonstrates increased cognitive fatigability of glioma patients},
journal = {Cortex},
volume = {151},
pages = {281-293},
year = {2022},
issn = {0010-9452},
doi = {https://doi.org/10.1016/j.cortex.2022.02.015},
url = {https://www.sciencedirect.com/science/article/pii/S0010945222000703},
author = {Valentine Facque and Antonius Wiehler and Emmanuelle Volle and Emmanuel Mandonnet and Mathias Pessiglione},
keywords = {Low-grade glioma, Fatigue, Impulsivity, Decision-making, Delay discounting, Cognitive control, Computational modelling},
abstract = {Fatigue is a frequent symptom in many clinical conditions that is still poorly understood despite having a major impact on quality of life. Here, we propose a novel approach using model-based analysis of choice behaviour to extract fatigue markers. We applied this approach to the case of low-grade glioma, with the aim of testing the hypothesis that fatigability in this condition may manifest as limited control over choice impulsivity. Patients with intact or resected glioma (n = 29) and matched healthy controls (n = 27) performed a series of behavioural tasks included in a 4 h-long neuropsychological assessment. Intertemporal choices, opposing smaller-sooner to larger-later monetary rewards, were intermixed with tasks designed to test cognitive and motor performance and to assess perceived fatigue with subjective ratings. All dependent variables were analysed with generalised linear models testing the main effects of group and time-on-task, as well as their interaction. While absent in standard measures of fatigue (subjective rating and objective performance), a significant group-by-time interaction was observed in the rate of impulsive choices: contrary to controls, patients developed a preference for the smaller-sooner option in the course of neuropsychological assessment. This preference shift was captured by computational modelling as an increase in the present bias, a parameter that assigns an additive bonus to immediate rewards. Thus, choice impulsivity was the only reliable marker that reflected the enhanced fatigability of patients relative to controls. These results suggest that the impact of glioma (or its resection) on brain functioning limits the exertion of cognitive control during decision-making. More generally, they pave the way to using model-based analysis of choice behaviour for future investigations of the many clinical conditions plagued with cognitive fatigue.}
}
@article{WRIGHT2025102048,
title = {PERSONALIZED PROFILES OF ORAL HEALTH AND DISEASE: USING HIGH-DIMENSIONAL VECTOR MODELS FOR GUIDING PRECISION DENTAL CARE},
journal = {Journal of Evidence-Based Dental Practice},
volume = {25},
number = {1, Supplement },
pages = {102048},
year = {2025},
issn = {1532-3382},
doi = {https://doi.org/10.1016/j.jebdp.2024.102048},
url = {https://www.sciencedirect.com/science/article/pii/S1532338224000988},
author = {CASEY D. WRIGHT and MARCUS G. WILD and REBECCA CUTLER and KIMON DIVARIS},
keywords = {Oral health-related quality of life, Dental patient reported outcomes, Operationalization, Precision modeling},
abstract = {Operationalizing the oral health experience is an ongoing effort with various clinical and patient-reported outcomes contributing to such conceptualizations. Computational technology has afforded advances in the ability to model complex interactions between various phenomena and provides an opportunity to reconsider the way oral health is conceptualized. High-dimensional vector space modeling is introduced and discussed as a theoretical way to incorporate all relative features associated with understanding oral health, including clinical, patient-reported, and demographic information. Specifically, a novel application of high-dimensional vector space models is proposed as a vehicle to operationalize the 3P model of oral health. Additionally, this paper outlines how this approach can 1) create more precise, person-level characterizations of oral health; 2) track oral health over time, offering greater opportunities for behavioral interventions to prevent, mitigate, or treat the negative impacts of dental, oral, and craniofacial diseases; and 3) offer comparisons to dynamically tuned comparison vectors which can define “good” oral health and quantify disparities and features on which to intervene to mitigate them.}
}
@article{ABUKHAIT202563,
title = {The association between psychotic symptoms and suicidal ideation in a sample of patients with schizophrenia: The moderating effect of the frequency of suicidal thoughts},
journal = {Archives of Psychiatric Nursing},
volume = {54},
pages = {63-72},
year = {2025},
issn = {0883-9417},
doi = {https://doi.org/10.1016/j.apnu.2025.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S0883941725000044},
author = {Abdallah {Abu Khait} and Austin Menger and Ghada Shahrour and Ayat ALhamdan and Esra'a Issa and Shaher H. Hamaideh},
keywords = {Suicidal ideation, Psychotic symptoms, Schizophrenia, Psychiatric nurses},
abstract = {Introduction
Suicidal ideation among patients with schizophrenia is ubiquitous and may lead to premature death. The ideation is a significant determinant of attempting and committing suicide.
Aim
This study aims to examine the moderating role of the frequency of suicidal thoughts on the relationship between psychotic symptoms and suicidal ideation in a sample of Jordanian patients with schizophrenia.
Materials and methods
This cross-sectional study used a non-experimental moderation design to recruit participants using convenience sampling. A total of 204 patients with schizophrenia completed self-administered questionnaires.
Results
The significant predictors of suicidal ideation were sex, whether or not the individual adhered to their medication prescription, age, the number of previous suicidal thoughts an individual had, and negative symptoms. For all suicidal ideation subscales except subscale 3 (suicide contemplation), positive psychotic symptoms were a significant predictor of suicidal ideation. The frequency of suicidal thoughts reduced (moderated) the effect of negative symptoms on suicidal ideation while amplifying the effect of positive psychotic symptoms on all suicidal ideation subscales except subscale 3 (suicide contemplation).
Conclusions
This study's results highlight the necessity of reducing suicidal thoughts to diminish the effect that positive psychotic symptoms have on suicidal ideation in patients with schizophrenia. Further research might explore the intricate relationship between psychotic symptoms and the mechanisms included in their complex link to suicidal ideation.
Implications for the practice
The results will help psychiatric nurses develop timely and accurate preventive strategies to fight suicidal ideation, assist in identifying which subgroups of patients with schizophrenia are vulnerable to suicidal ideation, and potentially lessen the suicide rate.}
}
@article{KELTYSTEPHEN2022104810,
title = {Turing’s cascade instability supports the coordination of the mind, brain, and behavior},
journal = {Neuroscience & Biobehavioral Reviews},
volume = {141},
pages = {104810},
year = {2022},
issn = {0149-7634},
doi = {https://doi.org/10.1016/j.neubiorev.2022.104810},
url = {https://www.sciencedirect.com/science/article/pii/S0149763422002998},
author = {Damian G. Kelty-Stephen and Madhur Mangalam},
keywords = {Criticality, Dynamic touch, Effortful touch, Executive function, Fractality, Multifractality, Multiscale, Multiplicativity, Multimodal perception, Neural avalanche, Perception and action, Posture, Power-law},
abstract = {Turing inspired a computer metaphor of the mind and brain that has been handy and has spawned decades of empirical investigation, but he did much more and offered behavioral and cognitive sciences another metaphor—that of the cascade. The time has come to confront Turing’s cascading instability, which suggests a geometrical framework driven by power laws and can be studied using multifractal formalism and multiscale probability density function analysis. Here, we review a rapidly growing body of scientific investigations revealing signatures of cascade instability and their consequences for a perceiving, acting, and thinking organism. We review work related to executive functioning (planning to act), postural control (bodily poise for turning plans into action), and effortful perception (action to gather information in a single modality and action to blend multimodal information). We also review findings on neuronal avalanches in the brain, specifically about neural participation in body-wide cascades. Turing’s cascade instability blends the mind, brain, and behavior across space and time scales and provides an alternative to the dominant computer metaphor.}
}
@article{ZHANG2019215,
title = {Food-energy-water (FEW) nexus for urban sustainability: A comprehensive review},
journal = {Resources, Conservation and Recycling},
volume = {142},
pages = {215-224},
year = {2019},
issn = {0921-3449},
doi = {https://doi.org/10.1016/j.resconrec.2018.11.018},
url = {https://www.sciencedirect.com/science/article/pii/S0921344918304361},
author = {Pengpeng Zhang and Lixiao Zhang and Yuan Chang and Ming Xu and Yan Hao and Sai Liang and Gengyuan Liu and Zhifeng Yang and Can Wang},
keywords = {Urban system, Food-energy-water nexus, Conceptual framework, Resilience},
abstract = {The emerging popularity of the nexus discussion reflects the ongoing transition from a sectoral or silo approach to an integrative approach to address the global challenges pertinent to the three essential resources: food, energy, and water (FEW). Cities are critically important for advancing regional sustainable development and are thus placed at the center of the FEW nexus. This paper provides a comprehensive literature review to debate the current concepts and methods of the FEW nexus at different scales, with the aim of developing a conceptual knowledgebase framework for scientific analysis and policy making associated with the urban FEW nexus. Although the concept of nexus thinking has been widely accepted, a consistent and explicit cognition of the FEW nexus is still lacking, and a sophisticated methodological modeling framework is urgently required at various scales. As such, we proposed a three-dimensional conceptual framework of the urban FEW nexus from the perspective of resource interdependency, resource provision and system integration. This framework is useful in steering the systematic modeling and integrative management of the complex nexus issues of urban systems with different perspectives. Finally, the future directions of urban nexus research are identified from four aspects, including systematic characterization, cross-region tele-connection mechanisms, co-decision model development, and governance transition.}
}
@article{ZHANG2024106073,
title = {Bayesian deep learning: An enhanced AI framework for legal reasoning alignment},
journal = {Computer Law & Security Review},
volume = {55},
pages = {106073},
year = {2024},
issn = {2212-473X},
doi = {https://doi.org/10.1016/j.clsr.2024.106073},
url = {https://www.sciencedirect.com/science/article/pii/S0267364924001390},
author = {Chuyue Zhang and Yuchen Meng},
keywords = {Legal AI, Legal reasoning, Deep learning, Bayesian deep learning, Bayesian neural networks},
abstract = {The integration of artificial intelligence into the field of law has penetrated the underlying logic of legal operations. Currently, legal AI systems face difficulties in representing legal knowledge, exhibit insufficient legal reasoning capabilities, have poor explainability, and are inefficient in handling causal inference and uncertainty. In legal practice, various legal reasoning methods (deductive reasoning, inductive reasoning, abductive reasoning, etc.) are often intertwined and used comprehensively. However, the reasoning modes employed by current legal AI systems are inadequate. Identifying AI models that are more suitable for legal reasoning is crucial for advancing the development of legal AI systems. Distinguished from the current high-profile large language models, we believe that Bayesian reasoning is highly compatible with legal reasoning, as it can perferm abductive reasoning, excel at causal inference, and admits the "defeasibility" of reasoning conclusions, which is consistent with the cognitive development pattern of legal professionals from apriori to posteriori. AI models based on Bayesian methods can also become the main technological support for legal AI systems. Bayesian neural networks have advantages in uncertainty modeling, avoiding overfitting, and explainability. Legal AI systems based on Bayesian deep learning frameworks can combine the advantages of deep learning and probabilistic graphical models, facilitating the exchange and supplementation of information between perception tasks and reasoning tasks. In this paper, we take perpetrator prediction systems and legal judegment prediction systems as examples to discuss the construction and basic operation modes of the Bayesian deep learning framework. Bayesian deep learning can enhance reasoning ability, improve the explainability of models, and make the reasoning process more transparent and visualizable. Furthermore, Bayesian deep learning framework is well-suited for human-machine collaborative tasks, enabling the complementary strengths of humans and machines.}
}
@article{COCHAND2023847,
title = {Systems Anesthesiology: Systems of Care Delivery and Optimization in the Operating Room},
journal = {Anesthesiology Clinics},
volume = {41},
number = {4},
pages = {847-861},
year = {2023},
note = {Perioperative Safety Culture},
issn = {1932-2275},
doi = {https://doi.org/10.1016/j.anclin.2023.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S1932227523000460},
author = {Laure Cochand and Mark G. Filipovic and Markus Huber and Markus M. Luedi and Richard D. Urman and Corina Bello},
keywords = {Systems anesthesiology, Perioperative care, Leadership, Operations management}
}
@article{KUBALAK2025104774,
title = {Simultaneous topology and toolpath optimization for layer-free multi-axis additive manufacturing of 3D composite structures},
journal = {Additive Manufacturing},
volume = {104},
pages = {104774},
year = {2025},
issn = {2214-8604},
doi = {https://doi.org/10.1016/j.addma.2025.104774},
url = {https://www.sciencedirect.com/science/article/pii/S2214860425001381},
author = {Joseph R. Kubalak and Alfred L. Wicks and Christopher B. Williams},
abstract = {Composite materials are extremely common in nature, with organic structures freely distributing and orienting anisotropic material properties in 3D to achieve a high degree of efficiency and functionality. Human-made composite structures do not leverage the same design thinking; they are frequently designed specifically for isotropic performance and with little geometric complexity due to limitations imposed by the manufacturing processes. While additive manufacturing (AM) provides unprecedented geometric flexibility, it typically deposits material in a series of stacked 2D layers (despite the moniker of “3D printing”); it does not enable the same freedoms of material placement and orientation seen in nature. Multi-axis (e.g., robotically-enabled) AM enables true 3D part fabrication such that material anisotropy can be advantageously oriented to enhance part performance (e.g., aligning fiber reinforcement to anticipated load paths), but existing methodologies separate the design of part geometry from its multi-axis printing toolpath. This paper presents a novel design and manufacturing workflow that integrates design optimization and multi-axis AM to algorithmically create optimal part topologies concurrently with their printing toolpaths. The workflow is aware of manufacturing and design considerations to maximize part performance while simultaneously guaranteeing multi-axis printability. Material is placed through an optimized, layer-free process to significantly improve the performance of additively manufactured composite structures. The design workflow is validated by optimizing, fabricating, and mechanically evaluating multi-axis structures and demonstrated a 56.9 % improvement in structural efficiency relative to a conventional, layer-wise AM process.}
}
@article{CARROLL200049,
title = {Use of student-constructed number stories in a reform-based curriculum},
journal = {The Journal of Mathematical Behavior},
volume = {19},
number = {1},
pages = {49-62},
year = {2000},
issn = {0732-3123},
doi = {https://doi.org/10.1016/S0732-3123(00)00038-9},
url = {https://www.sciencedirect.com/science/article/pii/S0732312300000389},
author = {William M Carroll and Karen C Fuson and Ann Diamond},
keywords = {Reform-based curriculum, Number stories, Invented procedures},
abstract = {Twelve classes using the reform-based curriculum, Everyday Mathematics (EM), were observed early in first grade. The two lessons observed involved students generating and solving addition and subtraction number stories. In these lessons, teachers were directed to help students link these number stories to representations (pictures or objects) and equations. Because this curriculum emphasizes invented procedures and number sense, the lessons also call for whole-class discussions of students' solutions. Further, the curriculum assumes that teachers will build upon and extend the children's mathematical thinking, highlighting these alternative solution methods and supporting the students' explanations. Results show that students were successful at making up, telling, and solving number stories and used a range of solution methods, including the mathematical representations available in the classrooms. However, only about three-quarters of the teachers established explicit links between the stories and mathematical representations, with fewer than half representing the stories as numbers and equations. Although student-based explanations play an important role in helping children develop solution procedures with understanding, solution methods were only elicited in half of the classes observed, and multiple methods in one-fourth of the classes. Implications for reform curricula, especially how they might clarify new goals for teachers, are discussed.}
}
@article{ROKOSZ2025106053,
title = {Moral-dilemma judgments by individuals and groups: Are many heads really more utilitarian than one?},
journal = {Cognition},
volume = {256},
pages = {106053},
year = {2025},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2024.106053},
url = {https://www.sciencedirect.com/science/article/pii/S0010027724003391},
author = {Marta Rokosz and Michał Białek and Michał M. Stefańczyk and Bertram Gawronski},
keywords = {CNI model, Deontology, Group decision-making, Moral judgment, Utilitarianism},
abstract = {Moral dilemmas often involve a conflict between action-options that maximize outcomes for the greater good (utilitarianism) and inaction-options that conform to moral norms (deontology). Previous research suggests that, compared to individuals, groups show stronger support for outcome-maximizing actions that violate moral norms. The current study used a computational modeling approach to investigate whether this difference is driven by (1) stronger sensitivity to consequences, (2) weaker sensitivity to moral norms, or (3) weaker action aversion in moral-dilemma judgments made by groups. The results suggest that groups show a stronger sensitivity to consequences than individuals. Groups and individuals did not differ in terms of their sensitivity to moral norms and their general action aversion. The findings challenge the idea that groups are less action averse and less concerned about violating moral norms than individuals and instead suggest that group decisions are more strongly guided by outcomes for the greater good.}
}
@article{ZHANG2022104630,
title = {Accurate band gap prediction based on an interpretable Δ-machine learning},
journal = {Materials Today Communications},
volume = {33},
pages = {104630},
year = {2022},
issn = {2352-4928},
doi = {https://doi.org/10.1016/j.mtcomm.2022.104630},
url = {https://www.sciencedirect.com/science/article/pii/S2352492822014714},
author = {Lingyao Zhang and Tianhao Su and Musen Li and Fanhao Jia and Shuobo Hu and Peihong Zhang and Wei Ren},
keywords = {2D materials, Bandgap, Machine learning, DFT calculation, Interpretable},
abstract = {Most materials science datasets are not so large that the accuracy of machine learning (ML) models is relatively limited if only simple features are used. Here, we constructed an interpretable ∆-machine learning (∆-ML) model to connect the hybrid functional HSE bandgap (EgHSE) with the PBE functional bandgap (EgPBE). The former can reproduce the band gap comparable with experiments, but the computational cost is much more challenging. The training is based on our high-throughput calculations on a set of two-dimensional semiconductors. Four complex descriptors, all based on the EgPBE are constructed using the sure independence screening and sparsifying operator (SISSO) algorithm. Using these descriptors, the ∆-ML can accurately predict the EgHSE of test set with a determination coefficient (R2) of 0.96. The error satisfies a normal distribution with a mean of zero. We provide a direct functional relationship between input descriptors and target properties. We find that EgHSE and the 5/6th power of EgPBE show a significant linear correlation, which may guide rapid prediction of EgHSE from EgPBE for materials with a EgHSE greater than 0.22 eV. We also discussed the correlation between the atomic radius and the EgHSE. Our work will provide an effective and interpretable model to construct the optimal physical descriptors for ML prediction on bandgaps in screening massive new 2D materials research.}
}
@article{CUI2022110595,
title = {Neural mechanisms of aberrant self-referential processing in patients with generalized anxiety disorder},
journal = {Progress in Neuro-Psychopharmacology and Biological Psychiatry},
volume = {119},
pages = {110595},
year = {2022},
issn = {0278-5846},
doi = {https://doi.org/10.1016/j.pnpbp.2022.110595},
url = {https://www.sciencedirect.com/science/article/pii/S0278584622000872},
author = {Qian Cui and Yuyan Chen and Qin Tang and Wei Sheng and Di Li and Yuhong Zeng and Kexing Jiang and Zongling He and Huafu Chen},
keywords = {Generalized anxiety disorder, Traits, Self-related processing, Task-related fMRI, Functional connectivity},
abstract = {Massive theoretical studies in clinical psychology have implicated the self in understanding internalizing disorders (i.e., anxiety and mood disorders), in which self-related tasks were frequently used to investigate internalizing psychopathology. As one of the most frequently seen internalizing disorder in primary care, patients with generalized anxiety disorder (GAD) are characterized by inappropriate self-related processing such as negative self-referential thinking. However, relevant neural mechanisms remain unknown. In this study, participants underwent a self-related task which they were presented with several positive and negative trait words and were required to judge the extent to which these traits matched themselves when compared to their average peers. Aberrant brain activation and functional connectivity of GAD were detected during processing positive and negative traits. Compared to healthy controls (HCs), patients with GAD exhibited abnormal self-processing which manifested as lower biased self-rating scores particularly for negative traits and weaker brain activity in the left dorsomedial prefrontal cortex, inferior frontal gyrus, superior temporal sulcus (STS), and bilateral lingual gyrus when processing trait words. Abnormal functional connections between these hypoactive regions and regions associated with reward, emotion, and theory of mind were observed in subsequent psychophysiological interaction analysis. An attenuation of connectivity between the left insula and left STS was associated with greater severity of anxiety symptom in GAD patients. These findings provide insight into the abnormal neurocognitive mechanisms of biased self-related processing in GAD patients, which involves distorted self-schema accompanied by abnormal activation and functional connections of regions implicated in self-related and social cognition processing.}
}
@incollection{MEY20065,
title = {Pragmatic Acts},
editor = {Keith Brown},
booktitle = {Encyclopedia of Language & Linguistics (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {5-12},
year = {2006},
isbn = {978-0-08-044854-1},
doi = {https://doi.org/10.1016/B0-08-044854-2/00386-2},
url = {https://www.sciencedirect.com/science/article/pii/B0080448542003862},
author = {J.L. Mey},
keywords = {Affordance, common sense, extralinguistic acts, intention, intentionality, interactional situation, irony, power, pract, pragmatic acts, pragmeme, presupposition, situated action, situated context, situation, speech acting, speech acts, utterance},
abstract = {Recently, the established way of thinking about speech acts (in the tradition of Austin, Searle, Grice, and their followers) has undergone a remarkable change. From being an effort to represent human words in terms of what they ‘do’ (Austin), or how they can be used to produce ‘speech acts’ (Searle) or to generate ‘implicatures’ (Grice), the focus has shifted to the situation in which words are spoken and how this contributes to understanding the utterance, or even how the situation can predefine and to a degree determine what can be said. The upshot of these considerations is that we need a new theory, one that takes into account the inter- and transactional aspects of speech acting. This article proposes such a theory under the label of ‘pragmatic acts’ – acts that work not just by their wording but also by their being embedded in a situation in which humans act, with everything that humans bring to their interactional forum, including body movements, emotions, and so on.}
}
@article{KIM2019141,
title = {AI for design: Virtual design assistant},
journal = {CIRP Annals},
volume = {68},
number = {1},
pages = {141-144},
year = {2019},
issn = {0007-8506},
doi = {https://doi.org/10.1016/j.cirp.2019.03.024},
url = {https://www.sciencedirect.com/science/article/pii/S0007850619300289},
author = {Sang-Gook Kim and Sang Min Yoon and Maria Yang and Jungwoo Choi and Haluk Akay and Edward Burnell},
keywords = {Design method, Machine learning, Hybrid intelligence},
abstract = {Engineering faces many wicked problems: irreducibly interdisciplinary with multiple competing objectives, and of such large scale and complexity that will require processes to deeply rely on human insights and power of computation. The resurgence of machine learning offers the possibility for new forms of human/computer collaboration where each fuels hybrid intelligence in complementary ways. A concept of virtual design assistant (VDA) is developed as a platform to bring the hybrid intelligence in solving complex design challenges. A deep learning-based abstraction process is developed to provide VDA a function to extract structured functional requirements from fragmental design specifications and customer needs.}
}
@article{SMITH2019473,
title = {Neurocomputational mechanisms underlying emotional awareness: Insights afforded by deep active inference and their potential clinical relevance},
journal = {Neuroscience & Biobehavioral Reviews},
volume = {107},
pages = {473-491},
year = {2019},
issn = {0149-7634},
doi = {https://doi.org/10.1016/j.neubiorev.2019.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S014976341930541X},
author = {Ryan Smith and Richard D. Lane and Thomas Parr and Karl J. Friston},
keywords = {Active inference, Emotional awareness, Somatic misattribution, Emotional working memory, Computational neuroscience},
abstract = {Emotional awareness (EA) is recognized as clinically relevant to the vulnerability to, and maintenance of, psychiatric disorders. However, the neurocomputational processes that underwrite individual variations remain unclear. In this paper, we describe a deep (active) inference model that reproduces the cognitive-emotional processes and self-report behaviors associated with EA. We then present simulations to illustrate (seven) distinct mechanisms that (either alone or in combination) can produce phenomena – such as somatic misattribution, coarse-grained emotion conceptualization, and constrained reflective capacity – characteristic of low EA. Our simulations suggest that the clinical phenotype of impoverished EA can be reproduced by dissociable computational processes. The possibility that different processes are at work in different individuals suggests that they may benefit from distinct clinical interventions. As active inference makes particular predictions about the underlying neurobiology of such aberrant inference, we also discuss how this type of modelling could be used to design neuroimaging tasks to test predictions and identify which processes operate in different individuals – and provide a principled basis for personalized precision medicine.}
}
@article{LI202261,
title = {Stochastic configuration networks for self-blast state recognition of glass insulators with adaptive depth and multi-scale representation},
journal = {Information Sciences},
volume = {604},
pages = {61-79},
year = {2022},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2022.04.061},
url = {https://www.sciencedirect.com/science/article/pii/S0020025522004133},
author = {Weitao Li and Qian Zhang and Dianhui Wang and Wei Sun and Qiyue Li},
keywords = {Self-blast state, Adaptive depth and multi-scale representation, Ensemble learning, Stochastic configuration networks, Feedback mechanism},
abstract = {The operating state of insulators is directly related to the stability of power transmission line. The existing methods for insulator state recognition cannot achieve satisfactory performance. In this paper, the self-blast state recognition of glass insulators is investigated by using an adaptive learning representation. To increase the adaptability of the network to different scales, we propose a solution based on multi-scale information throughout the entire process, beginning from a low-scale to high-scale subnetworks. The multi-scale information is aggregated in parallel way to take advantage of rich information representation. Then, an imitation of the human thinking pattern is employed. Utilizing entropy-based cost function, we update the parameters of the learner model in real-time. Based on the constraint of the evaluation index, adaptive depth representation for training glass insulators that are unsatisfied with the reliability evaluation is constructed to realize the self-optimizing regulation of feature space. Correspondingly, a stochastic configuration networks (SCNs) classifier is re-constructed to fit for the update multi-hierarchies knowledge space to carry out the re-recognition process. Finally, fuzzy integration is employed to ensemble multi-hierarchies network to improve the model’s generalization. The recognition results on aerial dataset of insulators images demonstrate the effectiveness of our proposed approach.}
}
@article{DAWKINS2012331,
title = {Metaphor as a possible pathway to more formal understanding of the definition of sequence convergence},
journal = {The Journal of Mathematical Behavior},
volume = {31},
number = {3},
pages = {331-343},
year = {2012},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2012.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S0732312312000156},
author = {Paul Christian Dawkins},
keywords = {Real analysis, Sequence convergence, Defining, Realistic Mathematics Education, Transition to advanced mathematical thinking},
abstract = {This study presents how the introduction of a metaphor for sequence convergence constituted an experientially real context in which an undergraduate real analysis student developed a property-based definition of sequence convergence. I use elements from Zandieh and Rasmussen's (2010) Defining as a Mathematical Activity framework to trace the transformation of the student's conception from a non-standard, personal concept definition rooted in the metaphor to a concept definition for sequence convergence compatible with the standard definition. This account of the development of the definition of sequence convergence differs from prior research in the sense that it began neither with examples or visual notions, nor with the statement of the formal definition. This study contributes to the Realistic Mathematics Education literature as it documents a student's progression through the definition-of and definition-for stages of mathematical activity in an interactive lecture classroom context.}
}
@article{BRAUN2013400,
title = {Custom fabric ventures: An instructional resource in job costing for the introductory managerial accounting course},
journal = {Journal of Accounting Education},
volume = {31},
number = {4},
pages = {400-429},
year = {2013},
issn = {0748-5751},
doi = {https://doi.org/10.1016/j.jaccedu.2013.07.004},
url = {https://www.sciencedirect.com/science/article/pii/S0748575113000511},
author = {Karen W. Braun},
keywords = {Job costing, Introductory managerial accounting, Active learning, Instructional resource},
abstract = {Job costing is a core foundational concept in the introductory managerial accounting course. The purpose of this instructional resource (IR) is to provide a thorough hands-on, active learning resource that will allow introductory students to experience a full set of accounting and management activities necessary to produce a job and assign production costs to it. For example, the IR requires students to analyze overhead costs, determine the optimal job size, schedule production, calculate the amount of materials to purchase, complete material requisitions, update raw materials records, analyze labor time records, complete a job cost record and address critical thinking questions. The IR was developed for use in a “flipped classroom” in which students work under the guidance of the instructor, but could alternatively be assigned as an unsupervised out-of-class assignment or on-line project. Since the IR was specifically developed as a learning tool for novice introductory managerial accounting students, adequate guidance is provided throughout the activity. However, to add realism and challenge students to think beyond the confines of simple mechanics, management and accounting issues are seeded throughout. Student feedback indicates that the IR not only helps students learn how a job costing system operates, but also helps students become aware of management decisions and accounting issues that impact the costs assigned to a job.}
}
@article{OUYANG2023101227,
title = {Using an integrated discourse analysis approach to analyze a group's collaborative argumentation},
journal = {Thinking Skills and Creativity},
volume = {47},
pages = {101227},
year = {2023},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2022.101227},
url = {https://www.sciencedirect.com/science/article/pii/S1871187122002280},
author = {Fan Ouyang and Zifan Tang and Mengting Cheng and Zixuan Chen},
keywords = {Collaborative argumentation, Collaborative learning, Knowledge construction, Discourse analysis, Informal learning},
abstract = {Collaborative argumentation is widely used in K-12 and higher education to foster students' argumentation skills, facilitate deep learning, and construct collective knowledge. Collaborative argumentation requires students to coordinate their social, cognitive, and metacognitive practices with peers through interactive discourses. Discourse analysis is a traditional analytic method that has been used to understand collaborative discourses. But most previous research has either integrated computational or statistical techniques to analyze frequencies of discourses or analyzed discourse from a qualitative perspective to demonstrate the microlevel, fine-grained attributes. There has been a lack of integrated discourse analysis method to holistically investigate and understand collaborative argumentation. To fill this gap, this case study used the scripted role strategy to support a group's collaborative argumentation and proposed an integrated discourse analysis approach to illustrate the temporal changes of the group's discourse moves, structures, and turn taking processes. The results showed that four participants had different discourse attributes. Based on the results, pedagogical and analytical implications are proposed to facilitate research and practice of collaborative argumentation.}
}
@article{PAN20241357,
title = {Construction and preliminary application of large language model for reservoir performance analysis},
journal = {Petroleum Exploration and Development},
volume = {51},
number = {5},
pages = {1357-1366},
year = {2024},
issn = {1876-3804},
doi = {https://doi.org/10.1016/S1876-3804(25)60546-5},
url = {https://www.sciencedirect.com/science/article/pii/S1876380425605465},
author = {Huanquan PAN and Jianqiao LIU and Bin GONG and Yiheng ZHU and Junhui BAI and Hu HUANG and Zhengbao FANG and Hongbin JING and Chen LIU and Tie KUANG and Yubo LAN and Tianzhi WANG and Tian XIE and Mingzhe CHENG and Bin QIN and Yujiang SHEN},
keywords = {reservoir performance analysis, artificial intelligence large model, application-specific large language model, incremental pre-training, fine-tuning, subsystems coupling, entity recognition, tool invocation},
abstract = {A large language model (LLM) is constructed to address the sophisticated demands of data retrieval and analysis, detailed well profiling, computation of key technical indicators, and the solutions to complex problems in reservoir performance analysis (RPA). The LLM is constructed for RPA scenarios with incremental pre-training, fine-tuning, and functional subsystems coupling. Functional subsystem and efficient coupling methods are proposed based on named entity recognition (NER), tool invocation, and Text-to-SQL construction, all aimed at resolving pivotal challenges in developing the specific application of LLMs for RDA. This study conducted a detailed accuracy test on feature extraction models, tool classification models, data retrieval models and analysis recommendation models. The results indicate that these models have demonstrated good performance in various key aspects of reservoir dynamic analysis. The research takes some injection and production well groups in the PK3 Block of the Daqing Oilfield as an example for testing. Testing results show that our model has significant potential and practical value in assisting reservoir engineers with RDA. The research results provide a powerful support to the application of LLM in reservoir performance analysis.}
}
@article{JOHN2022133624,
title = {How key-enabling technologies’ regimes influence sociotechnical transitions: The impact of artificial intelligence on decarbonization in the steel industry},
journal = {Journal of Cleaner Production},
volume = {370},
pages = {133624},
year = {2022},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2022.133624},
url = {https://www.sciencedirect.com/science/article/pii/S0959652622032024},
author = {Nikhil John and Joeri Hendrik Wesseling and Ernst Worrell and Marko Hekkert},
keywords = {Digitalization, Energy-intensive processing industries, Barriers to innovation, Multi-regime interactions, Sociotechnical systems, Multi-level perspective},
abstract = {Key Enabling Technologies (KETs) are pervasive groups of technologies expected to enable innovation. They have been promoted as technologies with tremendous potential for boosting economic growth and sustainability in all sectors of society – a claim whose validity remains underexplored. Building on systems thinking and the Multi-Level Perspective, we develop a novel approach to assess the socio-technical impact of a KET regime on a transitioning sectoral regime. This approach is applied to the case of the Artificial Intelligence (AI) KET impacting the decarbonization of the energy-intensive steel industry. To assess AI's technical impact, we compiled an inventory of AI tools based on reviewing technical scientific articles. Our analysis shows that AI adds technological value to the full range of areas in the steel industry, like predicting process parameters; optimizing operations, scheduling, and electrical energy; and forecasting product demand, quality, and site emissions. Semi-structured interviews were the primary data source to assess AI's broader socio-institutional impact. The results indicate that AI may currently be reinforcing path dependencies of the steel industry, as AI tools are more focused on incremental improvement for existing technologies rather than novel low-carbon technologies. However, AI also offers capabilities to reduce barriers to sustainability innovation, like system integration challenges, flexibility challenges, demand-side barriers, and risk-related barriers. Finally, we reflect on the generalizability of our approach for studying other transitions, and we induce characteristics of the AI-Digital KET regimes. We find that through these characteristics, the AI-Digital KET regime alters existing and creates new system structures (actors, networks, and institutions) within the impacted sector.}
}