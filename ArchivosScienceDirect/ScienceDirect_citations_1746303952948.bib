@article{RONGHUA2024e27753,
title = {Improved ant colony optimization for safe path planning of AUV},
journal = {Heliyon},
volume = {10},
number = {7},
pages = {e27753},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e27753},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024037848},
author = {Meng Ronghua and Cheng Xinhao and Wu Zhengjia and  {Du xuan}},
keywords = {Improved ant colony optimization, Safety factors, Dam inspections},
abstract = {In order to address the autonomous underwater vehicle navigation challenge for dam inspections, with the goal of enabling safe inspections and reliable obstacle avoidance, an improved smooth Ant Colony Optimization algorithm is proposed for path planning. This improved algorithm would optimize the smoothness of the path besides the robustness, avoidance of local optima, and fast computation speed. To achieve the goal of reducing turning time and improving the directional effect of path selection, a corner-turning heuristic function is introduced. Experimental simulation results show that the improved algorithm performs best than other algorithms in terms of path smoothness and iteration stability in path planning.}
}
@article{BISWAS2008127,
title = {Towards an agent-oriented approach to conceptualization},
journal = {Applied Soft Computing},
volume = {8},
number = {1},
pages = {127-139},
year = {2008},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2006.11.009},
url = {https://www.sciencedirect.com/science/article/pii/S1568494606001062},
author = {Pratik K. Biswas},
keywords = {Intelligent agents, Multi-agent systems, Agent-oriented software engineering, Agent-oriented thinking, Agent-oriented modeling, Extended agent model, Agent-oriented analysis, Agent-oriented design},
abstract = {Agent-oriented modeling provides a new technique for the conceptualization of agent-based systems. This paper extends and formalizes this agent-oriented modeling approach to the conceptualization process. It defines agent models and proposes a high-level methodology for agent-oriented analysis and design. It also includes analogies with the object-oriented and other existing agent-oriented methodologies, wherever applicable. The paper is concluded with a case study and an insight to future challenges.}
}
@incollection{SRIPRASADH202545,
title = {Chapter 3 - Review of existing neuromorphic systems},
editor = {Harish Garg and Jyotir {Moy Chatterjee} and R. Sujatha and Shatrughan Modi},
booktitle = {Primer to Neuromorphic Computing},
publisher = {Academic Press},
pages = {45-66},
year = {2025},
isbn = {978-0-443-21480-6},
doi = {https://doi.org/10.1016/B978-0-443-21480-6.00001-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780443214806000018},
author = {K. Sriprasadh},
keywords = {Deep learning, Neural network, Machine learning, Expert systems},
abstract = {Computer systems try to run in a similar manner like human brain. Interfacing the human brain activity with the computer device and making it to learn by thinking as human is coined the name neuromorphic system. Basically, neuromorphic form of computation performance ideology was initiated from the mathematical analysis started from the year of 1936, by mathematician and computer scientist Alan Turing, who created an algorithm to perform mathematical equation or solve mathematical problems through a machine. In 1949, he published the paper in name of intelligent machinery. The machine was named after him as Turing machine, which solved mathematical equations. This format was compared with humans; comparatively, humans were able to perform better than the system. The proposed model was coined the name cognitive modeling machinery. This model was the first step made by humans to create system model like the human brain. In 1949, Canadian psychologist Donald Hebb identified a supportive model of neuroscience correlating synaptic plasticity and learning, connecting human brain activity with an algorithm. In the year of 1950, Turing tested his Turing machine, which rendered his results. Based on the result, US navy created the Perceptron and human brain activity was mapped up to the level. But total activity of human brain cannot map due to lack of technology support. From 1980, neuromorphic research was taken through by Caltech professor Carver Mead. He created a analog silicon retina model and cochlea in 1981. Mead identified and proposed that computers can perform every action that human nervous system is capable of doing. In 2013, Henry Markram launched a system HBP like the human brain form, capable of understanding the human brain activity up to 10years and tried apply this format in science and technology. Recently, the neuromorphic system relies on AI and machine learning models and tries to support humans in detecting and decision-making in some critical situations. Neuromorphic systems basically make the decision through fuzzy neural and deep learning inputs. In this chapter, a review is made on features of trending neuromorphic system, and the future of the neuromorphic system is analyzed. The readers will gain the knowledge about the features of the neuromorphic systems and get an idea how it could be developed for different applications similar to decision-making and as expert system model in the form of humanoid.}
}
@article{VISWAN2023102808,
title = {Understanding molecular signaling cascades in neural disease using multi-resolution models},
journal = {Current Opinion in Neurobiology},
volume = {83},
pages = {102808},
year = {2023},
issn = {0959-4388},
doi = {https://doi.org/10.1016/j.conb.2023.102808},
url = {https://www.sciencedirect.com/science/article/pii/S0959438823001332},
author = {Nisha Ann Viswan and Upinder Singh Bhalla},
abstract = {If the genome defines the program for the operations of a cell, signaling networks execute it. These cascades of chemical, cell-biological, structural, and trafficking events span milliseconds (e.g., synaptic release) to potentially a lifetime (e.g., stabilization of dendritic spines). In principle almost every aspect of neuronal function, particularly at the synapse, depends on signaling. Thus dysfunction of these cascades, whether through mutations, local dysregulation, or infection, leads to disease. The sheer complexity of these pathways is matched by the range of diseases and the diversity of their phenotypes. In this review, we discuss how to build computational models, how these models are essential to tackle this complexity, and the benefits of using families of models at different levels of detail to understand signaling in health and disease.}
}
@article{ADAMO2024109162,
title = {Crop planting layout optimization in sustainable agriculture: A constraint programming approach},
journal = {Computers and Electronics in Agriculture},
volume = {224},
pages = {109162},
year = {2024},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2024.109162},
url = {https://www.sciencedirect.com/science/article/pii/S0168169924005532},
author = {Tommaso Adamo and Lucio Colizzi and Giovanni Dimauro and Emanuela Guerriero and Deborah Pareo},
keywords = {Constraint programming, Optimization crop planting layout, AI planning, Smart Agriculture, Intercropping systems},
abstract = {In sustainable agriculture, intercropping systems represent a valuable approach. These systems involve placing mutually beneficial plant types in close proximity to each other, with the goal of exploiting biodiversity to reduce pesticide and water usage, as well as improve soil nutrient utilization. Despite its potential, the optimization of intercropping systems has received limited attention in previous studies. One of the first steps in the design of an intercropping system is the solution of the crop planting layout problem, which involves meeting crop demand while maximizing positive interactions between adjacent plants. We perform a complexity analysis of this problem and solve it through constraint programming, an artificial intelligence technique, which relies on automated reasoning, constraint propagation and search heuristics. To this aim, we present two constraint programming models based on integer variables and interval variables, respectively. Through a computational study on real-life instances, we examine the impact of different modelling approaches on the difficulty of solving the crop planting layout problem with standard constraint programming solvers. This research work has also provided the groundwork for a sowing robotic arm (under development), aiming to automate intercropping systems and assist farm workers.}
}
@incollection{CHENG20251,
title = {Chapter 1 - A new milestone in artificial intelligence—ChatGPT},
editor = {Ge Cheng},
booktitle = {ChatGPT},
publisher = {Elsevier},
pages = {1-20},
year = {2025},
isbn = {978-0-443-27436-7},
doi = {https://doi.org/10.1016/B978-0-443-27436-7.00002-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780443274367000023},
author = {Ge Cheng},
keywords = {ChatGPT, OpenAI, transformer model, natural language processing (NLP), artificial general intelligence (AGI), GPT series, human feedback reinforcement learning (HFRL), large language models (LLMs), computational power, model limitations},
abstract = {This chapter provides an in-depth exploration of the evolution, capabilities, and impact of ChatGPT, a groundbreaking artificial intelligence (AI) application developed by OpenAI. The chapter traces the development history of ChatGPT from its early predecessors like GPT-1 and GPT-2 to the more advanced GPT-3 and GPT-4 models. It highlights the technological advancements that have made ChatGPT a powerful tool capable of complex tasks such as language comprehension, code generation, and multimodal reasoning. The chapter also discusses the architecture underpinning large language models (LLMs), focusing on the transition from traditional natural language processing techniques to Transformer-based models. Additionally, it addresses the significant computational and data requirements for training these models, alongside challenges such as model interpretability, biases, and privacy concerns. The chapter concludes by examining the broader implications of LLMs across various sectors and anticipates future trends in AI development.}
}
@article{DRACK2011150,
title = {System approaches of Weiss and Bertalanffy and their relevance for systems biology today},
journal = {Seminars in Cancer Biology},
volume = {21},
number = {3},
pages = {150-155},
year = {2011},
note = {Why Systems Biology and Cancer?},
issn = {1044-579X},
doi = {https://doi.org/10.1016/j.semcancer.2011.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S1044579X11000307},
author = {Manfred Drack and Olaf Wolkenhauer},
keywords = {Paul Alfred Weiss, Ludwig von Bertalanffy, Organismic biology, System theory of life, Systems biology},
abstract = {System approaches in biology have a long history. We focus here on the thinking of Paul A. Weiss and Ludwig von Bertalanffy, who contributed a great deal towards making the system concept operable in biology in the early 20th century. To them, considering whole living systems, which includes their organisation or order, is equally important as the dynamics within systems and the interplay between different levels from molecules over cells to organisms. They also called for taking the intrinsic activity of living systems and the conservation of system states into account. We compare these notions with today's systems biology, which is often a bottom-up approach from molecular dynamics to cellular behaviour. We conclude that bringing together the early heuristics with recent formalisms and novel experimental set-ups can lead to fruitful results and understanding.}
}
@article{BUHLER1990577,
title = {The COIN model for concurrent computation and its implementation},
journal = {Microprocessing and Microprogramming},
volume = {30},
number = {1},
pages = {577-584},
year = {1990},
note = {Proceedings Euromicro 90: Hardware and Software in System Engineering},
issn = {0165-6074},
doi = {https://doi.org/10.1016/0165-6074(90)90302-P},
url = {https://www.sciencedirect.com/science/article/pii/016560749090302P},
author = {Peter Buhler},
abstract = {COIN is a model for object-oriented programming with special emphasis on concurrent and distributed systems. It was developed to integrate design, implementation, and visualization of distributed applications. The distinguishing characteristics of COIN are: a) hierarchial object structures; b) multiple explicit object interfaces; c) explicit and dynamic binding of interface operations to operation implementations; d) generation of structures consisting of several objects and their interconnections as an atomic action. The paper gives an overview of the COIN model and its implementation in the COIN/L programming language.}
}
@article{YUAN2025129490,
title = {Global attention network with rain prior for real time single image deraining},
journal = {Neurocomputing},
volume = {625},
pages = {129490},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2025.129490},
url = {https://www.sciencedirect.com/science/article/pii/S0925231225001626},
author = {Yuan Yuan and Xuanbin Guo and Dandan Ma},
keywords = {Rain removal, Deep learning, Attention mechanism, Activation function},
abstract = {Poor visibility caused by rainy image can have a negative impact on the performance of computer vision applications. While several image deraining algorithms have been popularly adopted, most of them suffer from two main limitations: (1) they cannot well handle real and complex rain scenes by only focusing on one type of rain in images (e.g. raindrops or rain streaks) whereas the reality often coexists with both types, (2) they face significant difficulties in practical application because of ignoring the speed of inference. To address the above problems, we propose a global attention network (GANet) that can quickly and effectively separate rain streaks and raindrops. Inspired by the fact that rain in images often appears white, we leverage this prior to obtain an initial rain-free background image to guide neural network-based image deraining. Moreover, a new global attention block (GAB) is designed to simultaneously extract the rain features from spatial and channel dimensions. By cascading multiple GABs, the proposed method can effectively obtain the features of rain streaks and raindrops and progressively separates the rain-free image. Furthermore, owing to the nonlinear properties of GAB, the activation functions are omitted, which can speed up the inference time. And the depth-wise and point-wise convolutions are employed to promote computation efficiency as well. Extensive experiments on raindrop and rain streak datasets demonstrate that our method outperforms state-of-the-art methods, achieving up to 37.53 dB PSNR on Rain100L with an inference speed of 39 FPS, which is 2–30 times faster than competitors.}
}
@article{WU2024112197,
title = {A multi-strategy three-way decision approach for tri-state risk loss under q-rung orthopair fuzzy environment},
journal = {Applied Soft Computing},
volume = {167},
pages = {112197},
year = {2024},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2024.112197},
url = {https://www.sciencedirect.com/science/article/pii/S1568494624009712},
author = {Ping Wu and Yihua Zhong and Chuan Chen and Yanlin Wang and Chao Min},
keywords = {Three-way decision, q-rung orthopair fuzzy sets, Tri-state risk loss, Multi-strategy perspective, Threshold theorem},
abstract = {Addressing the decision-making challenge arising from the uncertainty of human cognition, three-way decision (3WD) and q-rung orthopair fuzzy sets (q-ROFSs) are integrated in this paper to propose a multi-strategy three-way decision approach (MS3WDA) for tri-state risk loss (TSRL) under q-rung orthopair fuzzy environment. Based on the ternary thinking of human cognition, the risk loss with hesitation state is considered and constructed under q-rung orthopair fuzzy environment. The TSRL with hesitation state is further constructed by combining the q-rung orthopair fuzzy (q-ROF) information. The conditional probability adopted by the original object classes is improved and extended by the three components of q-ROFSs. Next, the TSRL with q-ROF information and three components of q-ROFSs are integrated with decision-theoretic rough sets (DTRSs) to establish a novel 3WD model. Some relevant properties are also analyzed and discussed for the developed 3WD model. Then, its multi-strategy decision method is proposed based on the multi-strategy perspective. The related strategies with five different levels are designed by considering three different risk appetite perspectives and four different aspects of q-ROF information. The relevant threshold theorems are also given and proved to further provide the theoretical support for our MS3WDA. According to the five different strategies, we further derive the corresponding decision rules of MS3WDA. The key steps and specific algorithm are summarized for MS3WDA. Finally, a case study is provided to demonstrate the practicability and feasibility of MS3WDA. Meanwhile, the rationality, robustness and superiority of MS3WDA are further validated by the sensitivity analysis and comparative analysis.}
}
@article{XHAXHIU2024270,
title = {Seaweed boards as value-added natural waste product for insulation and building materials},
journal = {Energy Storage and Saving},
volume = {3},
number = {4},
pages = {270-277},
year = {2024},
issn = {2772-6835},
doi = {https://doi.org/10.1016/j.enss.2024.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S2772683524000359},
author = {Kledi Xhaxhiu and Avni Berisha and Nensi Isak and Besnik Baraj and Adelaida Andoni},
keywords = {Seaweed, Natural waste, Waste recycling, Building material, Insulation, Thermal and mechanical properties calculations},
abstract = {Large amounts of seaweed are deposited on shores worldwide daily. The presence of this natural pollutant on the coast is not only considered an environmental burden but also often hinders the development of tourism in the affected areas. Depending on the beach surface area, local governments worldwide spend considerable portions of their budgets to remove seaweed from beaches. Moreover, the removed seaweed occupies increasing space in landfills where it is disposed. Seaweed is noncombustible and decomposes slowly over long periods. In this study, we consider the use of seaweed (a natural waste) as a value-added product for insulation and building materials. Seaweed (Posidonia Oceanica) boards with dimensions of 250 mm × 60 mm × 10 mm were obtained by pressing a mixture of processed seaweed and an organic binder. The as-prepared boards were analyzed for their physical–mechanical properties according to the British standards. The boards with a mean humidity level of 9.15% and density of 404.5 g·cm−3 demonstrated a maximum bending resistance of 2.72 × 103 N·m−2 and mean expansion upon water adsorption of ∼10% with regards to length and width and ∼30% with regards to height. The tested samples showed significant humidity resistance according to the boiling test and an average thermal conductivity of 0.047 W·m−1·K−1, which is comparable to that of polystyrene. Computational analysis of the “seaweed material” model revealed significant thermal and mechanical properties. The mechanical strength of the computed material, including its high Young’s and shear moduli, renders it a promising candidate in construction.}
}
@incollection{RENNE2022147,
title = {Chapter 8 - Measuring and assessing resilience},
editor = {John L. Renne and Brian Wolshon and Anurag Pande and Pamela Murray-Tuite and Karl Kim},
booktitle = {Creating Resilient Transportation Systems},
publisher = {Elsevier},
pages = {147-192},
year = {2022},
isbn = {978-0-12-816820-2},
doi = {https://doi.org/10.1016/B978-0-12-816820-2.00005-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128168202000050},
author = {John L. Renne and Brian Wolshon and Anurag Pande and Pamela Murray-Tuite and Karl Kim},
keywords = {Resilience, Measurement, Assessment, Goals, Metrics, Data collection},
abstract = {The ability to effectively apply resilience-oriented thinking into practice starts with understanding, measuring, and evaluating the benefits and costs of resilience. With this knowledge it also becomes possible to comparatively assess potential planning, design, and maintenance options to plan and allocate financial and personnel resources most effectively to address needs. Other key components of practical and meaningful measurements and assessments of resilience are establishing metrics that quantify its performance and knowing what and how much data to collect. Then, understanding what these data mean so that goals, objectives, and expectations of resilience can be set, both within transportation organizations and for the consumers of the services they provide. Unfortunately, there is no universal agreement on what resilience even is, let alone how to systematically measure and assess it. However, recent reviews of practice and research show that ideas and methods to evaluate and assess resilience are evolving at a rapid pace, both within and outside of transportation. This chapter presents a summary of these ideas and compares and contrasts the effort they require to implement and the benefits they are expected to bring.}
}
@article{WU201655,
title = {Vertical position of Chinese power words influences power judgments: Evidence from spatial compatibility task and event-related Potentials},
journal = {International Journal of Psychophysiology},
volume = {102},
pages = {55-61},
year = {2016},
issn = {0167-8760},
doi = {https://doi.org/10.1016/j.ijpsycho.2016.03.005},
url = {https://www.sciencedirect.com/science/article/pii/S0167876016300253},
author = {Xiangci Wu and Huibin Jia and Enguo Wang and Chenguang Du and Xianghua Wu and Caiping Dang},
keywords = {Conceptual representation, Metaphor, Vertical position, Power},
abstract = {The present study used event-related potentials (ERPs) to explore the influence of vertical position on power judgments. Participants were asked to identify whether a Chinese word represented a powerful or powerless group (e.g., “king” or “servant”), which was presented in the top or bottom of the screen. The behavioral analysis showed that judging the power of powerful words were significantly faster when they were presented at the top position, compared with when they were presented at the bottom position. The ERP analysis showed enhanced N1 amplitude for congruent trials (i.e., the powerful words in the top and the powerless words in the bottom of the screen) and larger P300 and LPC amplitude for incongruent trials (i.e., the powerful words in the bottom and the powerless words in the top of the screen). The present findings provide further electrophysiological evidence that thinking about power can automatically activate the underlying spatial up-down (verticality) image schema and that the influence of vertical position on the power judgments not only occurs at the early perceptual stage of power word processing, but also at the higher cognitive stage (i.e., allocation of attention resources, conflict solving and response selection). This study revealed the neural underpinnings of metaphor congruent effect which have great significance to our understanding of the abstract concept power.}
}
@article{GIANNAKOS201777,
title = {Entertainment, engagement, and education: Foundations and developments in digital and physical spaces to support learning through making},
journal = {Entertainment Computing},
volume = {21},
pages = {77-81},
year = {2017},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2017.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S1875952117300307},
author = {Michail N. Giannakos and Monica Divitini and Ole Sejer Iversen},
keywords = {Maker movement, Learning technologies, Entertainment technologies, Creativity, Knowledge construction, Technological fluency, Constructionist},
abstract = {Making is a relatively new concept applied to describe the increasing attention paid to constructing activities to enable entertaining, and engaging learning. Making focuses on the process that occurs in digital and/or physical spaces that is not always learning oriented, but enables qualities such as problem solving, design thinking, collaboration, and innovation, to name a few. Contemporary technical and infrastructural developments, such as Hackerspaces, Makerspaces, TechShops, and FabLabs, and the appearance of tools such as wearable computing, robotics, 3D printing, microprocessors, and intuitive programming languages, posit making as a very promising research area to support learning processes, especially towards the acquisition of 21st-century learning competences. Collecting learning evidence via rigorous multidimensional and multidisciplinary case studies will allow us to better understand and improve the value of making and the role of the various digital and physical spaces. Drawing from our experience with a recent workshop that used making as a pathway to foster joyful engagement and creativity in learning (Make2Learn), we present the developments, as well as the four selected contributions of this special issue. The paper further draws attention to the great potential and need for research in the area of making to enable entertaining, and engaging, and learning.}
}
@article{LIU2025104441,
title = {Multi-TuneV: Fine-tuning the fusion of multiple modules for video action recognition},
journal = {Journal of Visual Communication and Image Representation},
volume = {109},
pages = {104441},
year = {2025},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2025.104441},
url = {https://www.sciencedirect.com/science/article/pii/S1047320325000550},
author = {Xinyuan Liu and Junyong Ye and Jingjing Wang and Guangyi Xu and Youwei Li and Chaoming Zheng},
keywords = {Multiple Fine-tuning, Vision Transformers, Video Action Recognition, Transfer Learning, Deep Learning},
abstract = {The current pre-trained models have achieved remarkable success, but they usually have complex structures and hundreds of millions of parameters, resulting in a huge computational resource requirement to train or fully fine-tune a pre-trained model, which limits its transfer learning on different tasks. In order to migrate pre-trained models to the field of Video Action Recognition (VAR), recent research uses parametric efficient transfer learning (PETL) approaches, while most of them are studied on a single fine-tuning module. For a complex task like VAR, a single fine-tuning method may not achieve optimal results. To address this challenge, we want to study the effect of joint fine-tuning with multiple modules, so we propose a method that merges multiple fine-tuning modules, namely Multi-TuneV. It combines five fine-tuning methods, including ST-Adapter, AdaptFormer, BitFit, VPT and LoRA. We design a particular architecture for Multi-TuneV and integrate it organically into the Video ViT model so that it can coordinate the multiple fine-tuning modules to extract features. Multi-TuneV enables pre-trained models to migrate to video classification tasks while maintaining improved accuracy and effectively limiting the number of tunable parameters, because it combines the advantages of five fine-tuning methods. We conduct extensive experiments with Multi-TuneV on three common video datasets, and show that it surpasses both full fine-tuning and other single fine-tuning methods. When only 18.7 % (16.09 M) of the full fine-tuning parameters are updated, the accuracy of Multi-TuneV on SSv2 and HMDB51 improve by 23.43 % and 16.46 % compared with the full fine-tuning strategy, and improve to 67.43 % and 75.84 %. This proves the effectiveness of joint multi-module fine-tuning. Multi-TuneV provides a new idea for PETL and a new perspective to address the challenge in video understanding tasks. Code is available at https://github.com/hhh123-1/Multi-TuneV.}
}
@article{EPIOTIS1989213,
title = {Lewis formulae for metallic systems: the Li tetramer paradigm},
journal = {Journal of Molecular Structure: THEOCHEM},
volume = {201},
pages = {213-238},
year = {1989},
issn = {0166-1280},
doi = {https://doi.org/10.1016/0166-1280(89)87077-0},
url = {https://www.sciencedirect.com/science/article/pii/0166128089870770},
author = {N.D. Epiotis},
abstract = {In previous works, we argued that metal atoms bind through a mechanism in which overlap is assisted by some other overlap-independent mode like dispersion or induction. The result is the formation of gas pairs (interstitial electron pairs). Unlike overlap, these mechanisms of bonding can be properly reproduced only at the MCSCF level. To draw the chemist away from one-electron thinking, we propose specific Lewis formulae for small metal clusters and we show how these change as we shift from a lower to a higher level of theory so as to project the key point. A qualitative (let alone quantitative) understanding of metallic bonding can only be achieved from examination of properly correlated wavefunctions. From the practical standpoint, we show how usage of these Lewis formulae can inspire analogies for explaining computational and experimental results.}
}
@article{CRANFORD2022100638,
title = {Navigating the “Kessel Run” of digital materials acceleration},
journal = {Patterns},
volume = {3},
number = {11},
pages = {100638},
year = {2022},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2022.100638},
url = {https://www.sciencedirect.com/science/article/pii/S2666389922002707},
author = {Steve Cranford},
abstract = {Computational methods such as machine learning, artificial intelligence, and big data in physical sciences, particularly materials science, have been exponentially growing in terms of progress, method development, and number of studies and related publications. This aggregate momentum of the community is palpable, and many exciting discoveries are likely on the horizon. But, like all endeavors, some thought should be given to the current trajectory of the field, ensuring the full potential of the new digital space.}
}
@article{DEVGUN2023141,
title = {Pre-cath Laboratory Planning for Left Atrial Appendage Occlusion – Optional or Essential?},
journal = {Cardiac Electrophysiology Clinics},
volume = {15},
number = {2},
pages = {141-150},
year = {2023},
note = {Left Atrial Appendage Occlusion},
issn = {1877-9182},
doi = {https://doi.org/10.1016/j.ccep.2023.01.009},
url = {https://www.sciencedirect.com/science/article/pii/S1877918223000205},
author = {Jasneet Devgun and Tom {De Potter} and Davide Fabbricatore and Dee Dee Wang},
keywords = {Left atrial appendage occlusion, Left atrial appendage, Atrial fibrillation, Cardiac CT, 3D printing, Imaging, Structural heart disease}
}
@incollection{BILLEN2023385,
title = {Chapter 16 - Lithosphere–Mantle Interactions in Subduction Zones},
editor = {João C. Duarte},
booktitle = {Dynamics of Plate Tectonics and Mantle Convection},
publisher = {Elsevier},
pages = {385-405},
year = {2023},
isbn = {978-0-323-85733-8},
doi = {https://doi.org/10.1016/B978-0-323-85733-8.00014-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780323857338000147},
author = {Magali I. Billen},
keywords = {Subduction dynamics, Rheology, Phase transitions, Numerical modeling, Mantle mixing},
abstract = {How does the interaction of sinking lithosphere with the mantle contribute to the motion of tectonics plates at the Earth's surface and to long-term mixing in the deep mantle? In the decades immediately following the acceptance of the theory of plate tectonics, these questions were pursued vigorously using analytical, laboratory, and numerical models. In the past two decades, attention has turned to building on this foundational knowledge using numerical simulations to more fully integrate the complexity of Earth materials including the effects of deformation mechanisms, composition, fluids, melting, and phase transitions. This ongoing transition to a more system-centered view of geodynamics and plate tectonics not only presents many challenges (computational, experimental, and theoretical) but also promises to bridge the gaps in our current understanding and address the still enigmatic behavior of sinking lithosphere.}
}
@article{SEDJELMACI2019101970,
title = {An efficient cyber defense framework for UAV-Edge computing network},
journal = {Ad Hoc Networks},
volume = {94},
pages = {101970},
year = {2019},
issn = {1570-8705},
doi = {https://doi.org/10.1016/j.adhoc.2019.101970},
url = {https://www.sciencedirect.com/science/article/pii/S1570870519302136},
author = {Hichem Sedjelmaci and Aymen Boudguiga and Inès Ben Jemaa and Sidi Mohammed Senouci},
keywords = {UAV-Edge computing, Detection, Stackelberg game, Energy consumption, Computation overhead},
abstract = {Mobile Edge Computing (MEC) is usually deployed in energy and delay constrained networks, such as internet of things networks and transportation systems to address the issues of energy consumption, computation capacity and network delay. In this work, we focus on a special case, which is Unmanned Aerial Vehicle Edge Computing (UEC) network. Addressing the security issues in UAV-Edge Computing network is mandatory due to the criticality of UEC services, such as network traffic monitoring, or search and rescue operations. However, cyber defense and protection of UEC network have not yet received sufficient research attention. Thereby, we propose and develop a cyber-defense solution based on a non-cooperative game to protect the UEC from network and offloading attacks, while taking into account nodes’ energy constraints and computation overhead. Simulation results show that, the deployment of our cyber defense system in UEC network requires low energy consumption and low computation overhead to obtain a high protection rate.}
}
@article{YONG2023e13529,
title = {Structure bionic topology design method based on biological unit cell},
journal = {Heliyon},
volume = {9},
number = {2},
pages = {e13529},
year = {2023},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2023.e13529},
url = {https://www.sciencedirect.com/science/article/pii/S2405844023007363},
author = {Yang Yong and Jiang Xue-tao and Zhu Qi-xin and Lu En-hui and Dong Xin-feng and Li Jing-bin},
keywords = {Biological unit cell, Substructure, Matter element, TRIZ},
abstract = {The mechanical structure topology design based on substructure always adopts the traditional substructure design method, which often comes from the experience and is limited by the inherent or stereotyped design thinking. A substructure design method based on biological unit cell (UC) is proposed, which draws inspiration from the biological efficient load-bearing topology structure. Especially, the thought of the formalized problem-solving of extension matter-element is introduced. Through the matter-element definition of UC substructure, the process model for the structure bionic topology design method based on biological UC is formed, which avoids the random or wild mental stimulation of the structure topology design method based on traditional substructure. In particular, in this proposed method, aiming at the problem about how to achieve the integration of high-efficiency load-bearing advantage of different organisms, furthermore, a biological UC hybridization method based on the principle of inventive problem solving theory (TRIZ) is proposed. The typical case is used to illustrate the process of this method in detail. The results from simulations and experiments both show that: the load-bearing capacity of structure design based on biology UC is improved than the initial design; on this basis, the load-bearing capacity of structure design is improved further through UC hybridization. All these show the feasibility and correctness of the proposed method.}
}
@article{MURTAGH201637,
title = {Direct Reading Algorithm for Hierarchical Clustering},
journal = {Electronic Notes in Discrete Mathematics},
volume = {56},
pages = {37-42},
year = {2016},
note = {TCDM 2016 - 1st IMA Conference on Theoretical and Computational Discrete Mathematics, University of Derby},
issn = {1571-0653},
doi = {https://doi.org/10.1016/j.endm.2016.11.006},
url = {https://www.sciencedirect.com/science/article/pii/S157106531630213X},
author = {Fionn Murtagh and Pedro Contreras},
keywords = {Analytics, hierarchical clustering, ultrametric topology, p-adic and m-adic number representation, linear time computational complexity},
abstract = {Reading the clusters from a data set such that the overall computational complexity is linear in both data dimensionality and in the number of data elements has been carried out through filtering the data in wavelet transform space. This objective is also carried out after an initial transforming of the data to a canonical order. Including high dimensional, high cardinality data, such a canonical order is provided by row and column permutations of the data matrix. In our recent work, we induce a hierarchical clustering from seriation through unidimensional representation of our observations. This linear time hierarchical classification is directly derived from the use of the Baire metric, which is simultaneously an ultrametric. In our previous work, the linear time construction of a hierarchical clustering is studied from the following viewpoint: representing the hierarchy initially in an m-adic, m = 10, tree representation, followed by decreasing m to smaller valued representations that include p-adic representations, where p is prime and m is a non-prime positive integer. This has the advantage of facilitating a more direct visualization and hence interpretation of the hierarchy. In this work we present further case studies and examples of how this approach is very advantageous for such an ultrametric topological data mapping.}
}
@article{OLADEJO2024111880,
title = {The Hiking Optimization Algorithm: A novel human-based metaheuristic approach},
journal = {Knowledge-Based Systems},
volume = {296},
pages = {111880},
year = {2024},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.111880},
url = {https://www.sciencedirect.com/science/article/pii/S0950705124005148},
author = {Sunday O. Oladejo and Stephen O. Ekwe and Seyedali Mirjalili},
keywords = {Optimization, Metaheuristics, Hiking, Tobler’s Hiking function, Algorithm, Benchmark, Problem solving},
abstract = {In this paper, a novel metaheuristic called ‘The Hiking Optimization Algorithm’ (HOA) is proposed. HOA is inspired by hiking, a popular recreational activity, in recognition of the similarity between the search landscapes of optimization problems and the mountainous terrains traversed by hikers. HOA’s mathematical model is premised on Tobler’s Hiking Function (THF), which determines the walking velocity of hikers (i.e. agents) by considering the elevation of the terrain and the distance covered. THF is employed in determining hikers’ positions in the course of solving an optimization problem. HOA’s performance is demonstrated by benchmarking with 29 well-known test functions (including unimodal, multimodal, fixed-dimension multimodal, and composite functions), three engineering design problems (EDPs), (including I-beam, tension/compression spring, and gear train problems) and two N-P Hard problems (i.e. Traveling Salesman’s and Knapsack Problems). Moreover, HOA’s results are verified by comparison to 14 other metaheuristics, including Teaching Learning Based Optimization (TLBO), Genetic Algorithm (GA), Differential Evolution (DE), Particle Swarm Optimization, Grey Wolf Optimizer (GWO) as well as newly introduced algorithms such as Komodo Mlipir Algorithm (KMA), Quadratic Interpolation Optimization (QIO), and Coronavirus Optimization Algorithm (COVIDOA). In this study, we employ statistical tests such as the Wilcoxon rank sum, Friedman test, and Dunn’s post hoc test for the performance evaluation. HOA’s results are competitive and, in many instances, outperform the aforementioned well-known metaheuristics. The source codes of HOA and related metaheuristics can be accessed publicly via this link: https://github.com/DayoSun/The-Hiking-Optimization-Algorithm.}
}
@article{KACZYNSKA20214290,
title = {A new multi-criteria model for ranking chess players},
journal = {Procedia Computer Science},
volume = {192},
pages = {4290-4299},
year = {2021},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 25th International Conference KES2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.09.205},
url = {https://www.sciencedirect.com/science/article/pii/S187705092101944X},
author = {Aleksandra Kaczyńska and Joanna Kołodziejczyk and Wojciech Sałabun},
keywords = {Chess, MCDA, COMET, players evaluation, decision making},
abstract = {Chess is a very demanding sport as it requires advanced planning and strategic thinking skills. The degree of difficulty of the game also depends on the time allotted for a game, which can range from a few minutes to several tens of minutes. For this reason, the games are divided into several categories: standard, blitz, and bullet. However, as many chess players specialize in only some of the categories, it is difficult to determine the best chess player. It is very important to keep a proper ranking of the players. One way to recognize their achievements is the FIDE (Fédération Internationale des Échecs) titles awarded to the best players. However, there is still the problem of how to determine the best among the Grandmasters. There are many very talented players competing in chess. Creating a single ranking for all types of chess, regardless of the time allotted for the game, is a difficult challenge, as many undeniably outstanding chess players do not specialize in all types. Creating a ranking for only one type would not accurately describe the level of players. Therefore, a ranking was created based on all of them using the COMET method, which belongs to the multi-criteria decision-making methods (MCDA). It is based on fuzzy logic and uses characteristic objects for the assessment of alternatives, which guarantees immunity to the paradox of reversal rankings. Expert opinion was used for correct evaluation. This article presents the ranking of chess players regardless of the type of game they specialize in, to prove that it should be possible to identify the single best chess player.}
}
@article{IMM2012130,
title = {Talking mathematically: An analysis of discourse communities},
journal = {The Journal of Mathematical Behavior},
volume = {31},
number = {1},
pages = {130-148},
year = {2012},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2011.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S0732312311000630},
author = {Kara Imm and Despina A. Stylianou},
keywords = {Discourse, Cognitively demand tasks, Local authority},
abstract = {Discourse has always been at the heart of teaching. In more recent years, the mathematics education community has also turned its attention towards understanding the role of discourse in mathematics teaching and learning. Using earlier classifications of discourse, in this paper, we looked at three types of classrooms: classrooms that engage in high discourse, low discourse and a hybrid of the two. We aimed to understand how the elements of each discourse affected classroom learning, relationships between teachers and students, and participatory structures for students. Overall, our findings highlight the important relationship between cognitively demanding tasks and mathematical talk, and the power of discourse as a “thinking device” as opposed to mere conduit of knowledge. Our work also points to the under-theorized nature of hybrid discourse in mathematics classrooms, thereby providing some directions for pedagogy and further research.}
}
@incollection{KIM2009332,
title = {Spatial Data Mining, Geovisualization},
editor = {Rob Kitchin and Nigel Thrift},
booktitle = {International Encyclopedia of Human Geography},
publisher = {Elsevier},
address = {Oxford},
pages = {332-336},
year = {2009},
isbn = {978-0-08-044910-4},
doi = {https://doi.org/10.1016/B978-008044910-4.00526-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780080449104005265},
author = {C. Kim},
keywords = {Exploratory spatial data analysis, Geovisualization, Knowledge discovery, Spatial autocorrelation, Spatial data mining, Spatial outliers, Spatial uncertainty, Visual data mining},
abstract = {Geovisualization in spatial data mining is one of the main methods that has recently been the subject of knowledge discovery research in geographic information science. Geovisualization is often referred to as knowledge discovery in that it produces previously unseen patterns from a larger set of data. Due to the increase in geospatial data, any techniques that can shift through large sets of data quickly and efficiently are in high demand. Geovisualization uses visual representations to facilitate thinking, understanding, and knowledge construction about human and physic environments, at geographic scales of measurement. It augments human visual ability in perceiving high complex structures, and detecting, exploring, and exploiting significant patterns. It integrates scientific visualization with traditional cartography, and can be utilized at data pre-processing, spatial data mining, and knowledge construction. The main purpose of geovisualization, however, is on insight rather than maps. Research needs in geovisualization are extensive as follows: geovisulation in spatiotemporal databases, the automated discovery of spatial knowledge, geovisualization in remote-sensing data and spatial object-oriented databases, effective geovisualizations of spatial relationships, and efficient geocomputation.}
}
@article{REVACH2021103229,
title = {Expanding the discussion: Revision of the fundamental assumptions framing the study of the neural correlates of consciousness},
journal = {Consciousness and Cognition},
volume = {96},
pages = {103229},
year = {2021},
issn = {1053-8100},
doi = {https://doi.org/10.1016/j.concog.2021.103229},
url = {https://www.sciencedirect.com/science/article/pii/S1053810021001550},
author = {Daniel Revach and Moti Salti},
keywords = {Consciousness, Awareness, Conscious perception, Unconscious perception, Cognition, Neuroscience, Assumptions, Premises, Neurobiological, Mechanism, Phenomenology},
abstract = {The way one asks a question is shaped by a-priori assumptions and constrains the range of possible answers. We identify and test the assumptions underlying contemporary debates, models, and methodology in the study of the neural correlates of consciousness, which was framed by Crick and Koch’s seminal paper (1990). These premises create a sequential and passive conception of conscious perception: it is considered the product of resolved information processing by unconscious mechanisms, produced by a singular event in time and place representing the moment of entry. The conscious percept produced is then automatically retained to be utilized by post-conscious mechanisms. Major debates in the field, such as concern the moment of entry, the all-or-none vs graded nature, and report vs no-report paradigms, are driven by the consensus on these assumptions. We show how removing these assumptions can resolve some of the debates and challenges and prompt additional questions. The potential non-sequential nature of perception suggests new ways of thinking about consciousness as a dynamic and dispersed process, and in turn about the relationship between conscious and unconscious perception. Moreover, it allows us to present a parsimonious account for conscious perception while addressing more aspects of the phenomenon.}
}
@article{VANZUNDERT2010270,
title = {Effective peer assessment processes: Research findings and future directions},
journal = {Learning and Instruction},
volume = {20},
number = {4},
pages = {270-279},
year = {2010},
note = {Unravelling Peer Assessment},
issn = {0959-4752},
doi = {https://doi.org/10.1016/j.learninstruc.2009.08.004},
url = {https://www.sciencedirect.com/science/article/pii/S0959475209000814},
author = {Marjo {van Zundert} and Dominique Sluijsmans and Jeroen {van Merriënboer}},
keywords = {Peer assessment, Development of peer assessment skills, Attitudes towards peer assessment, Training of peer assessment skills},
abstract = {Despite the popularity of peer assessment (PA), gaps in the literature make it difficult to describe exactly what constitutes effective PA. In a literature review, we divided PA into variables and then investigated their interrelatedness. We found that (a) PA's psychometric qualities are improved by the training and experience of peer assessors; (b) the development of domain-specific skills benefits from PA-based revision; (c) the development of PA skills benefits from training and is related to students' thinking style and academic achievement, and (d) student attitudes towards PA are positively influenced by training and experience. We conclude with recommendations for future research.}
}
@article{JUST20121292,
title = {Autism as a neural systems disorder: A theory of frontal-posterior underconnectivity},
journal = {Neuroscience & Biobehavioral Reviews},
volume = {36},
number = {4},
pages = {1292-1313},
year = {2012},
issn = {0149-7634},
doi = {https://doi.org/10.1016/j.neubiorev.2012.02.007},
url = {https://www.sciencedirect.com/science/article/pii/S0149763412000334},
author = {Marcel Adam Just and Timothy A. Keller and Vicente L. Malave and Rajesh K. Kana and Sashank Varma},
keywords = {Autism, Connectivity, Underconnectivity, 4CAPS, Computational model, fMRI},
abstract = {The underconnectivity theory of autism attributes the disorder to lower anatomical and functional systems connectivity between frontal and more posterior cortical processing. Here we review evidence for the theory and present a computational model of an executive functioning task (Tower of London) implementing the assumptions of underconnectivity. We make two modifications to a previous computational account of performance and brain activity in typical individuals in the Tower of London task (Newman et al., 2003): (1) the communication bandwidth between frontal and parietal areas was decreased and (2) the posterior centers were endowed with more executive capability (i.e., more autonomy, an adaptation is proposed to arise in response to the lowered frontal-posterior bandwidth). The autism model succeeds in matching the lower frontal-posterior functional connectivity (lower synchronization of activation) seen in fMRI data, as well as providing insight into behavioral response time results. The theory provides a unified account of how a neural dysfunction can produce a neural systems disorder and a psychological disorder with the widespread and diverse symptoms of autism.}
}
@article{ENRIQUEZHIDALGO2025123924,
title = {Evaluation of decision-support tools for coastal flood and erosion control: A multicriteria perspective},
journal = {Journal of Environmental Management},
volume = {373},
pages = {123924},
year = {2025},
issn = {0301-4797},
doi = {https://doi.org/10.1016/j.jenvman.2024.123924},
url = {https://www.sciencedirect.com/science/article/pii/S0301479724039112},
author = {Andrés M. Enríquez-Hidalgo and Andrés Vargas-Luna and Andrés Torres},
keywords = {Decision support tool, Coastal erosion and flood management, Development pathways, Coastal archetypes, Multi-criteria decision analysis},
abstract = {Coastal areas face significant challenges due to natural and anthropogenic changes, such as sea level rise, extreme events and coastal erosion. The coastal management requires the consideration of socioeconomic and environmental factors to address these variables. The selection of an appropriate Decision Support Tool (DST) based on decision matrix method plays a crucial role in implementing coastal management strategies to tackle climate change-related issues. This has posed considerable challenges for decision-makers, aligning with the Sustainable Development Goals (SDG). This review provides an overview of the practical experience in the application of DSTs for coastal erosion and flood risk, emphasizing the use of Multi-Criteria Decision Analysis (MCDA). DST choice depends on the coastal archetype, including its geographical features and sociocultural context. The purpose is to clarify how the integration of DSTs maximizes flexibility and supports the implementation of future Decision Support System (DSS) tailored to the needs of coastal cities with development pathways (DP). This review assesses different MCDA methods, highlighting their applicability, utility, and integration in coastal management, while evaluating each method's strengths, weaknesses, and specific applications, with a focus on sustainability and resilience. The review highlights the necessity of expert knowledge in accurately defining criteria and weighting factors to ensure that the chosen MCDA method reflects the complexities of the coastal environment. Depending on the scenario, methods like PROMETHEE and ELECTRE are recommended for their flexibility and robustness in handling complex decision-making processes, especially in data-rich and well-structured environments. In contrast, TOPSIS and AHP are suitable for scenarios with limited information or requiring minimal interaction with decision-makers. For more challenging contexts, where computational resources and expertise are constrained, methods like MAUT, VIKOR, and TODAIM emerge as viable alternatives due to their adaptability and reduced reliance on extensive datasets.}
}
@article{DULIC201654,
title = {Designing futures: Inquiry in climate change communication},
journal = {Futures},
volume = {81},
pages = {54-67},
year = {2016},
note = {Modelling and Simulation in Futures Studies},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2016.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S0016328716000057},
author = {Aleksandra Dulic and Jeannette Angel and Stephen Sheppard},
keywords = {Design inquiry, Designing futures, Climate change communication, Interaction design, 3D game simulation, Transdisciplinary research},
abstract = {There are many barriers and challenges associated with climate change communication focused on promoting community-based action for sustainable futures. Of particular interest is the challenge to embed community perspectives in a communication process of climate change solutions. In this paper we argue that 3D interactive simulations using design inquiry as a development process, can be an effective way of communicating climate change solutions and multiple community responses. People are more likely to engage with the challenges associated with complexity of climate change at the local level when their perspectives are integrated into viable and multiple pathways for action. Future scenarios of change processes situated in local experiences in compelling and interactive ways can be disseminated holistically by making links between scientific, social, political, economic and cultural elements. Design inquiry, as a research approach, integrates contextual knowledge into communication processes to aid imagining, re-thinking and reembodying viable pathways that explore the kinds of futures we collectively envision. This paper examines the contributions that design inquiry makes to climate change communication using an interactive simulation environment for designing futures. We discuss these ideas using the example of the Future Delta project, a virtual 3D environment that enables the exploration and simulation of multiple community-based climate change solutions in the Corporation of Delta, British Columbia.}
}
@article{YANG2023125877,
title = {Multi-parameter controlled mechatronics-electro-hydraulic power coupling electric vehicle based on active energy regulation},
journal = {Energy},
volume = {263},
pages = {125877},
year = {2023},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2022.125877},
url = {https://www.sciencedirect.com/science/article/pii/S0360544222027633},
author = {Jian Yang and Bo Liu and Tiezhu Zhang and Jichao Hong and Hongxin Zhang},
keywords = {Mechatronics-electro-hydraulic power coupling, Energy efficiency, K-means clustering analysis, Torque characteristic, Fuzzy control},
abstract = {To enhance the hydraulic energy utilization and torque output stability, a novel mechatronics-electro-hydraulic power coupling electric vehicle (MEH-PCEV) is proposed, integrating a hydraulic pump/motor and a motor into a single device for mutual energy conversion. For MEH-PCEVs equipped with multiple energy sources, a cluster analysis method is used to classify the actual road test dataset and provide guiding ideas for designing rule-based energy management strategies (RB-EMS). Simultaneously, for the output torque anomaly phenomenon in RB-EMS, an inverse thinking fuzzy logic optimization energy management strategy (FLO-EMS) conside ring multi-parameter objectives as input is used to adjust the electromagnetic torque in real-time and reasonably allocate the energy flow. The simulation results demonstrate that the electric and total torque output are more stable. The electric peak torque is relieved, with a corresponding increase in the percentage of electrical energy recovery. With the equal power demand, the overall efficiency of the motor working point is substantially improved, and the energy consumption rate is decreased by 24.42%. Under the active regulation of FLO-EMS, hydraulic energy is more reasonably utilized to meet the vehicle demand power while avoiding energy dissipation and waste. Moreover, this work is expected to reference the development and engineering applications of electro-hydraulic coupling systems.}
}
@article{KONDINSKI20241071,
title = {Hacking decarbonization with a community-operated CreatorSpace},
journal = {Chem},
volume = {10},
number = {4},
pages = {1071-1083},
year = {2024},
issn = {2451-9294},
doi = {https://doi.org/10.1016/j.chempr.2023.12.018},
url = {https://www.sciencedirect.com/science/article/pii/S2451929423006198},
author = {Aleksandar Kondinski and Sebastian Mosbach and Jethro Akroyd and Andrew Breeson and Yong Ren Tan and Simon Rihm and Jiaru Bai and Markus Kraft},
keywords = {decarbonization, chemistry, knowledge graphs, agents, CreatorSpace},
abstract = {Summary
The pressing challenge of decarbonization encompasses a vast combinatorial space of interlinked technologies, thus necessitating an increased reliance on artificial intelligence (AI)-assisted molecular modeling and data analytics. Our backcasting analysis proposes a future rich in efficient decarbonization technologies, such as sustainable fuels for aviation and shipping, as well as carbon capture and utilization. We then retrace the path to this proposed future with the guidance of two constraints: the maximization of scientists’ creative capacities and the evolution of a world-centric AI. Our exploration leads us to the concept of a “CreatorSpace,” a distributed digital system resembling existing hackerspaces and makerspaces known for accelerating the prototyping of new technologies worldwide. The CreatorSpace serves as a virtual, semantic platform where chemists, engineers, and materials scientists can freely collaborate, integrating chemical knowledge with cross-scale, cross-technology tools, and operations. This streamlined molecular-to-process-design pathway facilitates a diverse array of solutions for decarbonization and other sustainability technologies.}
}
@article{KRUSKOPF2024104574,
title = {Future teachers’ self-efficacy in teaching practical and algorithmic ICT competencies – Does background matter?},
journal = {Teaching and Teacher Education},
volume = {144},
pages = {104574},
year = {2024},
issn = {0742-051X},
doi = {https://doi.org/10.1016/j.tate.2024.104574},
url = {https://www.sciencedirect.com/science/article/pii/S0742051X24001069},
author = {Milla Kruskopf and Rekar Abdulhamed and Mette Ranta and Heidi Lammassaari and Kirsti Lonka},
keywords = {Teaching self-efficacy, Self-efficacy, ICT competence, Digital competence, Programming, 21st century competencies, Teacher students},
abstract = {Future teachers need to be confidently equipped to teach 21st century ICT skills. We investigated teaching self-efficacy (TSE) in ICT competencies among teacher students. We confirmed distinct ICT competencies among two cohorts from teacher training programs (n = 347; n = 428): practical (i.e., device and data management), and algorithmic (i.e., programming, and data security). Regression analyses indicated TSE-biases regarding younger age, male gender, and a background in natural sciences, with significant interactions between age, gender, and having learned such ICT-skills already in school. The findings point to a need for tailored strategies in teacher education to mitigate TSE disparities.}
}
@article{YU2023114721,
title = {Numerical investigation of splitter blades on the performance of a forward-curved impeller used in a pump as turbine},
journal = {Ocean Engineering},
volume = {281},
pages = {114721},
year = {2023},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2023.114721},
url = {https://www.sciencedirect.com/science/article/pii/S0029801823011058},
author = {He Yu and Tao Wang and Yuancheng Dong and Qiuqin Gou and Lei Lei and Yunqi Liu},
keywords = {Special impeller, Pump as turbine, Splitter blade, Entropy generation, Computational fluid dynamics},
abstract = {Abstract
As a type of economical energy recovery device, pump as turbine (PAT) is generally used in micro-hydropower plants and energy recovery. To study the influence of the splitter blade on a special impeller used in PAT, impellers without and with splitter blades are designed in this paper. The influences of splitter blade on the energy loss, external characteristics and internal flow field distribution of a PAT were simulated via a verified computational fluid dynamics (CFD) method. The consequences present that the shaft power, efficiency, and head corresponding to the BEP of the PAT with splitter blades are 16.4%, 1.3%, and 8.8% better than those of the PAT without splitter blades. The total entropy production of the PAT without splitter blade is higher than that of the PAT with splitter blades at the same flow rate. Adding splitter blade increased the number of effective blades, made the fluid flow more evenly along the impeller flow passage, and reduced the flow separation inside the impeller. This paper displays that adding splitter blades not only obviously increases hydraulic performance under large flow conditions but also significantly widens the high-efficiency range of PATs.}
}
@article{MONNAHAN2024107024,
title = {Toward good practices for Bayesian data-rich fisheries stock assessments using a modern statistical workflow},
journal = {Fisheries Research},
volume = {275},
pages = {107024},
year = {2024},
issn = {0165-7836},
doi = {https://doi.org/10.1016/j.fishres.2024.107024},
url = {https://www.sciencedirect.com/science/article/pii/S0165783624000882},
author = {Cole C. Monnahan},
keywords = {No-U-turn sampler (NUTS), Bayesian integration, Prior predictive checks, Posterior predictive checks, Cross validation},
abstract = {Bayesian inference has long been recognized as useful for fisheries stock assessments but it is less common than maximum likelihood approaches due to long run times and a lack of good practices. Recent computational advances leave developing good practices and user-friendly interfaces as the most important hurdles to wider use of this powerful statistical paradigm. Here, I argue that the modern Bayesian workflow proposed by Gelman et al. (2020) should form the basis for proposed good practices in fisheries sciences. Their workflow is a conceptual roadmap for iterative model building which includes the philosophical role of priors and how to apply statistical tools to construct them, how to validate and compare models, and how to overcome computational problems. Adapted for stock assessment, this leads to the following good practices for analysts. Diagnostics from multiple no-U-turn sampler (NUTS) chains (the recommended MCMC algorithm) should pass and be reported, specifically that the potential scale reduction Rˆ is <1.01 and the effective sample size is >400 for all parameters, and there are no NUTS divergences. When direct a priori information is unavailable on parameters, use prior predictive checking to build, assess, and adjust priors to enforce desired constraints on complexity, or to conform to a priori expectations or physical/biological limitations on derived quantities. Use posterior predictive checks to validate models by confirming simulated data and summaries (e.g., variance of compositional data) are similar to the observed counterparts. Process error variances can be estimated jointly with random effects and other parameters when desired, and should be for important model components. An approximate cross-validation technique called PSIS-LOO is the most practical tool for model selection, but can also provide important insights into model deficiencies. I also recommended that model developers build and parameterize models to have minimal parameter correlations and marginal variances close to one, have options for diverse (multivariate) priors, do predictive modeling, and ensure that the tools comprising a workflow are accessible and straightforward for routine use. I review, adapt, and illustrate a Bayesian workflow on AD Model Builder and Stock Synthesis models, but these good practices apply to models from any software platform, including Template Model Builder and Stan. Finally, I argue that the Bayesian and frequentist paradigms complement each other, with both helping analysts better understand different aspects of their models and data. Wider adoption of Bayesian methods using the good practices proposed here would therefore lead to improved scientific advice used to manage fisheries.}
}
@incollection{GIGERENZER2015515,
title = {Computers: Impact on the Social Sciences},
editor = {James D. Wright},
booktitle = {International Encyclopedia of the Social & Behavioral Sciences (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {515-518},
year = {2015},
isbn = {978-0-08-097087-5},
doi = {https://doi.org/10.1016/B978-0-08-097086-8.03202-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780080970868032025},
author = {Gerd Gigerenzer},
keywords = {Charles Babbage, Cognitive revolution, Computer simulation, Division of labor, H.A. Simon, Metaphor, Statistics},
abstract = {The social organization of labor in the nineteenth century served as the model for Babbage's first computers. In the second half of the twentieth century, when working computers were finally constructed and invaded the offices of social scientists, they turned into theories of mind. This mutual inspiration first changed the meaning of calculation and then led to a new understanding of thought as computation based on hierarchically organized subroutines. The computer as a research tool has changed the social sciences in a fundamental way, from enabling large-scale simulations of cognitive and social systems to allowing mindless and mechanical use of statistics.}
}
@article{SULLIVAN2020246,
title = {Maritime 4.0 – Opportunities in Digitalization and Advanced Manufacturing for Vessel Development},
journal = {Procedia Manufacturing},
volume = {42},
pages = {246-253},
year = {2020},
note = {International Conference on Industry 4.0 and Smart Manufacturing (ISM 2019)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.02.078},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920306430},
author = {Brendan P. Sullivan and Shantanoo Desai and Jordi Sole and Monica Rossi and Lucia Ramundo and Sergio Terzi},
keywords = {Maritime 4.0, Digitalization, Maritime Vessel Development, Industry 4.0},
abstract = {Maritime vessels are complex systems that generate and require the utilization of large amounts of data for maximum efficiency. The successful utilization of sensors and IoT in the industry requires a forward-thinking approach to leverage the benefits of Industry 4.0 in a more comprehensive manner. While processes and manufacturing processes can be improved and advanced through such efforts, in order the industry to be able to benefit from data generation, integrated approaches are necessary. In order to develop truly value-added vessels, we introduce a descriptive approach for understanding Maritime 4.0.}
}
@article{ZHANG2020259,
title = {Self-blast state detection of glass insulators based on stochastic configuration networks and a feedback transfer learning mechanism},
journal = {Information Sciences},
volume = {522},
pages = {259-274},
year = {2020},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2020.02.058},
url = {https://www.sciencedirect.com/science/article/pii/S0020025520301419},
author = {Qian Zhang and Weitao Li and Hua Li and Jianping Wang},
keywords = {Insulator self-blast state, Deep learning, Feedback transfer learning mechanism, Semantic error entropy},
abstract = {The self-blast state of a glass insulator directly affects the safety and reliability of transmission lines. To address the insufficient generalization ability of existing detection methods for insulator self-blast states and the drawbacks of deep neural network structures, the theories of transfer learning and closed-loop control are drawn upon to provide an intelligent detection method for the self-blast states of glass insulators. The method proposed in this paper is based on stochastic configuration networks and a feedback transfer learning mechanism. First, to reduce the redundancy of convolutional kernels in the channel extent, the interleaved group convolution strategy is employed to reconstruct the convolutional layers of the Inception network. Second, in view of the different feature applicabilities of different glass insulator images and based on the adaptive convolution module groups, the data structure of the dynamic feature space of insulator images is built with a certain mapping relationship from global to local. Then, the discriminative measure index is used to evaluate the discriminative information of the feature space to enhance the interpretability of the compact feature spance. Third, the fully connected feature vector of the compact feature space is sent to stochastic configuration networks (SCNs), which have universal approximation property to establish the classification criteria of the self-blast states of insulator images with generalization ability. Finally, an imitation of human thinking patterns is employed that exhibits repeated deliberation and comparison. Consequently, based on generalized error and entropy theories, the evaluation index of the objective function is established to evaluate the uncertain detection results of the self-blast states of glass insulator images in real time. Then, the dynamic transfer learning mechanism is constructed based on the constraint of the measurement index of uncertain detection results to realize self-optimizing regulation of the feature space that exhibits multihierarchy and discrimination and reconstructed classification criteria. The experimental results show that compared with other algorithms, the proposed method enhances the generalization ability and detection accuracy of the model.}
}
@article{KANDEMIR2025104990,
title = {Pre-service mathematics teachers' modelling processes within model eliciting activity through digital technologies},
journal = {Acta Psychologica},
volume = {256},
pages = {104990},
year = {2025},
issn = {0001-6918},
doi = {https://doi.org/10.1016/j.actpsy.2025.104990},
url = {https://www.sciencedirect.com/science/article/pii/S0001691825003038},
author = {Mehmet Ali Kandemir and Nurullah Eryilmaz},
keywords = {Digital technologies, Mathematical modelling, Model eliciting activity (MEA), Pre-service mathematics teachers (PSTs), Technology use, Mathematical content knowledge (MCK)},
abstract = {This study examines the modelling approaches of high school pre-service mathematics teachers (PSTs) and the types and purposes of digital technologies they use at different stages of the mathematical modelling process. Conducted during the 2018–2019 academic year, the study involved 26 PSTs working in eight groups as part of a course on computer technologies in mathematics education. The participants engaged in a model eliciting activity (MEA) focused on the obesity problem, integrating digital technologies and mathematical content knowledge. Findings indicate that while PSTs effectively utilized the internet, spreadsheets, calculators, and mathematical software for problem-solving, three distinct purposes of technology use emerged. However, challenges included overreliance on technological outputs, limiting critical evaluation and validation of models, and difficulties in transferring mathematical content knowledge to the modelling process. These results highlight the need for explicit instructional support in teacher education programs to enhance PSTs' critical engagement with digital tools and strengthen their ability to integrate mathematical knowledge in real-world problem-solving.}
}
@article{HUHN201642,
title = {Cognitive framing in action},
journal = {Cognition},
volume = {151},
pages = {42-51},
year = {2016},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2016.02.015},
url = {https://www.sciencedirect.com/science/article/pii/S0010027716300439},
author = {John M. Huhn and Cory Adam Potts and David A. Rosenbaum},
keywords = {Action, Cognitive framing, Heuristics, Object manipulation, Motor control, Bimanual actions},
abstract = {Cognitive framing effects have been widely reported in higher-level decision-making and have been ascribed to rules of thumb for quick thinking. No such demonstrations have been reported for physical action, as far as we know, but they would be expected if cognition for physical action is fundamentally similar to cognition for higher-level decision-making. To test for such effects, we asked participants to reach for a horizontally-oriented pipe to move it from one height to another while turning the pipe 180° to bring one end (the “business end”) to a target on the left or right. From a physical perspective, participants could have always rotated the pipe in the same angular direction no matter which end was the business end; a given participant could have always turned the pipe clockwise or counter-clockwise. Instead, our participants turned the business end counter-clockwise for left targets and clockwise for right targets. Thus, the way the identical physical task was framed altered the way it was performed. This finding is consistent with the hypothesis that cognition for physical action is fundamentally similar to cognition for higher-level decision-making. A tantalizing possibility is that higher-level decision heuristics have roots in the control of physical action, a hypothesis that accords with embodied views of cognition.}
}
@article{LUCKRING2024100998,
title = {Prediction of concentrated vortex aerodynamics: Current CFD capability survey},
journal = {Progress in Aerospace Sciences},
volume = {147},
pages = {100998},
year = {2024},
issn = {0376-0421},
doi = {https://doi.org/10.1016/j.paerosci.2024.100998},
url = {https://www.sciencedirect.com/science/article/pii/S0376042124000241},
author = {James M. Luckring and Arthur Rizzi},
abstract = {Concentrated vortex flows contribute to the aerodynamic performance of aircraft at elevated load conditions. For military interests, the vortex flows are exploited at maneuver conditions of combat aircraft and missiles. For transport interests, the vortex flows are exploited at takeoff and landing conditions as well as at select transonic conditions. Aircraft applications of these vortex flows are reviewed with a historical perspective followed by a discussion of the underlying physics of a concentrated vortex flow. A hierarchy of computational fluid dynamics simulation technology is then presented followed by findings from a capability survey for predicting concentrated vortex flows with computational fluid dynamics. Results are focused on military and civil fixed-wing aircraft; only limited results are included for missiles, and rotary-wing applications are not assessed. Opportunities for predictive capability advancement are then reported with comments related to digital transformation interests. A hierarchical approach that merges a physics-based perspective of the concentrated vortex flows with a systems engineering viewpoint of the air vehicle is also used to frame much of the discussion.}
}
@article{AQDA2011260,
title = {The impact of constructivist and cognitive distance instructional design on the learner’s creativity},
journal = {Procedia Computer Science},
volume = {3},
pages = {260-265},
year = {2011},
note = {World Conference on Information Technology},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2010.12.044},
url = {https://www.sciencedirect.com/science/article/pii/S1877050910004199},
author = {Mahnaz Fatemi Aqda and Farideh Hamidi and Farhad Ghorbandordinejad},
keywords = {Instructional design, Distance education, E-learning, Creativity, Cognitivism, Constructivism},
abstract = {Creativity is at the heart of the 21st century educational work. Learner’ creativity or learner’s creative thinking skills are among the most important skills they need to be prepared for the knowledge society. The rapid development of technology in the modern era sheds light on the place and importance of creativity in education. Technology also has brought change in the way the students learn (collaboration strategy) and recently the computer-based instruction associated with electrical technologies has been a popular way of instruction that learning is no longer confined to classrooms (distance learning). Also in the case of the students if they want to take effective advantage of technology, they have to use the constructivist and cognitive skills (psychological learning theory). Recently education experts have tried to show how a distance instructional can be designed. The main question of this paper is what effects the distance instructional design based on the views of constructivism and cognitivism have on the learners’ creativity.The definition of distance education (e-learning), the instruction design based on constructivist view and its function in education and distance learning (e-learning), the instruction design based on cognitive view and its function in education and distance learning (e-learning), the factors affecting the creativity development and accommodation (comparison) the characteristics of the instructural context, and the impact of the appropriate learning on the creativity development according to these settings are among the other main points of this review.}
}
@article{NORGAARD2023105308,
title = {Linked auditory and motor patterns in the improvisation vocabulary of an artist-level jazz pianist},
journal = {Cognition},
volume = {230},
pages = {105308},
year = {2023},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2022.105308},
url = {https://www.sciencedirect.com/science/article/pii/S0010027722002967},
author = {Martin Norgaard and Kevin Bales and Niels Chr. Hansen},
keywords = {Improvisation, Jazz, Music, Audiomotor coupling, Expectation, Entropy},
abstract = {Improvising musicians possess a stored library of musical patterns forming the basis for their improvisations. According to a prominent theoretical framework by Pressing (1988), this library includes linked auditory and motor information. Though examples of libraries of melodic patterns have been shown in extant recordings by some improvising musicians, the underlying motor component has not been experimentally investigated nor related to its auditory counterparts. Here we analyzed a large corpus of ∼100,000 notes from improvisations by one artist-level jazz pianist recorded during 11 live performances with audience. We compared the library identified from these recordings to a control corpus consisting of improvisations by 24 different advanced jazz pianists. In addition to pitch, our recordings included accurate micro-timing and key velocity (i.e., force) data. Following a previously validated procedure, this information was used to identify the underlying motor patterns through correlations between relative timing and velocity between notes in different iterations of the same pitch pattern. A computational model was, furthermore, used to estimate the information content and generated entropy exhibited by recurring pitch patterns with high and low timing and velocity correlations as perceived by a stylistically enculturated expert listener. Though both corpora contained a large number of recurring patterns, the single-player corpus showed stronger evidence that pitch patterns were linked to motor programs in that within-pattern timing and velocity correlations were significantly higher compared to the control corpus. Even when controlling for potentially greater baseline levels of motor self-consistency in the single-player corpus, this effect remained significant for velocity correlations. Amongst recurring 5-tone pitch patterns, those exhibiting more consistent motor schema also used less idiomatic pitch transitions that were both more unexpected and generated more uncertain expectations in enculturated experts than less consistently repeated patterns. Interestingly, we only found partial evidence for fixed pattern boundaries as predicted by the Pressing model and therefore suggest an expanded view in which the beginning and ends of idiomatic audio-motor patterns are not always clear-cut. Our results indicate that the library of melodic patterns may be idiosyncratic to the individual improviser and relies both on motor programming and predictive processing to promote stylistic distinctiveness.}
}
@incollection{QAZI2025245,
title = {Chapter 11 - Deep learning in clinical genomics-based cancer diagnosis},
editor = {Khalid Raza},
booktitle = {Deep Learning in Genetics and Genomics},
publisher = {Academic Press},
pages = {245-259},
year = {2025},
isbn = {978-0-443-27574-6},
doi = {https://doi.org/10.1016/B978-0-443-27574-6.00014-X},
url = {https://www.sciencedirect.com/science/article/pii/B978044327574600014X},
author = {Sahar Qazi and Raiyan Ali and Manoj Kumar Jana and Bimal Prasad Jit and Neeraj Gurung and Ashok Sharma},
keywords = {Artificial intelligence, Bioinformatics, Cancer, Deep learning, Diagnosis, Next generation sequencing, Variant calling},
abstract = {Deep learning, an artificial intelligence facet, has impacted distinct fields, including natural language processing and computer vision. Its advancements have transformed how computational and data scientists approach data, turning unstructured information into valuable insights. This is particularly impactful in clinical genomics, where high-throughput sequencing generates vast amounts of data. Techniques such as whole genome sequencing and transcriptomic profiling produce enormous datasets that are challenging to analyze manually. Deep learning tools like “Deep Variant” enhance accuracy in variant calling, improving diagnostic precision. By adapting to factors such as genetic mutations and disease progression, deep learning aids in early cancer diagnosis and better clinical outcomes. This chapter explores these transformative applications in clinical genomic research.}
}
@article{SEISING2006237,
title = {From vagueness in medical thought to the foundations of fuzzy reasoning in medical diagnosis},
journal = {Artificial Intelligence in Medicine},
volume = {38},
number = {3},
pages = {237-256},
year = {2006},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2006.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S0933365706001072},
author = {Rudolf Seising},
keywords = {History of science and technology, Fuzzy set theory, Vagueness, Medical diagnoses, Computer assistance, Medical philosophy, System theory},
abstract = {Summary
Objective
This article delineates a relatively unknown path in the history of medical philosophy and medical diagnosis. It is concerned with the phenomenon of vagueness in the physician's “style of thinking” and with the use of fuzzy sets, systems, and relations with a view to create a model of such reasoning when physicians make a diagnosis. It represents specific features of medical ways of thinking that were mentioned by the Polish physician and philosopher Ludwik Fleck in 1926. The paper links Lotfi Zadeh's work on system theory before the age of fuzzy sets with system-theory concepts in medical philosophy that were introduced by the philosopher Mario Bunge, and with the fuzzy-theoretical analysis of the notions of health, illness, and disease by the Iranian-German physician and philosopher Kazem Sadegh-Zadeh.
Material
Some proposals to apply fuzzy sets in medicine were based on a suggestion made by Zadeh: symptoms and diseases are fuzzy in nature and fuzzy sets are feasible to represent these entity classes of medical knowledge. Yet other attempts to use fuzzy sets in medicine were self-contained. The use of this approach contributed to medical decision-making and the development of computer-assisted diagnosis in medicine.
Conclusion
With regard to medical philosophy, decision-making, and diagnosis; the framework of fuzzy sets, systems, and relations is very useful to deal with the absence of sharp boundaries of the sets of symptoms, diagnoses, and phenomena of diseases. The foundations of reasoning and computer assistance in medicine were the result of a rapid accumulation of data from medical research. This explosion of knowledge in medicine gave rise to the speculation that computers could be used for the medical diagnosis. Medicine became, to a certain extent, a quantitative science. In the second half of the 20th century medical knowledge started to be stored in computer systems. To assist physicians in medical decision-making and patient care, medical expert systems using the theory of fuzzy sets and relations (such as the Viennese “fuzzy version” of the Computer-Assisted Diagnostic System, Cadiag, which was developed at the end of the 1970s) were constructed. The development of fuzzy relations in medicine and their application in computer-assisted diagnosis show that this fuzzy approach is a framework to deal with the “fuzzy mode of thinking” in medicine.}
}
@article{SIMS1991383,
title = {Computers and experiments in stress analysis: Eds G. M. Carlomagno and C. A. Brebbia Computational Mechanics Publications, Southampton, UK},
journal = {Engineering Structures},
volume = {13},
number = {4},
pages = {383-384},
year = {1991},
issn = {0141-0296},
doi = {https://doi.org/10.1016/0141-0296(91)90027-A},
url = {https://www.sciencedirect.com/science/article/pii/014102969190027A},
author = {P. Sims}
}
@article{BELABES2015639,
title = {Designing Islamic Finance Programmes in a Competitive Educational Space: The Islamic Economics Institute Experiment},
journal = {Procedia - Social and Behavioral Sciences},
volume = {191},
pages = {639-643},
year = {2015},
note = {The Proceedings of 6th World Conference on educational Sciences},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2015.04.300},
url = {https://www.sciencedirect.com/science/article/pii/S1877042815025604},
author = {Abderrazak Belabes and Ahmed Belouafi and Mohamed Daoudi},
keywords = {Curricula design, glocalization, competitiveness, Islamic finance, Islamic Economics Institute},
abstract = {This paper aims at exploring the experiment of the Islamic Economics Institute (IEI) of King Abdulaziz University in the design of the first ever Islamic finance higher educational programme at a Saudi Public University. An evaluative analytical framework has been utilized to meet this goal. Results show that the Institute has pursued a ‘glocalization’; thinking globally and acting locally approach in designing the programme. This approach aims at providing learners with ‘cutting-edge’ skills that will enhance their chances for employment at the local as well as regional markets. What are the advantages of this approach? And how can the Institute preserve its ‘distinctive research’ positioning that it has gained over the years, at the same time, being able to provide ‘world-class’ educational programmes?}
}
@article{MCLEAN20248,
title = {Autoantibodies against acetylcholine receptors are increased in archived serum samples from patients with schizophrenia},
journal = {Schizophrenia Research},
volume = {267},
pages = {8-13},
year = {2024},
issn = {0920-9964},
doi = {https://doi.org/10.1016/j.schres.2024.03.012},
url = {https://www.sciencedirect.com/science/article/pii/S0920996424001129},
author = {Ryan Thomas McLean and Elizabeth Buist and David {St. Clair} and Jun Wei},
keywords = {Neurotransmitter receptor, CHRM4, GRM3, CHRNA4, CHRNA5 neuroimmunology},
abstract = {Previous studies have demonstrated that the levels of IgG against neurotransmitter receptors are increased in patients with schizophrenia. Genome-wide association (GWA) studies of schizophrenia confirmed that 108 loci harbouring over 300 genes were associated with schizophrenia. Although the functional implications of genetic variants are unclear, theoretical functional alterations of these genes could be replicated by the presence of autoantibodies. This study examined the levels of plasma IgG antibodies against four neurotransmitter receptors, CHRM4, GRM3, CHRNA4 and CHRNA5, using an in-house ELISA in 247 patients with schizophrenia and 344 non-psychiatric controls. Four peptides were designed based on in silico analysis with computational prediction of HLA-DRB1 restricted and B-cell epitopes. The relationship between plasma IgG levels and psychiatric symptoms, as defined by the Operational Criteria Checklist for Psychotic Illness and Affective Illness (OPCRIT), were examined. The results showed that the levels of plasma IgG against peptides derived from CHRM4 and CHRNA4 were significantly increased in patients with schizophrenia compared with control subjects, but there was no significant association of plasma IgG levels with any symptom domain or any specific symptoms. These preliminary results suggest that CHRM4 and CHRNA4 may be novel targets for autoantibody responses in schizophrenia, although the pathogenic relationship between increased serum autoantibody levels and schizophrenia symptoms remains unclear.}
}
@article{FIROOZI2025104593,
title = {Developing urban infrastructure: Strategic integration of solar-heated pavement systems for enhanced resilience and sustainability},
journal = {Results in Engineering},
volume = {25},
pages = {104593},
year = {2025},
issn = {2590-1230},
doi = {https://doi.org/10.1016/j.rineng.2025.104593},
url = {https://www.sciencedirect.com/science/article/pii/S2590123025006711},
author = {Ali Akbar Firoozi and Ali Asghar Firoozi and D.O. Oyejobi and Siva Avudaiappan and Erick Saavedra Flores},
keywords = {Solar-Heated pavements, Urban resilience, Sustainable infrastructure, Smart city technologies, Thermal energy storage, Green urban planning},
abstract = {This research examines the integration and optimization of solar-heated pavement systems in urban environments, emphasizing their potential to enhance urban resilience and sustainability. Utilizing advanced materials and smart technologies, these pavements maintain safe, ice-free surfaces during winter, reducing reliance on traditional snow removal methods and their environmental impacts. The methodology focuses on a combination of experimental setups and computational simulations to analyze the design features, material advancements, and strategic integration of these systems. Quantitative findings from case studies across diverse climates demonstrate a significant reduction in energy consumption by up to 40 % and maintenance costs by 60 %, highlighting the economic and environmental benefits. The manuscript advocates broader implementation and recommends further research to optimize system efficiency and applicability in urban planning initiatives.}
}
@article{FELSCHE2023101530,
title = {Evidence for abstract representations in children but not capuchin monkeys},
journal = {Cognitive Psychology},
volume = {140},
pages = {101530},
year = {2023},
issn = {0010-0285},
doi = {https://doi.org/10.1016/j.cogpsych.2022.101530},
url = {https://www.sciencedirect.com/science/article/pii/S0010028522000664},
author = {Elisa Felsche and Patience Stevens and Christoph J. Völter and Daphna Buchsbaum and Amanda M. Seed},
keywords = {Overhypotheses, Abstraction, Generalization, Animal cognition, Computational modeling, Cognitive development},
abstract = {The use of abstract higher-level knowledge (also called overhypotheses) allows humans to learn quickly from sparse data and make predictions in new situations. Previous research has suggested that humans may be the only species capable of abstract knowledge formation, but this remains controversial. There is also mixed evidence for when this ability emerges over human development. Kemp et al. (2007) proposed a computational model of how overhypotheses could be learned from sparse examples. We provide the first direct test of this model: an ecologically valid paradigm for testing two species, capuchin monkeys (Sapajus spp.) and 4- to 5-year-old human children. We presented participants with sampled evidence from different containers which suggested that all containers held items of uniform type (type condition) or of uniform size (size condition). Subsequently, we presented two new test containers and an example item from each: a small, high-valued item and a large but low-valued item. Participants could then choose from which test container they would like to receive the next sample – the optimal choice was the container that yielded a large item in the size condition or a high-valued item in the type condition. We compared performance to a priori predictions made by models with and without the capacity to learn overhypotheses. Children's choices were consistent with the model predictions and thus suggest an ability for abstract knowledge formation in the preschool years, whereas monkeys performed at chance level.}
}
@article{HOLYOAK2021118,
title = {Emergence of relational reasoning},
journal = {Current Opinion in Behavioral Sciences},
volume = {37},
pages = {118-124},
year = {2021},
note = {Same-different conceptualization},
issn = {2352-1546},
doi = {https://doi.org/10.1016/j.cobeha.2020.11.012},
url = {https://www.sciencedirect.com/science/article/pii/S2352154620301716},
author = {Keith J Holyoak and Hongjing Lu},
abstract = {We review recent theoretical and empirical work on the emergence of relational reasoning, drawing connections among the fields of comparative psychology, developmental psychology, cognitive neuroscience, cognitive science, and machine learning. Relational learning appears to involve multiple systems: a suite of Early Systems that are available to human infants and are shared to some extent with nonhuman animals; and a Late System that emerges in humans only, at approximately age three years. The Late System supports reasoning with explicit role-governed relations, and is closely tied to the functions of a frontoparietal network in the human brain. Recent work in cognitive science and machine learning suggests that humans (and perhaps machines) may acquire abstract relations from nonrelational inputs by means of processes that enable re-representation.}
}
@article{KATONA2023115228,
title = {Accuracy of the robust design analysis for the flux barrier modelling of an interior permanent magnet synchronous motor},
journal = {Journal of Computational and Applied Mathematics},
volume = {429},
pages = {115228},
year = {2023},
issn = {0377-0427},
doi = {https://doi.org/10.1016/j.cam.2023.115228},
url = {https://www.sciencedirect.com/science/article/pii/S0377042723001723},
author = {Mihály Katona and Miklós Kuczmann and Tamás Orosz},
keywords = {Electrical machines, Optimisation, Finite element method, Robust design analysis, Design of Experiment methods},
abstract = {Mass-produced electrical machines are subjected to manufacturing uncertainties in terms of geometry. A robust design is inevitable to ensure the consistent performance of the electric motor. Some parts of the rotor geometry are often simplified, like the flux barrier at the end of the magnets. This paper presents a design optimisation regarding the torque ripple and the average torque. The aim is to assess the effects of the flux barrier on the main properties of a permanent magnet synchronous motor. Also, robust design analysis is presented on the flux barrier. The computational burden of the robust design analysis is immense, even if uniform uncertainties are assumed. In this case, different Design of Experiment (DoE) methods reduce the number of simulations. The efficiency of the DoE methods is compared in terms of simulation number and extreme value approximation. We found that the Central Composite method is the most accurate, while the Plackett–Burman is the most efficient in this particular case.}
}
@incollection{VALLERO20211,
title = {Chapter 1 - Systems science},
editor = {Daniel A. Vallero},
booktitle = {Environmental Systems Science},
publisher = {Elsevier},
pages = {1-24},
year = {2021},
isbn = {978-0-12-821953-9},
doi = {https://doi.org/10.1016/B978-0-12-821953-9.00014-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128219539000143},
author = {Daniel A. Vallero},
keywords = {Systems science, Scientific method, Data-intensive discovery, Computational methods, Emergence, Fuzziness, Ecotone, Ecocline, Environmental risk, Biosolids},
abstract = {This chapter manifests how some of the connotations of systems apply to environmental science. The discussion begins with the history and evolution of scientific methods and paradigms, especially the agreement on the scientific method, spatial and temporal complexity, and the first principles of thermodynamics and motion. These are compared to modern environmental applications, including computational methods, governance, and emergence. Scientific and technical communication approaches needed for environmental systems science are described.}
}
@article{HUANG2025134690,
title = {Operation optimization of Combined Heat and Power microgrid in buildings consider renewable energy, electric vehicles and hydrogen fuel},
journal = {Energy},
volume = {319},
pages = {134690},
year = {2025},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2025.134690},
url = {https://www.sciencedirect.com/science/article/pii/S0360544225003329},
author = {Yongyi Huang and Shoaib Ahmed and Soichiro Ueda and Xunyu Liang and Harun Or Rashid Howlader and Mohammed Elsayed Lotfy and Tomonobu Senjyu},
keywords = {Microgrid, Renewable energy, Electric vehicles, Monte Carlo simulation, K-means, Real-time pricing, Particle swarm optimization, Chance-constrained programming, Combined heat and power},
abstract = {This paper introduces a forward-thinking framework that integrates renewable energy, Electric Vehicles (EVs), and hydrogen within Combined Heat and Power (CHP) microgrids (MGs) for effective building energy management. By utilizing Particle Swarm Optimization (PSO) to find the optimal solution and incorporating Chance-Constrained Programming (CCP) to handle uncertainties in renewable energy generation and EV loads, this framework addresses the complexities of modern energy systems. The study employs Monte Carlo (MC) to simulate the EV load profile, applies K-means clustering to categorize load and renewable generation patterns, and uses a Sigmoid function-based model for Real-Time Pricing (RTP). The combination of PSO and CCP is used to optimize the system’s operating strategy. This evaluates the system’s economic benefits and impact on carbon emissions by analyzing different scenarios, such as weekdays versus weekends and various weather conditions (sunny, cloudy, rainy). The results show that due to the high price of hydrogen, it is currently costly to replace hydrogen completely. However, this integrated approach not only improves energy efficiency and reduces carbon footprint but also ensures system reliability under uncertain conditions, contributing to broader environmental sustainability.}
}
@article{LI2024120,
title = {Conformal structure-preserving SVM methods for the nonlinear Schrödinger equation with weakly linear damping term},
journal = {Applied Numerical Mathematics},
volume = {205},
pages = {120-136},
year = {2024},
issn = {0168-9274},
doi = {https://doi.org/10.1016/j.apnum.2024.06.024},
url = {https://www.sciencedirect.com/science/article/pii/S0168927424001727},
author = {Xin Li and Luming Zhang},
keywords = {Damped nonlinear Schrödinger equation, Conformal properties, Supplementary variable method, High-order accuracy, Optimization model},
abstract = {In this paper, by applying the supplementary variable method (SVM), some high-order, conformal structure-preserving, linearized algorithms are developed for the damped nonlinear Schrödinger equation. We derive the well-determined SVM systems with the conformal properties and they are then equivalent to nonlinear equality constrained optimization problems for computation. The deduced optimization models are discretized by using the Gauss type Runge-Kutta method and the prediction-correction technique in time as well as the Fourier pseudo-spectral method in space. Numerical results and some comparisons between this method and other reported methods are given to favor the suggested method in the overall performance. It is worthwhile to emphasize that the numerical strategy in this work could be extended to other conservative or dissipative system for designing high-order structure-preserving algorithms.}
}
@article{LESSARD20071754,
title = {Complexity and reflexivity: Two important issues for economic evaluation in health care},
journal = {Social Science & Medicine},
volume = {64},
number = {8},
pages = {1754-1765},
year = {2007},
issn = {0277-9536},
doi = {https://doi.org/10.1016/j.socscimed.2006.12.006},
url = {https://www.sciencedirect.com/science/article/pii/S0277953606006356},
author = {Chantale Lessard},
keywords = {Health economics, Complexity theory, Reflexivity, Economic evaluation in health care},
abstract = {Economic evaluations are analytic techniques to assess the relative costs and consequences of health care programmes and technologies. Their role is to provide rigorous data to inform the health care decision-making process. Economic evaluation may oversimplify complex health care decisions. These analyses often ignore important health consequences, contextual elements, relationships or other relevant modifying factors, which might not be appropriate in a multi-objective, multi-stakeholder issue. One solution would be to develop a new paradigm based on the issues of perspective and context. Complexity theory may provide a useful conceptual framework for economic evaluation in health care. Complexity thinking develops an awareness of issues including uncertainty, contextual issues, multiple perspectives, broader societal involvement, and transdisciplinarity. This points the economic evaluation field towards an accountability and epistemology based on pluralism and uncertainty, requiring new forms of lay-expert engagement and roles of lay knowledge into decision-making processes. This highlights the issue of reflexivity in economic evaluation in health care. A reflexive approach would allow economic evaluators to analyze how objective structures and subjective elements influence their practices. In return, this would point increase the integrity and reliability of economic evaluations. Reflexivity provides opportunities for critically thinking about the organization and activities of the intellectual field, and perhaps the potential of moving in new, creative directions. This paper argues for economic evaluators to have a less positivist attitude towards what is useful knowledge, and to use more imagination about the data and methodologies they use.}
}
@article{KARADAG2025,
title = {A new frontier in design studio: AI and human collaboration in conceptual design},
journal = {Frontiers of Architectural Research},
year = {2025},
issn = {2095-2635},
doi = {https://doi.org/10.1016/j.foar.2025.01.010},
url = {https://www.sciencedirect.com/science/article/pii/S2095263525000226},
author = {Derya Karadağ and Betül Ozar},
keywords = {Artificial intelligence, Design process, Conceptual development, Interior design education, Text-to-image generation},
abstract = {This study explores the role of artificial intelligence (AI) in the conceptual design phase of interior design education, focusing on AI's potential to help students visualise and refine creative ideas. Conducted within a design studio course, the research integrates text-to-image generators, particularly Midjourney to support students' design processes. Implemented in the fourth week of a 14-week course, a structured workshop introduced students to Midjourney, with surveys conducted both at this stage and during the final submission to capture changes in student perspectives. Using a two-phase case study involving a workshop, surveys, and interviews among senior undergraduate students in the bachelor's program of the Interior Architecture and Environmental Design Department, the study assesses the impact of AI prompts, from simple keywords to detailed narratives, on concept development and project outcomes. Findings indicate that AI broadens design possibilities, facilitates iterative ideation, and improves conceptual precision through high-fidelity visualizations. While students view AI as a valuable addition to their creative process, they also express concerns about ethics and the need to balance AI's benefits with preserving design authenticity. This research contributes to the broader discussion on AI's role in design, advocating for a balanced integration that respects both technological potential and human creativity.}
}
@article{LOPEZ2023104398,
title = {Facets of social problem-solving as moderators of the real-time relation between social rejection and negative affect in an at-risk sample},
journal = {Behaviour Research and Therapy},
volume = {169},
pages = {104398},
year = {2023},
issn = {0005-7967},
doi = {https://doi.org/10.1016/j.brat.2023.104398},
url = {https://www.sciencedirect.com/science/article/pii/S0005796723001468},
author = {Roberto López and Christianne Esposito-Smythers and Annamarie B. Defayette and Katherine M. Harris and Lauren F. Seibel and Emma D. Whitmyre},
keywords = {Social problem-solving, Social rejection, Negative affect},
abstract = {Social rejection predicts negative affect, and theoretical work suggests that problem-solving deficits strengthen this relation in real-time. Nevertheless, few studies have explicitly tested this relation, particularly in samples at risk for suicide. This may be particularly important as social rejection and negative affect are significant predictors of suicide. The aim of the current study was to examine whether cognitive (i.e., perceiving problems as threats) and behavioral (i.e., avoidance) facets of problem-solving deficits moderated the real-time relation between social rejection and negative affect. The sample consisted of 49 young adults with past-month suicidal ideation. Demographic information, social problem-solving deficits, as well as depressive/anxiety symptoms and stress levels were assessed at baseline. Social rejection and negative affect were assessed using ecological momentary assessment over the following 28 days. Dynamic structural equation modeling was used to assess relations among study variables. After accounting for depressive/anxiety symptoms, stress levels, sex, and age, only avoidance of problems bolstered the real-time positive relation between social rejection severity and negative affect (b = 0.04, 95% credibility interval [0.003, 0.072]). Individuals with suicidal ideation who possess an avoidant problem-solving style may be particularly likely to experience heightened negative affect following social rejection and may benefit from instruction in problem-solving skills.}
}
@article{DAEMS2019101110,
title = {Building communities. Presenting a model of community formation and organizational complexity in southwestern Anatolia},
journal = {Journal of Anthropological Archaeology},
volume = {56},
pages = {101110},
year = {2019},
issn = {0278-4165},
doi = {https://doi.org/10.1016/j.jaa.2019.101110},
url = {https://www.sciencedirect.com/science/article/pii/S027841651830237X},
author = {Dries Daems},
keywords = {Social complexity, Community formation, Sagalassos, Anatolian archaeology, Social interaction, Organizational complexity},
abstract = {In this paper, a model of community formation and organizational complexity is presented, focusing on the fundamental role of social interactions and information transmission for the development of complex social organisation. The model combines several approaches in complex systems thinking which has garnered increasing attention in archaeology. It is then outlined how this conceptual model can be applied in archaeology. In the absence of direct observations of constituent social interactions, archaeologists study the past through material remnants found in the archaeological record. People used their material surroundings to shape, structure and guide social interactions and practices in various ways. The presented framework shows how dynamics of social organisation and community formation can be inferred from these material remains. The model is applied on a case study of two communities, Sagalassos and Düzen Tepe, located in southwestern Anatolia during late Achaemenid to middle Hellenistic times (fifth to second centuries BCE). It is suggested that constituent interactions and practices can be linked to the markedly different forms of organizational structures and material surroundings attested in both communities. The case study illustrates how the presented model can help understand trajectories of socio-political structures and organizational complexity on a community level.}
}
@article{CUFFARO201235,
title = {Many worlds, the cluster-state quantum computer, and the problem of the preferred basis},
journal = {Studies in History and Philosophy of Science Part B: Studies in History and Philosophy of Modern Physics},
volume = {43},
number = {1},
pages = {35-42},
year = {2012},
issn = {1355-2198},
doi = {https://doi.org/10.1016/j.shpsb.2011.11.007},
url = {https://www.sciencedirect.com/science/article/pii/S1355219811000694},
author = {Michael E. Cuffaro},
keywords = {Quantum computation, Quantum mechanics, Many worlds, Everettian interpretation, Quantum parallelism, Quantum speed-up, Cluster state, Measurement-based, One-way, Preferred basis problem},
abstract = {I argue that the many worlds explanation of quantum computation is not licensed by, and in fact is conceptually inferior to, the many worlds interpretation of quantum mechanics from which it is derived. I argue that the many worlds explanation of quantum computation is incompatible with the recently developed cluster state model of quantum computation. Based on these considerations I conclude that we should reject the many worlds explanation of quantum computation.}
}
@article{BUI1997575,
title = {Computational modelling of thermophysical processes in the light metals industry},
journal = {Revue Générale de Thermique},
volume = {36},
number = {8},
pages = {575-591},
year = {1997},
issn = {0035-3159},
doi = {https://doi.org/10.1016/S0035-3159(97)89985-0},
url = {https://www.sciencedirect.com/science/article/pii/S0035315997899850},
author = {Rung T Bui},
keywords = {computer modelling, light metals, thermophysical processes, electrolysis, casting, furnaces, modèles numériques, métaux légers, procédés thermophysiques, électrolyse, coulée, fours},
abstract = {This survey focuses on the aluminium industry, mostly on process aspects as opposed to metallurgical aspects. It covers recent work on process models involving fluid flow and heat transfer, and extends to all important categories of processes encountered in the primary aluminium industry, from raw materials and reduction to cast shop and recycling. This includes a wide variety of processes from precipitators, calciners, rotary kilns, baking furnaces, reduction cells, casting and mixing furnaces to recycling furnaces and metal filtration. A review is carried out on the modelling work, the applications, and the needs expressed not only in analysis and design but also in process control, optimization and supervision, as well as operator training. A summary is given of the problems perceived, mainly in the field of model parameters and model validation. Indications on future trends are also given. Conclusions are drawn from the survey of this fast-expanding body of knowledge that suggests tough challenges as well as unprecedented opportunities. Suggestions are made as to how some of those challenges could be met.
Résumé
Cette synthèse concerne l'industrie de l'aluminium et s'intéresse surtout aux procédés de fabrication, par opposition aux aspects métallurgiques. Elle couvre les travaux récents sur les modèles de procédés impliquant les écoulements et le transfert de chaleur. Elle inclut toutes les catégories de procédés rencontrées dans l'industrie de l'aluminium de première fusion, allant des matières premières à la réduction, la coulée et le recyclage. On y retrouve les précipitateurs, les calcinateurs, les fours rotatifs, les fours de cuisson, les cuves d'électrolyse, les fours de coulée ou de mélange, les fours de recyclage, ainsi que la filtration du métal. Les travaux de modélisation et leurs applications sont brièvement énumérés, les besoins exprimés, concernant non seulement l'analyse et la conception des procédés, mais aussi le contrôle, sont examinés, ainsi que des aspects touchant à l'optimisation, à la supervision et à la formation du personnel. Un résumé est fait des problèmes, notamment en ce qui a trait à la détermination des paramètres de modélisation et à la validation des modèles. On tente de dégager les tendances d'avenir. Des conclusions sont tirées de cette étude, qui semble mettre en lumière des défis de taille aussi bien que des opportunités sans précédent ; quelques suggestions visant à relever certains de ces défis sont proposées.}
}
@article{CHAE2025105759,
title = {Enhancing nuclear power plant diagnostics: A comparative analysis of XAI-based feature selection methods for abnormal and emergency scenario detection},
journal = {Progress in Nuclear Energy},
volume = {185},
pages = {105759},
year = {2025},
issn = {0149-1970},
doi = {https://doi.org/10.1016/j.pnucene.2025.105759},
url = {https://www.sciencedirect.com/science/article/pii/S014919702500157X},
author = {Young Ho Chae and Seung Geun Kim and Jeonghun Choi and Seo Ryong Koo and Jonghyun Kim},
abstract = {This study introduces the application of explainable artificial intelligence (XAI) techniques to enhance nuclear power plant diagnostics through effective feature selection. We compared various XAI methods, including gradient-based techniques, layer-wise relevance propagation, DeepSHAP, integrated gradients, local interpretable model-agnostic explanation(LIME), and saliency maps, with traditional approaches such as principal component analysis (PCA). By applying these methods to data from an IAEA iPWR simulator, which includes 35 abnormal and emergency scenarios with 116 state variables, we demonstrated the superiority of XAI-based methods in selecting features that effectively distinguish between different plant conditions. Our approach successfully reduced the input dimensionality from 116 to 20 features while maintaining high diagnostic accuracy. XAI methods, particularly saliency map and DeepSHAP, outperformed traditional techniques by revealing distinct patterns for various abnormal situations. This reduction in dimensionality offers several benefits, including enhanced cybersecurity, improved human–machine interfaces, and increased computational efficiency. The findings have significant implications for developing more accurate, efficient, and interpretable diagnostic systems in nuclear power plants, potentially improving safety and operational effectiveness. Future work will focus on validating these methods across diverse plant designs and integrating this approach with advanced AI techniques for real-time adaptive diagnostics.}
}
@article{CUI2019305,
title = {Developing reflection analytics for health professions education: A multi-dimensional framework to align critical concepts with data features},
journal = {Computers in Human Behavior},
volume = {100},
pages = {305-324},
year = {2019},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2019.02.019},
url = {https://www.sciencedirect.com/science/article/pii/S0747563219300718},
author = {Yi Cui and Alyssa Friend Wise and Kenneth L. Allen},
keywords = {Reflection, Learning analytics, Natural language processing, Professional education, Dental education, Health professions education},
abstract = {Reflection is a key activity in self-regulated learning (SRL) and a critical part of health professions education that supports the development of effective lifelong-learning health professionals. Despite widespread use and plentiful theoretical models, empirical understanding of and support for reflection in health professions education remains limited due to simple manual assessment and rare feedback to students. Recent moves to digital reflection practices offer opportunities to computationally study and support reflection as a part of SRL. The critical task in such an endeavor, and the goal of this paper, is to align high-level reflection qualities that are valued conceptually with low-level features in the data that are possible to extract computationally. This paper approaches this goal by (a) developing a unified framework for conceptualizing reflection analytics in health professions education and (b) empirically examining potential data features through which these elements can be assessed. Synthesizing the prior literature yields a conceptual framework for health professions reflection comprised of six elements: Description, Analysis, Feelings, Perspective, Evaluation, and Outcome. These elements then serve as the conceptual grounding for the computational analysis in which 27 dental students’ reflections (in six reflective statement types) over the course of 4 years were examined using selected LIWC (Linguistic Inquiry and Word Count) indices. Variation in elements of reflection across students, years, and reflection-types supports use of the multi-dimensional analysis framework to (a) increase precision of research claims; (b) evaluate whether reflection activities are engaged in as intended; and (c) diagnose aspects of reflection in which specific students need support. Implications for the development of health professions reflection analytics that can contribute to SRL and promising areas for future research are discussed.}
}
@article{CHERNYSHOV20117408,
title = {System Identification Technique Application to Revealing Human-Operator Skills},
journal = {IFAC Proceedings Volumes},
volume = {44},
number = {1},
pages = {7408-7413},
year = {2011},
note = {18th IFAC World Congress},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20110828-6-IT-1002.00077},
url = {https://www.sciencedirect.com/science/article/pii/S1474667016447968},
author = {K.R. Chernyshov},
keywords = {Human factors and errors, Identification, Information correlation, Sampled data, Skills, Stochastic systems},
abstract = {Abstract
A new approach to abnormal situations with regard for the heuristic regularities of human-operator thinking process is proposed. The regularities are revealed on basis of recording the motions of the human-operator eyes over the information field of the control board and processing the experimental data obtained. For data processing, a probability theoretical approach is utilized. Such an approach is based on involving the notion of consistency of measures of dependence of random variables. Within the approach, a set of the so called information correlations has been proposed to serve as a quantitative performance index of human-operator skills.}
}
@article{NAARANOJA2015611,
title = {Multi-ontology Sense Making – Decision Making of Project Core Team},
journal = {Procedia Manufacturing},
volume = {3},
pages = {611-617},
year = {2015},
note = {6th International Conference on Applied Human Factors and Ergonomics (AHFE 2015) and the Affiliated Conferences, AHFE 2015},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2015.07.280},
url = {https://www.sciencedirect.com/science/article/pii/S2351978915002814},
author = {Marja Naaranoja},
keywords = {Sense making, Decision making, Ontology, Project, Core team, Construction industry},
abstract = {In order to understand core team's management task this paper studies the landscape of the decision making of the construction project core team. This paper uses multi-ontology sense making framework developed by Snowden. The four described situation illustrate the use of this framework. Firstly, a project core team create a project plan –timetable and cost estimate, that is supposed to be followed (rules and order) when making investment decision. Secondly, a project core team uses the plan but since the plan cannot be followed due to an unexpected situation the team changes the plan by calculating an optimal solution. In other words the team uses heuristic thinking when they change the rule (heuristics and order). Thirdly, the design group guides the design process by rules to get information for designing new facilities (rules and un-order). Fourthly, there are situations when the stakeholders have different kind of opinions in crisis and team cannot follow the preset orderly way of working (heuristics and un-order).}
}
@article{RICHEY2014857,
title = {A Complex Sociotechnical Systems Approach to Provisioning Educational Policies for Future Workforce},
journal = {Procedia Computer Science},
volume = {28},
pages = {857-864},
year = {2014},
note = {2014 Conference on Systems Engineering Research},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2014.03.102},
url = {https://www.sciencedirect.com/science/article/pii/S1877050914001653},
author = {Michael Richey and Marcus Nance and Leroy Hanneman and William Hubbard and Azad M. Madni and Marc Spraragen},
keywords = {Socio-technical Systems, Systems Engineering Education, Engineering, Worldbuilding and Workforce Development},
abstract = {Reforming the U.S. educational system and workforce is a national challenge. Both industry leaders 30 and academics 2,28 concur that improving the quality, quantity [and alignment] of U.S STEM graduates are national imperatives. Models of the U.S. educational system, using complex sociotechnical systems’ approaches and tools that instill systems thinking, offer a holistic perspective to the educational and workforce challenges we face as a nation and allow us to identify and understand challenges associated with workforce preparedness, and increasing the number and technical excellence of STEM graduates 9,13,29. These models represent a sociotechnical system of systems with various sub-systems, each one representing an inherently complex and interdisciplinary problem of maintaining bi-directional, non-linear feedback relationships between one another. Each system involves multiple disparate stakeholders that need to collaborate within a time-and resource-intensive process while embedded in a larger sociotechnical system, aligned with the people, ideas, and support required to support desired global outcomes, of the system of systems, society and industry in particular11.}
}
@article{OLTEANU20161,
title = {Opportunity to communicate: The coordination between focused and discerned aspects of the object of learning},
journal = {The Journal of Mathematical Behavior},
volume = {44},
pages = {1-12},
year = {2016},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2016.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S073231231630116X},
author = {Lucian Olteanu},
keywords = {Algebra, Communication, Experience, Critical aspects, Opportunity to communicate},
abstract = {There are extensive concerns pertaining to the idea that students do not develop sufficient communication abilities in algebra and in mathematics more generally. This problem is at least partially related to their algebraic thinking. Although teaching should give students the opportunity to develop their ability to communicate, there are limited research insights as to why some forms of communication work better than others, and how and why instruction influences such communication. Two case studies are reported on in this article. The analysis of the opportunity to communicate was grounded in variation theory. Differences between focused aspects and discerned aspects of the object of learning are described. The results show that the coordination between the aspects focused on by the teacher and discerned by the students provides students with the opportunity to successfully communicate the content in algebra. In addition, the structure of the lesson influences the opportunity to communicate aspects of the content.}
}
@article{BRODO202025,
title = {A Constraint-based Language for Multiparty Interactions},
journal = {Electronic Notes in Theoretical Computer Science},
volume = {351},
pages = {25-50},
year = {2020},
note = {Proceedings of LSFA 2020, the 15th International Workshop on Logical and Semantic Frameworks, with Applications (LSFA 2020)},
issn = {1571-0661},
doi = {https://doi.org/10.1016/j.entcs.2020.08.003},
url = {https://www.sciencedirect.com/science/article/pii/S1571066120300396},
author = {Linda Brodo and Carlos Olarte},
keywords = {Concurrency theory, constraints, multiparty interactions},
abstract = {Multiparty interactions are common place in today's distributed systems. An agent usually communicates, in a single session, with other agents to accomplish a given task. Take for instance an online transaction including the vendor, the client, the credit card system and the bank. When specifying this kind of system, we probably observe a single transaction including several (binary) communications leading to changes in the state of all the involved agents. Multiway synchronization process calculi, that move from a binary to a multiparty synchronization discipline, have been proposed to formally study the behavior of those systems. However, adopting models such as Bodei, Brodo, and Bruni's Core Network Algebra (CNA), where the number of participants in an interaction is not fixed a priori, leads to an exponential blow-up in the number of states/behaviors that can be observed from the system. In this paper we explore mechanisms to tackle this problem. We extend CNA with constraints that declaratively allow the modeler to restrict the interaction that should actually happen. Our extended process algebra, called CCNA, finds application in balancing the interactions in a concurrent system, leading to a simple, deadlock-free and fair solution for the Dinning Philosopher problem. Our definition of constraints is general enough and it offers the possibility of accumulating costs in a multiparty negotiation. Hence, only computations respecting the thresholds imposed by the modeler are observed. We use this machinery to neatly model a Service Level Agreement protocol. We develop the theory of CCNA including its operational semantics and a behavioral equivalence that we prove to be a congruence. We also propose a prototypical implementation that allows us to verify, automatically, some of the systems explored in the paper.}
}
@article{XU2025112998,
title = {Discrete Differentiated Creative Search for traveling salesman problem},
journal = {Applied Soft Computing},
volume = {174},
pages = {112998},
year = {2025},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2025.112998},
url = {https://www.sciencedirect.com/science/article/pii/S1568494625003096},
author = {Qi Xu and Kewen Xia and Xiaoyu Chu},
keywords = {Population-based algorithm, Traveling salesman problem, Edge-based operations, Random nearest neighbor replacement, Greedy beam search},
abstract = {A novel population-based Discrete Differentiated Creative Search (DDCS) is proposed in this paper for solving the traveling salesman problem (TSP). DDCS introduces greedy beam search to adaptively initialize the population and improve the quality of the initial solutions. Second, a multi-edge construction operator, edge-based mathematical operations and a similarity attraction operator are used to guide individuals from different population categories towards higher-quality solutions based on the current solutions. Finally, a random nearest neighbor replacement strategy is used to replace individuals with the same distance heuristically, reducing the assimilation rate of the population. DDCS is tested with 50 instances from TSPLIB and compared with a variety of state-of-the-art and variants of classical algorithms. The results demonstrate that DDCS exhibits superior optimization capability and higher stability.}
}
@article{FERRES2025,
title = {AI in the Era of GPT: Transforming the Future of Work and Discovery},
journal = {Journal of the American College of Radiology},
year = {2025},
issn = {1546-1440},
doi = {https://doi.org/10.1016/j.jacr.2025.02.007},
url = {https://www.sciencedirect.com/science/article/pii/S1546144025001097},
author = {Juan M.Lavista Ferres and Elliot K. Fishman and Linda C. Chu and Felipe Lopez-Ramirez and Charles K. Crawford and Steven P. Rowe}
}
@article{RASS202385,
title = {Adaptive dynamical systems modelling of transformational organizational change with focus on organizational culture and organizational learning},
journal = {Cognitive Systems Research},
volume = {79},
pages = {85-108},
year = {2023},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2023.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S1389041723000049},
author = {Lars Rass and Jan Treur and Wioleta Kucharska and Anna Wiewiora},
keywords = {Transformational Change, Organizational Culture, Organizational Learning, Safety Culture},
abstract = {Transformative Organizational Change becomes more and more significant both practically and academically, especially in the context of organizational culture and learning. However computational modeling and a formalization of organizational change and learning processes are still largely unexplored. This paper aims to provide an adaptive network model of transformative organizational change and translate a selection of organizational learning and change processes into computationally modelled processes. Additionally, it sets out to connect the dynamic systems view of organizations to self-modelling network models. The creation of the model and the implemented mechanisms of organizational processes are based on extrapolations of an extensive literature study and grounded in related work in this field, and then applied to a specified hospital-related case scenario in the context of safety culture. The model was evaluated by running several simulations and variations thereof. The results of these were investigated by qualitative analysis and comparison to expected emergent behaviour based on related available academic literature. The simulations performed confirmed the occurrence of an organizational transformational change towards a constant learning culture by offering repeated and effective learning and changes to organizational processes. Observations about various interplays and effects of the mechanism have been made, and they exposed that acceptance of mistakes as a part of learning culture facilitates transformational change and may foster sustainable change in the long run. Further, the model confirmed that the self-modelling network model approach applies to a dynamic systems view of organizations and a systems perspective of organizational change. The created model offers the basis for the further creation of self-modelling network models within the field of transformative organizational change and the translated mechanisms of this model can further be extracted and reused in a forthcoming academic exploration of this field.}
}
@article{GHABOUSSI201275,
title = {Unifying Principles for Sudden Transitions in All Systems},
journal = {Procedia Computer Science},
volume = {8},
pages = {75-80},
year = {2012},
note = {Conference on Systems Engineering Research},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2012.01.016},
url = {https://www.sciencedirect.com/science/article/pii/S1877050912000178},
author = {Jamshid Ghaboussi},
keywords = {Complex systems, Sudden transitions, System properties, Tipping points},
abstract = {All physical, natural, biological and socio-economic systems – also referred to as complex systems - have system-level properties that result from the interactions between their components. In most cases it is not possible to determine the complete system-level properties with the current state of our knowledge. Where there are system-level properties, the uncoupled form of those properties is the eigen-system consisting of system eigenvalues and eigenfunctions, even though at the present we are not able to determine them through modelling or observation. All systems operate in equilibrium states; small perturbations cause small changes. While these systems normally undergo gradual changes in their system-level properties, they can also undergo sudden transitions to new equilibrium states. New insights into these important transitions are proposed in this paper. In some mechanical systems transition occur when the smallest system eigenvalue goes to zero. It is proposed that the same principles apply to all systems. Transitions in all systems occur when at least one system eigenvalue goes to zero. Generalization of these principles to all systems will encourage new ways of thinking about systems and will suggest new research directions in studying these important major transitions, potentially leading to reliable methods for predicting their onset.}
}
@article{BAUER2024100002,
title = {What if? Numerical weather prediction at the crossroads},
journal = {Journal of the European Meteorological Society},
volume = {1},
pages = {100002},
year = {2024},
issn = {2950-6301},
doi = {https://doi.org/10.1016/j.jemets.2024.100002},
url = {https://www.sciencedirect.com/science/article/pii/S2950630124000024},
author = {Peter Bauer},
keywords = {Numerical weather prediction, Machine learning, High-performance computing},
abstract = {This paper provides an outlook on the future of operational weather prediction given the recent evolution in science, computing and machine learning. In many parts, this evolution strongly deviates from the strategy operational centres have formulated only several years ago. New opportunities in digital technology have greatly accelerated progress, and the full integration of computational science in numerical weather prediction centres is common knowledge now. Within the last few years, a vast machine learning research community has emerged for creating new and tailor-made products, accelerating processing and – most of all – creating emulators for the entire production of global forecasts that outperform traditional systems at the spatial resolution of the training data. In this context, the role of both numerical models and observations is changing from being equation to data driven. Model simulations and reanalyses are becoming the new currency for training machine learning, and operational centres are in a powerful position as they generate these datasets based on decades worth of experience. This environment creates incredible opportunities to progress much faster than in the past but also uncertainties about what the strategic implications on defining cost-effective and sustainable research and operations are, and how to achieve sufficient high-performance computing and data handling capacities. It will take individual national public services a while to understand what to focus on and how to coordinate their substantial investments in staff and infrastructure at institutional, national and international level. This paper addresses this new situation operational weather prediction finds itself in through formulating the most likely “what if?” scenarios for the near future. It also provides an outline for how weather centres could adapt.}
}
@article{SALEM2025141924,
title = {Novel eco-friendly nicotinonitrile derivative as a corrosion inhibitor for carbon steel: Synthesis, inhibitive efficiency, and DFT analysis},
journal = {Journal of Molecular Structure},
volume = {1335},
pages = {141924},
year = {2025},
issn = {0022-2860},
doi = {https://doi.org/10.1016/j.molstruc.2025.141924},
url = {https://www.sciencedirect.com/science/article/pii/S0022286025006106},
author = {Aya M. Salem and Ahmed Nassef and Ahmed M. Wahba and Samar M. Mohammed},
keywords = {Corrosion, Inhibition, C-steel, HCl, Nicotinonitrile derivatives, Langmuir isotherm},
abstract = {Two new variations of Nicotinonitrile were synthesized, namely: "4-(4-Chlorophenyl)-3-cyano-6-(thien-2-yl)-1H-pyridin-2-one (3A) and 4-(4-Chlorophenyl)-2-oxo-1-(prop‑2-yn-1-yl)-6-(thien-2-yl)-1,2-dihydropyridine-3-carbonitrile (4A). The chemical structures were examined and confirmed using IR and 1H NMR. This method's notable features include being solvent-free, catalyst-free, economical, and having great yields without the need for a catalyst. These compounds were then evaluated as corrosion inhibitors for carbon steel (CS) in 1 M HCl media. Both weight loss (WL) and electrochemical methods such as potentiodynamic polarization (PDP) and electrochemical impedance spectroscopy (EIS) were employed for the investigation. The synthesis and assessment of a novel series of organic compounds with related chemical structures as hydrochloric acid corrosion inhibitors of C-steel is a novel aspect of this study. The results showed that the Nicotinonitrile derivatives were effective corrosion inhibitors, with inhibition efficiencies ( %η) of 86.4 % and 90.7 % for 3A and 4A, respectively, at a concentration of 15×10−5 M. Theoretical computations are applied using the density functional theory. The experiments' findings show that these compounds are effective corrosion inhibitors, and that the concentration of the substances increases the inhibition efficiency. When compared to 3A, the 4A molecule has the maximum efficiency. Monte Carlo simulations and quantum chemical calculations were also performed to analyse and discuss the behaviour of these derivatives. Surface analysis using Scanning Electron Microscopy (SEM) and Energy Dispersive X-ray (EDX) was conducted to verify the results obtained from atomic force microscope measurements. Excellent agreement is found between the outcomes of theoretical computations and experimental measurements.}
}
@article{SHER1992505,
title = {A computational normative theory of scientific evidence},
journal = {International Journal of Approximate Reasoning},
volume = {6},
number = {4},
pages = {505-524},
year = {1992},
issn = {0888-613X},
doi = {https://doi.org/10.1016/0888-613X(92)90002-H},
url = {https://www.sciencedirect.com/science/article/pii/0888613X9290002H},
author = {David B. Sher},
keywords = {interval probability, evidence combination, experimental evidence, probabilistic logic, statistical inference},
abstract = {A scientific reasoning system makes decisions using objective evidence in the form of independent experimental trials, propositional axioms, and constraints on the probabilities of events. I propose a collection of algorithms that derive probability intervals and estimate conditional probabilities from objective evidence in those forms. This reasoning system can manage uncertainty about data and rules in a rule-based expert system. I expect that the system will be particularly applicable to diagnosis and analysis in domains with a wealth of experimental evidence such as medicine. The algorithms currently apply to systems with arbitrary amounts of experimental evidence but with less than 20 variables. I discuss limitations of this solution and propose future directions for this research. This work can be considered a generalization of Nilsson's “probabilistic logic” to intervals and experimental observations.}
}
@article{HAN2025104938,
title = {Synergizing artificial intelligence and probiotics: A comprehensive review of emerging applications in health promotion and industrial innovation},
journal = {Trends in Food Science & Technology},
volume = {159},
pages = {104938},
year = {2025},
issn = {0924-2244},
doi = {https://doi.org/10.1016/j.tifs.2025.104938},
url = {https://www.sciencedirect.com/science/article/pii/S0924224425000743},
author = {Xin Han and Qingqiu Liu and Yun Li and Meng Zhang and Kaiyang Liu and Lai-Yu Kwok and Heping Zhang and Wenyi Zhang},
keywords = {Artificial intelligence, Gastrointestinal health, Personalized medicine, Probiotic metabolite},
abstract = {Background
Probiotics play a vital role in human health, garnering significant scientific and public interest. The integration of artificial intelligence (AI) into probiotic research and applications promises to revolutionize strain discovery, health outcomes, and food industry innovations.
Scope and approach
This review explores the intersection of AI and probiotics, focusing on AI-powered machine learning models that revolutionize strain screening, biomarker prediction, and metabolite analysis. Artificial intelligence enables early diagnosis and personalized nutrition by predicting biomarkers for conditions like inflammatory bowel disease and irritable bowel syndrome. It also identifies key probiotic metabolites, such as antimicrobial peptides, exopolysaccharides, and phenolic compounds, advancing fermentation technology and probiotic efficacy. Challenges, including data quality, computational demands, and experimental validation, are also discussed.
Key findings and conclusions
Artificial intelligence outperforms conventional methods, offering rapid, high-precision screening, scalable data analysis, and automated strain optimization. Case studies demonstrate AI models achieving over 97% accuracy in bacterial identification and accelerated metabolite discovery. However, challenges like data quality, computational costs, and model interpretability remain. Overcoming these will strengthen the role of AI in precision nutrition, functional food development, and personalized medicine. This review concludes with future perspectives, emphasizing the potential of AI to revolutionize gut microbiome research and probiotic-based therapeutics.}
}
@article{GILL2019556,
title = {Holons on the Horizon: Re-Understanding Automation and Control},
journal = {IFAC-PapersOnLine},
volume = {52},
number = {25},
pages = {556-561},
year = {2019},
note = {19th IFAC Conference on Technology, Culture and International Stability TECIS 2019},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2019.12.605},
url = {https://www.sciencedirect.com/science/article/pii/S2405896319325261},
author = {Karamjit S Gill},
keywords = {data science, human-machine interaction, cybernetics, systems architecture, holon, symbiosis, valorisation, cultural architectures},
abstract = {In Re-Understanding Automation and Control in the era of digital automation of societal systems, we need to understand the inter-connected relations between knowledge, culture, technology and society. This in turn demands the exploration of social and cultural architectures, which facilitate them. Whilst computational model of data systems is built upon the bottom-up architecture, it is the top-down architecture of social and cultural contexts that synchronises the processing and outcomes of data systems, may they relate to organisational systems, heath and welfare systems, or institutional systems. It is this notion of the inter-dependence of the bottom-up and top-down architectures that makes us act beyond the linear gaze worldview of automation and control of production systems, and explore the multiplicity of interconnections between and across societal systems. In these horizons, we see the inter-connectedness between the unit and the whole, between the horizontal and vertical, and a symbiosis of hand and brain- an augmentation of the human and the machine. The ideas of inter-connectedness, augmentation and symbiosis lie at the core of holonic horizons. These horizons allow us to transcend the limit of the calculation and control model of automation, and enable the design of human-centred systems that valorise differences whilst utilising the richness and diversity of human-machine collaborations. When we envision these interactions and collaborations as a systems developmental process, we begin to visualise systems design from an interdependent perspective, which goes beyond the linear gaze of “utility”. The paper explores the ways holonic architectures engage us in the design process.}
}
@article{ROSSITER20237555,
title = {A suite of MATLAB livescript files to support learning of elementary control and feedback concepts},
journal = {IFAC-PapersOnLine},
volume = {56},
number = {2},
pages = {7555-7560},
year = {2023},
note = {22nd IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2023.10.657},
url = {https://www.sciencedirect.com/science/article/pii/S2405896323010315},
author = {J.A. Rossiter},
keywords = {Virtual laboratories, independent learning, visualisation, livescripts},
abstract = {This paper builds on a body of work in the community which is focussed on sharing learning and teaching resources, especially those which might support a first course in control. Here attention is given to some of the mathematical, analytical and numerical computations which are required to support simple system and feedback analysis and design. The aim is to provide resources which allow students to focus on core concepts and understanding so that the numerical computations are not an obstacle to their investigations. More specifically, this paper focuses on a number of MATLAB livescript files which have been produced to help students visualise the impact of parameter and design choices on system behaviour, while simultaneously empowering them to understand the source code and thus upskill them for the future. The paper gives an overview of the livescripts available so users can decide whether these could be useful in their own context; all are freely available on the author's website (Rossiter, 2021).}
}
@article{GAO2025111002,
title = {Learning and knowledge-guided evolutionary algorithm for the large-scale buffer allocation problem in production lines},
journal = {Computers & Industrial Engineering},
volume = {203},
pages = {111002},
year = {2025},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2025.111002},
url = {https://www.sciencedirect.com/science/article/pii/S0360835225001482},
author = {Sixiao Gao and Fan Zhang and Shuo Shi},
keywords = {Buffer allocation, Large-scale, Evolutionary algorithm, Learning-guided, Knowledge-guided},
abstract = {The large-scale buffer allocation problem (LBAP) in production lines represents a significant optimization challenge, centered on the efficient allocation of limited temporary storage areas. Prior research has predominantly addressed the LBAP through dynamic programming, search algorithms, and metaheuristics. However, these methodologies are often problem-specific and inefficient when applied to large-scale scenarios. Consequently, there is a pressing need to investigate innovative algorithms beyond existing approaches. This paper presents a novel learning and knowledge-guided evolutionary algorithm designed for the LBAP in production lines. The proposed algorithm develops an adaptive genetic algorithm and a variable neighborhood search algorithm, incorporating a simulated annealing-based strategy. An online Q-learning algorithm is employed to dynamically select the more effective of the two preceding algorithms for solution updates, while the simulated annealing-based strategy regulates the acceptance of these updated solutions. Furthermore, The proposed algorithm dynamically adjusts crossover, mutation, and shaking rates to adapt to the neighborhood structure. It also leverages conflict knowledge obtained from prior update experiences to inform the search process, thereby enhancing solution quality and computational efficiency. Numerical results indicate that the proposed algorithm surpasses state-of-the-art methods in addressing the LBAP. Additionally, empirical ablation studies demonstrate that the knowledge-guided approach efficiently explores promising solution regions by eliminating low-value solutions, while the learning-guided approach effectively generates improved solutions by selecting optimal strategies. This proposed algorithm significantly advances dynamic production resource allocation in large-scale systems.}
}
@article{DOSHI2020103202,
title = {Recursively modeling other agents for decision making: A research perspective},
journal = {Artificial Intelligence},
volume = {279},
pages = {103202},
year = {2020},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2019.103202},
url = {https://www.sciencedirect.com/science/article/pii/S000437021930027X},
author = {Prashant Doshi and Piotr Gmytrasiewicz and Edmund Durfee},
keywords = {Decision theory, Game theory, Hierarchical beliefs, Multiagent systems, Recursive modeling, Theory of mind},
abstract = {Individuals exhibit theory of mind, attributing beliefs, intent, and mental states to others as explanations of observed actions. Dennett's intentional stance offers an analogous abstraction for computational agents seeking to understand, explain, or predict others' behaviors. These recognized theories provide a formal basis to ongoing investigations of recursive modeling. We review and situate various frameworks for recursive modeling that have been studied in game- and decision- theories, and have yielded methods useful to AI researchers. Sustained attention given to these frameworks has produced new analyses and methods with an aim toward making recursive modeling practicable. Indeed, we also review some emerging uses and the insights these yielded, which are indicative of pragmatic progress in this area. The significance of these frameworks is that higher-order reasoning is critical to correctly recognizing others' intent or outthinking opponents. Such reasoning has been utilized in academic, business, military, security, and other contexts both to train and inform decision-making agents in organizational and strategic contexts, and also to more realistically predict and best respond to other agents' intent.}
}
@article{MILANEZ200793,
title = {A new method for real time computation of power quality indices based on instantaneous space phasors},
journal = {Electric Power Systems Research},
volume = {77},
number = {1},
pages = {93-98},
year = {2007},
issn = {0378-7796},
doi = {https://doi.org/10.1016/j.epsr.2006.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S0378779606000265},
author = {Dalgerti L. Milanez and Rade M. Ciric},
keywords = {Instantaneous space phasors, Instantaneous complex power, Buchholz–Goodhue effective apparent power, Power definitions, Unbalance, Dispersed generators},
abstract = {One of the important issues about using renewable energy is the integration of dispersed generation in the distribution networks. Previous experience has shown that the integration of dispersed generation can improve voltage profile in the network, decrease loss, etc. but can create safety and technical problems as well. This work report the application of the instantaneous space phasors and the instantaneous complex power in observing performances of the distribution networks with dispersed generators in steady state. New IEEE apparent power definition, the so-called Buchholz–Goodhue effective apparent power, as well as new proposed power quality (oscillation) index in the three-phase distribution systems with unbalanced loads and dispersed generators, are applied. Results obtained from several case studies using IEEE 34 nodes test network are presented and discussed.}
}
@article{HU2024112598,
title = {Unraveling the dynamics of stacking fault nucleation in ceramics: A case study of aluminum nitride},
journal = {Computational Materials Science},
volume = {231},
pages = {112598},
year = {2024},
issn = {0927-0256},
doi = {https://doi.org/10.1016/j.commatsci.2023.112598},
url = {https://www.sciencedirect.com/science/article/pii/S092702562300592X},
author = {Yixuan Hu and Yumeng Zhang and Simanta Lahkar and Xiaodong Wang and Qi An and Kolan {Madhav Reddy}},
keywords = {Ceramics, Aluminum nitride, Deformation, Stacking faults, Generalized stacking fault energy},
abstract = {Stacking fault (SF), originating from the emission of partial dislocations, wields significant influence over the structural and physicochemical traits of ceramic materials. Yet, the intricate atomic dynamics driving SF nucleation remain obscured. Here, we introduce an improved methodology for computing the generalized stacking fault energy (GSFE) in ceramics, integrating uneven Degrees of Freedom (DOFs) for distinct lattice sites. This refinement has yielded substantial energy advantages over the traditional rigid shift method inherited from metallic systems. Our findings underscore that the relaxation of nonmetallic N atoms within the SF region is pivotal for achieving a more realistic SF simulation. This, in turn, unveils the involvement of N atom migration within the SF region between different aluminum tetrahedral sites during SF nucleation. By alleviating the energy barrier, this relaxation contrasts with previous simulations where nonmetallic elements remained more rigid. This work demonstrates the atomic dynamics of SF nucleation in ceramics and breaks the conventional wisdom of uniformly applying constraints for GSFE computations.}
}
@article{SMIRNOV20142507,
title = {Domain Ontologies Integration for Virtual Modelling and Simulation Environments},
journal = {Procedia Computer Science},
volume = {29},
pages = {2507-2514},
year = {2014},
note = {2014 International Conference on Computational Science},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2014.05.234},
url = {https://www.sciencedirect.com/science/article/pii/S1877050914004116},
author = {Pavel A. Smirnov and Sergey V. Kovalchuk and Alexey V. Dukhanov},
keywords = {Virtual simulation objects, semantic technologies, computational experiment, knowledge base},
abstract = {This paper presents a model of semantic ontologies integration into workflow co mposition design process via Virtual Simu lation Objects (VSO) concept and technology. Doma in knowledge distributed over open linked data sources may be usefully applied for new VSO-images design and used for organization co mputational-intensive simulation e xpe riments. In this paper we describe the VSO- architecture e xtended with novel functionality regarding integration with lin ked open data sources. We also provide a computational-scientific e xa mp le of do ma in-specific use-case offering a solution for some public-transportation domain problem.}
}
@article{TSUTSUI201856,
title = {A Bayesian network model for supporting the formation of PSS design knowledge},
journal = {Procedia CIRP},
volume = {73},
pages = {56-60},
year = {2018},
note = {10th CIRP Conference on Industrial Product-Service Systems, IPS2 2018, 29-31 May 2018, Linköping, Sweden},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2018.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S2212827118305171},
author = {Yusuke Tsutsui and Yosuke Kubota and Yoshiki Shimomura},
keywords = {Product-service systems, Design knowledge, Bayesian network},
abstract = {Recently, product-service systems (PSS) have drawn the interest of the manufacturing industry. Designing PSS to enhance the value of their core products, manufacturers should assume that their products are their strength or constraint and also derive the service solution logically. However, PSS design knowledge to determine the services suitable for manufacturers’ core products is unclear. As a result, determining a service solution that is compatible with their core products is difficult. This difficulty consequently prevents the manufacturing industry from realising high-quality PSS. To form PSS design knowledge efficiently, this study aims to support the analysis of the complicated and diverse relationships between product characteristics and service contents. Specifically, a Bayesian network model that represents the logical structure between the product characteristics and service contents common among existing PSS cases is constructed through computational learning based on statistical data on PSS cases.}
}
@article{CARLSON201888,
title = {Ghosts in machine learning for cognitive neuroscience: Moving from data to theory},
journal = {NeuroImage},
volume = {180},
pages = {88-100},
year = {2018},
note = {New advances in encoding and decoding of brain signals},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2017.08.019},
url = {https://www.sciencedirect.com/science/article/pii/S1053811917306663},
author = {Thomas Carlson and Erin Goddard and David M. Kaplan and Colin Klein and J. Brendan Ritchie},
keywords = {Multivariate pattern analysis, Brain decoding, Exploratory methods, fMRI, Magnetoencephalography},
abstract = {The application of machine learning methods to neuroimaging data has fundamentally altered the field of cognitive neuroscience. Future progress in understanding brain function using these methods will require addressing a number of key methodological and interpretive challenges. Because these challenges often remain unseen and metaphorically “haunt” our efforts to use these methods to understand the brain, we refer to them as “ghosts”. In this paper, we describe three such ghosts, situate them within a more general framework from philosophy of science, and then describe steps to address them. The first ghost arises from difficulties in determining what information machine learning classifiers use for decoding. The second ghost arises from the interplay of experimental design and the structure of information in the brain – that is, our methods embody implicit assumptions about information processing in the brain, and it is often difficult to determine if those assumptions are satisfied. The third ghost emerges from our limited ability to distinguish information that is merely decodable from the brain from information that is represented and used by the brain. Each of the three ghosts place limits on the interpretability of decoding research in cognitive neuroscience. There are no easy solutions, but facing these issues squarely will provide a clearer path to understanding the nature of representation and computation in the human brain.}
}
@article{MCLEAN2020,
title = {Simulation Modeling as a Novel and Promising Strategy for Improving Success Rates With Research Funding Applications: A Constructive Thought Experiment},
journal = {JMIR Nursing},
volume = {3},
number = {1},
year = {2020},
issn = {2562-7600},
doi = {https://doi.org/10.2196/18983},
url = {https://www.sciencedirect.com/science/article/pii/S2562760020000125},
author = {Allen McLean and Wade McDonald and Donna Goodridge},
keywords = {simulation modeling, computational science, funding application, grant funding, grant writing, nursing, research, thought experiment, persuasive technology, peripheral vascular disease},
abstract = {Writing a successful grant or other funding applications is a requirement for continued employment, promotion, and tenure among nursing faculty and researchers. Writing successful applications is a challenging task, with often uncertain results. The inability to secure funding not only threatens the ability of nurse researchers to conduct relevant health care research but may also negatively impact their career trajectories. Many individuals and organizations have offered advice for improving success with funding applications. While helpful, those recommendations are common knowledge and simply form the basis of any well-considered, well-formulated, and well-written application. For nurse researchers interested in taking advantage of innovative computational methods and leading-edge analytical techniques, we propose adding the results from computer-based simulation modeling experiments to funding applications. By first conducting a research study in a virtual space, nurse researchers can refine their study design, test various assumptions, conduct experiments, and better determine which elements, variables, and parameters are necessary to answer their research question. In short, simulation modeling is a learning tool, and the modeling process helps nurse researchers gain additional insights that can be applied in their real-world research and used to strengthen funding applications. Simulation modeling is well-suited for answering quantitative research questions. Still, the design of these models can benefit significantly from the addition of qualitative data and can be helpful when simulating the results of mixed methods studies. We believe this is a promising strategy for improving success rates with funding applications, especially among nurse researchers interested in contributing new knowledge supporting the paradigm shift in nursing resulting from advances in computational science and information technology.}
}
@article{HONG2015671,
title = {Free will: A case study in reconciling phenomenological philosophy with reductionist sciences},
journal = {Progress in Biophysics and Molecular Biology},
volume = {119},
number = {3},
pages = {671-727},
year = {2015},
note = {Integral Biomathics: Life Sciences, Mathematics, and Phenomenological Philosophy},
issn = {0079-6107},
doi = {https://doi.org/10.1016/j.pbiomolbio.2015.08.008},
url = {https://www.sciencedirect.com/science/article/pii/S0079610715001212},
author = {Felix T. Hong},
keywords = {Free will, Determinism, Quantum indeterminacy, Downward causation, Naturalizing phenomenology, Visual thinking},
abstract = {Phenomenology aspires to philosophical analysis of humans' subjective experience while it strives to avoid pitfalls of subjectivity. The first step towards naturalizing phenomenology — making phenomenology scientific — is to reconcile phenomenology with modern physics, on the one hand, and with modern cellular and molecular neuroscience, on the other hand. In this paper, free will is chosen for a case study to demonstrate the feasibility. Special attention is paid to maintain analysis with mathematical precision, if possible, and to evade the inherent deceptive power of natural language. Laplace's determinism is re-evaluated along with the concept of microscopic reversibility. A simple and transparent version of proof demonstrates that microscopic reversibility is irreconcilably incompatible with macroscopic irreversibility, contrary to Boltzmann's claim. But the verdict also exalts Boltzmann's statistical mechanics to the new height of a genuine paradigm shift, thus cutting the umbilical cord linking it to Newtonian mechanics. Laplace's absolute determinism must then be replaced with a weaker form of causality called quasi-determinism. Biological indeterminism is also affirmed with numerous lines of evidence. The strongest evidence is furnished by ion channel fluctuations, which obey an indeterministic stochastic phenomenological law. Furthermore, quantum indeterminacy is shown to be relevant in biology, contrary to the opinion of Erwin Schrödinger. In reconciling phenomenology of free will with modern sciences, three issues — alternativism, intelligibility and origination — of free will must be accounted for. Alternativism and intelligibility can readily be accounted for by quasi-determinism. In order to account for origination of free will, the concept of downward causation must be invoked. However, unlike what is commonly believed, there is no evidence that downward causation can influence, shield off, or overpower low-level physical forces already known to physicists. Quasi-determinism offers an escape route: The possibility that downward causation arising from hierarchical organization of biological structures can modify dispersions of physical laws remains open. Empirical evidence in support of downward causation is scanty but nevertheless exists. Still, origination of free will must be considered an unsolved problem at present. It is demonstrated that objectivity does not guarantee scientific rigor in the study of complex phenomena, such as human creativity. In its replacement, universality and overall consistency between a theory and empirical evidence must be maintained. Visual thinking is proposed as a reasoning tool to ensure universality and overall consistency through inference to the best explanation.}
}
@article{GERSHMAN2023104825,
title = {The molecular memory code and synaptic plasticity: A synthesis},
journal = {Biosystems},
volume = {224},
pages = {104825},
year = {2023},
issn = {0303-2647},
doi = {https://doi.org/10.1016/j.biosystems.2022.104825},
url = {https://www.sciencedirect.com/science/article/pii/S0303264722002064},
author = {Samuel J. Gershman},
keywords = {Memory, Free energy, Synaptic plasticity, Learning, Inference},
abstract = {The most widely accepted view of memory in the brain holds that synapses are the storage sites of memory, and that memories are formed through associative modification of synapses. This view has been challenged on conceptual and empirical grounds. As an alternative, it has been proposed that molecules within the cell body are the storage sites of memory, and that memories are formed through biochemical operations on these molecules. This paper proposes a synthesis of these two views, grounded in a computational model of memory. Synapses are conceived as storage sites for the parameters of an approximate posterior probability distribution over latent causes. Intracellular molecules are conceived as storage sites for the parameters of a generative model. The model stipulates how these two components work together as part of an integrated algorithm for learning and inference.}
}
@article{KELLYPITOU201723,
title = {Microgrids and resilience: Using a systems approach to achieve climate adaptation and mitigation goals},
journal = {The Electricity Journal},
volume = {30},
number = {10},
pages = {23-31},
year = {2017},
issn = {1040-6190},
doi = {https://doi.org/10.1016/j.tej.2017.11.008},
url = {https://www.sciencedirect.com/science/article/pii/S1040619017303007},
author = {Katrina M. Kelly-Pitou and Anais Ostroski and Brandon Contino and Brandon Grainger and Alexis Kwasinski and Gregory Reed},
keywords = {Sustainable energy, Microgrid, Sustainable development, Energy policy, Climate change policy, Climate change adaptation, Adaptive capacity, Ecological modernization},
abstract = {Although energy resource sustainability has been researched extensively, the understanding of how we use and interact with electricity sustainably is less understood. New electrical designs, like microgrids, provide opportunities to better address the immediate needs of electrical sustainability and urban development. This paper analyzes the role of microgrids in urban development and examines how greater systemic thinking between infrastructure planning and energy policymaking can increase a city’s resilience.}
}
@article{ZHOU2024124298,
title = {Hyperspectral imaging combined with blood oxygen saturation for in vivo analysis of small intestinal necrosis tissue},
journal = {Spectrochimica Acta Part A: Molecular and Biomolecular Spectroscopy},
volume = {315},
pages = {124298},
year = {2024},
issn = {1386-1425},
doi = {https://doi.org/10.1016/j.saa.2024.124298},
url = {https://www.sciencedirect.com/science/article/pii/S1386142524004645},
author = {Yao Zhou and LeChao Zhang and DanFei Huang and Yong Zhang and LiBin Zhu and Xiaoqing Chen and Guihua Cui and Qifan Chen and XiaoJing Chen and Shujat Ali},
keywords = {Hyperspectral imaging, Tissue oxygenation, Small intestine tissue, Isosbestic points},
abstract = {Acute mesenteric ischemia (AMI) is a clinically significant vascular and gastrointestinal condition, which is closely related to the blood supply of the small intestine. Unfortunately, it is still challenging to properly discriminate small intestinal tissues with different degrees of ischemia. In this study, hyperspectral imaging (HSI) was used to construct pseudo-color images of oxygen saturation about small intestinal tissues and to discriminate different degrees of ischemia. First, several small intestine tissue models of New Zealand white rabbits were prepared and collected their hyperspectral data. Then, a set of isosbestic points were used to linearly transform the measurement data twice to match the reference spectra of oxyhemoglobin and deoxyhemoglobin, respectively. The oxygen saturation was measured at the characteristic peak band of oxyhemoglobin (560 nm). Ultimately, using the oxygenated hemoglobin reflectance spectrum as the benchmark, we obtained the relative amount of median oxygen saturation in normal tissues was 70.0 %, the IQR was 10.1 %, the relative amount of median oxygen saturation in ischemic tissues was 49.6 %, and the IQR was 14.6 %. The results demonstrate that HSI combined with the oxygen saturation computation method can efficiently differentiate between normal and ischemic regions of the small intestinal tissues. This technique provides a powerful support for internist to discriminate small bowel tissues with different degrees of ischemia, and also provides a new way of thinking for the diagnosis of AMI.}
}
@article{CARUSO1996135,
title = {Reported earliest memory age: Relationships with personality and coping variables},
journal = {Personality and Individual Differences},
volume = {21},
number = {1},
pages = {135-142},
year = {1996},
issn = {0191-8869},
doi = {https://doi.org/10.1016/0191-8869(96)00021-9},
url = {https://www.sciencedirect.com/science/article/pii/0191886996000219},
author = {John C. Caruso and Charles L. Spirrison},
abstract = {The present study examined the viability of motivated retrieval failure as a mechanism for childhood amnesia. A total of 115 undergraduates completed the Constructive Thinking Inventory, the NEO Personality Inventory, and answered questions regarding their earliest memory in order to assess the relationships between personality and coping variables and the subject's reported age of earliest memory. The personality and coping inventories were divided into four groups defined by the stated age of each subject's earliest memory. Analyses indicated that scores on two Constructive Thinking Inventory scales (Emotional Coping and Categorical Thinking) and two NEO Personality Inventory scales (Neuroticism and Openness to Experience) were significantly different between groups. These differences were consistent with the motivated retrieval failure hypothesis. Subscales of these four scales were then analyzed individually to further examine the differences between groups. Results are discussed in the context of personality assessment and the repression of early memories.}
}
@article{MA2024100647,
title = {Design of online teaching interaction mode for vocational education based on gamified-learning},
journal = {Entertainment Computing},
volume = {50},
pages = {100647},
year = {2024},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2024.100647},
url = {https://www.sciencedirect.com/science/article/pii/S1875952124000156},
author = {Zhongbao Ma and Wei Li},
keywords = {Gamified-learning, Traveler-type problems, Genetic-based algorithms, Game design},
abstract = {Along with the process of building China's modern vocational education system, China's higher vocational education has made great progress. With the development of computer and Internet technology, gamified learning, as a new way of learning, combines the advantages of computer games and online learning, which not only meets the needs of people to learn anytime and anywhere, but also increases the fun of learning activities. In this paper, we developed a gamified learning software with traveler-type problems as the research content, through the interaction with the game, so that students can think in the game and learn knowledge through the game. Through the questionnaire for research and analysis, this game is good game fun and can stimulate learning interest well. In addition, this paper carries out an in-depth study of the game's help system, optimizes the algorithm for the help system, and proposes an improved genetic algorithm. The reverse learning method is adopted to improve the accuracy and convergence speed of the optimal solution; then the Metropolis criterion is used to improve the crossover and mutation operators to enhance the local search ability of the algorithm; finally, the concept of realistic elite learning is introduced to further enhance the local search ability of the algorithm. The simulation results show that the algorithm is effectively improved in convergence performance and solution accuracy, which can significantly improve the response speed of the help system, effectively improve the game's fun, and improve the game's playability.}
}
@article{MARTIN1993141,
title = {Neural connections, mental computation: Lynn Nadel, Lynn A. Cooper, Peter Culicover and R. Michael Harnish, eds.},
journal = {Artificial Intelligence},
volume = {62},
number = {1},
pages = {141-151},
year = {1993},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(93)90052-D},
url = {https://www.sciencedirect.com/science/article/pii/000437029390052D},
author = {Benjamin Martin}
}
@article{GADZHIEV2025101314,
title = {Creating A dynamic cognovisor – Brain activity recognition using principal Component analysis and Machine learning models},
journal = {Cognitive Systems Research},
volume = {89},
pages = {101314},
year = {2025},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2024.101314},
url = {https://www.sciencedirect.com/science/article/pii/S1389041724001086},
author = {Ismail M. Gadzhiev and Alexander S. Makarov and Vadim L. Ushakov and Vyacheslav A. Orlov and Georgy A. Ivanitsky and Sergei A. Dolenko},
keywords = {Brain Activity, Cognitive States, Machine Learning, Principal Component Analysis},
abstract = {This study explores the feasibility of developing a dynamic cognovisor capable of recognizing cognitive states and transitions using fMRI data. Data were collected from 31 participants performing spatial and verbal tasks during fMRI scanning and were preprocessed using a nine-step algorithm for artifact removal and denoising. Three types of classification problems were examined, with machine learning methods and dimensionality reduction techniques applied to classify activity states. The best-performing models were identified for each classification problem, providing insights into their applicability. Notably, binary classification of resting versus active states achieved good quality with relatively simple methods. A key finding underscores the importance of accounting for temporal history of the signal prior to the prediction moment to improve model performance.}
}
@article{STRYCKER2020e04358,
title = {K-12 art teacher technology use and preparation},
journal = {Heliyon},
volume = {6},
number = {7},
pages = {e04358},
year = {2020},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2020.e04358},
url = {https://www.sciencedirect.com/science/article/pii/S2405844020312020},
author = {Jesse Strycker},
keywords = {Applications in the subject area, Art education, Educational technology, Elementary education, Instructional technology, Post-secondary education, Secondary education, Teaching/learning strategies, Educational development, Evaluation in education, Media education, Pedagogy, Teaching research, Education},
abstract = {Largely absent from educational/instructional technology journals, this study focused on how K-12 art teachers in a southern state used technology to support teaching and learning, uses they found to be the best, and what kinds of technology training they received as part of their initial teacher preparation. Findings indicated that presentation and resource access technologies had transformed the way art teachers in the study work with students and materials. They also had little use of technology to support students with special needs and had limited technology experiences in their own training. Elementary art teachers were found to have more examples of student higher-order thinking skills promoting technology use, while secondary art teachers had more student media creation and a desire to implement digital portfolios. Additional findings and interpretations are offered.}
}
@article{GROSS2019116125,
title = {Is perception the missing link between creativity, curiosity and schizotypy? Evidence from spontaneous eye-movements and responses to auditory oddball stimuli},
journal = {NeuroImage},
volume = {202},
pages = {116125},
year = {2019},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2019.116125},
url = {https://www.sciencedirect.com/science/article/pii/S1053811919307165},
author = {Madeleine E. Gross and Draulio B. Araujo and Claire M. Zedelius and Jonathan W. Schooler},
keywords = {Creativity, Curiosity, Schizotypy, Eye-tracking, Eye-gaze, Salience, Perception},
abstract = {What is the relationship between creativity, curiosity, and schizotypy? Schizophrenia-spectrum conditions and creativity have been linked to deficits in filtering sensory information, and curiosity is associated with information-seeking. This raises the possibility of a perception-based link between all three concepts. Here, we investigated whether the same individual differences in perceptual encoding explain variance in creativity, curiosity, and schizotypy. We administered an active auditory oddball task and a free viewing eye-tracking paradigm (N = 88). Creativity was measured with the figural portion of the Torrance Tests of Creative Thinking (TTCT) and two self-report scales. Schizotypy and curiosity were measured with self-reports. We found that creativity was associated with increased reaction time to the rare tone in the oddball task and was positively associated with the number and duration of fixations in the free viewing task. Schizotypy, on the other hand, showed a negative trend with the number and duration of fixations. Both creativity and curiosity were positively associated with explorative eye movements (unique number of regions visited) and Shannon entropy, while schizotypy was negatively associated with entropy. We further compared saliency maps finding that individuals high versus low in creativity and curiosity, respectively, exhibit differences in where they look. These findings may suggest a perception-based link between creativity and curiosity, but not schizotypy. Implications and limitations of these findings are discussed.}
}
@article{CLANCY2008248,
title = {Applications of complex systems theory in nursing education, research, and practice},
journal = {Nursing Outlook},
volume = {56},
number = {5},
pages = {248-256.e3},
year = {2008},
note = {Special Issue Informatics: Science and Practice},
issn = {0029-6554},
doi = {https://doi.org/10.1016/j.outlook.2008.06.010},
url = {https://www.sciencedirect.com/science/article/pii/S0029655408001619},
author = {Thomas R. Clancy and Judith A. Effken and Daniel Pesut},
abstract = {The clinical and administrative processes in today's healthcare environment are becoming increasingly complex. Multiple providers, new technology, competition, and the growing ubiquity of information all contribute to the notion of health care as a complex system. A complex system (CS) is characterized by a highly connected network of entities (e.g., physical objects, people or groups of people) from which higher order behavior emerges. Research in the transdisciplinary field of CS has focused on the use of computational modeling and simulation as a methodology for analyzing CS behavior. The creation of virtual worlds through computer simulation allows researchers to analyze multiple variables simultaneously and begin to understand behaviors that are common regardless of the discipline. The application of CS principles, mediated through computer simulation, informs nursing practice of the benefits and drawbacks of new procedures, protocols and practices before having to actually implement them. The inclusion of new computational tools and their applications in nursing education is also gaining attention. For example, education in CSs and applied computational applications has been endorsed by The Institute of Medicine, the American Organization of Nurse Executives and the American Association of Colleges of Nursing as essential training of nurse leaders. The purpose of this article is to review current research literature regarding CS science within the context of expert practice and implications for the education of nurse leadership roles. The article focuses on 3 broad areas: CS defined, literature review and exemplars from CS research and applications of CS theory in nursing leadership education. The article also highlights the key role nursing informaticists play in integrating emerging computational tools in the analysis of complex nursing systems.}
}
@article{HEINZE2021R1381,
title = {Fly navigation: Yet another ring},
journal = {Current Biology},
volume = {31},
number = {20},
pages = {R1381-R1383},
year = {2021},
issn = {0960-9822},
doi = {https://doi.org/10.1016/j.cub.2021.09.009},
url = {https://www.sciencedirect.com/science/article/pii/S0960982221012495},
author = {Stanley Heinze},
abstract = {Summary
Flies keep track of a food site by path integration. A novel behavioral paradigm has been combined with computational models to show that Drosophila can track at least three food patches simultaneously by using the center of gravity of all food sites as the reference point for their path integrator.}
}
@article{OSMAN2013188,
title = {21st Century Biology: An Interdisciplinary Approach of Biology, Technology, Engineering and Mathematics Education},
journal = {Procedia - Social and Behavioral Sciences},
volume = {102},
pages = {188-194},
year = {2013},
note = {6th International Forum on Engineering Education (IFEE 2012)},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2013.10.732},
url = {https://www.sciencedirect.com/science/article/pii/S1877042813042687},
author = {Kamisah Osman and Lee Chuo Hiong and Rian Vebrianto},
keywords = {interdisciplinary, BTEM (Biology, Technology, Engineering, Mathematics), inquiry-discovery, 21st century skills},
abstract = {The principal goal of interdisciplinary approach for Biology, Technology, Engineering and Mathematics (BTEM) is to cultivate scientific inquiry that requires coordination of both knowledge and skills simultaneously. The dominant activity for BTEM is inquiry-discovery on the authentic problems. This is intended to enhance the students’ abilities to construct their own knowledge through the relevant hands-on and minds-on activities. The essence of engineering is inventive problem solving. The Integration of advanced information communication technologies believed to be able to fulfill current Net Generation learning styles. Mathematics plays an important role as computational tools. The expected outcome of BTEM implementation is the inculcation of 21st century skills.}
}