@article{TRICHKOVAKASHAMOVA2024123,
title = {Criteria and Approaches for Optimization of Innovative Methods for STEM Education},
journal = {IFAC-PapersOnLine},
volume = {58},
number = {3},
pages = {123-128},
year = {2024},
note = {22nd IFAC Conference on Technology, Culture and International Stability TECIS 2024},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2024.07.137},
url = {https://www.sciencedirect.com/science/article/pii/S2405896324002209},
author = {E. Trichkova-Kashamova and E. Paunova-Hubenova and Y. Boneva and S. Dimitrov},
keywords = {STEM education, innovative educational methods, optimisation, data analyses, technology-based learning},
abstract = {The proposed research aims to evaluate the modern learning process in STEM subjects in a technology-rich environment. The study examines contemporary teaching methods and evaluates their application in different educational levels in Bulgaria. The aim is to provide information for developing a concept of a modern technology-based learning process and integrating innovative methods with appropriate technological tools.}
}
@article{PENG2025104791,
title = {From GPT to DeepSeek: Significant gaps remain in realizing AI in healthcare},
journal = {Journal of Biomedical Informatics},
volume = {163},
pages = {104791},
year = {2025},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2025.104791},
url = {https://www.sciencedirect.com/science/article/pii/S1532046425000206},
author = {Yifan Peng and Bradley A. Malin and Justin F. Rousseau and Yanshan Wang and Zihan Xu and Xuhai Xu and Chunhua Weng and Jiang Bian},
keywords = {DeepSeek, ChatGPT, AI in Healthcare}
}
@article{ELMAIZI2019126,
title = {A novel information gain based approach for classification and dimensionality reduction of hyperspectral images},
journal = {Procedia Computer Science},
volume = {148},
pages = {126-134},
year = {2019},
note = {THE SECOND INTERNATIONAL CONFERENCE ON INTELLIGENT COMPUTING IN DATA SCIENCES, ICDS2018},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.01.016},
url = {https://www.sciencedirect.com/science/article/pii/S187705091930016X},
author = {Asma Elmaizi and Hasna Nhaila and Elkebir Sarhrouni and Ahmed Hammouch and Chafik Nacir},
keywords = {Hyperspectral images, dimentionality reduction, information gain, classification accuracy},
abstract = {Recently, the hyperspectral sensors has improved our ability to monitor the earth surface with high spectral resolution. However, the high dimensionality of spectral data brings challenges for the image processing. Consequently, the dimensionality reduction is a necessary step in order to reduce the computational complexity and increase the classification accuracy. In this paper, we propose a new filter approach based on information gain for dimensionality reduction and classification of hyperspectral images. A special strategy based on hyperspectral bands selection is adopted to pick the most informative bands and discard the irrelevant and noisy ones. The algorithm evaluates the relevancy of the bands based on the information gain function with the support vector machine classifier. The proposed method is compared using two benchmark hyperspectral datasets (Indiana, Pavia) with three competing methods. The comparison results showed that the information gain filter approach outperforms the other methods on the tested datasets and could significantly reduce the computation cost while improving the classification accuracy.}
}
@article{FUSHING2023129227,
title = {Multiscale major factor selections for complex system data with structural dependency and heterogeneity},
journal = {Physica A: Statistical Mechanics and its Applications},
volume = {630},
pages = {129227},
year = {2023},
issn = {0378-4371},
doi = {https://doi.org/10.1016/j.physa.2023.129227},
url = {https://www.sciencedirect.com/science/article/pii/S0378437123007823},
author = {Hsieh Fushing and Elizabeth P. Chou and Ting-Li Chen},
keywords = {Broken symmetry, Conditional entropy, Contingency table, Major factor selection, Multiclass Classification, Pitching dynamics},
abstract = {The unknown multiscale structure hidden in large complex systems is explored bottom-up through discovered heterogeneity under structural dependency embedded within structured data sets. Via two real complex systems, we demonstrate computed hierarchical structures with broken symmetry constituting data’s information content. Through graphic displays, such information content indirectly, but efficiently resolves system-related scientific issues that are difficult to resolve directly. All bottom-up explorations and computations are based on conditional entropy and mutual information evaluated upon contingency table platforms after categorizing all quantitative features. Categorical Exploratory Data Analysis (CEDA) first extracts global major factors that share significant mutual information with the targeted response (Re) variable against many covariate (Co) features under the presence of structural dependency. Then each global major factor is taken as one perspective of heterogeneity to subdivide the entire data set according to its categories into sub-collections. This simple “de-associating” protocol significantly reduces structural dependency among the rest of the features such that another run of major factor selection performed on the sub-collection scale can precisely identify which feature sets could provide extra information beyond the global major factor. Finally, informative patterns collected from multiple perspectives of heterogeneity are displayed to explicitly resolve issues of prediction, classification, and detecting minute dynamic changes.}
}
@incollection{ESCHE2014699,
title = {Systematic Modeling for Optimization},
editor = {Mario R. Eden and John D. Siirola and Gavin P. Towler},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {34},
pages = {699-704},
year = {2014},
booktitle = {Proceedings of the 8th International Conference on Foundations of Computer-Aided Process Design},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-444-63433-7.50101-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780444634337501012},
author = {Erik Esche and David Müller and Günter Wozny},
keywords = {Multiple-Scale Modeling, Optimization, Convexification, Linearization},
abstract = {Optimization usually requires models, which are computationally speaking less expensive than models commonly used for simulations. At the same time, process optimization and model predictive control etc. require dependable accuracies in addition to the fastness. To demystify the art of preparing process models for optimization, a workflow is presented in this contribution, which systematically deduces models based on simplification of existing models and experiment based deduction of computationally inexpensive correlations.}
}
@article{PANETSOS2011314,
title = {Physical measurement of brain perception abilities. Foundations of a working methodology for the design of “intelligent” beings},
journal = {Procedia Computer Science},
volume = {7},
pages = {314-316},
year = {2011},
note = {Proceedings of the 2nd European Future Technologies Conference and Exhibition 2011 (FET 11)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2011.09.052},
url = {https://www.sciencedirect.com/science/article/pii/S1877050911006120},
author = {F. Panetsos and S.L. Andino Gonzalez and P.C. Marijuan and C. Herrera-Rincon},
keywords = {Emergent properties, complexity, artificial brain, synthetic approach},
abstract = {Most of the important properties of the brain (thinking, consciousness, music, etc.) are severely ill-defined. They are not the direct output of biological sensors or their combinations but emerge from complex computations at the network level and are not necessarily represented in the sensory input or the activity of individual cells. They are emergent properties arising from dynamic interactions between neurons in the different relay stations of the sensory pathways where recognition of basic physical properties of incoming stimuli take place. Emergent properties and interactions between them range from physical properties of stimuli to cognitive operations as emotions or consciousness and gradually involve interactions between sensory pathways, associative cortexes, hippocampus, or the amygdala. Here we propose to build neural tissues from embryonic stem cells in “in vitro” controlled environments to determine the way physical inputs are transformed into what humans perceive and measure. We will start with “low complexity” tissues able to perform low level recognition of physical properties, to gradually increase the complexity of the tissue to investigate how the physical characteristics of the incoming stimuli correspond at higher levels to the emergent properties of the system. Mathematical methods based on networks theory, nonlinear dynamics, fractal theory and chaos among other will be used to determine and measure the emergent properties of the nervous tissue at different complexity levels. We expected to provide criteria and methodologies to measure human-like perception variables and use them for the design of future living artifacts (autonomous robots, intelligent sensors, hybrid systems, etc.).}
}
@incollection{BIR202197,
title = {Chapter Four - Generic quantum hardware accelerators for conventional systems},
editor = {Shiho Kim and Ganesh Chandra Deka},
series = {Advances in Computers},
publisher = {Elsevier},
volume = {122},
pages = {97-133},
year = {2021},
booktitle = {Hardware Accelerator Systems for Artificial Intelligence and Machine Learning},
issn = {0065-2458},
doi = {https://doi.org/10.1016/bs.adcom.2021.01.007},
url = {https://www.sciencedirect.com/science/article/pii/S0065245821000322},
author = {Parth Bir},
keywords = {Quantum mechanics, Computational basis, State space, Deterministic model, Probabilistic model, QA, GQHA},
abstract = {Quantum mechanics proposes, universe is a sum of a generic building block. Different orientation (i.e., angle, phase, amplitude, etc.) and summation of blocks forms entities. When differentiated, building blocks used for formation of entity are termed as basis. Following computational theory, these basis are termed as computational basis. Classical computers possess binary basis. Quantum system possess exponential computational power because of infinite computational basis. When computing solution to a problem, it's found in state space. Deterministic model (Conventional) requires both correct and incorrect solution set. For problems of probabilistic nature with plenty of variables (NP and P problems), computing solution requires exponential time, as entire state space is scanned. Furthermore, if solution is incomputable, the computation will never complete as solution is missing from both sets. Probabilistic model (Quantum) conducts a guided state space search and possess greater information carrying capacity per bit. Therefore, Quantum Accelerators (QA) are ideal for solving such problems. Resulting implementation of a Generic Quantum Hardware Accelerator (GQHA) is described via algorithms, mathematical models and microarchitecture. Next, a competitive industrial analysis and virtual implementation in a cloud environment is defined. Finally, it's proven that GQHA can replace conventional accelerators to produce faster and reliable results.}
}
@article{LEE2025111886,
title = {Project symphony: Composing a masterpiece in a science laboratory},
journal = {iScience},
volume = {28},
number = {2},
pages = {111886},
year = {2025},
issn = {2589-0042},
doi = {https://doi.org/10.1016/j.isci.2025.111886},
url = {https://www.sciencedirect.com/science/article/pii/S2589004225001464},
author = {Sunghee Lee and Jamie Gudyka and Marnie Skinner and Jasmin Ceja-Vega and Amani Rabadi and Christopher Poust and Caroline Scott and Micaela Panella and Elizabeth Andersen and Jessica Said},
abstract = {In the spirit of collaborative science, Prof. Sunghee Lee (Chemistry Professor at Iona University in New York, USA) embarked on her academic career with a vision to bring an interdisciplinary approach to undergraduate education. At a Predominantly Undergraduate Institution (PUI) such as Iona, she saw a unique opportunity to weave together teaching and research, creating a rich tapestry of learning experiences for students. Her goal was simple yet ambitious: to use research as a bridge connecting classroom theory to real-world interdisciplinary scientific practice. In this Backstory, Sunghee and her students and recent graduates reflect on the development and experiences that shaped their journey through Project Symphony and the resulting skills they’ve learned. The symphony they’ve created together is a testament to the transformative power of collaborative undergraduate research – a melody of discovery that continues to evolve and inspire.}
}
@incollection{PETROVIC2025163,
title = {A Historical and Current Look at Chemical Design for Reduced Hazard},
editor = {Béla Török},
booktitle = {Encyclopedia of Green Chemistry (First Edition)},
publisher = {Elsevier},
edition = {First Edition},
address = {Oxford},
pages = {163-172},
year = {2025},
isbn = {978-0-443-28923-1},
doi = {https://doi.org/10.1016/B978-0-443-15742-4.00072-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780443157424000727},
author = {Predrag V. Petrovic and Philip Coish and Paul T. Anastas},
keywords = {Benign by design, CAMD, First do no harm, Green chemistry, Hazard reduction, Safer chemical design, Sustainable chemistry},
abstract = {Green chemistry aims to design chemical products and processes that reduce or eliminate the use and generation of hazardous substances. The design of chemicals for reduced hazard selects the properties and attributes that provide the desired function while avoiding unintended consequences for human health and the environment. The tools and knowledge that are available to designers have increased considerably in recent years. Importantly, the approach and goals of design have evolved and become more holistic, multi-disciplinary, and inclusive. This is notable as the global population׳s continued economic and social well-being cannot be maintained with measures that destroy our natural environment. Sustainable chemistry promotes, advances, enables, and empowers the implementation of the chemistry of sustainability and includes chemical design for reduced hazard. In this chapter, we consider the design of chemicals for reduced hazard (safer chemical design) focusing on the past and present, and importantly, looking to the future on what lies ahead.}
}
@article{SHI2024100685,
title = {Drug development in the AI era: AlphaFold 3 is coming!},
journal = {The Innovation},
volume = {5},
number = {5},
pages = {100685},
year = {2024},
issn = {2666-6758},
doi = {https://doi.org/10.1016/j.xinn.2024.100685},
url = {https://www.sciencedirect.com/science/article/pii/S2666675824001231},
author = {Yi Shi}
}
@article{ANDERSON2024108366,
title = {Trichotomy revisited: A monolithic theory of attentional control},
journal = {Vision Research},
volume = {217},
pages = {108366},
year = {2024},
issn = {0042-6989},
doi = {https://doi.org/10.1016/j.visres.2024.108366},
url = {https://www.sciencedirect.com/science/article/pii/S0042698924000105},
author = {Brian A. Anderson},
keywords = {Attentional control, Visual attention, Selection history, Memory, Learning},
abstract = {The control of attention was long held to reflect the influence of two competing mechanisms of assigning priority, one goal-directed and the other stimulus-driven. Learning-dependent influences on the control of attention that could not be attributed to either of those two established mechanisms of control gave rise to the concept of selection history and a corresponding third mechanism of attentional control. The trichotomy framework that ensued has come to dominate theories of attentional control over the past decade, replacing the historical dichotomy. In this theoretical review, I readily affirm that distinctions between the influence of goals, salience, and selection history are substantive and meaningful, and that abandoning the dichotomy between goal-directed and stimulus-driven mechanisms of control was appropriate. I do, however, question whether a theoretical trichotomy is the right answer to the problem posed by selection history. If we reframe the influence of goals and selection history as different flavors of memory-dependent modulations of attentional priority and if we characterize the influence of salience as a consequence of insufficient competition from such memory-dependent sources of priority, it is possible to account for a wide range of attention-related phenomena with only one mechanism of control. The monolithic framework for the control of attention that I propose offers several concrete advantages over a trichotomy framework, which I explore here.}
}
@incollection{ZELINSKY2005395,
title = {CHAPTER 65 - Specifying the Components of Attention in a Visual Search Task},
editor = {Laurent Itti and Geraint Rees and John K. Tsotsos},
booktitle = {Neurobiology of Attention},
publisher = {Academic Press},
address = {Burlington},
pages = {395-400},
year = {2005},
isbn = {978-0-12-375731-9},
doi = {https://doi.org/10.1016/B978-012375731-9/50069-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780123757319500690},
author = {Gregory J. Zelinsky},
abstract = {ABSTRACT
Although commonly treated as a unitary process, attention is more likely a collection of task-related but separable operations. Three components of attention (set, selection, and movement) are identified and defined within the context of a computationally explicit model of eye movements during visual search. The model compares filter-based representations of the target and search displays to derive a salience map indicating likely target candidates in a scene. Eye position is defined as the centroid of activity on this saliency map. As this map is thresholded over time, the changing centroid produces a sequence of movements that eventually cause simulated gaze to become aligned with the target. By adopting a more computational language and making explicit the underlying operations of the task, visual search, a behavior that has long been hobbled to the concept of attention, can be well described without appeal to an abstracted attention theory.}
}
@article{PLUZHNIKOVA202334,
title = {The Human Factor and the Problem of Transport Safety in Modern Conditions},
journal = {Transportation Research Procedia},
volume = {68},
pages = {34-39},
year = {2023},
note = {XIII International Conference on Transport Infrastructure: Territory Development and Sustainability},
issn = {2352-1465},
doi = {https://doi.org/10.1016/j.trpro.2023.02.005},
url = {https://www.sciencedirect.com/science/article/pii/S2352146523000078},
author = {N.N. Pluzhnikova},
keywords = {Transport, transport robotics, intelligence, IT-technologies, human},
abstract = {The article is devoted to the analysis of the interaction between man and artificial intelligence to ensure the safety of transport. The author analyzes intelligence and IT technologies on the example of the development of transport robotics. To consider this problem the author refers to cognitive developments in this area, and also indicates the philosophical problems of the development of transport robotics. The article uses such methods as comparative and structural analysis.}
}
@article{ROBINSON2009310,
title = {Children's understanding of the inverse relation between multiplication and division},
journal = {Cognitive Development},
volume = {24},
number = {3},
pages = {310-321},
year = {2009},
issn = {0885-2014},
doi = {https://doi.org/10.1016/j.cogdev.2008.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S0885201408000889},
author = {Katherine M. Robinson and Adam K. Dubé},
keywords = {Arithmetic, Inversion, Conceptual knowledge, Procedural knowledge, Factual knowledge, Multiplication, Division},
abstract = {Children's understanding of the inversion concept in multiplication and division problems (i.e., that on problems of the form d * e/e no calculations are required) was investigated. Children in Grades 6, 7, and 8 completed an inversion problem-solving task, an assessment of procedures task, and a factual knowledge task of simple multiplication and division. Application of the inversion concept in the problem-solving task was low and constant across grades. Most participants approved of the inversion-based shortcut but only a slight majority preferred it. Three clusters of children were identified based on their performance on the three tasks. The inversion cluster used and approved of the inversion shortcut the most and had high factual knowledge. The negation cluster used the negation strategy, had lower approval of the inversion shortcut, and had medium factual knowledge. The computation cluster used computation and had the lowest approval and the weakest factual knowledge. The findings highlight the importance of addressing the multiplication and division inversion concept in theories of children's mathematical competence.}
}
@article{ALLGOWER2019147,
title = {Position paper on the challenges posed by modern applications to cyber-physical systems theory},
journal = {Nonlinear Analysis: Hybrid Systems},
volume = {34},
pages = {147-165},
year = {2019},
issn = {1751-570X},
doi = {https://doi.org/10.1016/j.nahs.2019.05.007},
url = {https://www.sciencedirect.com/science/article/pii/S1751570X19300603},
author = {Frank Allgöwer and João {Borges de Sousa} and James Kapinski and Pieter Mosterman and Jens Oehlerking and Patrick Panciatici and Maria Prandini and Akshay Rajhans and Paulo Tabuada and Philipp Wenzelburger},
keywords = {cyber–physical systems theory},
abstract = {Cyber-physical systems theory offers a powerful framework for modeling, analyzing, and designing real engineering systems integrating communication, control, and computation functionalities (the cyber part) within a natural and/or man-made system governed by the laws of physics (the physical part). New methodological developments in cyber-physical systems theory are required by traditional application domains such as manufacturing, transportation, and energy systems, which are currently experiencing significant and – to some extent – revolutionary changes to address the needs of our modern society. The goal of this position paper is to provide the cyber-physical systems community, and especially young researchers, a clear view on what are research directions worth pursuing motivated by the challenges posed by modern applications.}
}
@incollection{YADEN2023849,
title = {Reintroducing “development” into theories of the acquisition and growth of early literacy: developmental science approaches and the cultural-historical perspective of L. S. Vygotsky},
editor = {Robert J Tierney and Fazal Rizvi and Kadriye Ercikan},
booktitle = {International Encyclopedia of Education (Fourth Edition)},
publisher = {Elsevier},
edition = {Fourth Edition},
address = {Oxford},
pages = {849-865},
year = {2023},
isbn = {978-0-12-818629-9},
doi = {https://doi.org/10.1016/B978-0-12-818630-5.07103-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128186305071037},
author = {David B. Yaden and Camille Martinez-Yaden},
keywords = {Microgenetic, Developmental science, Optimizing equilibration, Process-relational, Relational-developmental-system, Backward transition, Early writing, Overlapping waves theory, Prospective models, Retrospective models, Hyperbolic geometry},
abstract = {This chapter contrasts the differences between computational “retrospective” models of early writing achievement whose elements represent static states of being and “prospective” models based upon the principles of developmental science and a process-relational-developmental framework which characterizes early writing performances always in the process of “becoming.” The chapter highlights these differences using examples from Spanish-speaking and Chinese/English emergent bilinguals to illustrate the various patterns of writing development captured in a Piagetian/Vygotskian-inspired early writing assessment. The children's simultaneous display of multiple conceptualizations of the notational system in Spanish, English, and Chinese is interpreted as reflecting aspects of Siegler's “overlapping waves theory” and Piaget's “optimizing equilibration.”}
}
@article{SAENZROYO2024121922,
title = {Ordering vs. AHP. Does the intensity used in the decision support techniques compensate?},
journal = {Expert Systems with Applications},
volume = {238},
pages = {121922},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.121922},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423024247},
author = {Carlos Sáenz-Royo and Francisco Chiclana and Enrique Herrera-Viedma},
keywords = {AHP, IBR, Decision Support Systems, Expertise, Intensity Judgment},
abstract = {The manifestation of the intensity in the judgment of one alternative versus another in the peer comparison processes is a central element in some decision support techniques, such as the Analytical Hierarchy Process (AHP). However, its contribution regarding quality (expected performance) with respect to the priority vector has not been evaluated so far. Using the Intentional Bounded Rationality Methodology (IBRM), this work analyzes the gains obtained from requiring the decision-maker to report an intensity judgment in pairs (AHP) with respect to a technique that only requires expressing a preference (Ordering). The results show that when decision-makers have low levels of expertise, it is possible that a less informative and computational cheap technique (Ordering) performs better than a more informative and computational expensive one (AHP). When decision-makers have medium and high levels of expertise, AHP technique obtains modest gains with respect to the Ordering technique. This study proposes a cost-benefit analysis of decision support techniques contrasting the gains of a technique that requires more resources (AHP) against other that require less resources (Ordering). Our results can change the managing approach of the information obtained from experts’ judgments.}
}
@article{MCCRADDEN2023100864,
title = {A normative framework for artificial intelligence as a sociotechnical system in healthcare},
journal = {Patterns},
volume = {4},
number = {11},
pages = {100864},
year = {2023},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2023.100864},
url = {https://www.sciencedirect.com/science/article/pii/S2666389923002489},
author = {Melissa D. McCradden and Shalmali Joshi and James A. Anderson and Alex John London},
abstract = {Summary
Artificial intelligence (AI) tools are of great interest to healthcare organizations for their potential to improve patient care, yet their translation into clinical settings remains inconsistent. One of the reasons for this gap is that good technical performance does not inevitably result in patient benefit. We advocate for a conceptual shift wherein AI tools are seen as components of an intervention ensemble. The intervention ensemble describes the constellation of practices that, together, bring about benefit to patients or health systems. Shifting from a narrow focus on the tool itself toward the intervention ensemble prioritizes a “sociotechnical” vision for translation of AI that values all components of use that support beneficial patient outcomes. The intervention ensemble approach can be used for regulation, institutional oversight, and for AI adopters to responsibly and ethically appraise, evaluate, and use AI tools.}
}
@article{JOHANN2016420,
title = {Soil moisture modeling based on stochastic behavior of forces on a no-till chisel opener},
journal = {Computers and Electronics in Agriculture},
volume = {121},
pages = {420-428},
year = {2016},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2015.12.020},
url = {https://www.sciencedirect.com/science/article/pii/S0168169915004020},
author = {André L. Johann and Augusto G. {de Araújo} and Hevandro C. Delalibera and André R. Hirakawa},
keywords = {Soil physics, Computational models, Precision agriculture, Soft computing, Force sensors},
abstract = {Crop-yield variability is frequently associated with soil moisture and its real-time measurement can be an alternative for the automatic control of no-till seeding to improve soil–crop conditions. Soil moisture has a significant influence on soil behavior, markedly on its temporal and spatial variability; however, the measurement of soil moisture is generally time consuming and expensive. Many studies employ electric, electromagnetic, optical, or radiometric sensors for the direct measurement of soil moisture. It is also possible to develop an estimation method employing existing machinery components using mechanical sensors such as load cells. Auto-regressive error function (AREF) combined with computational models is applied in this study for estimating soil moisture using a data set of forces acting on a chisel and speed as inputs to assess the feasibility of achieving more accurate results than previously obtained by Sakai et al. (2005). AREF is a stochastic method that can be applied to the analysis of soil-force patterns acting on a tool. Three computational models are developed, including two artificial neural networks (a Multi-Layer Perceptron (MLP) and a Radial Basis Function (RBF)) and one Neuro-Fuzzy model (ANFIS). These are compared with two multiple linear regression (MLR) models with two and six independent variables. The models’ performances are evaluated using root mean square error (RMSE), determination coefficient (R2), and average percentage error (APE). The computational models demonstrated superior performance compared to MLR, confirming the hypothesis. The neural network models had similar performances with RMSE between 1.27% and 1.30%, R2 around 0.80, and APE between 3.77% and 3.75% for testing data. These results indicate that using AREF parameters combined with computational models may be a suitable technique to estimate soil moisture and has potential to be used in control systems applied to no-till machinery.}
}
@article{JALALIAN2023103602,
title = {Learning about me and you: Only deterministic stimulus associations elicit self-prioritization},
journal = {Consciousness and Cognition},
volume = {116},
pages = {103602},
year = {2023},
issn = {1053-8100},
doi = {https://doi.org/10.1016/j.concog.2023.103602},
url = {https://www.sciencedirect.com/science/article/pii/S1053810023001393},
author = {Parnian Jalalian and Marius Golubickis and Yadvi Sharma and C. {Neil Macrae}},
keywords = {Self, Instrumental learning, Probabilistic selection task, Self-prioritization, Drift diffusion model},
abstract = {Self-relevant material has been shown to be prioritized over stimuli relating to others (e.g., friend, stranger), generating benefits in attention, memory, and decision-making. What is not yet understood, however, is whether the conditions under which self-related knowledge is acquired impacts the emergence of self-bias. To address this matter, here we used an associative-learning paradigm in combination with a stimulus-classification task to explore the effects of different learning experiences (i.e., deterministic vs. probabilistic) on self-prioritization. The results revealed an effect of prior learning on task performance, with self-prioritization only emerging when participants acquired target-related associations (i.e., self vs. friend) under conditions of certainty (vs. uncertainty). A further computational (i.e., drift diffusion model) analysis indicated that differences in the efficiency of stimulus processing (i.e., rate of information uptake) underpinned this self-prioritization effect. The implications of these findings for accounts of self-function are considered.}
}
@article{ZOU2024134011,
title = {Synthesis and mechanism of quaternary ammonium salts based on porphyrin as high-performance copper levelers},
journal = {Tetrahedron},
volume = {159},
pages = {134011},
year = {2024},
issn = {0040-4020},
doi = {https://doi.org/10.1016/j.tet.2024.134011},
url = {https://www.sciencedirect.com/science/article/pii/S0040402024001911},
author = {Peikun Zou and Xuyang Li and Xin Chen and Wenhao Zhou and Kexin Du and Limin Wang},
keywords = {Porphyrin, Porphyrin quaternary ammonium salts, Through-hole electroplating, Electroplating leveler, Quantum chemical calculations},
abstract = {The molecular structure and energy distribution of organic compounds have a great influence on their adsorption capacity on the metal surface. However, there are still insufficient researches on the influence of energy distribution on adsorption properties of organic molecules. Herein, a family of porphyrin derivatives (TPyP-Et, TPyP-Oct, TPyP-Bn and TPyP-Al) bearing quaternary ammonium groups were synthesized for the first time as promising levelers for through-hole copper electrodeposition. Electrochemical tests revealed that all four TPyP derivatives displayed enhanced electrochemical properties. Theoretical calculations and molecular dynamics simulations were carried out to investigate the physisorption capacity and chemical reaction activity of the TPyP molecules, as well as the adsorption capacity on the surface of the copper layer. Through optical and scanning electron microscopy as well as X-ray diffractometry, it was demonstrated that TPyP molecules are effective electroplating levelers. TPyP-Oct, with its longer carbon chain substituent, exhibited superior hole-filling performance in practical PCB experiments among the four compounds. This study expandes the application range of porphyrin compounds, analyzes the influence of organic molecular adsorption properties in copper electrodeposition, and provides theoretical guidance for the future study of organic compounds adsorbed on metal surfaces.}
}
@article{FRENCH2023100030,
title = {Reflections on 50 years of MCDM: Issues and future research needs},
journal = {EURO Journal on Decision Processes},
volume = {11},
pages = {100030},
year = {2023},
issn = {2193-9438},
doi = {https://doi.org/10.1016/j.ejdp.2023.100030},
url = {https://www.sciencedirect.com/science/article/pii/S2193943823000031},
author = {Simon French},
keywords = {Behavioural decision studies, Bayesian analysis, Conflicting objectives, Cynefin, Multiple criteria decision-making (MCDM), Uncertainty},
abstract = {Modern discussions of multiple criteria decision-making extend back about half a century. I reflect on key developments, schools of thought and controversies that have taken place over the period, arguing that perhaps those of us in different schools focus too much on our differences and do not capitalise enough on what we share in common. Moreover, the differences between schools are indications of their respective weaknesses and can drive improvements in each. The discussion points to a number of issues and research needs that the community needs to address.}
}
@incollection{WANG2017259,
title = {Chapter 14 - Reason and Emotion in Xunzi’s Moral Psychology},
editor = {T.-W. Hung and T.J. Lane},
booktitle = {Rationality},
publisher = {Academic Press},
address = {San Diego},
pages = {259-276},
year = {2017},
isbn = {978-0-12-804600-5},
doi = {https://doi.org/10.1016/B978-0-12-804600-5.00014-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128046005000143},
author = {E.H. Wang},
keywords = {Xunzi, moral rationalism, emotion},
abstract = {In this paper I explore the extent to which Xunzi may, or may not, be a moral rationalist by investigating the roles reason and emotion play in Xunzi’s moral psychology. To this end, I address Soek’s and Slingerland’s recent work on this subject. Seok (2013) recently characterized two contrasting models of moral psychology: “reason based” and “emotion based”; the former takes the reflective and conscious reasoning ability to be the essence of one’s moral judgment and action, while emotions and affective mechanisms play only minor roles (if any); the latter takes emotional states to be essential or at least necessary. Soek understands Confucian ethics in general to operate with the emotion-based model, but his argument mainly concerns Mencius’ work. Slingerland (2010), on the other hand, categorizes Xunzi’s moral psychology as a theory that presumes what he calls the “high reason model,” which significantly resembles the reason-based model in Soek’s account. Slingerland understands that, on Xunzi’s account, rational faculties and emotional faculties are competitive in the reasoning process. Moreover, he takes Xunzi to prioritize the rational faculties, thinking that they can and should monitor emotional responses, and override them when needed. Slingerland also cites recent empirical studies to criticize this model. I argue that Xunzi’s moral psychology cannot be captured by either of the two models Soek characterizes, but presents to us a third alternative: it gives us a good example of a hybrid model of these two. Indeed, Xunzi’s emphasis on ritual practices in the cultivation of xin and qing toward sagehood sheds light on a possible interplay between reason and emotion in ideal moral judgment/decision. This discussion inspires further consideration of what a moral rationalist may be, and the extent to which Xunzi may, or may not, be a moral rationalist.}
}
@article{SCHOLL201856,
title = {Understanding psychiatric disorder by capturing ecologically relevant features of learning and decision-making},
journal = {Behavioural Brain Research},
volume = {355},
pages = {56-75},
year = {2018},
note = {SI: MCC 2016},
issn = {0166-4328},
doi = {https://doi.org/10.1016/j.bbr.2017.09.050},
url = {https://www.sciencedirect.com/science/article/pii/S0166432817305673},
author = {Jacqueline Scholl and Miriam Klein-Flügge},
keywords = {Reinforcement learning, Decision-making, Computational psychiatry},
abstract = {Recent research in cognitive neuroscience has begun to uncover the processes underlying increasingly complex voluntary behaviours, including learning and decision-making. Partly this success has been possible by progressing from simple experimental tasks to paradigms that incorporate more ecological features. More specifically, the premise is that to understand cognitions and brain functions relevant for real life, we need to introduce some of the ecological challenges that we have evolved to solve. This often entails an increase in task complexity, which can be managed by using computational models to help parse complex behaviours into specific component mechanisms. Here we propose that using computational models with tasks that capture ecologically relevant learning and decision-making processes may provide a critical advantage for capturing the mechanisms underlying symptoms of disorders in psychiatry. As a result, it may help develop mechanistic approaches towards diagnosis and treatment. We begin this review by mapping out the basic concepts and models of learning and decision-making. We then move on to consider specific challenges that emerge in realistic environments and describe how they can be captured by tasks. These include changes of context, uncertainty, reflexive/emotional biases, cost-benefit decision-making, and balancing exploration and exploitation. Where appropriate we highlight future or current links to psychiatry. We particularly draw examples from research on clinical depression, a disorder that greatly compromises motivated behaviours in real-life, but where simpler paradigms have yielded mixed results. Finally, we highlight several paradigms that could be used to help provide new insights into the mechanisms of psychiatric disorders.}
}
@article{FRY2010218,
title = {On the nature of tetraalkylammonium ions in common electrochemical solvents: General and specific solvation – Quantitative aspects},
journal = {Journal of Electroanalytical Chemistry},
volume = {638},
number = {2},
pages = {218-224},
year = {2010},
issn = {1572-6657},
doi = {https://doi.org/10.1016/j.jelechem.2009.11.011},
url = {https://www.sciencedirect.com/science/article/pii/S0022072809004288},
author = {Albert J. Fry and L. Kraig Steffen},
keywords = {Computational electrochemistry, Tetraalkylammonium ions, Specific solvation, Inner sphere solvation, General solvation},
abstract = {The free energies of each of 80 tetraalkylammonium ion/solvent complexes [R4N+/(solv)n], with R ranging from methyl through butyl and n ranging from 1 through 4, were computed by density functional theory (DFT) in five common electrochemical solvents: dimethylformamide (DMF), dimethylsulfoxide (DMSO), acetonitrile (AN), dichloromethane (DCM), and methanol (MeOH). The energies of the complexes were computed both with and without their solvation energies. Additional computations of the energies of the individual components, both solvated and unsolvated, were also carried out. The resulting data permit construction of a thermodynamic cycle for each R4N+/solvent pair that in turn allows the determination of the extent of general and specific solvation energies for that pair. An additional series of computations for pentane as solvent were carried out. Since this solvent should not coordinate with tetraalkylammonium ions, these computations provide a test of the validity of the computational method. This work represents a useful new general protocol for assessing the relative importance of general and specific solvation in chemical systems.}
}
@article{XU2024102292,
title = {Hierarchical spatio-temporal graph convolutional neural networks for traffic data imputation},
journal = {Information Fusion},
volume = {106},
pages = {102292},
year = {2024},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2024.102292},
url = {https://www.sciencedirect.com/science/article/pii/S1566253524000708},
author = {Dongwei Xu and Hang Peng and Yufu Tang and Haifeng Guo},
keywords = {Traffic data imputation, Hierarchical representation, Graph convolution network, Spatio-temporal features},
abstract = {The quality of traffic services depends on the accuracy and completeness of the collected traffic data. However,the existing traffic data imputation methods usually only rely on the predefined road network structure to capture the spatio-temporal features and only consider the imputation effect from a single perspective, which are very limited for imputation of different missing patterns of road traffic data. In this paper, we propose a novel deep learning framework called Hierarchical Spatio-temporal Graph Convolutional Neural Networks(HSTGCN) to impute traffic data,through the macro layer and the road layer. The model constructs macro graph of the road network based on the data temporal correlation clustering, which can mine the temporal dependencies of road traffic data from a hierarchical perspective. Besides, a temporal attention mechanism and adaptive adjacency matrix are introduced in the road layer to better extract the spatio-temporal information of the road traffic data. Finally, we use graph convolution neural networks to learn the spatio-temporal feature representations of the road layer and macro layer, which are then fused to achieve data imputation. To illustrate the efficient performance of the model, experiments are conducted on traffic data collected from California and Seattle. The proposed model performs better than the comparison model for traffic data imputation.}
}
@article{LI2021711,
title = {Prediction of BLEVE blast loading using CFD and artificial neural network},
journal = {Process Safety and Environmental Protection},
volume = {149},
pages = {711-723},
year = {2021},
issn = {0957-5820},
doi = {https://doi.org/10.1016/j.psep.2021.03.018},
url = {https://www.sciencedirect.com/science/article/pii/S0957582021001324},
author = {Jingde Li and Qilin Li and Hong Hao and Ling Li},
keywords = {ANN, BLEVE, Blast wave, Peak pressure, CFD, Neural networks},
abstract = {Boiling Liquid Expanding Vapour Explosions (BLEVEs) are extreme explosions driven by nonlinear physical processes associated with explosively expanded vapour and flashed liquid. Blast loading generated from BLEVEs may severely harm structures and people. Prediction of such strong explosions is not currently feasible using simple tools. Physics-based Computational Fluid Dynamics (CFD) methods are commonly utilized to predict the blast loading of BLEVE by going through many empirical formulas that map input variables to the target progressively. The calculation is often time-consuming, and it is therefore impractical to apply these methods to predict explosion loads from BLEVE in normal design analysis. Thinking of the composition of empirical relations in CFD models as a complex and nonlinear function, it is necessary to find an approximation of this function that can be efficiently calculated. The Artificial Neural Network (ANN) is a data-driven computational model that is capable of approximating any functions by learning from training data. Once properly trained, ANN can produce accurate predictions even for unseen inputs. This article presents the development of an ANN model to predict blast loading of BLEVEs in an open environment. A rigorous validation process is presented for the design of ANN structure, and the selected ANN is trained using validated simulation data from CFD models. Extensive evaluation of the network predictive performance is conducted, and it shows that the developed ANN can reproduce the result of CFD models effectively and efficiently, not only on simulation data but also on real experimental data. The prediction of ANN has a percentage error around 6 % and R2 value over 0.99 with the result of CFD simulated data. It speeds up the processing time from hours to seconds and only increases the error from 26.3 %–27.6 %, compared to the CFD simulations of real experimental data. Therefore, the developed ANN model can be potentially applied in the process engineering to generate a large number of reliable data for safety and risk assessment of BLEVEs in a more efficient way.}
}
@article{PANTALEON201579,
title = {Taylor series expansion using matrices: An implementation in MATLAB®},
journal = {Computers & Fluids},
volume = {112},
pages = {79-82},
year = {2015},
issn = {0045-7930},
doi = {https://doi.org/10.1016/j.compfluid.2015.01.009},
url = {https://www.sciencedirect.com/science/article/pii/S0045793015000183},
author = {Carlos Pantaleón and Amitabha Ghosh},
keywords = {Taylor series, Finite differences, Truncation error, Modified equation, Symbolic computation},
abstract = {Taylor series expansions are widely used in engineering approximations, for instance, to develop finite differences schemes or numerical integration methods. This technical note presents a novel technique to generate, display and manipulate Taylor series expansion by using matrices. The resulting approach allows algebraic manipulation as well as differentiation in a very intuitive manner in order to experiment with different numerical schemes, their truncation errors and their structures, while avoiding manual calculation errors. A detailed explanation of the mathematical procedure to generate a matrix form of the Taylor series expansion for a function of two variables is presented along with the algorithm of an implementation in MATLAB®. Example cases of different orders are tabulated to illustrate the generation and manipulation capabilities of this technique. Additionally, an extended application is developed to determine the modified equations of finite difference schemes for partial differential equations, with one-dimensional examples of the wave equation and the heat equation using explicit and implicit schemes.}
}
@article{YANG2023105120,
title = {Thoughts of brain EEG signal-to-text conversion using weighted feature fusion-based Multiscale Dilated Adaptive DenseNet with Attention Mechanism},
journal = {Biomedical Signal Processing and Control},
volume = {86},
pages = {105120},
year = {2023},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2023.105120},
url = {https://www.sciencedirect.com/science/article/pii/S1746809423005530},
author = {Jing Yang and Muhammad Awais and Md. Amzad Hossain and Por {Lip Yee} and Ma. Haowei and Ibrahim M. Mehedi and A.I.M. Iskanderani},
keywords = {Thought-to-text conversion, Electroencephalography signal, Optimal weighted feature fusion, Eurasian oystercatcher wild geese migration optimization, Multiscale Dilated Adaptive DenseNet with Attention Mechanism},
abstract = {Individuals with visual inefficiencies or different abilities face difficulties using their hands to operate smartphones and computers, necessitating reliance on others to enter data. Such dependence may lead to security and privacy issues, especially when sensitive information is shared with helpers. To address this problem, we present Think2Type, an efficient Brain-Computer Interface (BCI) that enables users to translate their active intentions into text format based on Morse code. BCI leverages brain activity to facilitate interaction with computers, often captured via Electroencephalography (EEG). This work proposes an enhanced attention-based deep learning strategy to develop an efficient text conversion mechanism from EEG signals. We begin by collecting EEG signals from standard benchmark datasets and extracting spectral and statistical features in phase 1, concatenating them into concatenated feature set 1 (F1). In phase 2, we extract spatial and temporal features via a One-Dimensional Convolutional Neural Network (1DCNN) and a Recurrent Neural Network (RNN), respectively, concatenating them into concatenated feature set 2 (F2). Weighted feature fusion is performed on concatenated features F1 and F2, with the hybrid optimization algorithm Eurasian Oystercatcher Wild Geese Migration Optimization (EOWGMO) optimizing the weight for improved fusion efficiency. The text conversion phase utilizes the Multiscale Dilated Adaptive DenseNet with Attention Mechanism (MDADenseNet-AM) to obtain the converted text information. The MDADenseNet-A's parameters are optimized to improve thought-to-text conversion performance. The developed model's performance is evaluated via experimental analysis and compared to conventional techniques, resulting in a higher accuracy value of 96.41%, facilitating appropriate text conversion.}
}
@article{LIU2024100129,
title = {Extracting multi-objective multigraph features for the shortest path cost prediction: Statistics-based or learning-based?},
journal = {Green Energy and Intelligent Transportation},
volume = {3},
number = {1},
pages = {100129},
year = {2024},
issn = {2773-1537},
doi = {https://doi.org/10.1016/j.geits.2023.100129},
url = {https://www.sciencedirect.com/science/article/pii/S2773153723000658},
author = {Songwei Liu and Xinwei Wang and Michal Weiszer and Jun Chen},
keywords = {Multi-objective multigraph, Feature extraction, Shortest path cost prediction, Node patterns, Node embeddings, Regression},
abstract = {Efficient airport airside ground movement (AAGM) is key to successful operations of urban air mobility. Recent studies have introduced the use of multi-objective multigraphs (MOMGs) as the conceptual prototype to formulate AAGM. Swift calculation of the shortest path costs is crucial for the algorithmic heuristic search on MOMGs, however, previous work chiefly focused on single-objective simple graphs (SOSGs), treated cost enquires as search problems, and failed to keep a low level of computational time and storage complexity. This paper concentrates on the conceptual prototype MOMG, and investigates its node feature extraction, which lays the foundation for efficient prediction of shortest path costs. Two extraction methods are implemented and compared: a statistics-based method that summarises 22 node physical patterns from graph theory principles, and a learning-based method that employs node embedding technique to encode graph structures into a discriminative vector space. The former method can effectively evaluate the node physical patterns and reveals their individual importance for distance prediction, while the latter provides novel practices on processing multigraphs for node embedding algorithms that can merely handle SOSGs. Three regression models are applied to predict the shortest path costs to demonstrate the performance of each. Our experiments on randomly generated benchmark MOMGs show that (i) the statistics-based method underperforms on characterising small distance values due to severe overestimation; (ii) A subset of essential physical patterns can achieve comparable or slightly better prediction accuracy than that based on a complete set of patterns; and (iii) the learning-based method consistently outperforms the statistics-based method, while maintaining a competitive level of computational complexity.}
}
@incollection{BARBER20251,
title = {Cultural contributions to cognitive aging},
editor = {Jordan Henry Grafman},
booktitle = {Encyclopedia of the Human Brain (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {1-16},
year = {2025},
isbn = {978-0-12-820481-8},
doi = {https://doi.org/10.1016/B978-0-12-820480-1.00042-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128204801000425},
author = {Nicolette Barber and Ioannis Valoumas and Chaipat Chunharas and Sirawaj Itthipuripat and Angela Gutchess},
keywords = {Aging, Attention, Autobiographical memory, Cognition, Cognitive aging, Cognitive neuroscience, Cross-cultural, Culture, Long-term memory, Memory},
abstract = {In this article, we review research on the influences of culture on cognitive aging, with a focus on long-term memory and attention. Given the small number of studies directly investigating cognitive aging across cultures, we draw on existing cross-cultural studies comparing brain and behavior in young adult samples. We outline the potential for future research and discuss the importance of adopting a cross-cultural lens to support cognition and well-being in older adults in diverse cultural contexts.}
}
@article{OMIZO2020102578,
title = {Machining Topoi: Tracking Premising in Online Discussion Forums with Automated Rhetorical Move Analysis},
journal = {Computers and Composition},
volume = {57},
pages = {102578},
year = {2020},
note = {Composing Algorithms: Writing (with) Rhetorical Machines},
issn = {8755-4615},
doi = {https://doi.org/10.1016/j.compcom.2020.102578},
url = {https://www.sciencedirect.com/science/article/pii/S8755461520300396},
author = {Ryan M. Omizo},
keywords = {computational rhetoric, Docuscope, Faciloscope, discussion forums, topoi},
abstract = {This article interrogates recent computational work on discovering and analyzing topoi through the use of topic modeling in the discipline of the literary digital humanities against the long history of topical research and pedagogy in rhetoric and composition. While significant work has been done in the literary digital humanities to advance the study of texts through topic modeling, this article argues that the emphasis on the textuality of topoi in computational research neglects situated rhetorical actions and the dynamics of audience interaction. In response to this deemphasis, this article proposes an algorithmic alternative to the identification and explanation of the rhetorical topoi through the integrated use of a computational rhetorical move classifier called the Faciloscope (Omizo et al., 2016) and the pattern-matching program, Docuscope (Kaufer and Ishizaki, 1998).}
}
@article{SCHMID2019178,
title = {Representing stuff in the human brain},
journal = {Current Opinion in Behavioral Sciences},
volume = {30},
pages = {178-185},
year = {2019},
note = {Visual perception},
issn = {2352-1546},
doi = {https://doi.org/10.1016/j.cobeha.2019.10.007},
url = {https://www.sciencedirect.com/science/article/pii/S2352154619300816},
author = {Alexandra C Schmid and Katja Doerschner},
abstract = {Our experience of materials does not merely comprise judgments of single properties such as glossiness or roughness but is rather made up of a multitude of simultaneous impressions of qualities. To understand the neural mechanisms yielding such complex impressions, we suggest that it is necessary to extend existing experimental approaches to those that view material perception as a distributed and dynamic process. A distributed representations framework not only fits better with our perceptual experience of material qualities, it is commensurate with recent psychophysics and neuroimaging results.}
}
@incollection{WALSH2017,
title = {Sensory Systems},
booktitle = {Reference Module in Neuroscience and Biobehavioral Psychology},
publisher = {Elsevier},
year = {2017},
isbn = {978-0-12-809324-5},
doi = {https://doi.org/10.1016/B978-0-12-809324-5.06867-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012809324506867X},
author = {V. Walsh},
keywords = {Auditory system, Multisensory integration, Nerves, Somatosensation, Visual system},
abstract = {Sensory systems have an old school ring to them, a very old school ring to them. In the 15th century Benedetti was able to write, “By means of nerves, the pathways of the senses are distributed like the roots and fibers of a tree” (Alessandro Benedetti, 1497). This is still a good place to start because it gives one a feel for the 3D structure of our sensory apparatus, but the challenge of understanding the senses has, of course, gone well beyond structure (which is not to imply that all structural descriptions are complete or that we have joined all the dots of structure–function relationships), and any serious scholar needs to have a working knowledge of the development, physiology, psychophysics (physiology without the blood), genetics, pathology, and computational models of the senses.}
}
@article{JOHNSON2022105743,
title = {Metacognition for artificial intelligence system safety – An approach to safe and desired behavior},
journal = {Safety Science},
volume = {151},
pages = {105743},
year = {2022},
issn = {0925-7535},
doi = {https://doi.org/10.1016/j.ssci.2022.105743},
url = {https://www.sciencedirect.com/science/article/pii/S0925753522000832},
author = {Bonnie Johnson},
keywords = {Metacognition, Artificial intelligence systems, Machine learning, System safety, Complexity},
abstract = {Advances in computational thinking and data science have led to a new era of artificial intelligence systems being engineered to adapt to complex situations and develop actionable knowledge. These learning systems are meant to reliably understand the essence of a situation and construct critical decision recommendations to support autonomous and human–machine teaming operations. In parallel, the increasing volume, velocity, variety, veracity, value, and variability of data is confounding the complexity of these new systems – creating challenges in terms of their development and implementation. For artificial systems supporting critical decisions with higher consequences, safety has become an important concern. Methods are needed to avoid failure modes and ensure that only desired behavior is permitted. This paper discusses an approach that promotes self-awareness, or metacognition, within the artificial intelligence systems to understand their external and internal operational environments and use this knowledge to identify potential failures and enable self-healing and self-management for safe and desired behavior.}
}
@article{NOLAN2025108094,
title = {Efficient Bayesian functional principal component analysis of irregularly-observed multivariate curves},
journal = {Computational Statistics & Data Analysis},
volume = {203},
pages = {108094},
year = {2025},
issn = {0167-9473},
doi = {https://doi.org/10.1016/j.csda.2024.108094},
url = {https://www.sciencedirect.com/science/article/pii/S0167947324001786},
author = {Tui H. Nolan and Sylvia Richardson and Hélène Ruffieux},
keywords = {Functional principal component analysis, Hierarchical modelling, Multivariate functional data, Variational message passing},
abstract = {The analysis of multivariate functional curves has the potential to yield important scientific discoveries in domains such as healthcare, medicine, economics and social sciences. However, it is common for real-world settings to present longitudinal data that are both irregularly and sparsely observed, which introduces important challenges for the current functional data methodology. A Bayesian hierarchical framework for multivariate functional principal component analysis is proposed, which accommodates the intricacies of such irregular observation settings by flexibly pooling information across subjects and correlated curves. The model represents common latent dynamics via shared functional principal component scores, thereby effectively borrowing strength across curves while circumventing the computationally challenging task of estimating covariance matrices. These scores also provide a parsimonious representation of the major modes of joint variation of the curves and constitute interpretable scalar summaries that can be employed in follow-up analyses. Estimation is conducted using variational inference, ensuring that accurate posterior approximation and robust uncertainty quantification are achieved. The algorithm also introduces a novel variational message passing fragment for multivariate functional principal component Gaussian likelihood that enables modularity and reuse across models. Detailed simulations assess the effectiveness of the approach in sharing information from sparse and irregularly sampled multivariate curves. The methodology is also exploited to estimate the molecular disease courses of individual patients with SARS-CoV-2 infection and characterise patient heterogeneity in recovery outcomes; this study reveals key coordinated dynamics across the immune, inflammatory and metabolic systems, which are associated with long-COVID symptoms up to one year post disease onset. The approach is implemented in the R package bayesFPCA.}
}
@article{VERDUZCO2022103189,
title = {CALRECOD — A software for Computed Aided Learning of REinforced COncrete structural Design},
journal = {Advances in Engineering Software},
volume = {172},
pages = {103189},
year = {2022},
issn = {0965-9978},
doi = {https://doi.org/10.1016/j.advengsoft.2022.103189},
url = {https://www.sciencedirect.com/science/article/pii/S0965997822000965},
author = {Luis Fernando Verduzco and Jaime Horta and Miguel A. Pérez Lara y Hernández and Juan Bosco Hernández},
keywords = {CALRECOD, Reinforced concrete structures, High education, Computed aided learning, Optimization methods},
abstract = {It is presented the development and implementation of a new computed aided learning MatLab Toolbox for the design of reinforced concrete structures named as CALRECOD for their abbreviation Computer Aided Learning of Reinforced Concrete Design. Such development emerges as the result of a series of research works in the Autonomous University of Queretaro with the main purpose of improving the way in which the design of reinforced concrete structures is taught in high education institutions. CALRECOD uses optimization methods and algorithms to aid students in their design interaction learning so that they are able to compare their own designs and what commercial software delivers with optimal ones given certain load conditions on the elements or structures. The software consists almost entirely of MatLab functions (.m files) and the ACI 318-19 code is taken as their main design reference to make it internationally useful, although in some cases the Mexican code NTC-17 specifications are used. Besides MatLab functions, the software consists as well of ANSYS SpaceClaim script functions (.scscript files) as an additional tool for the aid in the visualization of design results in a 3D space in the software ANSYS SpaceClaim. CALRECOD has proven to be versatile, flexible and of easy use with a huge potential to increase learning outcomes for students in high education programs related with the design of reinforced concrete structures as well as to enhance the creation of efficient interactive environments for researchers and academics focused on the development of new design and analysis methods for such structures. With their optimization design functions, a solid comparison platform of designs’ performance could be laid out, and with its extended function design packages for structural systems, reinforced concrete design courses could be enhanced in a great deal regarding their program content’s scope. The software can be found at: https://github.com/calrecod/CALRECOD.}
}
@article{CORTENBACH2024104869,
title = {The Dial-a-Ride problem with meeting points: A problem formulation for shared demand–responsive transit},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {169},
pages = {104869},
year = {2024},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2024.104869},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X24003905},
author = {L.E. Cortenbach and K. Gkiotsalitis and E.C. {van Berkum} and E. Walraven},
keywords = {Dial-a-Ride problem, Meeting points, Demand–responsive transit, Tabu search},
abstract = {In this paper, a formulation for the Dial-a-Ride Problem with Meeting Points (DARPmp) is introduced. The problem consists of defining routes that satisfy trip requests between pick-up and drop-off points while complying with time window, ride time, vehicle load, and route duration constraints. A set of meeting points is defined, and passengers may be asked to use these meeting points as alternative pickup or drop-off points if this results in routes with lower costs. Incorporating meeting points into the DARP is achieved by formulating a mixed-integer linear program. Two preprocessing steps and three valid inequalities are introduced, which improve the computational performance when solving the DARPmp to global optimality. Two versions of the Tabu Search metaheuristic are proposed to approximate the optimal solution in large-scale networks due to the NP-hardness of DARPmp. Performing numerical experiments with benchmark instances, this study demonstrates the benefits of DARPmp compared to DARP in terms of reducing vehicle running costs.}
}
@article{READ201952,
title = {Using neural networks as models of personality process: A tutorial},
journal = {Personality and Individual Differences},
volume = {136},
pages = {52-67},
year = {2019},
note = {Dynamic Personality Psychology},
issn = {0191-8869},
doi = {https://doi.org/10.1016/j.paid.2017.11.015},
url = {https://www.sciencedirect.com/science/article/pii/S0191886917306724},
author = {Stephen J. Read and Vita Droutman and Benjamin J. Smith and Lynn C. Miller},
keywords = {Neural networks, Computational modeling, Within-subjects variability, Connectionist modeling, Personality dynamics},
abstract = {This paper presents a tutorial for creating neural network models of personality processes. Such models enable researchers to create explicit models of both personality structure and personality dynamics, and to address issues of recent concern in personality, such as, “If personality is stable, then how is it possible that within subject variability in personality states can be as large as or larger than between subject variability in personality?” or “Is it possible to understand personality dynamics and personality structure within a common framework?” We discuss why one should want to use neural networks, review what a neural network model is, review a previous model we have constructed, discuss how to conceptualize issues in such a way that they can be computationally modeled, show how that conceptualization can be translated into a model, and discuss the utility of such models for understanding personality structure and personality dynamics. To build our model we use a neural network modeling package called emergent that is freely available, and a specific architecture called Leabra to build a runnable model that addresses one of the questions posed above: How can within subject variability in personality related states be as large as between subject variability in personality?}
}
@article{DELLACORTE2016209,
title = {Referential description of the evolution of a 2D swarm of robots interacting with the closer neighbors: Perspectives of continuum modeling via higher gradient continua},
journal = {International Journal of Non-Linear Mechanics},
volume = {80},
pages = {209-220},
year = {2016},
note = {Dynamics, Stability, and Control of Flexible Structures},
issn = {0020-7462},
doi = {https://doi.org/10.1016/j.ijnonlinmec.2015.06.016},
url = {https://www.sciencedirect.com/science/article/pii/S0020746215001468},
author = {Alessandro {Della Corte} and Antonio Battista and Francesco dell׳Isola},
keywords = {Swarm robot, Second gradient continua, Generalized continua, Deformable bodies},
abstract = {In the present paper a discrete robotic system model whose elements interact via a simple geometric law is presented and some numerical simulations are provided and discussed. The main idea of the work is to show the resemblance between the cases of first and second neighbors interaction with (respectively) first and second gradient continuous deformable bodies. Our numerical results showed indeed that the interaction and the evolution process described is suitable to closely reproduce some basic characteristics of the behavior of bodies whose deformation energy depends on first or on higher gradients of the displacement. Moreover, some specific qualitative characteristics of the continuous deformation are also reproduced. The model introduced here will need further investigation and generalization in both theoretical and numerical directions.}
}
@incollection{GROSSBERG19873,
title = {The Qijantized Geometry of Visual Space: The Coherent Computation of Depth, Form and Lightness},
editor = {Stephen Grossberg},
series = {Advances in Psychology},
publisher = {North-Holland},
volume = {43},
pages = {3-79},
year = {1987},
booktitle = {The Adaptive Brain II},
issn = {0166-4115},
doi = {https://doi.org/10.1016/S0166-4115(08)61756-2},
url = {https://www.sciencedirect.com/science/article/pii/S0166411508617562},
author = {Stephen Grossberg},
keywords = {binocular vision, brightness perception, figure-ground, feature extraction, form perception, neural network, nonlinear resonance, receptive field, short-term memory, spatial scales, visual completion},
abstract = {A theory is presented of how global visual interactions between depth, length, lightness, and form percepts can occur. The theory suggests how quantized activity patterns which reflect these visual properties can coherently fill-in, or complete, visually ambiguous regions starting with visually informative data features. Phenomena such as the Cornsweet and Craik-O'Brien effects, phantoms and subjective contours, binocular brightness summation, the equidistance tendency, Emmert's law, allelotropia, multiple spatial frequency scaling and edge detection, figure-ground completion, coexistence of depth and binocular rivalry, reflectance rivalry, Fechner's paradox, decrease of threshold contrast with increased number of cycles in a grating pattern, hysteresis, adaptation level tuning, Weber law modulation, shift of sensitivity with background luminance, and the finite capacity of visual short term memory are discussed in terms of a small set of concepts and mechanisms. Limitations of alternative visual theories which depend upon Fourier analysis, Laplacians, zero-crossings, and cooperative depth planes are described. Relationships between monocular and binocular processing of the same visual patterns are noted, and a shift in emphasis from edge and disparity computations toward the characterization of resonant activity-scaling correlations across multiple spatial scales is recommended. This recommendation follows from the theory's distinction between the concept of a structural spatial scale, which is determined by local receptive field properties, and a functional spatial scale, which is defined by the interaction between global properties of a visual scene and the network as a whole. Functional spatial scales, but not structural spatial scales, embody the quantization of network activity that reflects a scene's global visual representation. A functional scale is generated by a filling-in resonant exchange, or FIRE, which can be ignited by an exchange of feedback signals among the binocular cells where monocular patterns are binocularly matched.}
}
@article{UMAIR2024106224,
title = {Emotion Fusion-Sense (Emo Fu-Sense) – A novel multimodal emotion classification technique},
journal = {Biomedical Signal Processing and Control},
volume = {94},
pages = {106224},
year = {2024},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2024.106224},
url = {https://www.sciencedirect.com/science/article/pii/S1746809424002829},
author = {Muhammad Umair and Nasir Rashid and Umar {Shahbaz Khan} and Amir Hamza and Javaid Iqbal},
keywords = {EEG, ECG, GSR, Respiration amplitude, Body temperature, LSTM, Feature fusion, Modality biasing, Multimodal emotion classification},
abstract = {Human emotions play a vital role in overall well-being. With the advent of advance technologies growing interest has been observed in developing a multimodal emotion classification system that can accurately interpret human emotions. The article presents a comprehensive overview of a multimodal emotion classification system (Emo Fu-Sense) designed to capture the rich and nuanced nature of human emotions. Objective of Emo Fu-Sense is to integrates information from Electrocardiogram (ECG), Galvanic Skin Response (GSR), Electroencephalograph (EEG), respiration amplitude and body temperature to achieve holistic understanding of emotional states. To effectively extract information from multimodal data, designed system employs conventional methods with sophisticated machine learning algorithms including Long Short-Term Memory (LSTM), a variety of Recurrent Neural Network (RNN). Recommended solution extracts column wise features independently from different modalities based on the windowing operation. Finally, feature fusion and modality biasing were used to combine the information from different modalities. The proposed method has not only highlighted the limitations of unimodal system but has achieved a classification accuracy of 92.62 %, with an average F1-Score of 93 % and 9.2 % of Mean Absolute Error (MAE). Obtained results are better than existing state-of-the-art approaches. Evaluation of the multimodal emotion classification system was conducted on MAHNOB-HCI dataset, which encompasses a wide range of emotional expressions across various contexts and individuals. The integration of multiple modalities and advanced machine learning techniques enables a more comprehensive understanding of emotional states and highlights the significance of research and development in the field of affective computing.}
}
@article{2024100678,
title = {Erratum regarding missing Declaration of Competing Interest statements in previously published articles},
journal = {International Journal of Child-Computer Interaction},
volume = {41},
pages = {100678},
year = {2024},
issn = {2212-8689},
doi = {https://doi.org/10.1016/j.ijcci.2024.100678},
url = {https://www.sciencedirect.com/science/article/pii/S2212868924000473}
}
@article{KOTYRA2023105613,
title = {High-performance watershed delineation algorithm for GPU using CUDA and OpenMP},
journal = {Environmental Modelling & Software},
volume = {160},
pages = {105613},
year = {2023},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2022.105613},
url = {https://www.sciencedirect.com/science/article/pii/S1364815222003139},
author = {Bartłomiej Kotyra},
keywords = {Watershed delineation, GIS, Parallel algorithms, GPU, CUDA, OpenMP},
abstract = {Watershed delineation is one of the fundamental tasks in hydrological studies. Tools for extracting watersheds from digital elevation models and flow direction rasters are commonly implemented in GIS software packages. However, the performance of available techniques and algorithms often turns out to be far from sufficient, especially when working with large datasets. While modern hardware offers high computing performance through massive parallelism, there is still a need for algorithms that can effectively use these capabilities. This paper proposes an algorithm for rapid watershed delineation directly from flow direction rasters, using the possibilities offered by modern GPU devices. Performance measurements show a significant reduction in execution time compared to other parallel solutions proposed for this task in the literature. Moreover, this implementation makes it possible to delineate multiple watersheds from the same dataset simultaneously, each having one or more outlet cells, with virtually no additional computational cost.}
}
@article{LI2025112016,
title = {The neural correlates of logical-mathematical symbol systems processing resemble those of spatial cognition more than language processing},
journal = {iScience},
volume = {28},
number = {4},
pages = {112016},
year = {2025},
issn = {2589-0042},
doi = {https://doi.org/10.1016/j.isci.2025.112016},
url = {https://www.sciencedirect.com/science/article/pii/S2589004225002767},
author = {Yuannan Li and Shan Xu and Jia Liu},
keywords = {Neuroscience, Cognitive neuroscience},
abstract = {Summary
The ability to use logical-mathematical symbols (LMS), encompassing tasks such as calculation, reasoning, and programming, is special to humans with recent emergence. LMS processing was suggested to build upon fundamental cognitive systems through neuronal recycling, with natural language processing and spatial cognition as key candidates. This study used meta-analyses and synthesized neural maps of representative LMS tasks, including reasoning, calculation, and mental programming, to compare their neural correlates with those of the two systems. Our results revealed greater activation overlap and multivariate similarity between LMS and spatial cognition than with language processing. Hierarchical clustering further indicated that LMS tasks were indistinguishable from spatial tasks at the neural level, suggesting an inherent connection. Our findings support the hypothesis that spatial cognition is the basis of LMS processing, shedding light on the logical reasoning limitations of large language models, particularly those lacking explicit spatial representations.}
}
@article{TRONCOSOGARCIA2023108387,
title = {Explainable hybrid deep learning and Coronavirus Optimization Algorithm for improving evapotranspiration forecasting},
journal = {Computers and Electronics in Agriculture},
volume = {215},
pages = {108387},
year = {2023},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2023.108387},
url = {https://www.sciencedirect.com/science/article/pii/S0168169923007755},
author = {A.R. Troncoso-García and I.S. Brito and A. Troncoso and F. Martínez-Álvarez},
keywords = {XAI, Deep learning, Evapotranspiration forecasting, Hyperparameter optimization},
abstract = {Reference evapotranspiration is a critical hydrological measurement closely associated with agriculture. Accurate forecasting is vital in effective water management and crop planning in sustainable agriculture. In this study, the future values of reference evapotranspiration are forecasted by applying a recurrent long short-term memory neural network optimized using the Coronavirus Optimization Algorithm, a novel bioinspired metaheuristic based on the spread of COVID-19. The input data is sourced from the Sistema Agrometeorológico para a Gestão da Rega no Alentejo, in Portugal, with meteorological data such as air temperature or wind speed. Several baseline models are applied to the same problem to facilitate comparisons, including support vector machines, multi-layer perceptron, Lasso and decision tree. The results demonstrate the successful forecasting performance of the proposed model and its potential in this field. In turn, to gain deeper insights into the model’s inner workings, the SHapley Additive exPlanation tool is applied for explainability. Consequently, the study identifies the most relevant variables for reference evapotranspiration forecasting, including previously measured evapotranspiration values. Additionally, a univariable model is tested using historic evapotranspiration values as input, offering a comparable performance with a considerable reduction of computational time.}
}
@article{MOTSA2023116912,
title = {A data-driven, machine learning scheme used to predict the structural response of masonry arches},
journal = {Engineering Structures},
volume = {296},
pages = {116912},
year = {2023},
issn = {0141-0296},
doi = {https://doi.org/10.1016/j.engstruct.2023.116912},
url = {https://www.sciencedirect.com/science/article/pii/S0141029623013275},
author = {Siphesihle Mpho Motsa and Georgios Ε. Stavroulakis and Georgios Α. Drosopoulos},
keywords = {FEM, Machine Learning, Artificial Neural Network, Multi-hinge failure, Damage Prediction, Masonry Arches, Data-driven Mechanics, Digital Twin},
abstract = {A data-driven methodology is proposed, for the investigation of the ultimate response of masonry arches. Aiming to evaluate their structural response in a computationally efficient framework, machine learning metamodels, in the form of artificial neural networks, are adopted. Datasets are numerically built, integrating Matlab, Python and commercial finite element software. Heyman’s assumptions are adopted within non-linear finite element analysis, incorporating contact-friction laws between adjacent stones, to capture failure in the arch. The artificial neural networks are trained, validated, and tested using the least square minimization technique. It is shown that the proposed scheme can be used to provide a fast and accurate prediction of the deformed geometry, the collapse mechanism and the ultimate load. Cases studies demonstrate the efficiency of the method in random, new arch geometries. Relevant Matlab/Python scripts and datasets are provided. The method can be extended towards structural health monitoring and the concept of digital twin.}
}
@article{DISESSA198067,
title = {Computation as a physical and intellectual environment for learning physics},
journal = {Computers & Education},
volume = {4},
number = {1},
pages = {67-75},
year = {1980},
issn = {0360-1315},
doi = {https://doi.org/10.1016/0360-1315(80)90009-3},
url = {https://www.sciencedirect.com/science/article/pii/0360131580900093},
author = {A.A. DiSessa}
}
@article{MOHANAN2024100997,
title = {Integrating Ayurveda and modern mainstream medicine},
journal = {Journal of Ayurveda and Integrative Medicine},
volume = {15},
number = {5},
pages = {100997},
year = {2024},
issn = {0975-9476},
doi = {https://doi.org/10.1016/j.jaim.2024.100997},
url = {https://www.sciencedirect.com/science/article/pii/S0975947624001128},
author = {K.P. Mohanan},
abstract = {This article is an attempt to understand the challenge of integrating the education provided by BAMS programs and MBBS programs, in order to initiate the process of integrating research and practice in Ayurveda and Modern Mainstream Medicine. The specific issues discussed in the article are framed within the broader context of the challenge of integrating any two bodies of knowledge, theories, or knowledge systems in education and research.}
}
@article{HALL1996115,
title = {The role of creativity within best practice manufacturing},
journal = {Technovation},
volume = {16},
number = {3},
pages = {115-121},
year = {1996},
issn = {0166-4972},
doi = {https://doi.org/10.1016/0166-4972(95)00050-X},
url = {https://www.sciencedirect.com/science/article/pii/016649729500050X},
author = {David J. Hall},
abstract = {‘Best practice’ manufacturing is linked directly to aspects of creativity, through an appreciation of the operation of the practitioner's brain. It is suggested that the introduction of techniques such as benchmarking or business process re-engineering cannot succeed in the long term, unless the correct understanding is developed within the management team. Concepts of mind set and lateral thinking are related to the top-down introduction of step change, whilst ‘total quality’ programmes develop the culture necessary for bottom-up continuous improvement. Successful companies will run the two approaches in parallel.}
}
@article{POLHEMUS2020,
title = {Human-Centered Design Strategies for Device Selection in mHealth Programs: Development of a Novel Framework and Case Study},
journal = {JMIR mHealth and uHealth},
volume = {8},
number = {5},
year = {2020},
issn = {2291-5222},
doi = {https://doi.org/10.2196/16043},
url = {https://www.sciencedirect.com/science/article/pii/S2291522220003046},
author = {Ashley Marie Polhemus and Jan Novák and Jose Ferrao and Sara Simblett and Marta Radaelli and Patrick Locatelli and Faith Matcham and Maximilian Kerz and Janice Weyer and Patrick Burke and Vincy Huang and Marissa Fallon Dockendorf and Gergely Temesi and Til Wykes and Giancarlo Comi and Inez Myin-Germeys and Amos Folarin and Richard Dobson and Nikolay V Manyakov and Vaibhav A Narayan and Matthew Hotopf},
keywords = {human-centric design, design thinking, patient centricity, device selection, technology selection, remote patient monitoring, remote measurement technologies},
abstract = {Background
Despite the increasing use of remote measurement technologies (RMT) such as wearables or biosensors in health care programs, challenges associated with selecting and implementing these technologies persist. Many health care programs that use RMT rely on commercially available, “off-the-shelf” devices to collect patient data. However, validation of these devices is sparse, the technology landscape is constantly changing, relative benefits between device options are often unclear, and research on patient and health care provider preferences is often lacking.
Objective
To address these common challenges, we propose a novel device selection framework extrapolated from human-centered design principles, which are commonly used in de novo digital health product design. We then present a case study in which we used the framework to identify, test, select, and implement off-the-shelf devices for the Remote Assessment of Disease and Relapse-Central Nervous System (RADAR-CNS) consortium, a research program using RMT to study central nervous system disease progression.
Methods
The RADAR-CNS device selection framework describes a human-centered approach to device selection for mobile health programs. The framework guides study designers through stakeholder engagement, technology landscaping, rapid proof of concept testing, and creative problem solving to develop device selection criteria and a robust implementation strategy. It also describes a method for considering compromises when tensions between stakeholder needs occur.
Results
The framework successfully guided device selection for the RADAR-CNS study on relapse in multiple sclerosis. In the initial stage, we engaged a multidisciplinary team of patients, health care professionals, researchers, and technologists to identify our primary device-related goals. We desired regular home-based measurements of gait, balance, fatigue, heart rate, and sleep over the course of the study. However, devices and measurement methods had to be user friendly, secure, and able to produce high quality data. In the second stage, we iteratively refined our strategy and selected devices based on technological and regulatory constraints, user feedback, and research goals. At several points, we used this method to devise compromises that addressed conflicting stakeholder needs. We then implemented a feedback mechanism into the study to gather lessons about devices to improve future versions of the RADAR-CNS program.
Conclusions
The RADAR device selection framework provides a structured yet flexible approach to device selection for health care programs and can be used to systematically approach complex decisions that require teams to consider patient experiences alongside scientific priorities and logistical, technical, or regulatory constraints.}
}
@incollection{BAREISS1993157,
title = {The Evolution of a Case-Based Computational Approach to Knowledge Representation, Classification, and Learning},
editor = {Glenn V. Nakamura and Roman Taraban and Douglas L. Medin},
series = {Psychology of Learning and Motivation},
publisher = {Academic Press},
volume = {29},
pages = {157-186},
year = {1993},
booktitle = {Categorization by Humans and Machines},
issn = {0079-7421},
doi = {https://doi.org/10.1016/S0079-7421(08)60139-5},
url = {https://www.sciencedirect.com/science/article/pii/S0079742108601395},
author = {Ray Bareiss and Brian M. Slator}
}
@article{SPARKES202515,
title = {Quantum-inspired algorithm could give us better weather forecasts},
journal = {New Scientist},
volume = {265},
number = {3529},
pages = {15},
year = {2025},
issn = {0262-4079},
doi = {https://doi.org/10.1016/S0262-4079(25)00218-0},
url = {https://www.sciencedirect.com/science/article/pii/S0262407925002180},
author = {Matthew Sparkes}
}
@article{DAVIS2024307,
title = {AI rising in higher education: opportunities, risks and limitations},
journal = {Asian Education and Development Studies},
volume = {13},
number = {4},
pages = {307-319},
year = {2024},
issn = {2046-3162},
doi = {https://doi.org/10.1108/AEDS-01-2024-0017},
url = {https://www.sciencedirect.com/science/article/pii/S2046316224000154},
author = {Adrian John Davis},
keywords = {Human mind, Human intelligence, Human consciousness, Artificial intelligence (AI), Artificial consciousness, Quality teaching},
abstract = {Purpose
The aim of this paper is twofold: to explore the significance and implications of the rise of AI technology for the field of tertiary education in general and, in particular, to answer the question of whether teachers can be replaced by intelligent AI systems such as androids, what that requires in terms of human capabilities and what that might mean for teaching and learning in higher education.
Design/methodology/approach
Given the interdisciplinary nature of this conceptual paper, a literature review serves as a methodological tool to access data pertaining to the research question posed in the paper.
Findings
This exploratory paper gathers a range of evidence from the philosophy of mind (the mind-body problem), Kahneman’s (2011) System 1 and System 2 models of the mind, Gödel’s (1951) Two Incompleteness Theorems, Polanyi’s (1958, 1966) theory of tacit knowing and Searle’s (1980) Chinese Room thought experiment to the effect that no AI system can ever fully replace a human being because no machine can replicate the human mind and its capacity for intelligence, consciousness and highly developed social skills such as empathy and cooperation.
Practical implications
AI is rising, but there are inherent limits to what machines can achieve when compared to human capabilities. An android can at most attain “weak AI”, that is, it can be smart but lack awareness or empathy. Therefore, an analysis of good teaching at the tertiary level shows that learning, knowledge and understanding go far beyond any quantitative processing that an AI machine does so well, helping us to appreciate the qualitative dimension of education and knowledge acquisition. ChatGPT is robotic, being AI-generated, but human beings thrive on the human-to-human interface – that is, human relationships and meaningful connections – and that is where the true qualitative value of educational attainment will be gauged.
Social implications
This paper has provided evidence that human beings are irreplaceable due to our unique strengths as meaning-makers and relationship-builders, our capacity for morality and empathy, our creativity, our expertise and adaptability and our capacity to build unity and cooperate in building social structures and civilization for the benefit of all. Furthermore, as society is radically automated, the purpose of human life and its reevaluation will also come into question. For instance, as more and more occupations are replaced by ChatGPT services, more and more people will be freed up to do other things with their time, such as caring for relatives, undertaking creative projects, studying further and having children.
Originality/value
The investigation of the scope and limitations of AI is significant for two reasons. First, the question of the nature and functions of a mind becomes critical to the possibility of replication because if the human mind is like a super-sophisticated computer, then the relationship between a brain and mind is similar (if not identical) to the relationship between a computer as machine hardware and its programme or software (Dreyfus, 1979). [ ] If so, it should be theoretically possible to understand its mechanism and reproduce it, and then it is just a matter of time before AI research and development can replicate the human mind and eventually replace a human teacher, especially if an AI machine can teach just as intelligently yet more efficiently and economically. But if AI has inherent limitations that preclude the possibility of ever having a human-like mind and thought processes, then our investigation can at least clarify in what ways AI/AGI – such as ChatGPT – could support teaching and learning at universities.}
}
@article{MOHAN2020771,
title = {Spread Spectrum Hop Count analyzing technique based code-division multiple access for data frequencies examining in wireless network},
journal = {Computer Communications},
volume = {150},
pages = {771-776},
year = {2020},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2019.12.010},
url = {https://www.sciencedirect.com/science/article/pii/S0140366419304980},
author = {N. Mohan},
keywords = {Data transfer, Quality improvement, Spread Spectrum, Hop Count, CDMA},
abstract = {Code-division multiple access (CDMA) is a bandwidth access technique used by different radio waves and signal advancements. CDMA is a way of providing multiple access, where transmitters can send data at the same time, where a single clock channel can be completed. It enables us to share data frequencies with a few systems (refer to the data transfer and capacity). From the multiple backward spaces allow this CDMA uses a wide range of novelty innovation and exceptional coding scheme (where each transmitter is allocated code). In this work, the different application of the Spread Spectrum Hop Count Analyzing Technique (SSHCA–CDMA) is presented which organizes information testing techniques to create accessible assessment data, with the ultimate goal of providing the most efficient techniques for execution improvement thinking. The underlying area of eligibility testing and the evaluation of metadata inquiry are the expectation space, the data that select the most effective regulatory function. Similarly, in this work, master-based techniques have been demonstrated to validate and analyze SSHCA cells. Long, most recent developments have retained a perspective and the nature of customer correspondence management.}
}
@article{LI2025124674,
title = {Aerodynamic analysis and hygrothermal transfer characteristics of cellulose evaporative cooling pads: A case study applied to protected agriculture},
journal = {Applied Thermal Engineering},
volume = {258},
pages = {124674},
year = {2025},
issn = {1359-4311},
doi = {https://doi.org/10.1016/j.applthermaleng.2024.124674},
url = {https://www.sciencedirect.com/science/article/pii/S1359431124023421},
author = {He Li and Chengji Zong and Jiarui Lu and Shumei Zhao and Weitang Song and Dongyan Yang},
keywords = {Direct evaporative cooling, CFD, Saturation efficiency, Pressure drop, Energy saving, Wind tunnel experiment},
abstract = {Evaporative cooling pads are clean media for cost-effective temperature management. However, the spatially integrated microclimate resulting from heat and mass transfer in the evaporative cooling pad is an uncertain and nonlinear complexity. Therefore, this purpose of this study is to present an innovative prediction model with computational fluid dynamics and evaluate the operating scenarios and geometrical properties of wet pads from quantitative metrics such as saturation efficiency and pressure drop. The reliability of the established numerical model of wet pads was verified by wind tunnel experiments and existing experiments, which predicted the outlet conditions in satisfactory conformity. The results showed that a wet pad with 8 mm ripple height and 19.6 mm ripple distance exhibits the best prospective, with energy consumption and specific water consumption reduced by 45.28 % and 26.26 %, respectively. Meanwhile, the cross strategy of 45_45° corrugation obliquity reduces the pressure drop by 18.95 %, providing uniform supply air distributions. The heat exchange of the wet pad is mainly concentrated within the first 35 mm from the inlet section, while the mass exchange of the wet pad is concentrated within the first 60 mm. The range of frontal air velocity with 0.9–2.5 m/s is recommend for the evaporative cooling system.}
}
@article{ALIJAH2007193,
title = {On the N3O2- paradigm},
journal = {Journal of Molecular Structure},
volume = {844-845},
pages = {193-199},
year = {2007},
note = {STUDIES IN HYDROGEN-BONDED SYSTEMS – A collection of Invited Papers in honour of Professor Lucjan Sobcyk, on the occasion of his 80th Birthday},
issn = {0022-2860},
doi = {https://doi.org/10.1016/j.molstruc.2007.04.024},
url = {https://www.sciencedirect.com/science/article/pii/S0022286007003316},
author = {Alexander Alijah and Eugene S. Kryachko},
keywords = {, Theoretical calculations, Isomers, Electron detachment},
abstract = {A survey of the existing experimental and theoretical data on the trinitrogen dioxide anion N3O2- that manifests a controversy as to the number of isomers and their chemical structures is presented. To resolve the controversy, new computational studies are performed at the MP2/aug-cc-pVTZ computational level. Two hitherto unknown isomers are predicted, one with singlet and one with triplet spin multiplicity. The singlet isomer, structurally characterized as N2·[ONO]−, is the most stable among all known isomers and accounts for fragmentation patterns observed in the recent dissociative photodetachment experiments.}
}
@article{ONKAL2013772,
title = {Scenarios as channels of forecast advice},
journal = {Technological Forecasting and Social Change},
volume = {80},
number = {4},
pages = {772-788},
year = {2013},
note = {Scenario Method: Current developments in theory and practice},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2012.08.015},
url = {https://www.sciencedirect.com/science/article/pii/S0040162512002090},
author = {Dilek Önkal and Kadire Zeynep Sayım and Mustafa Sinan Gönül},
keywords = {Forecast, Scenario, Group, Judgment, Advice taking},
abstract = {Today's business environment provides tougher competition than ever before, stressing the important role played by information and forecasts in decision-making. The scenario method has been popular for focused organizational learning, decision making and strategic thinking in business contexts, and yet, its use in communicating forecast information and advice has received little research attention. This is surprising since scenarios may provide valuable tools for communication between forecast providers and users in organizations, offering efficient platforms for information exchange via structured storylines of plausible futures. In this paper, we aim to explore the effectiveness of using scenarios as channels of forecast advice. An experimental study is designed to investigate the effects of providing scenarios as forecast advice on individual and group-based judgmental predictions. Participants are given time series information and model forecasts, along with (i) best-case, (ii) worst-case, (iii) both, or (iv) no scenarios. Different forecasting formats are used (i.e., point forecast, best-case forecast, worst-case forecast, and surprise probability), and both individual predictions and consensus forecasts are requested. Forecasts made with and without scenarios are compared for each of these formats to explore the potential effects of providing scenarios as forecast advice. In addition, group effects are investigated via comparisons of composite versus consensus predictions. The paper concludes with a discussion of results and implications for future research on scenario use in forecasting.}
}
@incollection{BERNINGER2002273,
title = {Chapter 10 - Building a Computing Brain Pedagogically},
editor = {Virginia W. Berninger and Todd L. Richards},
booktitle = {Brain Literacy for Educators and Psychologists},
publisher = {Academic Press},
address = {San Diego},
pages = {273-294},
year = {2002},
series = {Practical Resources for the Mental Health Professional},
issn = {18730450},
doi = {https://doi.org/10.1016/B978-012092871-2/50011-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780120928712500115},
author = {Virginia W. Berninger and Todd L. Richards},
abstract = {Publisher Summary
This chapter focuses on building a computing brain pedagogically. The brain, as it interacts with the world, constructs more concepts that are represented as mental models in distributed neural networks. Both the quantitative dimension and logical structures contribute to how these mental models are constructed and represented in the brain. The brain draws on both inductive thinking. During the construction process, the brain also uses multiple codes to represent and understand this emerging conceptual domain. The hand that is instrumental in development of the written language system also plays a major role in development of conceptual representations of the world. Working memory also plays an important role in conceptual development in the math domain. Like the writing brain, the computing brain also develops from both play and conscious work. The chapters conclude that development of the computing brain requires guided assistance in translating implicit knowledge based on experience into explicit knowledge that can be used for the hard work of math problem solving, which is conducted in resource-limited, temporally constrained working memory.}
}
@article{FANG2025101300,
title = {Generative AI-enhanced human-AI collaborative conceptual design: A systematic literature review},
journal = {Design Studies},
volume = {97},
pages = {101300},
year = {2025},
issn = {0142-694X},
doi = {https://doi.org/10.1016/j.destud.2025.101300},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X25000122},
author = {Cong Fang and Yujie Zhu and Le Fang and Yonghao Long and Huan Lin and Yangfan Cong and Stephen Jia Wang},
keywords = {Human-AI collaboration, AI-enhanced design, Design methodology, Design process, Conceptual design},
abstract = {Generative Artificial Intelligence (GenAI) has gained increasing attention, enhancing design productivity by elevating creativity within the conceptual design process. Despite these advancements, how GenAI will influence the conceptual design process and methods remains ambiguous, hindering its full potential. This study introduces a systematic literature review to explore GenAI's role in the conceptual design process, emphasizing the GenAI-human interactions and collaborations. We offer a critical evaluation of the current state of GenAI-human collaboration, identifying challenges, opportunities, and future research directions to leverage GenAI's design potential for enhancing creativity in conceptual design practice. Finally, a Generative AI Enhanced Conceptual Design framework was further proposed to clarify the potential collaborative design process, which can serve as a guideline for effective human-AI collaboration in the conceptual design process.}
}
@article{MUTH1992278,
title = {Extraneous information and extra steps in arithmetic word problems},
journal = {Contemporary Educational Psychology},
volume = {17},
number = {3},
pages = {278-285},
year = {1992},
issn = {0361-476X},
doi = {https://doi.org/10.1016/0361-476X(92)90066-8},
url = {https://www.sciencedirect.com/science/article/pii/0361476X92900668},
author = {K.Denise Muth},
abstract = {To determine how middle school students cope with some of the demands imposed on them by arithmetic word problems, 140 eighth graders were asked to solve word problems modeled after those used by the National Assessment of Educational Progress. Processing demands were imposed on the students by adding extraneous information and extra steps to the problems. Results indicated that the presence of extraneous information and extra steps reduced the accuracy of students' solutions. Thinking-out-loud protocols also revealed several misconceptions that students have about solving word problems.}
}
@article{JYOTSNA20231270,
title = {IntelEye: An Intelligent Tool for the Detection of Stressful State based on Eye Gaze Data While Watching Video},
journal = {Procedia Computer Science},
volume = {218},
pages = {1270-1279},
year = {2023},
note = {International Conference on Machine Learning and Data Engineering},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.01.105},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923001059},
author = {C. Jyotsna and J. Amudha and Amritanshu Ram and Giandomenico Nollo},
keywords = {Eye Tracking, Mental Health, Real Time Monitoring, K-Nearest Neighbour, Eye Gaze Measures, Welch Two Sample t-test},
abstract = {Technology to monitor mental health is gaining popularity as it helps to improve the cognitive and behavioral performance of an individual. Considering the growing need to monitor mental health, there is subsequent research in continuous and real-time monitoring technologies that can increase the quality of life by reducing the cost of health care. Eye tracking technology has played a significant role in monitoring a person's mental health. An intelligent system can apply several computational procedures to extract meaningful information from the massive physiological data obtained from eye tracking. The proposed model IntelEye is a tool to detect the stressful states of an individual while watching calm and stressful videos. The eye gaze measures based on pupil diameter, fixation, and blink were used for detecting stressful conditions. The data was collected from hospital employees, and the K Nearest Neighbor algorithm could successfully recognize the stressful states and the corresponding gaze location during stressful situations. IntelEye is not only identifying the stressful states but also has the novelty of identifying the scene and gaze location, making them stressful while watching the video.}
}
@article{URSINO2015234,
title = {A neural network for learning the meaning of objects and words from a featural representation},
journal = {Neural Networks},
volume = {63},
pages = {234-253},
year = {2015},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2014.11.009},
url = {https://www.sciencedirect.com/science/article/pii/S0893608014002639},
author = {Mauro Ursino and Cristiano Cuppini and Elisa Magosso},
keywords = {Semantic memory, Lexical memory, Conceptual representation, Hebb rule, Dominant features, Category formation},
abstract = {The present work investigates how complex semantics can be extracted from the statistics of input features, using an attractor neural network. The study is focused on how feature dominance and feature distinctiveness can be naturally coded using Hebbian training, and how similarity among objects can be managed. The model includes a lexical network (which represents word-forms) and a semantic network composed of several areas: each area is topologically organized (similarity) and codes for a different feature. Synapses in the model are created using Hebb rules with different values for pre-synaptic and post-synaptic thresholds, producing patterns of asymmetrical synapses. This work uses a simple taxonomy of schematic objects (i.e., a vector of features), with shared features (to realize categories) and distinctive features (to have individual members) with different frequency of occurrence. The trained network can solve simple object recognition tasks and object naming tasks by maintaining a distinction between categories and their members, and providing a different role for dominant features vs. marginal features. Marginal features are not evoked in memory when thinking of objects, but they facilitate the reconstruction of objects when provided as input. Finally, the topological organization of features allows the recognition of objects with some modified features.}
}
@article{HARA20239703,
title = {Reorganizing Cyber-Physical Configurations using User Activities for Human-in-the-Loop Cyber-Physical Systems},
journal = {IFAC-PapersOnLine},
volume = {56},
number = {2},
pages = {9703-9708},
year = {2023},
note = {22nd IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2023.10.281},
url = {https://www.sciencedirect.com/science/article/pii/S240589632300633X},
author = {Tatsunori Hara and Yuki Okada and Jun Ota},
keywords = {Design, modelling and analysis of HMS, human-centered automation and design, cyber-physical system, design structure matrix, data utilization, product service system, smart home},
abstract = {This paper proposes a “human-in-the-loop design structure matrix (DSM)” method for understanding and reorganizing the structures of multiple cyber-physical systems (CPSs), focusing on user activities. By incorporating user activities as integral components in system engineering techniques, this method contributes to the design literature on human-in-the-loop CPS. Using the illustrative case of a smart home, we obtained the following types of clusters to review the structure and cyber-physical configurations of CPSs: clusters that retain the target user activity, clusters decoupled from the target user activity, and clusters across two different user activities. In the second type, we found a hub cluster regarding the cyber process of notifications to users, which enabled diverse data utilization among the clusters.}
}
@article{ZHENG2018214,
title = {An improved genetic approach for composing optimal collaborative learning groups},
journal = {Knowledge-Based Systems},
volume = {139},
pages = {214-225},
year = {2018},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2017.10.022},
url = {https://www.sciencedirect.com/science/article/pii/S0950705117304914},
author = {Yaqian Zheng and Chunrong Li and Shiyu Liu and Weigang Lu},
keywords = {Collaborative learning, Learner group formation, Genetic algorithm, Optimal solution},
abstract = {Collaborative learning is an effective strategy for promoting learning in both traditional face-to-face and online environments. When applying it, students should be assigned to best collaborative groups at the first step, which is called the learner group formation task. In previous studies, various approaches have been proposed to solve this problem. However, they failed to meet all the problem requirements. To address this problem, a generic group formation method that covers all aspects of the problem is proposed in this study. In this method, all requirements of the learner group formation problem are formulated into an integrated mathematical model and an improved genetic algorithm is proposed to solve the model and obtain optimal learning groups to meet various grouping requirements for different educational contexts. To analyse the performance of the proposed approach from a computational perspective, a series of computational experiments are conducted based on eight simulation datasets with different levels of complexity. The simulation results indicate that the proposed method is effective and stable for solving the learner group formation problem. An empirical study is also carried out to validate the proposed approach from a pedagogical view by comparing it with two traditional group formation strategies. The results show that groups formed through the proposed method produce better outcomes than others in terms of group grades, individual grades and student satisfaction.}
}
@article{SECCHI2024105891,
title = {Modeling and theorizing with agent-based sustainable development},
journal = {Environmental Modelling & Software},
volume = {171},
pages = {105891},
year = {2024},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2023.105891},
url = {https://www.sciencedirect.com/science/article/pii/S1364815223002773},
author = {D. Secchi and V. Grimm and D.B. Herath and F. Homberg},
keywords = {Sustainable development, Theory development, Human–environment interaction, Agent-based modeling, Common language, ODD protocol},
abstract = {Sustainable development is an expression that permeates large areas of knowledge. For it to be meaningful, environmental aspects must be considered as intertwined with economic and social aspects. This is a multidisciplinary effort that is made challenging by the task of synthesizing the many emerging contributions. This has limited theory development where the definition of mechanisms, assumptions, dynamics and the determination of the entities involved are largely left to the reader’s imagination. We suggest to engage with the rationale of agent-based modeling to better define the assumptions, mechanisms, and boundaries of sustainable development. For this, the O-part of the widely used ODD protocol for describing agent-based models (ABM) provides a standardized structure, which we here augment to OsDD to specifically take into account sustainability issues. Even without formulating and implementing the full ABM, using OsDD requires to be explicit about the mechanisms, assumptions, dynamics and the entities involved and thereby provides a common language for theory development.}
}
@article{NAIK2017509,
title = {Metastability in Senescence},
journal = {Trends in Cognitive Sciences},
volume = {21},
number = {7},
pages = {509-521},
year = {2017},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2017.04.007},
url = {https://www.sciencedirect.com/science/article/pii/S1364661317300797},
author = {Shruti Naik and Arpan Banerjee and Raju S. Bapi and Gustavo Deco and Dipanjan Roy},
keywords = {healthy aging, whole-brain computational modeling, metastability},
abstract = {The brain during healthy aging exhibits gradual deterioration of structure but maintains a high level of cognitive ability. These structural changes are often accompanied by reorganization of functional brain networks. Existing neurocognitive theories of aging have argued that such changes are either beneficial or detrimental. Despite numerous empirical investigations, the field lacks a coherent account of the dynamic processes that occur over our lifespan. Taking advantage of the recent developments in whole-brain computational modeling approaches, we hypothesize that the continuous process of aging can be explained by the concepts of metastability − a theoretical framework that gives a systematic account of the variability of the brain. This hypothesis can bridge the gap between existing theories and the empirical findings on age-related changes.}
}
@article{BENBOW19841643,
title = {Computational analysis of polymer processing: (Edited by J.R.A. Pearson and S.M. Richardson). Applied Science, 1983. £36.00. 343pp},
journal = {Chemical Engineering Science},
volume = {39},
number = {11},
pages = {1643},
year = {1984},
issn = {0009-2509},
doi = {https://doi.org/10.1016/0009-2509(84)80093-7},
url = {https://www.sciencedirect.com/science/article/pii/0009250984800937},
author = {J.J. Benbow}
}
@article{LU2025154,
title = {Simulation and classification optimization design of ultra-low temperature heat exchangers in dilution refrigerator using local thermal nonequilibrium model},
journal = {International Journal of Refrigeration},
volume = {174},
pages = {154-164},
year = {2025},
issn = {0140-7007},
doi = {https://doi.org/10.1016/j.ijrefrig.2025.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S0140700725000866},
author = {Yian Lu and Jun Shen and Ya'nan Zhao and Danyang Liu},
keywords = {Dilution refrigeration, Ultra-low temperature heat exchanger, Classification optimization design, Numerical simulation, Local thermal nonequilibrium},
abstract = {Dilution refrigeration is widely used in a variety of cutting-edge scientific fields such as quantum computations. The ultra-low temperature heat exchangers play a crucial role for the ultimate performance of dilution refrigerator. To study the classification optimization design of ultra-low temperature heat exchangers, an integrated local thermal nonequilibrium model have used. By considering the ultra-low temperature heat exchangers as an integrated unit consisting of a number of interconnected sub-modules, an optimized design study of the geometrical parameter configurations of continuous heat exchanger, three-stage step heat exchanger, and connecting tubes has been carried out. Using the optimized configuration for continuous heat exchanger with a length of 19 m, an inner tube diameter of 10 mm, and an outer tube diameter of 15 mm at a molar flow rate of 1.5 mmol/s, the classification optimization of step heat exchanger is carried out. By adjusting the ratio of the sintered volume to the whole sintered volume (g1=0.15, g2=0.3 and g3=0.55) and the ratio of concentrated heat transfer area to the total heat transfer area of this stage (f1=0.35, f2=0.4 and f3=0.45), the heat transfer efficiency of the three-stage step heat exchanger can be further improved without additional resources. And the impact of length and diameter of connecting tubes has also been discussed in this paper. This study provides quantitative guidance for the design of ultra-low temperature heat exchangers, which is important for improving the overall performance of dilution refrigerators.}
}
@article{GREENWOOD2021106597,
title = {Exploring a causal model in observational cohort data: The role of parents and peers in shaping substance use trajectories},
journal = {Addictive Behaviors},
volume = {112},
pages = {106597},
year = {2021},
issn = {0306-4603},
doi = {https://doi.org/10.1016/j.addbeh.2020.106597},
url = {https://www.sciencedirect.com/science/article/pii/S0306460320307279},
author = {C.J. Greenwood and G.J. Youssef and P. Letcher and E.A. Spry and K.C. Thomson and L.J. Hagg and D.M. Hutchinson and J.A. Macdonald and J. McIntosh and A. Sanson and J.W. Toumbourou and C.A. Olsson},
keywords = {Causal modeling, substance use, Adolescence, Young adulthood, Trajectory, Parents, Peers},
abstract = {Aims
To explore the process of applying counterfactual thinking in examining causal determinants of substance use trajectories in observational cohort data. Specifically, we examine the extent to which quality of the parent-adolescent relationship and affiliations with deviant peers are causally related to trajectories of alcohol, tobacco, and cannabis use across adolescence and into young adulthood.
Methods
Data were drawn from the Australian Temperament Project, a population-based cohort study that has followed a sample of young Australians from infancy to adulthood since 1983. Parent-adolescent relationship quality and deviant peer affiliations were assessed at age 13–14 years. Latent curve models were fitted for past month alcohol, tobacco, and cannabis use (n = 1590) from age 15–16 to 27–28 years (5 waves). Confounding factors were selected in line with the counterfactual framework.
Results
Following confounder adjustment, higher quality parent-adolescent relationships were associated with lower baseline cannabis use, but not alcohol or tobacco use trajectories. In contrast, affiliations with deviant peers were associated with higher baseline binge drinking, tobacco, and cannabis use, and an earlier peak in the cannabis use trajectory.
Conclusions
Despite careful application of the counterfactual framework, interpretation of associations as causal is not without limitations. Nevertheless, findings suggested causal effects of both parent-adolescent relationships and deviant peer affiliations on the trajectory of substance use. Causal effects were more pervasive (i.e., more substance types) and protracted for deviant peer affiliations. The exploration of causal relationships in observational cohort data is encouraged, when relevant limitations are transparently acknowledged.}
}
@article{ROY2022105849,
title = {EEG based stress analysis using rhythm specific spectral feature for video game play},
journal = {Computers in Biology and Medicine},
volume = {148},
pages = {105849},
year = {2022},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2022.105849},
url = {https://www.sciencedirect.com/science/article/pii/S0010482522006023},
author = {Shidhartho Roy and Monira Islam and Md. Salah Uddin Yusuf and Nushrat Jahan},
keywords = {Beta–Alpha ratio, EEG, Stress-relaxation modeling, Topography, Video gameplay},
abstract = {Background and objective:
For the emerging significance of mental stress, various research directives have been established over time to understand better the causes of stress and how to deal with it. In recent years, the rise of video gameplay has been unprecedented, further triggered by the lockdown imposed due to the COVID-19 pandemic. Several researchers and organizations have contributed to the practical analysis of the impacts of such extended periods of gameplay, which lacks coordinated studies to underline the outcomes and reflect those in future game designing and public awareness about video gameplay. Investigations have mainly focused on the “gameplay stress” based on physical syndromes. Some studies have analyzed the effects of video gameplay with Electroencephalogram (EEG), Magnetic resonance imaging (MRI), etc., without concentrating on the relaxation procedure after video gameplay.
Methods:
This paper presents an end-to-end stress analysis for video gaming stimuli using EEG. The power spectral density (PSD) of the Alpha and Beta bands is computed to calculate the Beta-to-Alpha ratio (BAR). The Alpha and Beta band power is computed, and the Beta-to-Alpha band power ratio (BAR) has been determined. In this article, BAR is used to denote mental stress. Subjects are chosen based on various factors such as gender, gameplay experience, age, and Body mass index (BMI). EEG is recorded using Scan SynAmps2 Express equipment. There are three types of video gameplay: strategic, puzzle, and combinational. Relaxation is accomplished in this study by using music of various pitches. Two types of regression analysis are done to mathematically model stress and relaxation curve. Brain topography is rendered to indicate the stressed and relaxed region of the brain.
Results:
In the relaxed state, the subjects have BAR 0.701, which is considered the baseline value. Non-gamer subjects have an average BAR of 2.403 for 1 h of strategic video gameplay, whereas gamers have 2.218 BAR concurrently. After 12 minutes of listening to low-pitch music, gamers achieved 0.709 BAR, which is nearly the baseline value. In comparison to Quartic regression, the 4PL symmetrical sigmoid function performs regression analysis with fewer parameters and computational power.
Conclusion:
Non-gamers experience more stress than gamers, whereas strategic games stress the human brain more. During gameplay, the beta band in the frontal region is mostly activated. For relaxation, low pitch music is the most useful medium. Residual stress is evident in the frontal lobe when the subjects have listened to high pitch music. Quartic regression and 4PL symmetrical sigmoid function have been employed to find the model parameters of the relaxation curve. Among them, quartic regression performs better in terms of Akaike information criterion (AIC) and R2 measure.}
}
@article{FALCONE2020110269,
title = {Soft computing techniques in structural and earthquake engineering: a literature review},
journal = {Engineering Structures},
volume = {207},
pages = {110269},
year = {2020},
issn = {0141-0296},
doi = {https://doi.org/10.1016/j.engstruct.2020.110269},
url = {https://www.sciencedirect.com/science/article/pii/S0141029619322540},
author = {Roberto Falcone and Carmine Lima and Enzo Martinelli},
keywords = {Structural engineering, Earthquake engineering, Fuzzy logic, Neural network, Swarm intelligence, Evolutionary computing},
abstract = {Although civil engineering problems are often characterized by significant levels of complexity, they are generally approached and solved by combining several practitioners’ skills, such as intuition, past experience, logical reasoning, mathematical elaborations, and physical sense. This is also the case of problems in structural and earthquake engineering whose solution is generally based on the so-called “engineer’s judgment”. However, heuristic theories and algorithms within the framework of “soft computing” can provide a more rational and systematic way to approach and solve problems in these areas. As a matter of fact, the aforementioned algorithms have been recently utilized in several branches of engineering and applied sciences. This paper proposes a state-of-the-art review of the main applications of soft computing techniques to relevant structural and earthquake engineering problems. Specifically, the applications of fuzzy computing, evolutionary computing, swarm intelligence, and neural networks, as well as their hybrid combinations, are analyzed with the aim to examine their capability and limitations in modeling, simulation, and optimization problems.}
}
@article{MA2024109594,
title = {Robust adaptive learning framework for semi-supervised pattern classification},
journal = {Signal Processing},
volume = {224},
pages = {109594},
year = {2024},
issn = {0165-1684},
doi = {https://doi.org/10.1016/j.sigpro.2024.109594},
url = {https://www.sciencedirect.com/science/article/pii/S0165168424002135},
author = {Jun Ma and Guolin Yu},
keywords = {Non-convex distance metric, Semi-supervised robust classification, Generalized adaptive robust loss function, Outliers, Kernel method},
abstract = {Hessian scatter regularized twin support vector machine (HSR-TSVM) employs hinge loss function and L2-distance metric, which makes it ineffective in dealing with outliers and noise data problems. Aiming to this problem, this paper a novel robust adaptive learning framework CL2,pHSR-TSVM is developed for semi-supervised classification tasks. In CL2,pHSR-TSVM, the generalized adaptive robust loss function Lδ(u) is first innovatively introduced to overcome the problem that hinge loss function is not sensitive to noise and outliers. Intuitively, Lδ(u) can improve the robustness of the model by selecting different robust loss functions for different learning tasks during the learning process via the adaptive parameter δ. Secondly, the robust distance metric capped L2,p-norm is introduced in CL2,pHSR-TSVM to reduce and eliminate the exaggerated influence of L2-distance metric on the learning process of outliers, especially when the outliers are far from the normal data distribution, by setting the appropriate parameters. Furthermore, to improve the computational efficiency of CL2,pHSR-TSVM, the fast CL2,pHSR-TSVM is presented for semi-supervised classification tasks. Finally, two effective algorithms are designed to solve our methods respectively, and the convergence and computational complexity are analyzed theoretically. Experimental results demonstrate the effectiveness and robustness of our methods.}
}
@article{JIA2011445,
title = {Evolutionary level set method for structural topology optimization},
journal = {Computers & Structures},
volume = {89},
number = {5},
pages = {445-454},
year = {2011},
issn = {0045-7949},
doi = {https://doi.org/10.1016/j.compstruc.2010.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S0045794910002567},
author = {Haipeng Jia and H.G. Beom and Yuxin Wang and Song Lin and Bo Liu},
keywords = {Evolutionary structure optimization, Structure topology optimization, Intelligent computation, Level set method},
abstract = {This paper proposes an evolutionary accelerated computational level set algorithm for structure topology optimization. It integrates the merits of evolutionary structure optimization (ESO) and level set method (LSM). Traditional LSM algorithm is largely dependent on the initial guess topology. The proposed method combines the merits of ESO techniques with those of LSM algorithm, while allowing new holes to be automatically generated in low strain energy within the nodal neighboring region during optimization. The validity and robustness of the new algorithm are supported by some widely used benchmark examples in topology optimization. Numerical computations show that optimization convergence is accelerated effectively.}
}
@incollection{SEN201629,
title = {3 - History of zero including its representation and role},
editor = {Syamal K. Sen and Ravi P. Agarwal},
booktitle = {Zero},
publisher = {Academic Press},
pages = {29-75},
year = {2016},
isbn = {978-0-08-100774-7},
doi = {https://doi.org/10.1016/B978-0-08-100774-7.00003-X},
url = {https://www.sciencedirect.com/science/article/pii/B978008100774700003X},
author = {Syamal K. Sen and Ravi P. Agarwal},
keywords = {Algorithms for arithmetic operations, alphabetical positional number system, assumption versus axioms, avoidance of subtraction, Brahmagupta’s rule to compute with zero, building block of matter, direction separator, driver of calculus, dwarf and machine epsilon, exponential growth of computing power, Godel’s incompleteness theorem, Gregorian calendar, history of zero, image of the earth, infinite versus finite precisions, infinitive universe, Maya numbers and long count, mean value theorem, Mohanjodaro and Harappa civilization, most pervasive global symbol, object of zero dimension, Quipu, representation of nothingness, Rolle’s theorem, sexagesimal (base 60) positional number system, stone/copper plate inscription, Vedas and Puranas, violation of a law of nature, Zeno’s paradoxes, zero as a number, zero as a vacant position, zero-free system, zero with its eternal spiritual significance},
abstract = {The chronological development of the history of zero over the centuries is a tough job due to both poor man to man communication and also poor publication machinery. However, the time period 7000 BC–2015 AD is broadly divided into four parts based on the landmark innovations in each part. During 7000–2000 BC, the most important contribution, that is, the modern decimal based place value system with 0 as a number due to Aryabhatta was developed and used. The Maya numbers and Long Count days that were tallied in a modified radix-20 number system are notable. Zero with representation and arithmetic operations was fully developed during 2000 BC–1000 AD. Brahmagupta’s rules for arithmetic operations were developed. The Romans and the Greeks had no zero then and their system was order-valued. Egyptian numerals were base-10 while Babylonian mathematics had a base-60 positional number system. With better understanding of zero, calculus was born. Arab and Persian mathematicians were active and became an important interface between the east and the west in promoting number systems with arithmetic. The period 1000–1900 AD saw the introduction of the Hindu–Arabic numeral system in Europe. The link between the system and European mathematics is the Italian mathematician Fibonacci. During the late eleventh century AD, Shen Gua introduced infinitesimal and exhaustion. He described piling up very small things. During 1900–2015 AD, increasingly high-speed modern digital computing made its presence felt very intensely globally by one and all. Specifically due to its finite precision, unlike the infinite precision which the regular and natural mathematics have, the advent of numerical zero, as opposed to the exact zero, changed the face of all real-world computation. Understanding natural, regular, and computational mathematics with calculus, and specifically the role of zero, became extraordinarily important in all engineering computations. The computational error (implying quality of solution) and computational complexity (cost of computation) due to the presence of numerical zero became integral parts of any algorithm to justify the acceptability of a solution. During the early twentieth century, Ramanujan, to whom each number is a living being and his personal friend carrying an important distinct message, felt intensely the eternal spiritual significance of zero and its inverse infinity. He built a theory of reality around zero and infinity.}
}
@article{ARNOLD2018581,
title = {Combining conscious and unconscious knowledge within human-machine-interfaces to foster sustainability with decision-making concerning production processes},
journal = {Journal of Cleaner Production},
volume = {179},
pages = {581-592},
year = {2018},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2018.01.070},
url = {https://www.sciencedirect.com/science/article/pii/S0959652618300787},
author = {Marlen Gabriele Arnold},
keywords = {Exploratory design, Structural systemic constellations, Cognitive human biases, HMI, Sustainable production contexts},
abstract = {At present, sustainability science is mainly based on conscious information and strongly focused on analytical tools or strategies. Neuroscience has made obvious that human decisions are prepared by the unconsciousness. Intuition plays an important role in early and late stages of learning processes and has a crucial impact on decision-making. Thus, intuitive and unconscious thinking is crucial for management processes in general and production planning processes in the main. However, unconscious knowledge and human behaviour is predominantly neglected in production research. Especially the addressing of human machine interfaces (HMI), human cognitive biases have a crucial impact on decision making processes. Constellation work is based on unconscious knowledge and intuition. Thus, systemic structural constellations are an innovative tool to integrate unconscious knowledge in a research context. In systemic structural constellations specific foci of complex systems, such as a production system, can be simulated and represented through spatial arrangements of persons or symbols. So, the method was used to reveal relevant patterns of relationships, structures, interaction, implicit knowledge, including hidden or underlying dynamics and influences that are relevant to and within a production system to understand how the raised problems in HMI can be better solved. The guiding research question is: How can the use of structural systemic constellations improve decision-making processes in HMI contexts in production environments in order to increase sustainability? Results show sustainability seems to be a matter of consciousness and is closely linked to the bias group not enough meaning. Sustainability and complexity resemble more than being linked by trade-offs. The recognition of human biases can be trained to improve human-machine-interfaces and sustainability. Constellation work contributes to decision theory by supporting effectuation.}
}
@article{LOPEZPERSEM2023273,
title = {Conceptual promises and mechanistic challenges of the creative metacognition framework: Comment on “A systematic framework of creative metacognition” by Izabela Lebuda and Mathias Benedek},
journal = {Physics of Life Reviews},
volume = {47},
pages = {273-275},
year = {2023},
issn = {1571-0645},
doi = {https://doi.org/10.1016/j.plrev.2023.11.007},
url = {https://www.sciencedirect.com/science/article/pii/S1571064523001860},
author = {Alizée Lopez-Persem and Marion Rouault and Emmanuelle Volle}
}
@article{SURESHBABU2006277,
title = {Modeling and simulation in signal transduction pathways: a systems biology approach},
journal = {Biochimie},
volume = {88},
number = {3},
pages = {277-283},
year = {2006},
issn = {0300-9084},
doi = {https://doi.org/10.1016/j.biochi.2005.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S0300908405001999},
author = {C.V. {Suresh Babu} and Eun {Joo Song} and Young Sook Yoo},
keywords = {Biological systems, Signal transduction, Systems biology, Modeling and simulation},
abstract = {Modeling, the heart of systems biology, of complex processes (example: signal transduction) is a wide scientific discipline where many approaches from different areas are confronted with the aim of better understanding, identifying and modeling of complex data coming from various sources. The purpose of this paper is to introduce the basic steps of systems biology view towards signaling pathways, which mainly deals with the computational tools. The paper emphasizes the modeling and simulation approach in the signal transduction pathways using the topologies of the biochemical reactions with an overview of the different types of software platforms. Finally, we demonstrated the epidermal growth factor receptor signaling pathway model as an example to study the growth factor mediated signaling system with biological experiments. This paper will enables new comers to underline the strengths of the computational approaches towards signal transduction, as well as to highlight the systems biology research directions.}
}
@incollection{GOLDSCHMIDT201146,
title = {Architecture},
editor = {Mark A. Runco and Steven R. Pritzker},
booktitle = {Encyclopedia of Creativity (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {San Diego},
pages = {46-51},
year = {2011},
isbn = {978-0-12-375038-9},
doi = {https://doi.org/10.1016/B978-0-12-375038-9.00010-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780123750389000108},
author = {G. Goldschmidt},
keywords = {Architectural design, Architectural education, Culture, Digital design, Form, Function, Ideas, Leading idea, Starchitect, Style},
abstract = {Architecture is a cultural arena based on ideas, which communally produce styles and individually, at their best, generate outstanding buildings. Every building tackles form and function. In our era architecture is expected to innovate in its forms, while ensuring perfect functionality. Form and function handling are rough correlates of originality and practicality, by which we measure design creativity. Architecture is also a product of the technological state of its time. At present we experience computational advances that promise to fundamentally change buildings and the manner in which they are designed. Architectural education is groping to adjust to the changes.}
}
@article{LIU2025103366,
title = {LoViT: Long Video Transformer for surgical phase recognition},
journal = {Medical Image Analysis},
volume = {99},
pages = {103366},
year = {2025},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2024.103366},
url = {https://www.sciencedirect.com/science/article/pii/S1361841524002913},
author = {Yang Liu and Maxence Boels and Luis C. Garcia-Peraza-Herrera and Tom Vercauteren and Prokar Dasgupta and Alejandro Granados and Sébastien Ourselin},
keywords = {Surgical phase recognition, Long videos, Temporally-rich spatial feature, Multi-scale, Phase transition-aware},
abstract = {Online surgical phase recognition plays a significant role towards building contextual tools that could quantify performance and oversee the execution of surgical workflows. Current approaches are limited since they train spatial feature extractors using frame-level supervision that could lead to incorrect predictions due to similar frames appearing at different phases, and poorly fuse local and global features due to computational constraints which can affect the analysis of long videos commonly encountered in surgical interventions. In this paper, we present a two-stage method, called Long Video Transformer (LoViT), emphasizing the development of a temporally-rich spatial feature extractor and a phase transition map. The temporally-rich spatial feature extractor is designed to capture critical temporal information within the surgical video frames. The phase transition map provides essential insights into the dynamic transitions between different surgical phases. LoViT combines these innovations with a multiscale temporal aggregator consisting of two cascaded L-Trans modules based on self-attention, followed by a G-Informer module based on ProbSparse self-attention for processing global temporal information. The multi-scale temporal head then leverages the temporally-rich spatial features and phase transition map to classify surgical phases using phase transition-aware supervision. Our approach outperforms state-of-the-art methods on the Cholec80 and AutoLaparo datasets consistently. Compared to Trans-SVNet, LoViT achieves a 2.4 pp (percentage point) improvement in video-level accuracy on Cholec80 and a 3.1 pp improvement on AutoLaparo. Our results demonstrate the effectiveness of our approach in achieving state-of-the-art performance of surgical phase recognition on two datasets of different surgical procedures and temporal sequencing characteristics. The project page is available at https://github.com/MRUIL/LoViT.}
}
@incollection{WOOLLISCROFT2020153,
title = {Chapter 12 - Precision medicine},
editor = {James O. Woolliscroft},
booktitle = {Implementing Biomedical Innovations into Health, Education, and Practice},
publisher = {Academic Press},
pages = {153-167},
year = {2020},
isbn = {978-0-12-819620-5},
doi = {https://doi.org/10.1016/B978-0-12-819620-5.00012-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128196205000126},
author = {James O. Woolliscroft},
keywords = {Precision medicine, Environment, Behavior, Microbiome, Genome, Pharmacogenomics},
abstract = {The convergence of computational, technologic and biomedical advances has enabled the development of precision medicine. Growing out of an understanding that there is a need for a new taxonomy of disease, the vision for precision medicine is to better understand the complex relationships in health and disease through the assemblage of massive databases that include individuals’ genomes, microbiomes, exposomes (a subsection of the environment), epigenomes, physiologic data, signs and symptoms, and other relevant information. Through the development of a holistic picture of genomic, microbiota, environmental and behavioral factors leading to disease, the intent is to intervene before disease becomes manifest to maintain or restore to health. Precision medicine will drive not only disruptive changes in the practice of clinical medicine, but also changes in our very conceptualization of health and disease.}
}
@article{CAPONNETTO2021104823,
title = {Examining nursing student academic outcomes: A forty-year systematic review and meta-analysis},
journal = {Nurse Education Today},
volume = {100},
pages = {104823},
year = {2021},
issn = {0260-6917},
doi = {https://doi.org/10.1016/j.nedt.2021.104823},
url = {https://www.sciencedirect.com/science/article/pii/S0260691721000800},
author = {Valeria Caponnetto and Angelo Dante and Vittorio Masotta and Carmen {La Cerra} and Cristina Petrucci and Celeste Marie Alfes and Loreto Lancia},
keywords = {Academic failure, Academic success, Attrition, Bachelor's degree, Determinants, Factors, Nursing student},
abstract = {Objectives
To synthesize the definitions of nursing students' academic outcomes and provide a quantitative synthesis of their associated and predictive factors.
Design
Systematic review and meta-analysis.
Data sources
Four scientific databases were searched until January 2020.
Review methods
Observational studies describing undergraduate nursing students' academic outcomes were included. Studies were analytically synthesized and meta-analyses were performed utilizing the Odds Ratio or Cohen's d as effect sizes.
Results
Eighteen studies, published from 1979 to 2018, were included in the review, nine were meta-analyzed. Studies involved 10,024 undergraduate nursing students and were mostly retrospective cohort (55.6%). Students were mostly female (75.4%) with a mean age ranging from 21.3 to 27.0 years. Meta-analysis revealed that being female (OR = 1.65, 95% CI = 1.26 to 2.12), having attended a Classical, Scientific or Academic high school (OR = 1.30, 95% IC = 1.16 to 1.46), and having reported higher final grades at the upper-secondary high school (Cohen's d = 0.42, 95% CI = 0.18 to 0.65) was significantly associated with student's ability to graduate within the regular duration of the program. Sensitivity analyses confirmed meta-analytic results and meta-analyses heterogeneity depended on study design. Contrasting and limited evidence were found for other investigated factors, and for academic outcomes different from graduation within the regular duration of the program.
Conclusions
Despite meta-analytic results, gender and upper-secondary school would be unethical students' entry selection criteria. Final upper-secondary school grades should be considered for this scope and purpose. Conflicting and limited evidence found for other factors, such as students' background, suggested the influence of local contexts on the phenomenon and its investigation. Investigating the role of modifiable individual variables, such as empathy and critical thinking, could contribute to the open debate about students' entry selection strategies. An improvement in methodological quality of future studies is recommended and expected.}
}
@article{LEE201618,
title = {Affective Computing as Complex Systems Science},
journal = {Procedia Computer Science},
volume = {95},
pages = {18-23},
year = {2016},
note = {Complex Adaptive Systems Los Angeles, CA November 2-4, 2016},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2016.09.288},
url = {https://www.sciencedirect.com/science/article/pii/S1877050916324607},
author = {William Lee and Michael D. Norman},
keywords = {Affective Computing, Computational Models, Complexity, Emotion, Apprasial},
abstract = {Pioneered in the early ‘90s by Rosalind Picard, a professor and IEEE Fellow of the MIT Media Lab, Affective Computing – rooted originally in artificial intelligence – now branches into wearable computing, big data, psychology, neuroscience, and modeling in order to advance the knowledge, understanding, and development of systems for sensing, recognizing, categorizing, and reacting to human emotion. Yet, the challenges of sensing multiple modalities simultaneously, disambiguating complex emotional states non-linearly, and modeling multiple individuals’ emotional states dynamically have continued to ring true, despite dramatic advances in affective computing. This paper seeks to serve two objectives. The first objective is to discuss how these three challenges are related to the three characteristics of complex systems – namely multiple components, non-linearity, and emergent behaviors. The second objective is to identify opportunities from the complex systems domain to address these challenges in novel and comprehensive ways. Recent advances in the utilization of Dynamical Systems Theory (an applied complexity science methodology) have shown that complex human interaction can be rigorously studied and modeled. Coupling the technological advances that cloud-based affective computing have brought with the emerging complex systems science-perspective may well catalyze a new era of human-machine and human-human collaboration.}
}
@article{ALGERAFI202461,
title = {Designing of an effective e-learning website using inter-valued fuzzy hybrid MCDM concept: A pedagogical approach},
journal = {Alexandria Engineering Journal},
volume = {97},
pages = {61-87},
year = {2024},
issn = {1110-0168},
doi = {https://doi.org/10.1016/j.aej.2024.04.012},
url = {https://www.sciencedirect.com/science/article/pii/S1110016824003867},
author = {Mohammed Abdulwahab Al-Gerafi and Shankha Shubhra Goswami and Mohammad Amir Khan and Quadri Noorulhasan Naveed and Ayodele Lasisi and Abdulaziz AlMohimeed and Ahmed Elaraby},
keywords = {E-Learning, Pedagogy, IVF, COPRAS, EDAS, PIV, MCDM},
abstract = {The demand for effective e-learning platforms requires prioritizing pedagogical excellence in online educational websites. Current approaches struggle with uncertainties, hindering optimal e-learning environments due to a lack of comprehensive evaluation in traditional methods. An integrated approach is crucial to avoid inefficiencies and incomplete understanding of learner needs. This research introduces a pioneering methodology integrating Inter-Valued Fuzzy (IVF) COPRAS-EDAS-PIV hybrid Multiple Criteria Decision-Making (MCDM) techniques, addressing existing limitations. Leveraging the IVF concept allows a holistic assessment of pedagogical parameters, ensuring a thorough understanding of the decision-making landscape. The study involves an extensive literature review, parameter identification, and data acquisition through group decision-making. The selection of a suitable e-learning website is based on seven conflicting parameters, and preference ranking orders are prescribed using EDAS, COPRAS, and PIV MCDM model. Rigorous analysis using these techniques facilitates precise ranking and informed decision-making. The findings underscore the efficacy of the proposed IVF-MCDM approach for the design of a pedagogical e-learning website. Final results reveal that alternative 5 as the most preferable, followed by alternative 2, while alternative 3 is the least favored option among the group. Comparative and sensitivity analyses validate the approach’s superiority, enabling stakeholders to make well-informed decisions for optimal e-learning websites that cater to diverse learner needs, thus enhancing the overall online learning experience.}
}
@article{MILLER2016102,
title = {Provision for income tax expense ASC 740: A teaching note},
journal = {Journal of Accounting Education},
volume = {35},
pages = {102-126},
year = {2016},
issn = {0748-5751},
doi = {https://doi.org/10.1016/j.jaccedu.2015.11.007},
url = {https://www.sciencedirect.com/science/article/pii/S0748575115000858},
author = {Tad Miller and Lindsay Miller and Jeffrey Tolin},
keywords = {Provision for income tax expense, Accounting Standards Codification, Deferred tax assets, Deferred tax liabilities, Deductible temporary differences, Taxable temporary differences},
abstract = {This project requires students to think critically to synthesize concepts they learned in their financial reporting and tax classes. They will use and interpret accounting standards to prepare tax provisions, comparative financial statements and the appropriate footnote disclosures. Even a simple tax provision results in a challenging project.}
}
@article{LI202514,
title = {Paradigm shifts from data-intensive science to robot scientists},
journal = {Science Bulletin},
volume = {70},
number = {1},
pages = {14-18},
year = {2025},
issn = {2095-9273},
doi = {https://doi.org/10.1016/j.scib.2024.09.029},
url = {https://www.sciencedirect.com/science/article/pii/S2095927324006807},
author = {Xin Li and Yanlong Guo}
}
@article{RAVALI2022100045,
title = {A systematic review of artificial intelligence for pediatric physiotherapy practice: Past, present, and future},
journal = {Neuroscience Informatics},
volume = {2},
number = {4},
pages = {100045},
year = {2022},
issn = {2772-5286},
doi = {https://doi.org/10.1016/j.neuri.2022.100045},
url = {https://www.sciencedirect.com/science/article/pii/S2772528622000073},
author = {Ravula Sahithya Ravali and Thangavel Mahalingam Vijayakumar and Karunanidhi {Santhana Lakshmi} and Dinesh Mavaluru and Lingala Viswanath Reddy and Mervin Retnadhas and Tintu Thomas},
keywords = {Artificial intelligence, Systematic review, Pediatric physical therapy, Physiotherapy education},
abstract = {Background: Artificial intelligence (AI) is one of the active research fields to develop systems that mimic human intelligence and is helpful in many fields, particularly in medicine. (“Role of Artificial Intelligence Techniques ... - PubMed”) Physiotherapy is mainly involving in curing bone-related pain and injuries. The recent emergence of artificially intelligent machines has seen human cognitive capacity enhanced by computational agents that can recognize previously hidden patterns within massive data sets. (“(PDF) Artificial intelligence in clinical practice ...”) In this context, artificial intelligence in pediatric physiotherapy could be one of the most important modalities in delivering better medical and healthcare services to needy people. It is an attempt to identify the types, as well as to assess the effectiveness of interventions provided by artificial intelligence on pediatric physical therapy optimization-related outcomes. Methods: Data acquisition was carried out by systematic searches from various academic and research databases i.e., google scholar, PubMed, and IEEE from March 2011 to March 2021. Besides, numerous trial registries and grey literature resources were also explored. A total of 187 titles/abstracts were screened, and forty-eight full-text articles were assessed for eligibility. Conclusions: This research describes some of the possible influences of artificial intelligence technologies on pediatric physiotherapy practice, and the subsequent ways in which physiotherapy education will need to change to graduate professionals who are fit for practice in the 21st century health system for promoting safe and effective use of artificial intelligence and the delivery of Pediatric Physical Therapy care to people.}
}
@article{LI20221,
title = {Computing for Chinese Cultural Heritage},
journal = {Visual Informatics},
volume = {6},
number = {1},
pages = {1-13},
year = {2022},
issn = {2468-502X},
doi = {https://doi.org/10.1016/j.visinf.2021.12.006},
url = {https://www.sciencedirect.com/science/article/pii/S2468502X21000644},
author = {Meng Li and Yun Wang and Ying-Qing Xu},
keywords = {Cultural computing, Chinese Cultural Heritage, Computable cultural ecosystem, Mogao caves, Guqin},
abstract = {Implementing computational methods for preservation, inheritance, and promotion of Cultural Heritage (CH) has become a research trend across the world since the 1990s. In China, generations of scholars have dedicated themselves to studying the country’s rich CH resources; there are great potential and opportunities in the field of computational research on specific cultural artefacts or artforms. Based on previous works, this paper proposes a systematic framework for Chinese Cultural Heritage Computing that consists of three conceptual levels which are Chinese CH protection and development strategy, computing process, and computable cultural ecosystem. The computing process includes three modules: (1) data acquisition and processing, (2) digital modeling and database construction, and (3) data application and promotion. The modules demonstrate the computing approaches corresponding to different phases of Chinese CH protection and development, from digital preservation and inheritance to presentation and promotion. The computing results can become the basis for the generation of cultural genes and eventually the formation of computable cultural ecosystem Case studies on the Mogao caves in Dunhuang and the art of Guqin, recognized as world’s important tangible and intangible cultural heritage, are carried out to elaborate the computing process and methods within the framework. With continuous advances in data collection, processing, and display technologies, the framework can provide constructive reference for building up future research roadmaps in Chinese CH computing and related fields, for sustainable protection and development of Chinese CH in the digital age.}
}
@article{FOUGERES2021100025,
title = {Fuzzy engineering design semantics elaboration and application},
journal = {Soft Computing Letters},
volume = {3},
pages = {100025},
year = {2021},
issn = {2666-2221},
doi = {https://doi.org/10.1016/j.socl.2021.100025},
url = {https://www.sciencedirect.com/science/article/pii/S2666222121000149},
author = {Alain-Jérôme Fougères and Egon Ostrosi},
keywords = {Fuzzy collaborative design, Fuzzy collaborative system, Natural language processing, Fuzzy requirement engineering, Fuzzy engineering design platform},
abstract = {Product design activities are predicated on fuzzy modelling, given that verbalising and interpreting engineering requirements are inherently fuzzy processes. The aim of this paper is to present a method for fuzzy intelligent requirement engineering from natural language to Computer-Aided Design (CAD) models. The field exploring the dynamics of computational processes from fuzzy linguistic modelling to fuzzy design modelling is complex and remains under-explored. No existing research has been identified which focuses specifically on fuzzy requirements engineering from natural language to CAD modelling. This paper seeks to address this by providing a design formalisation system based on five key principles. These principles are used to set out a computing procedure which follows a method broken up into six phases. The results of these six phases are fuzzy semantic graphs, which provide engineering requirements according to reliable design information. The approach is put into practice using the fuzzy agent-based tool developed by the authors, called F-EGEON (Fuzzy Engineering desiGn sEmantics elabOration and applicatioN). The proposed method is illustrated through an application from the automotive industry.}
}
@article{ZHANG2024127373,
title = {Adaptive emotion neural network based on ITCSO and grey correlation contribution},
journal = {Neurocomputing},
volume = {577},
pages = {127373},
year = {2024},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.127373},
url = {https://www.sciencedirect.com/science/article/pii/S0925231224001449},
author = {Wei Zhang and Wanfeng Wei},
keywords = {Emotion neural network, Hormone regulation, Competitive swarm optimization, Grey correlation contribution, Convergence analysis},
abstract = {In order to further improve the performance of emotion neural network (ENN), a novel adaptive hormone regulation emotion neural network (HRENN) is proposed, which is based on the improved triple competitive swarm optimization (ITCSO) algorithm and grey correlation contribution. Firstly, the structure of HRENN is designed that is inspired by the biological mechanism of hormone regulation. The fast response characteristic of emotion processing and the feedback effect of hormone regulation can effectively improve the learning ability of HRENN. Secondly, the ITCSO algorithm is proposed for optimizing the parameters of HRENN. In order to strike a well balance between exploration and exploitation, triple competition mechanism is adopted. Two-thirds of particles participate in the optimization and different learning strategies for different particles are provided. These operations can greatly improve the optimization efficiency and the convergence accuracy. Moreover, grey correlation contribution is used to add or delete the dimension of particles. It means that the structure and parameters of HRENN can be adjusted simultaneously and the compact structure can be obtained. Finally, the stability and the convergence of ITCSO are proved using the Banach space and the principle of compression mapping. Experiment results show that the proposed ITCSO-HRENN has good self-organization ability, compact network structure, high convergence accuracy and superior computation efficiency compared with other methods.}
}
@article{LEAHY2019102422,
title = {The digital frontier: Envisioning future technologies impact on the classroom},
journal = {Futures},
volume = {113},
pages = {102422},
year = {2019},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2019.04.009},
url = {https://www.sciencedirect.com/science/article/pii/S0016328718304166},
author = {Sean M. Leahy and Charlotte Holland and Francis Ward},
keywords = {Artificial intelligence, Augmented reality, Smart materials, Educational technology, Education futures, Education},
abstract = {Global advances in technology and information are purportedly propelling transformations and disruptions across many sectors, including education. However, historical reviews of technology integration in education mainly reveal weak or ineffectual impacts on learning, and only minor reforms to date within the education system. This study adopted a futures studies methodological approach to explore how K-12 educational spaces and experiences might be shaped by emerging and emergent technologies. In this regard, a series of vignettes are presented which critically examine the potential of augmented reality technologies, artificial intelligence, and smart materials technologies to transform future learning experiences and learning environments across K-12 education contexts, while also challenging assumptions about, and considering influences on, these futures. The focus of the study was not to predict a single or desired future for education, but rather to critically consider a range of possible education futures informed by the articulation of these three vignettes. The paper concludes with discourse on an emergent pedagogic approach that has the potential to prepare teachers and learners to interact and flourish within radically re-configured learning spaces that lean on the aforementioned technologies to support transitions within and beyond the school and its connected communities.}
}
@incollection{LUDLOW2025,
title = {I-Language and E-Language},
booktitle = {Reference Module in Social Sciences},
publisher = {Elsevier},
year = {2025},
isbn = {978-0-443-15785-1},
doi = {https://doi.org/10.1016/B978-0-323-95504-1.00558-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780323955041005585},
author = {Peter Ludlow},
keywords = {I-language, E-Language, Internalism, Externalism, Individualism, Intensional, Language faculty, Scientific method, Chomsky, Language organ},
abstract = {Chomsky (1986) introduced the distinction between I-language and E-language—a distinction that has consequences for how scientific investigation proceeds. In the former case we are investigating internal mechanisms of the language faculty. In the latter case we are investigating external, social constructs. In this essay we examine the distinguishing features of these approaches, raise questions about the coherence of each approach, and finally raise questions about whether the distinction itself stands up to close scrutiny.}
}
@article{GIOVANNINI20143280,
title = {Approach for the rationalisation of product lines variety},
journal = {IFAC Proceedings Volumes},
volume = {47},
number = {3},
pages = {3280-3291},
year = {2014},
note = {19th IFAC World Congress},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20140824-6-ZA-1003.02226},
url = {https://www.sciencedirect.com/science/article/pii/S1474667016421130},
author = {A. Giovannini and A. Aubry and H. Panetto and H. El Haouzi and L. Pierrel and M. Dassisti},
keywords = {Mass customization, Product variety, Knowledge representation, Knowledge-based system},
abstract = {The product variety management is a key process to deal with the flexibility requested by the mass customisation. In this paper we show that current variety-modelling methods miss a customer representation: without a proper assessment of the customers is not possible to define the product variety that has to be developed to meet the requirements of a customer segment. Here we present an innovative approach to rationalise the product variety, i.e. to link each product variant to the customer profile who needs it. The aim is to optimise the product variety avoiding excesses (variants not related to a customer), lacks (customers not related to a variant) or redundancies (two or more variants proposed to a customer). An overview of customer modelling approaches in the classic product design (non-customisable) is presented. The innovative approach is here developed using system-thinking concepts. A knowledge-based system that uses this approach is designed. Finally the approach is explained using a real industrial case of a quasi-real coil design process.}
}
@article{NI2011100,
title = {Influence of curriculum reform: An analysis of student mathematics achievement in Mainland China},
journal = {International Journal of Educational Research},
volume = {50},
number = {2},
pages = {100-116},
year = {2011},
note = {Curricular effect on the teaching and learning of mathematics: Findings from two longitudinal studies in China and the United States},
issn = {0883-0355},
doi = {https://doi.org/10.1016/j.ijer.2011.06.005},
url = {https://www.sciencedirect.com/science/article/pii/S0883035511000413},
author = {Yujing Ni and Qiong Li and Xiaoqing Li and Zhong-Hua Zhang},
keywords = {Curriculum reform, Primary mathematics, Curriculum evaluation, Student mathematics achievement, Cognitive, Affective},
abstract = {This study investigated curriculum influences on student mathematics achievement by following two groups of students from fifth to sixth grade that were taught either the reformed curriculum or the conventional curriculum. Analyses with three-level modeling were conducted to examine learning outcomes of the students who were assessed three times over a period of 18 months. Achievement was measured with regard to computation, routine problem solving, and complex problem solving. Affective aspects included self-reported interest in learning mathematics, classroom participation, views of the nature of mathematics, and views of learning mathematics. The results showed overall improved performance among all the students over the time on computation, routine problem solving, and complex problem solving but not on the affective measures. There were differentiated patterns of performance between the groups. On the initial assessment, the reform group performed better than the non-reform group on calculation, complex problem solving, and indicated higher interest in learning mathematics. The two groups did not differ on the other achievement and affective measures at the first time of assessment. There was no significant difference in growth rate between the groups on the cognitive and affective measures except that the non-reform group progressed at a faster pace on calculation. Therefore, the non-reform group outperformed the reform group on computation at the third (last) assessment. These results are discussed with respect to the possible influence of the curriculum on student learning.}
}
@article{BLACKSCHAFFER20162374289516665393,
title = {Training Pathology Residents to Practice 21st Century Medicine: A Proposal},
journal = {Academic Pathology},
volume = {3},
pages = {2374289516665393},
year = {2016},
issn = {2374-2895},
doi = {https://doi.org/10.1177/2374289516665393},
url = {https://www.sciencedirect.com/science/article/pii/S2374289521002189},
author = {W. Stephen Black-Schaffer and Jon S. Morrow and Michael B. Prystowsky and Jacob J. Steinberg},
keywords = {competency, progressive responsibility, residency training},
abstract = {Scientific advances, open information access, and evolving health-care economics are disrupting extant models of health-care delivery. Physicians increasingly practice as team members, accountable to payers and patients, with improved efficiency, value, and quality. This change along with a greater focus on population health affects how systems of care are structured and delivered. Pathologists are not immune to these disruptors and, in fact, may be one of the most affected medical specialties. In the coming decades, it is likely that the number of practicing pathologists will decline, requiring each pathologist to serve more and often sicker patients. The demand for increasingly sophisticated yet broader diagnostic skills will continue to grow. This will require pathologists to acquire appropriate professional training and interpersonal skills. Today’s pathology training programs are ill designed to prepare such practitioners. The time to practice for most pathology trainees is typically 5 to 6 years. Yet, trainees often lack sufficient experience to practice independently and effectively. Many studies have recognized these challenges suggesting that more effective training for this new century can be implemented. Building on the strengths of existing programs, we propose a redesign of pathology residency training that will meet (and encourage) a continuing evolution of American Board of Pathology and Accreditation Council for Graduate Medical Education requirements, reduce the time to readiness for practice, and produce more effective, interactive, and adaptable pathologists. The essence of this new model is clear definition and acquisition of core knowledge and practice skills that span the anatomic and clinical pathology continuum during the first 2 years, assessed by competency-based metrics with emphasis on critical thinking and skill acquisition, followed by individualized modular training with intensively progressive responsibility during the final years of training. We anticipate that implementing some or all aspects of this model will enable residents to attain a higher level of competency within the current time-based constraints of residency training.}
}
@article{FIELDS2025310,
title = {Paradox or illusion? A comment on “The paradox of the self-studying brain” by Battaglia, Servajean, and Friston},
journal = {Physics of Life Reviews},
volume = {53},
pages = {310-311},
year = {2025},
issn = {1571-0645},
doi = {https://doi.org/10.1016/j.plrev.2025.04.009},
url = {https://www.sciencedirect.com/science/article/pii/S1571064525000600},
author = {Chris Fields},
keywords = {Free energy principle, Introspection, Perception}
}
@incollection{SANCHEZSILVA2013437,
title = {17 - Risk assessment and management of civil infrastructure networks: a systems approach},
editor = {S. Tesfamariam and K. Goda},
booktitle = {Handbook of Seismic Risk Analysis and Management of Civil Infrastructure Systems},
publisher = {Woodhead Publishing},
pages = {437-464},
year = {2013},
series = {Woodhead Publishing Series in Civil and Structural Engineering},
isbn = {978-0-85709-268-7},
doi = {https://doi.org/10.1533/9780857098986.4.437},
url = {https://www.sciencedirect.com/science/article/pii/B9780857092687500177},
author = {M. Sánchez-Silva and C. Gómez},
keywords = {infrastructure, transportation networks, systems thinking, risk assessment, decision-making, optimization},
abstract = {Abstract:
Infrastructure networks are complex systems due to the large number of components that interact in a nonlinear way. Detecting and understanding the properties of such systems is of paramount importance to make effective decisions about risk management and sustainable development. This chapter presents a systems approach to risk management and risk-based decision making in infrastructure networks. In the proposed approach, the internal structure of a network is detected via pattern recognition (clustering), and structured information is used to enhance conceptual and computational analyses of reliability, vulnerability, damage propagation, and resource allocation. The approach can be applied to network analysis of complex infrastructure systems subjected to extreme events, such as earthquakes.}
}
@article{FARHADINIA2016135,
title = {Multiple criteria decision-making methods with completely unknown weights in hesitant fuzzy linguistic term setting},
journal = {Knowledge-Based Systems},
volume = {93},
pages = {135-144},
year = {2016},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2015.11.008},
url = {https://www.sciencedirect.com/science/article/pii/S0950705115004359},
author = {B. Farhadinia},
keywords = {Multi-criteria decision making, Hesitant fuzzy linguistic term set, Entropy measure, Similarity measure, Distance measure},
abstract = {As for multi-criteria decision making problems with hesitant fuzzy linguistic information, it is common that the criteria involved in the problems are associated with the predetermined weights, whereas the information about criteria weights is generally incomplete. This is because of the complexity and the inherent subjective nature of human thinking. In this circumstance, the weights of criteria can be derived by means of information entropy from the evaluation values of criteria for alternatives. To the best of our knowledge, up to now, there is no work having introduced the concept of entropy measure for hesitant fuzzy linguistic term sets (HFLTSs). Hence, in this paper, we are going to fill in this gap by developing information about how entropy measures of HFLTSs can be designed.}
}
@article{RATTEN2023100857,
title = {Generative artificial intelligence (ChatGPT): Implications for management educators},
journal = {The International Journal of Management Education},
volume = {21},
number = {3},
pages = {100857},
year = {2023},
issn = {1472-8117},
doi = {https://doi.org/10.1016/j.ijme.2023.100857},
url = {https://www.sciencedirect.com/science/article/pii/S1472811723000952},
author = {Vanessa Ratten and Paul Jones},
keywords = {Academic research, Teaching, Learning, Digital transformation, Management education, Artificial intelligence, ChatGPT},
abstract = {ChatGPT has been one of the most talked about computer programs amongst management educators in recent weeks due to its transformative ability to change how assessments are undertaken and graded. Unlike other educational technologies that can be tracked when used, ChatGPT has superior abilities that make it virtually untraceable when used. This creates a dilemma for management educators wanting to utilise the technology whilst staying relevant but also interested in authentic learning. Thus, it is critical for management educators to quickly implement policies regarding ChatGPT and subsequent new generative artificial intelligence because of its ease of use and affordability. This article is conceptual in nature and discusses ChatGPT as a generative form of artificial intelligence that presents challenges for management educators that need to be addressed through appropriate strategies. Thereby contributing to the literature on how technological innovations can be included in curriculum design and management learning practices. Practical and managerial implications are stated that highlight the critical need to re-examine existing education practices as a way of incorporating new technological innovation that can be utilised in a beneficial way.}
}
@article{XU2024100549,
title = {Prediction of environmental pollution hazard index of water conservancy system based on fuzzy logic},
journal = {International Journal of Thermofluids},
volume = {21},
pages = {100549},
year = {2024},
issn = {2666-2027},
doi = {https://doi.org/10.1016/j.ijft.2023.100549},
url = {https://www.sciencedirect.com/science/article/pii/S2666202723002641},
author = {Bingshu Xu},
keywords = {Water Conservancy System, Fuzzy Logic, Combined Forecasting, Environmental Pollution, Hazard Index Prediction},
abstract = {The water conservancy framework is a significant foundation for China's financial and social turn of events. Simultaneously monetary and social turn of events, likewise tremendously affect the biological climate. In the prediction method of the environmental pollution hazard index of water conservancy projects, a risk assessment must be carried out to enhance its applicability. This article mainly focuses on traditional mathematical prediction models and combines the characteristics of fuzzy logic mathematics (good nonlinear quality) to establish a new combination prediction method. By comparing the convergence errors of traditional methods with this method, the results show that the algorithm proposed in this paper can effectively reduce prediction errors with fewer iterations, making it stable at around 500 times, with good convergence and high computational accuracy. The fuzzy logic-based prediction method for environmental pollution hazards in water conservancy systems proposed in this article can overcome the difficulty of nonlinear combination prediction. This achieves real-time prediction of environmental pollution hazards in water conservancy systems and shortens response time, greatly improving the accuracy of prediction.}
}