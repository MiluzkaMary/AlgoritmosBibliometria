@article{ALI2024101172,
title = {Physics-informed neural networks in groundwater flow modeling: Advantages and future directions},
journal = {Groundwater for Sustainable Development},
volume = {25},
pages = {101172},
year = {2024},
issn = {2352-801X},
doi = {https://doi.org/10.1016/j.gsd.2024.101172},
url = {https://www.sciencedirect.com/science/article/pii/S2352801X2400095X},
author = {Ahmed Shakir Ali Ali and Farhad Jazaei and T. Prabhakar Clement and Brian Waldron},
keywords = {Artificial intelligence, Physics-informed neural network, PINN, Groundwater modeling, MODFLOW},
abstract = {In recent years, there has been enormous development in soft computing, especially artificial intelligence (AI), which has developed robust methods for solving complex engineering problems. Researchers in the field of water resources engineering have applied these AI methods to solve a variety of hydrological problems. Despite their widespread use in the surface and atmospheric hydrology fields, groundwater hydrologists have not widely used AI methods in their routine field-scale modeling efforts. This is because AI models have been primarily considered black box models that lack physical meaning. Furthermore, using AI models to generate the space-time distribution of transient groundwater level variations is challenging and requires further flux balance and mass transport analyses. More recently, a new type of physics-informed neural network (PINN) model has been developed to address several limitations by integrating governing physics (groundwater flow equations) into the AI tools. This study presents the systematic advantages of the PINN algorithm for solving groundwater problems using a set of classic test problems. As discussed in detail in the article, these advantages and potentials are associated with the meshless nature of PINN, its continuous time and space dimensions, its independence from time-stepping and incremental marching in space, and its efficiency in running time. However, despite PINN's promising attributes, it is important to acknowledge its nascent stage of development and the inherent limitations of all neural network models, such as training challenges and hyperparameter selection. Thus, collaborative efforts between groundwater modelers and computer scientists are imperative to explore and exploit the full potential of PINN in tackling increasingly complex groundwater problems and nurturing PINN into a dependable modeling tool in industry and academia.}
}
@article{DAS2022104116,
title = {Role of non-motorized transportation and buses in meeting climate targets of urban regions},
journal = {Sustainable Cities and Society},
volume = {86},
pages = {104116},
year = {2022},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2022.104116},
url = {https://www.sciencedirect.com/science/article/pii/S2210670722004292},
author = {Deepjyoti Das and Pradip P. Kalbar and Nagendra R. Velaga},
keywords = {Decarbonization, Life cycle thinking, Avoided trip and material, Carbon budget, Climate change, Sustainable transportation},
abstract = {Studies examining the potential of low-carbon modes of passenger transportation for achieving climate goals are limited. The study is one of the first to assess the potential of non-motorized transportation (NMT) and buses to meet regional climate targets representing 2 °C, 1.5 °C, and Intended Nationally Determined Contributions from 2018 to 2050. Also, the approach towards quantifying contribution from avoided trips and materials in holistically understanding the potential of NMT and buses is novel. Data from the transportation model of Mumbai Metropolitan Region's Comprehensive Mobility Plan is used to assess multiple scenarios of upgrading NMT and bus infrastructure to reduce cumulative carbon dioxide emissions (CCE) from passenger transportation. The assessment is based on three push levels, i.e., conservative, moderate, and aggressive. Results show that upgrading bus infrastructure contributes higher to reducing CCE than NMT. As NMT also contributes significantly to decreasing CCE, it is recommended that bus and NMT development should be integrated. However, their combined contribution will not meet the climate targets. Since avoided materials contribute considerably more than avoided trips, high emission materials such as aluminum used in light-weighting should be questioned. The results provide policy guidance to authorities in prioritizing buses and NMT infrastructure development during city planning.}
}
@article{FOO201410,
title = {Evolution of acquired resistance to anti-cancer therapy},
journal = {Journal of Theoretical Biology},
volume = {355},
pages = {10-20},
year = {2014},
issn = {0022-5193},
doi = {https://doi.org/10.1016/j.jtbi.2014.02.025},
url = {https://www.sciencedirect.com/science/article/pii/S0022519314001003},
author = {Jasmine Foo and Franziska Michor},
keywords = {Drug resistance, Cancer, Evolution, Mathematical modeling, Optimal dosing strategies},
abstract = {Acquired drug resistance is a major limitation for the successful treatment of cancer. Resistance can emerge due to a variety of reasons including host environmental factors as well as genetic or epigenetic alterations in the cancer cells. Evolutionary theory has contributed to the understanding of the dynamics of resistance mutations in a cancer cell population, the risk of resistance pre-existing before the initiation of therapy, the composition of drug cocktails necessary to prevent the emergence of resistance, and optimum drug administration schedules for patient populations at risk of evolving acquired resistance. Here we review recent advances towards elucidating the evolutionary dynamics of acquired drug resistance and outline how evolutionary thinking can contribute to outstanding questions in the field.}
}
@incollection{HUNG2017227,
title = {Chapter 12 - Rationality and Escherichia Coli},
editor = {T.-W. Hung and T.J. Lane},
booktitle = {Rationality},
publisher = {Academic Press},
address = {San Diego},
pages = {227-240},
year = {2017},
isbn = {978-0-12-804600-5},
doi = {https://doi.org/10.1016/B978-0-12-804600-5.00012-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012804600500012X},
author = {T.-W. Hung},
keywords = {practical rationality, procedural rationality, , human uniqueness, nonhuman rationality},
abstract = {If rationality is the defining characteristic of the human species, as Aristotle asserts, why is this trait rarely noticed in Eastern traditions? How should we interpret the reasoning ability in problem solving that is increasingly reported in other animals? In this chapter, I focus on descriptive-practical-procedural rationality (one’s action is described as rational if it is determined by internal processes that conform to logical or Bayesian rules). I argue that this rationality can be found in all organisms with adaptive capacity, including unicellular bacteria. To this end, I first review three seemingly true claims and explain why they lead to an inconsistency: (1) Escherichia coli are computational systems in a nontrivial sense, (2) E. coli are not creatures that can be rational or irrational, and (3) rationality is a matter of computational facts in that nontrivial sense, and organisms of the same computation are the type of creatures that can be rational or irrational. I then suggest rejecting claim (2) by examining recent microbiological data on E. coli, explaining the extent to which they satisfy this type of rationality. I also discuss some objections to and implications of this view. Instead of concluding that humans and bacteria both evolve with rationality at the same level, I argue that organisms’ rationality capacities comes in degree.}
}
@article{ZHAO2024102465,
title = {GA-GGD: Improving semantic discriminability in graph contrastive learning via Generative Adversarial Network},
journal = {Information Fusion},
volume = {110},
pages = {102465},
year = {2024},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2024.102465},
url = {https://www.sciencedirect.com/science/article/pii/S1566253524002434},
author = {Jitao Zhao and Dongxiao He and Meng Ge and Yongqi Huang and Lianze Shan and Yongbin Qin and Zhiyong Feng},
keywords = {Graph representation learning, Generative Adversarial Network, Graph contrastive learning, Adversarial Machine Learning, Semantic discriminability},
abstract = {Graph contrastive learning has garnered considerable research interest due to its ability to effectively embed graph data without manual labels. Among them, methods based on Deep Graph Infomax (DGI) have been widely studied and favored in the industry because of their fast training speed, applicability to large-scale data. DGI-based methods usually obtain a noise graph through node shuffling. The proxy task of these methods encourages the encoder to distinguish whether the nodes come from the original graph or the noise graph, thereby maximizing the mutual information between the node representation and the graph it belongs to, while also maximizing the Jenson–Shannon divergence between the nodes of original graph and noise graph. However, we argue that these approaches only enable the encoder to differentiate between semantically meaningful graphs and noise graphs, but not to effectively identify different semantic graphs. This leads to the inability of the encoder to effectively embed information between different semantics, significantly reducing the robustness and affecting the performance of downstream tasks. In addition, this training mode makes the model more sensitive to attacks. To improve their semantic discriminability, we take advantage of the natural ability of generative adversarial networks to generate semantic data, proposing a method called Generative Adversarial Graph Group Discrimination (GA-GGD). Specifically, it consists of a graph group discriminator and a semantic attack generator. The discriminator aims to encode the graph and identify whether nodes originate from the original graph. The goal of the generator is to use random features and graph structure to find vulnerabilities of discriminator and generate node representations with similar but wrong semantic to confuse the discriminator. GA-GGD can improve the model’s semantic information embedding without significantly increasing computational overhead and memory occupancy. We test the effectiveness of the proposed model on commonly used data sets and large-scale datasets, as well as in various downstream tasks such as classification, clustering, and adversarial attacks defence. A wealth of experimental results confirm the efficacy of the proposed model.}
}
@incollection{SIEGEL20223,
title = {Chapter 1 - Introduction: Defining the Role of Statistics in Business},
editor = {Andrew F. Siegel and Michael R. Wagner},
booktitle = {Practical Business Statistics (Eighth Edition)},
publisher = {Academic Press},
edition = {Eighth Edition},
pages = {3-18},
year = {2022},
isbn = {978-0-12-820025-4},
doi = {https://doi.org/10.1016/B978-0-12-820025-4.00001-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128200254000014},
author = {Andrew F. Siegel and Michael R. Wagner},
abstract = {We begin this chapter with an overview of the competitive advantage provided by a knowledge of statistical methods, followed by some basic facts about statistics and probability and their role in business. Statistical activities can be grouped into five main activities (designing, exploring, modeling, estimating, and hypothesis testing), and one way to clarify statistical thinking is to be able to match the business task at hand with the correct collection of statistical methods. This chapter sets the stage for the rest of the book, which follows up with many important detailed procedures for accomplishing business goals that involve these activities. Next follows an overview of data mining of Big Data (which involves these main activities) and its importance in business. Then we distinguish the field of probability (where, based on assumptions, we reach conclusions about what is likely to happen—a useful exercise in business where nobody knows for sure what will happen) from the field of statistics (where we know from the data what happened, from which we infer conclusions about the system that produced these data) while recognizing that probability and statistics will work well together in future chapters. The chapter concludes with some words of advice on how to integrate statistical thinking with other business viewpoints and activities.}
}
@article{THANHEISER2024101176,
title = {Introduction to the virtual special issue: Mathematics that underpins social issues},
journal = {The Journal of Mathematical Behavior},
volume = {75},
pages = {101176},
year = {2024},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2024.101176},
url = {https://www.sciencedirect.com/science/article/pii/S0732312324000531},
author = {Eva Thanheiser and Ami Mamolo},
keywords = {Mathematics in Society, Educational Research, Social Issues, Mathematical Worldview, Numeracy, Mathematical Literacy},
abstract = {This Virtual Special Issue on Mathematics in Society: Exploring the Mathematics that Underpins Social Issues features 13 articles which expand our understanding of how people build, retain, communicate, apply, and comprehend mathematical ideas as they relate to social and societal issues. The focus is on education research that explores the ways in which mathematics and a mathematical worldview can influence choices, on educational, personal and societal levels. We take a broad view and raise questions about what it means to be mathematical in society, and we consider the multifaceted ways in which abilities to derive and interpret information presented mathematically are also necessary in and for society.}
}
@incollection{NEWMAN2020183,
title = {Chapter 7 - Cognitive developmental theories},
editor = {Barbara M. Newman and Philip R. Newman},
booktitle = {Theories of Adolescent Development},
publisher = {Academic Press},
pages = {183-211},
year = {2020},
isbn = {978-0-12-815450-2},
doi = {https://doi.org/10.1016/B978-0-12-815450-2.00007-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128154502000073},
author = {Barbara M. Newman and Philip R. Newman},
keywords = {Problem solving, Equilibrium, Schemes, Adaptation, Egocentrism, Formal operational thought, Moral reasoning, Social reasoning, Metacognition, Education},
abstract = {The chapter focuses on cognitive developmental theories which address the emerging nature of concept formation, reasoning, planning, and problem solving, and the increasingly complex structures that support changing and flexible capacities for thinking about multidimensional problems with probabilistic outcomes. This chapter summarizes the work of Jean Piaget and the extension of his ideas among neo-Piagetian theorists including Deanna Kuhn, Paul Klacziynski, and Robbie Case. The following key concepts are explained: equilibrium, schemes, organization, adaptation, stages of development, and egocentrism. The stage of Formal Operational Reasoning and elaboration of cognitive capacities in adolescence are described in detail. Application of these theories to moral reasoning, social reasoning, metacognition, and educational initiatives are discussed. Experimental approaches and paper and pencil measures of cognitive reasoning are described. The strengths and limitations of cognitive developmental theories are reviewed.}
}
@incollection{CANCES20033,
title = {Computational quantum chemistry: A primer},
series = {Handbook of Numerical Analysis},
publisher = {Elsevier},
volume = {10},
pages = {3-270},
year = {2003},
booktitle = {Special Volume, Computational Chemistry},
issn = {1570-8659},
doi = {https://doi.org/10.1016/S1570-8659(03)10003-8},
url = {https://www.sciencedirect.com/science/article/pii/S1570865903100038},
author = {Eric Cancès and Mireille Defranceschi and Werner Kutzelnigg and Claude {Le Bris} and Yvon Maday},
abstract = {Publisher Summary
This chapter discusses basic modeling. The chapter illustrates that quantum chemistry aims at understanding the properties of matter through the modeling of its behavior at a subatomic scale, where matter is described as an assembly of nuclei and electrons. At this scale, the equation that rules the interactions between these constitutive elements is the Schrödinger equation. It can be considered as a universal model for at least three reasons. First, it contains all the physical information of the system under consideration so that any of the properties of this system can be deduced in theory from the Schrödinger equation associated to it. Second, the Schrödinger equation does not involve any empirical parameter, except some fundamental constants of Physics; it can thus be written for any kind of molecular system provided its chemical composition, in terms of natures of nuclei and number of electrons, is known. Third, this model enjoys remarkable predictive capabilities, as confirmed by comparisons with a large amount of experimental data of various types.}
}
@article{DING2025121721,
title = {Endogenous dynamics of rumor spreading and debunking considering the influence of attitude: An agent-based modeling approach},
journal = {Information Sciences},
volume = {694},
pages = {121721},
year = {2025},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2024.121721},
url = {https://www.sciencedirect.com/science/article/pii/S0020025524016359},
author = {Haixin Ding},
keywords = {Rumor, Rumor debunking, Innovation diffusion, Social Judgement theory, Balance theory, Agent-based Modelling},
abstract = {Rumor and debunking help create one another, but their endogenous dynamics are still worth further exploration. Relatedly, although attitude is important during the communication process, this indispensable element has been underrepresented for rumors as a specific kind of communication. The study first uses the balance logic to reveal the fundamental influence of attitude on rumor-related interactions at the individual level; Based on the innovation diffusion perspective and social judgment theory, the study constructs a conceptual framework and mathematical models of endogenous dynamics of rumor spreading and debunking considering the influence of attitude. Using an agent-based modeling approach, the study conducts systematic computational experiments. The results show that both attitude distribution and attitude interaction structure influence the endogenous dynamics, and the two types of factors interact with each other. The study explains debunking without official rebuttal endogenously, exposes debunking may not result in the intended outcome, and illustrates focusing only on rumor spreaders would always underestimate the impact of rumors systematically.}
}
@article{WANG2025111994,
title = {A lightweight progressive joint transfer ensemble network inspired by the Markov process for imbalanced mechanical fault diagnosis},
journal = {Mechanical Systems and Signal Processing},
volume = {224},
pages = {111994},
year = {2025},
issn = {0888-3270},
doi = {https://doi.org/10.1016/j.ymssp.2024.111994},
url = {https://www.sciencedirect.com/science/article/pii/S0888327024008926},
author = {Changdong Wang and Jingli Yang and Huamin Jie and Zhen Tao and Zhenyu Zhao},
keywords = {Class imbalance, Ensemble learning, Fault diagnosis, Markov process, Progressive joint-transfer strategy},
abstract = {Owing to safety limitations and data collection costs, scenarios with imbalanced data usually arise, posing a great challenge for precise fault diagnosis. Targeting imbalanced fault diagnosis and the high computational cost of mainstream ensemble learning methods currently used, this article proposes a lightweight and accurate scheme based on a progressive joint-transfer ensemble network (PJTEN) and a Markov-lightweight strategy (MLS). Specifically, a PJTEN is developed, incorporating a multiple excitation-channel attention basic estimator and progressive joint-transfer strategy (PJTS) to maintain diversity of basic estimators better and focus more on key information from minority classes. Besides, the MLS guided by Markov transition probabilities is for the first time constructed for ensemble learning to reduce the network redundancy by alternating optimization. Using a standard dataset and a brand-new dataset of a real ship propulsion system, the proposed method achieves leading results in Accuracy, F1 score and MCC, compared with eight cutting-edge methods, thereby validating its substantial value. In terms of lightweight operation, such as temporal complexity (TC), spatial complexity (SC), and time efficiency, it is also ahead of the latest ensemble-based methods.}
}
@article{LABO2018185,
title = {Application of low-invasive techniques and incremental seismic rehabilitation to increase the feasibility and cost-effectiveness of seismic interventions},
journal = {Procedia Structural Integrity},
volume = {11},
pages = {185-193},
year = {2018},
note = {XIV INTERNATIONAL CONFERENCE ON BUILDING PATHOLOGY AND CONSTRUCTIONS REPAIR, FLORENCE, ITALY, JUNE 20-22, 2018},
issn = {2452-3216},
doi = {https://doi.org/10.1016/j.prostr.2018.11.025},
url = {https://www.sciencedirect.com/science/article/pii/S2452321618301264},
author = {S. Labò and E. Casprini and C. Passoni and J. Zanni and A. Belleri and A. Marini and P. Riva},
keywords = {Incremental rehabilitation, seismic retrofit, renovation strategy, low-invasive techniques, life cycle thinking, diagrid, school buildings},
abstract = {The high seismic risk connected to the existing construction heritage requires a wide-scale renovation action to ensure structural resilience and avoid future human and economic losses. Given the urgency and the scale of the problem and the lack of available resources, a new strategy for the renovation of the obsolete European building stock should be envisioned, accounting for both safety and environmental, social and economic sustainability. This research aims at exploring new cost-effective seismic retrofit solutions based on the principles of low-invasiveness and incremental seismic rehabilitation, as envisioned by FEMA P-420 (2009). The incremental rehabilitation approach allows to plan repair and retrofit actions along with the maintenance works expected during the building's lifetime, thereby spreading them in time and reducing costs. In addition, low-invasiveness of the solutions is required to reduce the impacts on the functionality of the building, thus cutting the costs connected to downtime. A possible solution is represented by the introduction of an exoskeleton entirely carried out from outside. In this paper, a new sustainable technique is proposed, where the existing structure is connected to a self-supporting exoskeleton adopting demountable dry techniques, which may be assembled and activated in different phases of the building lifetime. As a proof of concept, the approach is then applied to a school building.}
}
@article{ZHU2023110006,
title = {Deep reinforcement learning-based edge computing offloading algorithm for software-defined IoT},
journal = {Computer Networks},
volume = {235},
pages = {110006},
year = {2023},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2023.110006},
url = {https://www.sciencedirect.com/science/article/pii/S1389128623004516},
author = {Xiaojuan Zhu and Tianhao Zhang and Jinwei Zhang and Bao Zhao and Shunxiang Zhang and Cai Wu},
keywords = {Edge computing, Computing offloading, Software defined network, Internet of things, Deep reinforcement learning},
abstract = {Edge computing offloading can effectively solve the problem of insufficient computing resources for terminal devices and improve the performance and efficiency of the system. When network states and tasks change rapidly, data-driven intelligent algorithms have difficulty obtaining comprehensive statistics for accurate prediction, resulting in degraded performance of computational offloading and difficulty in adaptive adjustment. It is a current challenge to improve the environment-aware, intelligent optimization so that the computational offloading algorithm can adapt to the dynamic changes in network state and task demands, thus achieving global multi-objective optimization. This paper presents optimized edge computing offloading algorithm for software-defined IoT. First, to provide global state for making decisions, a software defined edge computing (SDEC) architecture is proposed. The edge layer is integrated into the control layer of software-defined IoT, and multiple controllers share the global network state information via east–west message exchange. Moreover, an edge computing offloading algorithm in software-defined IoT (ECO-SDIoT) based on deep reinforcement learning is proposed. It enables the controllers to offload the computing task to the most appropriate edge server according to the global states, task requirements, and reward. Finally, the performance metrics for edge computing offloading were evaluated in terms of unit task processing latency, load balancing of edge servers, task processing energy consumption, and task completion rate, respectively. Simulation results show that ECO-SDIoT can effectively reduce task completion time and energy consumption compared with other strategies.}
}
@article{WOLFENGAGEN2016353,
title = {Concordance in the Crowdsourcing Activity},
journal = {Procedia Computer Science},
volume = {88},
pages = {353-358},
year = {2016},
note = {7th Annual International Conference on Biologically Inspired Cognitive Architectures, BICA 2016, held July 16 to July 19, 2016 in New York City, NY, USA},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2016.07.448},
url = {https://www.sciencedirect.com/science/article/pii/S1877050916317045},
author = {Viacheslav E. Wolfengagen and Larisa Yu. Ismailova and Sergey Kosikov},
keywords = {crowdsourcing, Big Data, Thick Data, variable domains, cognition model, concordance, computational model},
abstract = {A concordance in cognition activity of possibly interrelated crowdsourcers aimed to property recognition in the voluminous data sources is considered. Data sources are of either usual nature or manually generated with the crowdsourcing. The proposed model is based on the variable domains assumption. A general layout is able to take into account an interaction of crowdsourcers and properties when they are varying with the evolving the events. The cognition model is of stage-by-stage type and has the representable functor. This model as may be shown is faithfully embedded into a category of indexed sets. Using the proposed neighborhood for cognition activity leads to a flexible computing model.}
}
@article{VALJAK2023191,
title = {Functional modelling through Function Class Method: A case from DfAM domain},
journal = {Alexandria Engineering Journal},
volume = {66},
pages = {191-209},
year = {2023},
issn = {1110-0168},
doi = {https://doi.org/10.1016/j.aej.2022.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S1110016822007852},
author = {Filip Valjak and Nenad Bojčetić},
keywords = {Functional modelling, Function structure, Function class, Design for Additive Manufacturing},
abstract = {Functional modelling is an essential part of systematic design approaches and is often prescribed in engineering design textbooks. However, function models created with current function modelling techniques often lack formal and repeatable representation, limiting their use in computational reasoning. Therefore, this paper presents a new functional modelling method to support function models' creation with formal and repeatable representation. The key element of the proposed method is a Function Class – a function-modelling element that categorises defined functions on a function block level by specifying operating flow, input and output flows, and integrates primary rules for functional modelling such as conservation law. The formalisation on a function block level reduces the number of morphological errors and provides a theoretical framework for future computational processing of function models. This paper proposes a protocol for developing Function Classes and defines a theoretical function modelling framework through Function Class Method. The development and use of the Function Class Method are demonstrated through the development of Function Classes for the Design for Additive Manufacturing domain as the first step toward a universal function modelling approach.}
}
@article{BHADURI2025100723,
title = {Community partnership design of a maker-related camp for underserved youth: Impacts on youths’ present and future learning trajectories},
journal = {International Journal of Child-Computer Interaction},
volume = {44},
pages = {100723},
year = {2025},
issn = {2212-8689},
doi = {https://doi.org/10.1016/j.ijcci.2025.100723},
url = {https://www.sciencedirect.com/science/article/pii/S2212868925000030},
author = {Srinjita Bhaduri and Quentin Biddy and Melissa Rummel and Mimi Recker},
keywords = {Collaborative design, Maker technologies, 3D modeling and printing, Community partnership, Informal learning, Middle school youth STEM activities},
abstract = {This paper describes the design of Science, Technology, Engineering, and Mathematics (STEM) maker-related activities offered to middle school youth as part of a free, four-week summer camp. The camp was aimed at academically at-risk youth in a rural, tourism-oriented mountain community with significant income disparities. Guided by an educational model focused on enhancing youths’ present and future interests in and visions of STEM and computing fields, camp activities were collaboratively designed by a community partnership comprised of a local camp provider, the local school district, and researchers. Situating design in a community partnership helped highlight and integrate locally relevant resources, careers, and community opportunities. The paper also reports findings from a study examining how the STEM maker camp activities, which leveraged 3D modeling and printing practices, impacted youths’ perceptions of their disciplinary identity, engagement, and their present and future visions of the relevance of these STEM practices to themselves and their communities. The study also explores design tensions that emerged during the camp design process and identified barriers and opportunities that arose from balancing the needs of each partner, the research team’s focus on youth-centered learning, and the overall program goals.}
}
@article{DOSSOU2021476,
title = {Development of a decision support tool for sustainable urban logistics optimization},
journal = {Procedia Computer Science},
volume = {184},
pages = {476-483},
year = {2021},
note = {The 12th International Conference on Ambient Systems, Networks and Technologies (ANT) / The 4th International Conference on Emerging Data and Industry 4.0 (EDI40) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.03.060},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921006918},
author = {Paul-Eric Dossou and Axel Vermersch},
keywords = {Type your keywords here, separated by semicolons},
abstract = {Traffic flows are increasing in cities, partially due to congestions provoked by trucks. These congestions cause many problems such as pollution (gasoil, Carbon, etc.), noise, waste of time. Indeed, cities like Paris, Hamburg, Milan, and “Grand Paris Sud” conurbation are thinking about a sustainable alternative solution to road transportation. Then, a research based on co-creation methodology integrating all stakeholders (local authorities, companies, citizens) for elaborating an alternative solution to road transportation has been defined. This paper presents the architecture and the development of a decision aided tool for simulating and optimizing alternative solutions to road transportation}
}
@article{MAO2025102712,
title = {A survey on pragmatic processing techniques},
journal = {Information Fusion},
volume = {114},
pages = {102712},
year = {2025},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2024.102712},
url = {https://www.sciencedirect.com/science/article/pii/S1566253524004901},
author = {Rui Mao and Mengshi Ge and Sooji Han and Wei Li and Kai He and Luyao Zhu and Erik Cambria},
keywords = {Pragmatic processing, Metaphor understanding, Sarcasm detection, Personality recognition, Aspect extraction, Sentiment polarity detection},
abstract = {Pragmatics, situated in the domains of linguistics and computational linguistics, explores the influence of context on language interpretation, extending beyond the literal meaning of expressions. It constitutes a fundamental element for natural language understanding in machine intelligence. With the advancement of large language models, the research focus in natural language processing has predominantly shifted toward high-level task processing, inadvertently downplaying the importance of foundational pragmatic processing tasks. Nevertheless, pragmatics serves as a crucial medium for unraveling human language cognition. The exploration of pragmatic processing stands as a pivotal facet in realizing linguistic intelligence. This survey encompasses important pragmatic processing techniques for subjective and emotive tasks, such as personality recognition, sarcasm detection, metaphor understanding, aspect extraction, and sentiment polarity detection. It spans theoretical research, the forefront of pragmatic processing techniques, and downstream applications, aiming to highlight the significance of these low-level tasks in advancing natural language understanding and linguistic intelligence.}
}
@article{TAPIA2019170,
title = {Design of biomass value chains that are synergistic with the food–energy–water nexus: Strategies and opportunities},
journal = {Food and Bioproducts Processing},
volume = {116},
pages = {170-185},
year = {2019},
issn = {0960-3085},
doi = {https://doi.org/10.1016/j.fbp.2019.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S0960308519300641},
author = {John Frederick D. Tapia and Sheila Samsatli and Stephen S. Doliente and Elias Martinez-Hernandez and Wan Azlina Binti Wan Ab Karim Ghani and Kean Long Lim and Helmi Zulhaidi Mohd Shafri and Nur Shafira Nisa Binti Shaharum},
keywords = {Biomass value chains (BVCs), Food–energy–water (FEW) nexus, Mathematical modelling, Biomass supply chains, Optimisation, Process systems engineering, Sustainable land use, Bioenergy},
abstract = {Humanity’s future sustainable supply of energy, fuels and materials is aiming towards renewable sources such as biomass. Several studies on biomass value chains (BVCs) have demonstrated the feasibility of biomass in replacing fossil fuels. However, many of the activities along the chain can disrupt the food–energy–water (FEW) nexus given that these resource systems have been ever more interlinked due to increased global population and urbanisation. Essentially, the design of BVCs has to integrate the systems-thinking approach of the FEW nexus; such that, existing concerns on food, water and energy security, as well as the interactions of the BVCs with the nexus, can be incorporated in future policies. To date, there has been little to no literature that captures the synergistic opportunities between BVCs and the FEW nexus. This paper presents the first survey of process systems engineering approaches for the design of BVCs, focusing on whether and how these approaches considered synergies with the FEW nexus. Among the surveyed mathematical models, the approaches include multi-stage supply chain, temporal and spatial integration, multi-objective optimisation and uncertainty-based risk management. Although the majority of current studies are more focused on the economic impacts of BVCs, the mathematical tools can be remarkably useful in addressing critical sustainability issues in BVCs. Thus, future research directions must capture the details of food–energy–water interactions with the BVCs, together with the development of more insightful multi-scale, multi-stage, multi-objective and uncertainty-based approaches.}
}
@article{ULRICH1988309,
title = {Computation and conceptual design},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {4},
number = {3},
pages = {309-315},
year = {1988},
note = {Special Issue Manufacturing Systems and Technology of the Future},
issn = {0736-5845},
doi = {https://doi.org/10.1016/0736-5845(88)90002-6},
url = {https://www.sciencedirect.com/science/article/pii/0736584588900026},
author = {Karl Ulrich and Warren Seering},
abstract = {Design is the transformation between a functional and a structural description of a device. Conceptual design is the initial stage of this transformation. We hypothesize that most new design are derived from knowledge of existing designs. We identify a special case of this process and call it novel combination. By describing an implemented program which designs novel mechanical fasteners, we explain how knowledge of existing devices can be represented and used. We highlight the issues arising from this implementation and propose four areas of future research. This work is important for establishing a fundamental understanding of conceptual design, leading to enhanced design teaching and better design tools.}
}
@article{LEITE20231,
title = {Interval incremental learning of interval data streams and application to vehicle tracking},
journal = {Information Sciences},
volume = {630},
pages = {1-22},
year = {2023},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2023.02.027},
url = {https://www.sciencedirect.com/science/article/pii/S0020025523002165},
author = {Daniel Leite and Igor Škrjanc and Sašo Blažič and Andrej Zdešar and Fernando Gomide},
keywords = {Granular machine learning, Online learning, Granular computing, Interval analysis, Data stream},
abstract = {This paper presents a method called Interval Incremental Learning (IIL) to capture spatial and temporal patterns in uncertain data streams. The patterns are represented by information granules and a granular rule base with the purpose of developing explainable human-centered computational models of virtual and physical systems. Fundamentally, interval data are either included into wider and more meaningful information granules recursively, or used for structural adaptation of the rule base. An Uncertainty-Weighted Recursive-Least-Squares (UW-RLS) method is proposed to update affine local functions associated with the rules. Online recursive procedures that build interval-based models from scratch and guarantee balanced information granularity are described. The procedures assure stable and understandable rule-based modeling. In general, the model can play the role of a predictor, a controller, or a classifier, with online sample-per-sample structural adaptation and parameter estimation done concurrently. The IIL method is aligned with issues and needs of the Internet of Things, Big Data processing, and eXplainable Artificial Intelligence. An application example concerning real-time land-vehicle localization and tracking in an uncertain environment illustrates the usefulness of the method. We also provide the Driving Through Manhattan interval dataset to foster future investigation.}
}
@article{GERSHMAN2020104394,
title = {Origin of perseveration in the trade-off between reward and complexity},
journal = {Cognition},
volume = {204},
pages = {104394},
year = {2020},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2020.104394},
url = {https://www.sciencedirect.com/science/article/pii/S0010027720302134},
author = {Samuel J. Gershman},
keywords = {Decision making, Information theory, Reinforcement learning},
abstract = {When humans and other animals make repeated choices, they tend to repeat previously chosen actions independently of their reward history. This paper locates the origin of perseveration in a trade-off between two computational goals: maximizing rewards and minimizing the complexity of the action policy. We develop an information-theoretic formalization of policy complexity and show how optimizing the trade-off leads to perseveration. Analysis of two data sets reveals that people attain close to optimal trade-offs. Parameter estimation and model comparison supports the claim that perseveration quantitatively agrees with the theoretically predicted functional form (a softmax function with a frequency-dependent action bias).}
}
@article{YANG2010209,
title = {Creativity of student information system projects: From the perspective of network embeddedness},
journal = {Computers & Education},
volume = {54},
number = {1},
pages = {209-221},
year = {2010},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2009.08.004},
url = {https://www.sciencedirect.com/science/article/pii/S0360131509001997},
author = {Heng-Li Yang and Hsiu-Hua Cheng},
keywords = {Project team creativity, Network embeddedness, Affiliation network, Innovation climate, Centrality},
abstract = {Many companies have pursued innovation to obtain a competitive edge. Thus, educational reform focuses mainly on training creative students. This study adopted the concept of an affiliated network of projects to investigate how project embeddedness influences project team creativity. This work surveys 60 projects in a Management Information Systems Department of a University. Validity of the specific study hypotheses is tested by using moderate hierarchical regression analysis to determine how project embeddedness affects project team creativity and assess how the team innovation climate moderates the relationships between project embeddedness and project team creativity. Analytical results indicate a positive association between structural embeddedness and project team creativity, a negative relationship between positional embeddedness and project team creativity, and a positive influence of team innovation climate on the relationships between network embeddedness and project team creativity. An attempt is also made to understand the role of positional embeddedness by classifying the interactions based on the content of interactions. According to those results, positional embeddedness is positively related to project team creativity during problem–identification interaction; during solution–design interaction, positional embeddedness is negatively related to project team creativity. Results of this study explain the phenomena of divergent thinking and convergent thinking during creative development.}
}
@article{199064,
title = {Natural languages: Berwick, R ‘Natural language computational complexity and generative capacity’ Comput. Artif. Intell. Vol 8 No 5 (1989) pp 423–441},
journal = {Knowledge-Based Systems},
volume = {3},
number = {1},
pages = {64},
year = {1990},
issn = {0950-7051},
doi = {https://doi.org/10.1016/0950-7051(90)90091-U},
url = {https://www.sciencedirect.com/science/article/pii/095070519090091U}
}
@article{MARKMAN20181,
title = {Combining the Strengths of Naturalistic and Laboratory Decision-Making Research to Create Integrative Theories of Choice},
journal = {Journal of Applied Research in Memory and Cognition},
volume = {7},
number = {1},
pages = {1-10},
year = {2018},
issn = {2211-3681},
doi = {https://doi.org/10.1016/j.jarmac.2017.11.005},
url = {https://www.sciencedirect.com/science/article/pii/S2211368117301778},
author = {Arthur B. Markman},
keywords = {Decision making, Naturalistic decision making, External validity, Internal validity},
abstract = {Naturalistic decision-making research contrasts with traditional laboratory research along a number of dimensions. It is typically more observational, more focused on expert performance, and more attentive to the context in which decisions are made than laboratory studies. This approach helps to shore up some of the weaknesses of laboratory research by providing incentive to develop integrative theories of choice and examining strong methods of problem solving in a choice domain. This paper contrasts the strengths and weaknesses of laboratory and naturalistic approaches to decision making. Then, it explores strategies for using both of these approaches as well as mathematical and computational modeling to find the optimal tradeoff between internal and external validity for research projects.}
}
@incollection{GORI2024339,
title = {Chapter 6 - Learning with constraints},
editor = {Marco Gori and Alessandro Betti and Stefano Melacci},
booktitle = {Machine Learning (Second Edition)},
publisher = {Morgan Kaufmann},
edition = {Second Edition},
pages = {339-442},
year = {2024},
isbn = {978-0-323-89859-1},
doi = {https://doi.org/10.1016/B978-0-32-389859-1.00013-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780323898591000131},
author = {Marco Gori and Alessandro Betti and Stefano Melacci},
keywords = {Support constraint machines, Learning from constraints, Lifelong learning, Constraint satisfaction, Penalty functions, Logic constraints, t-norms, Recurrent neural networks, Long short term memory networks (LSTM), Graphical models},
abstract = {This chapter provides a unified view of learning and inference in structured environments that are formally expressed as constraints that involve both data and tasks. A preliminary discussion has been put forward in Section 1.1.5, where we began proposing an abstract interpretation of the ordinary notion of constraint that characterizes human-based learning, reasoning, and decision processes. Here we make an effort to formalize those processes and explore the corresponding computational aspects. A first fundamental remark for the formulation of a sound theory is that most interesting real-world problems correspond with learning environments that are heavily structured, a feature that has been mostly neglected in the previous chapters on linear and kernel machines, as well as on deep networks. So far we have been mostly concerned with machine learning models where the agent takes a decision on patterns represented by x∈Rd, whereas we have mostly neglected the issue of constructing appropriate representations from the environmental information e∈E. The discussion in Section 1.1.5 has already stimulated the need of processing information organized as lists, trees, and graphs. Interestingly, in this chapter, it is shown that computational models, like recurrent neural networks and graph neural networks can also be regarded as a way for expressing appropriate constraints on environmental data by means of diffusion processes. In these cases the distinguishing feature of the computational model is that the focus is on uniform diffusion processes, whereas one can think of constraints that involve both data and tasks in a more general way. Basically, different vertexes of a graph that model the environment can be involved in different relations, thus giving rise to a different treatment. As a result, this yields richer computational mechanisms that involve the meaning attached to the different relations.}
}
@article{RADTKE2022102355,
title = {Smart energy systems beyond the age of COVID-19: Towards a new order of monitoring, disciplining and sanctioning energy behavior?},
journal = {Energy Research & Social Science},
volume = {84},
pages = {102355},
year = {2022},
issn = {2214-6296},
doi = {https://doi.org/10.1016/j.erss.2021.102355},
url = {https://www.sciencedirect.com/science/article/pii/S2214629621004461},
author = {Jörg Radtke},
keywords = {Smart city, Smart energy governmentality, Social power framework, Energy transition conflict, Energy communities, Energy democracy, Michel Foucault},
abstract = {The Corona pandemic has led to the increased use of online tools throughout society, whether in business, education, or daily life. This shift to an online society has led social scientists to question the extent to which increased forms of control, surveillance and enforced conformity to ways of thinking, attitudes and behaviors can be promoted through online activities. This question arises overtly amidst a pandemic, but it also lurks behind the widespread diffusion of smart energy systems throughout the world and the increased use of smart meters in those systems. The extent to which forms of monitoring, disciplining and sanctioning of energy behavior and practices could come to reality is thus an important question to consider. This article does so using the ideas of Michel Foucault, together with research on smart energy systems and current trends in energy policy. The article closes with a discussion of energy democracy and democratic legitimacy in the context of possible effects of smart technologies on community energy systems.}
}
@incollection{JANELLE2015415,
title = {Time-Space in Geography},
editor = {James D. Wright},
booktitle = {International Encyclopedia of the Social & Behavioral Sciences (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {415-420},
year = {2015},
isbn = {978-0-08-097087-5},
doi = {https://doi.org/10.1016/B978-0-08-097086-8.72070-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780080970868720708},
author = {Donald G. Janelle},
keywords = {Activity patterns, Communication, Cyberinfrastructure, Geographic information systems, Human extensibility, Space-time path, Space-time prism, Time geography, Time-space compression, Time-space convergence, Time-space distanciation, Transportation, Travel},
abstract = {This article reviews the development of time-space perspectives in geography and exposes linkages between these perspectives and society's prevailing technologies for travel and communication. Special attention is given to the role of information, computation, and visualization technologies that shape the research practices that advance the potential to understand social organization and human activity behavior in a time-space context.}
}
@article{COMPANY2009592,
title = {Computer-aided sketching as a tool to promote innovation in the new product development process},
journal = {Computers in Industry},
volume = {60},
number = {8},
pages = {592-603},
year = {2009},
note = {Computer Aided Innovation},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2009.05.018},
url = {https://www.sciencedirect.com/science/article/pii/S016636150900133X},
author = {Pedro Company and Manuel Contero and Peter Varley and Nuria Aleixos and Ferran Naya},
keywords = {Engineering design and innovation, CAI software, Computer-aided sketching},
abstract = {Sketching is an established part of engineering culture. Sketches assist product designers during the creative stages of design and help them to develop inventions. Paper-and-pencil sketching is highly useful but lacks functionalities, mainly because it is disconnected from the rest of the (computer-aided) design process. However, CAS tools are not yet as usable as paper-and-pencil, although they provide full integration with the subsequent phases of the design processes (CAD, CAE, CAM, etc.) and other interesting functionalities. We desire computer-aided sketching (CAS) tools which furnish users with the sketching environment they require to make full use of their conceptual design and innovation talents, while providing full integration with the subsequent phases of the design processes (CAD, CAE, CAM, etc.). In this paper we discuss the importance of sketching in conceptual design, we review the current situation of engineering sketching, and we then analyze the main characteristics which a successful and fully integrated CAS tool should include. We consider CAS, not as a single problem, but as at least three: thinking, prescriptive and talking sketches require different approaches to functionality. Finally, we present the current state of the art in CAS tools by describing the main features and outstanding problems of our own applications.}
}
@article{BELLANTE2025104341,
title = {Evaluating the potential of quantum machine learning in cybersecurity: A case-study on PCA-based intrusion detection systems},
journal = {Computers & Security},
volume = {154},
pages = {104341},
year = {2025},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2025.104341},
url = {https://www.sciencedirect.com/science/article/pii/S0167404825000306},
author = {Armando Bellante and Tommaso Fioravanti and Michele Carminati and Stefano Zanero and Alessandro Luongo},
keywords = {Quantum computing, Quantum machine learning, QML, Evaluation, Framework, Impact, PCA, Principal component analysis, Network intrusion detection, Network security},
abstract = {Quantum computing promises to revolutionize our understanding of the limits of computation, and its implications in cryptography have long been evident. Today, cryptographers are actively devising post-quantum solutions to counter the threats posed by quantum-enabled adversaries. Meanwhile, quantum scientists are innovating quantum protocols to empower defenders. However, the broader impact of quantum computing and quantum machine learning (QML) on other cybersecurity domains still needs to be explored. In this work, we investigate the potential impact of QML on cybersecurity applications of traditional ML. First, we explore the potential advantages of quantum computing in machine learning problems specifically related to cybersecurity. Then, we describe a methodology to quantify the future impact of fault-tolerant QML algorithms on real-world problems. As a case study, we apply our approach to standard methods and datasets in network intrusion detection, one of the most studied applications of machine learning in cybersecurity. Our results provide insight into the conditions for obtaining a quantum advantage and the need for future quantum hardware and software advancements.}
}
@article{YU2024107998,
title = {Bridging the gap: Geometry-centric discriminative manifold distribution alignment for enhanced classification in colorectal cancer imaging},
journal = {Computers in Biology and Medicine},
volume = {170},
pages = {107998},
year = {2024},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2024.107998},
url = {https://www.sciencedirect.com/science/article/pii/S0010482524000829},
author = {Weiwei Yu and Nuo Xu and Nuanhui Huang and Houliang Chen},
keywords = {Medical image analysis, Colorectal cancer detection, Domain adaptation, Transfer learning, Manifold learning, Computational oncology},
abstract = {The early detection of colorectal cancer (CRC) through medical image analysis is a pivotal concern in healthcare, with the potential to significantly reduce mortality rates. Current Domain Adaptation (DA) methods strive to mitigate the discrepancies between different imaging modalities that are critical in identifying CRC, yet they often fall short in addressing the complexity of cancer's presentation within these images. These conventional techniques typically overlook the intricate geometrical structures and the local variations within the data, leading to suboptimal diagnostic performance. This study introduces an innovative application of the Discriminative Manifold Distribution Alignment (DMDA) method, which is specifically engineered to enhance the medical image diagnosis of colorectal cancer. DMDA transcends traditional DA approaches by focusing on both local and global distribution alignments and by intricately learning the intrinsic geometrical characteristics present in manifold space. This is achieved without depending on the potentially misleading pseudo-labels, a common pitfall in existing methodologies. Our implementation of DMDA on three distinct datasets, involving several unique DA tasks, has consistently demonstrated superior classification accuracy and computational efficiency. The method adeptly captures the complex morphological and textural nuances of CRC lesions, leading to a significant leap in domain adaptation technology. DMDA's ability to reconcile global and local distributional disparities, coupled with its manifold-based geometrical structure learning, signals a paradigm shift in medical imaging analysis. The results obtained are not only promising in terms of advancing domain adaptation theory but also in their practical implications, offering the prospect of substantially improved diagnostic accuracy and faster clinical workflows. This heralds a transformative approach in personalized oncology care, aligning with the pressing need for early and accurate CRC detection.}
}
@article{CHEN2022101380,
title = {An automated quality evaluation framework of psychotherapy conversations with local quality estimates},
journal = {Computer Speech & Language},
volume = {75},
pages = {101380},
year = {2022},
issn = {0885-2308},
doi = {https://doi.org/10.1016/j.csl.2022.101380},
url = {https://www.sciencedirect.com/science/article/pii/S0885230822000213},
author = {Zhuohao Chen and Nikolaos Flemotomos and Karan Singla and Torrey A. Creed and David C. Atkins and Shrikanth Narayanan},
keywords = {Cognitive behavioral therapy, Computational linguistics, Hierarchical framework, Local quality estimates},
abstract = {Text-based computational approaches for assessing the quality of psychotherapy are being developed to support quality assurance and clinical training. However, due to the long durations of typical conversation based therapy sessions, and due to limited annotated modeling resources, computational methods largely rely on frequency-based lexical features or dialogue acts to assess the overall session level characteristics. In this work, we propose a hierarchical framework to automatically evaluate the quality of transcribed Cognitive Behavioral Therapy (CBT) interactions. Given the richly dynamic nature of the spoken dialog within a talk therapy session, to evaluate the overall session level quality, we propose to consider modeling it as a function of local variations across the interaction. To implement that empirically, we divide each psychotherapy session into conversation segments and initialize the segment-level qualities with the session-level scores. First, we produce segment embeddings by fine-tuning a BERT-based model, and predict segment-level (local) quality scores. These embeddings are used as the lower-level input to a Bidirectional LSTM-based neural network to predict the session-level (global) quality estimates. In particular, we model the global quality as a linear function of the local quality scores, which allows us to update the segment-level quality estimates based on the session-level quality prediction. These newly estimated segment-level scores benefit the BERT fine-tuning process, which in turn results in better segment embeddings. We evaluate the proposed framework on automatically derived transcriptions from real-world CBT clinical recordings to predict session-level behavior codes. The results indicate that our approach leads to improved evaluation accuracy for most codes when used for both regression and classification tasks.}
}
@article{MARIN2021100015,
title = {Human macrophage polarization in the response to Mycobacterium leprae genomic DNA},
journal = {Current Research in Microbial Sciences},
volume = {2},
pages = {100015},
year = {2021},
issn = {2666-5174},
doi = {https://doi.org/10.1016/j.crmicr.2020.100015},
url = {https://www.sciencedirect.com/science/article/pii/S2666517420300171},
author = {Alberto Marin and Kristopher {Van Huss} and John Corbett and Sangjin Kim and Jonathon Mohl and Bo-young Hong and Jorge Cervantes},
keywords = {RNAseq, , Leprosy, Macrophage polarization},
abstract = {Infection with Mycobacterium leprae, the causative organism of leprosy, is still endemic in numerous parts of the world including the southwestern United States. The broad variation of symptoms in the leprosy disease spectrum range from the milder tuberculoid leprosy (paucibacillary) to the more severe and disfiguring lepromatous leprosy (multibacillary). The established thinking in the health community is that host response, rather than M. leprae strain variation, is the reason for the range of disease severity. More recent discoveries suggest that macrophage polarization also plays a significant role in the spectrum of leprosy disease but to what degree it contributes is not fully established. In this study, we aimed to analyze if different strains of M. leprae elicit different transcription responses in human macrophages, and to examine the role of macrophage polarization in these responses. Genomic DNA from three different strains of M. leprae DNA (Strains NHDP, Br4923, and Thai-53) were used to stimulate human macrophages under three polarization conditions (M1, M1-activated, and M2). Transcriptome analysis revealed a large number of differentially expressed (DE) genes upon stimulation with DNA from M. leprae strain Thai-53 compared to strains NHDP and Br4923, independent of the macrophage polarization condition. We also found that macrophage polarization affects the responses to M. leprae DNA, with up-regulation of numerous interferon stimulated genes. These findings provide a deeper understanding of the role of macrophage polarization in the recognition of M. leprae DNA, with the potential to improve leprosy treatment strategies.}
}
@article{SREENATH1992121,
title = {A hybrid computation environment for multibody simulation},
journal = {Mathematics and Computers in Simulation},
volume = {34},
number = {2},
pages = {121-140},
year = {1992},
issn = {0378-4754},
doi = {https://doi.org/10.1016/0378-4754(92)90049-M},
url = {https://www.sciencedirect.com/science/article/pii/037847549290049M},
author = {N. Sreenath},
abstract = {A simulation architecture capable of generating the dynamical equations of a multibody system symbolically, automatically creating the computer code to simulate these equations numerically, run the simulation and display the results using animation and graphics is discussed. The power of object-oriented programming is used systematically to manipulate the symbolic, numeric and graphic modules and produce an effective tool for understanding the complicated motions of multibody systems. The architecture has been implemented in OOPSS (Object-Oriented Planar System Simulator) a software package written in a multilanguage (macsyma–fortran–lisp) environment. The package supports user interface capable of interactively modifying system parameters, change runtime initial conditions and introduce feedback control.}
}
@article{CARBONELL2016145,
title = {The role of metaphors in the development of technologies. The case of the artificial intelligence},
journal = {Futures},
volume = {84},
pages = {145-153},
year = {2016},
note = {SI: Metaphors in FS},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2016.03.019},
url = {https://www.sciencedirect.com/science/article/pii/S0016328715300902},
author = {Javier Carbonell and Antonio Sánchez-Esguevillas and Belén Carro},
keywords = {CLA, Metaphors, Artificial intelligence, Lakoff and Johnson},
abstract = {Technology plays a prominent role in configuring the way we live and work. In this paper we go further and think that it is a first level driver in the configuration of our deepest perceptions and has a paramount influence on shaping our worldviews and metaphors, though this aspect goes unnoticed for most of the population. In this paper we analyze how metaphors take action in the characterization of technologies, mainly emerging technologies, and in their evolution, and furthermore the impact of technologies and metaphors on the way we perceive our daily life. We analyze metaphors underlying brain nature and artificial intelligence, raising the connections between them and showing how metaphors in one of these fields impact on the way we understand the other. This fact has important consequences, for instance it conditions the evolution of computational systems, and we propose two scenarios for this evolution. This paper relies on the conceptual model and classification of metaphors proposed by Lakoff and Johnson in “Metaphors we live by”, from the orientational metaphors that show values and mantras, to the deepest structural metaphors that are reconfiguring how life is conceived. It also relies on CLA (Causal Layered Analysis) and to its reference book “CLA 2.0” in order to insert this analysis in a wider and future oriented framework and to analyze scenarios.}
}
@article{FAIRHALL2014ix,
title = {The receptive field is dead. Long live the receptive field?},
journal = {Current Opinion in Neurobiology},
volume = {25},
pages = {ix-xii},
year = {2014},
note = {Theoretical and computational neuroscience},
issn = {0959-4388},
doi = {https://doi.org/10.1016/j.conb.2014.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S0959438814000361},
author = {Adrienne Fairhall},
abstract = {Advances in experimental techniques, including behavioral paradigms using rich stimuli under closed loop conditions and the interfacing of neural systems with external inputs and outputs, reveal complex dynamics in the neural code and require a revisiting of standard concepts of representation. High-throughput recording and imaging methods along with the ability to observe and control neuronal subpopulations allow increasingly detailed access to the neural circuitry that subserves neural representations and the computations they support. How do we harness theory to build biologically grounded models of complex neural function?}
}
@article{SOPER2022126712,
title = {Quantifying the effect of solvent on the morphology of organic crystals using a statistical thermodynamics approach},
journal = {Journal of Crystal Growth},
volume = {591},
pages = {126712},
year = {2022},
issn = {0022-0248},
doi = {https://doi.org/10.1016/j.jcrysgro.2022.126712},
url = {https://www.sciencedirect.com/science/article/pii/S0022024822002007},
author = {Eleanor M. Soper and Radoslav Y. Penchev and Stephen M. Todd and Frank Eckert and Marc Meunier},
keywords = {A2. Solvent screening, A2. Particle engineering, A1. Surface chemistry, A1. Cosmo-RS, A1. Morphology},
abstract = {A method for predicting the effect of solvent on the morphology of organic crystals is presented, providing an efficient screening tool for identifying ideal crystallization solvents. The solvent effect is estimated by the computation of chemical potentials and activity coefficients of crystal surfaces using a first principles-based statistical thermodynamics approach. Density functional theory and COSMO-RS are utilized to determine the activity coefficients of the crystal growth faces of a selection of active pharmaceutical ingredients (APIs) in solvents across a broad range of polarities. The ability of COSMO-RS to predict and quantify the effects of solvent on crystal growth and morphology is assessed using hierarchical clustering to classify the solvents according to their overall interaction strength with the crystal faces. The COSMO-RS approach allows for a physical interpretation of the predictions in terms of surface polarity and is confirmed by comparison to published experimental data. Herein a methodology is reported for automated computation of the activity coefficients of all solvent-surface pairs directly from the drug crystal structure. The procedure goes beyond the traditional trial-and-error solvent selection process and has the potential to be used as a rapid computational screening tool in pharmaceutical drug development.}
}
@article{BASHIRPOURBONAB2023122642,
title = {In complexity we trust: A systematic literature review of urban quantum technologies},
journal = {Technological Forecasting and Social Change},
volume = {194},
pages = {122642},
year = {2023},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2023.122642},
url = {https://www.sciencedirect.com/science/article/pii/S004016252300327X},
author = {Aysan {Bashirpour Bonab} and Maria Fedele and Vincenzo Formisano and Ihor Rudko},
keywords = {Quantum City, Uncertainty, Duality, Parallelism, Quantum mechanics, Systematic literature review},
abstract = {Today's cities are facing increasingly complex challenges. The growing uncertainty and complexity—caused by the unremitted differentiation of social, environmental, and technological orders—call for novel ways of conceptualizing urban reality. Although technology-oriented solutions shape the most efficient strategies to manage complexity in contemporary cities, ensuring an effective transition toward a Quantum City paradigm can grant considerable advantages for city administrators and managers facing looming urban challenges. In this article, we introduce the Quantum City metaphor—grounded in fundamental notions of quantum mechanics—as a new conceptual lens for investigating urban complexity. We then build upon the metaphor, theorizing a set of assumptions grounded in three fundamental concepts of quantum theory: relativity, uncertainty, and duality/parallelism. Finally, we propose an empirical conceptualization of Quantum Cities based on the concrete adoption of quantum technologies to deal with urban complexity. This is achieved through a systematic literature review of scholarly records on quantum technologies in the context of social sciences, emphasizing related urban problematics and challenges. Principal component analysis and agglomerative hierarchical clustering reveal two types of quantum technologies most useful for city planners and managers: quantum communication and quantum computing. Accordingly, we perform a qualitative thematic synthesis of related scholarly records, emphasizing the negative and positive aspects of both types of urban quantum technologies.}
}
@article{NAZI2025100124,
title = {Evaluation of open and closed-source LLMs for low-resource language with zero-shot, few-shot, and chain-of-thought prompting},
journal = {Natural Language Processing Journal},
volume = {10},
pages = {100124},
year = {2025},
issn = {2949-7191},
doi = {https://doi.org/10.1016/j.nlp.2024.100124},
url = {https://www.sciencedirect.com/science/article/pii/S2949719124000724},
author = {Zabir Al Nazi and Md. Rajib Hossain and Faisal Al Mamun},
keywords = {Large language models, Zero-shot, Few-shot, Chain-of-thought, GPT-4, Llama 3, Ablation studies, Prompting, LLM reasoning, Low-resource, Bangla},
abstract = {As the global deployment of Large Language Models (LLMs) increases, the demand for multilingual capabilities becomes more crucial. While many LLMs excel in real-time applications for high-resource languages, few are tailored specifically for low-resource languages. The limited availability of text corpora for low-resource languages, coupled with their minimal utilization during LLM training, hampers the models’ ability to perform effectively in real-time applications. Additionally, evaluations of LLMs are significantly less extensive for low-resource languages. This study offers a comprehensive evaluation of both open-source and closed-source multilingual LLMs focused on low-resource language like Bengali, a language that remains notably underrepresented in computational linguistics. Despite the limited number of pre-trained models exclusively on Bengali, we assess the performance of six prominent LLMs, i.e., three closed-source (GPT-3.5, GPT-4o, Gemini) and three open-source (Aya 101, BLOOM, LLaMA) across key natural language processing (NLP) tasks, including text classification, sentiment analysis, summarization, and question answering. These tasks were evaluated using three prompting techniques: Zero-Shot, Few-Shot, and Chain-of-Thought (CoT). This study found that the default hyperparameters of these pre-trained models, such as temperature, maximum token limit, and the number of few-shot examples, did not yield optimal outcomes and led to hallucination issues in many instances. To address these challenges, ablation studies were conducted on key hyperparameters, particularly temperature and the number of shots, to optimize Few-Shot learning and enhance model performance. The focus of this research is on understanding how these LLMs adapt to low-resource downstream tasks, emphasizing their linguistic flexibility and contextual understanding. Experimental results demonstrated that the closed-source GPT-4o model, utilizing Few-Shot learning and Chain-of-Thought prompting, achieved the highest performance across multiple tasks: an F1 score of 84.54% for text classification, 99.00% for sentiment analysis, a F1bert score of 72.87% for summarization, and 58.22% for question answering. For transparency and reproducibility, all methodologies and code from this study are available on our GitHub repository: https://github.com/zabir-nabil/bangla-multilingual-llm-eval.}
}
@article{KRAUSE20211094,
title = {The challenge of ensuring affordability, sustainability, consistency, and adaptability in the common metrics agenda},
journal = {The Lancet Psychiatry},
volume = {8},
number = {12},
pages = {1094-1102},
year = {2021},
issn = {2215-0366},
doi = {https://doi.org/10.1016/S2215-0366(21)00122-X},
url = {https://www.sciencedirect.com/science/article/pii/S221503662100122X},
author = {Karolin Rose Krause and Sophie Chung and Maria da Luz {Sousa Fialho} and Peter Szatmari and Miranda Wolpert},
abstract = {Summary
Mental health research grapples with research waste and stunted field progression caused by inconsistent outcome measurement across studies and clinical settings, which means there is no common language for considering findings. Although recognising that no gold standard measures exist and that all existing measures are flawed in one way or another, anxiety and depression research is spearheading a common metrics movement to harmonise measurement, with several initiatives over the past 5 years recommending the consistent use of specific scales to allow read-across of measurements between studies. For this approach to flourish, however, common metrics must be acceptable and adaptable to a range of contexts and populations, and global access should be as easy and affordable as possible, including in low-income countries. Within a measurement landscape dominated by fixed proprietary measures and with competing views of what should be measured, achieving this goal poses a range of challenges. In this Personal View, we consider tensions between affordability, sustainability, consistency, and adaptability that, if not addressed, risk undermining the common metrics agenda. We outline a three-pronged way forward that involves funders taking more direct responsibility for measure development and dissemination; a move towards managing measure dissemination and adaptation via open-access measure hubs; and transitioning from fixed questionnaires to item banks. We argue that now is the time to start thinking of mental health metrics as 21st century tools to be co-owned and co-created by the mental health community, with support from dedicated infrastructure, coordinating bodies, and funders.}
}
@article{NAGANANDHINI2019548,
title = {Effective Diagnosis of Alzheimer’s Disease using Modified Decision Tree Classifier},
journal = {Procedia Computer Science},
volume = {165},
pages = {548-555},
year = {2019},
note = {2nd International Conference on Recent Trends in Advanced Computing ICRTAC -DISRUP - TIV INNOVATION , 2019 November 11-12, 2019},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.01.049},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920300570},
author = {S. Naganandhini and P. Shanmugavadivu},
keywords = {Alzheimer’s Disease, Feature Selection, Classification, Decision Tree, Early Detection, Hyper Parameter Tuning},
abstract = {Alzheimer’s disease (AD) is described as a severe form of the neural disorder that collectively degenerate the essential cognitive activities of a human being (thinking, memory retention, etc.,) in particular among the elderly individuals and eventually results in death. In addition to the adverse ill-health effects on the patients, AD imposes paramount responsibility and burden on the caretakers too. Several genetic and pathological traits and non-invasive diagnostic strategies are being vigorously investigated and explored to discover the early onset of this debilitating disease. The prognosis of AD assumes importance, as the deterioration of health due to its progression may be either contained or controlled. Moreover, early and accurate detection of AD helps medical practitioners to prescribe case-specific medical treatment procedure. Among the popular machine learning algorithms, decision tree technique is widely used for classification/prediction, due to its accuracy and speed.This research article presents a novel decision tree-based classification technique, with optimum hyper parametertuning, that is ideally suitable for AD diagnosis, even at the early stages of development. The performance of this newly proposed Decision Tree Classifier with Hyper Parameters Tuning (DTC-HPT) is validated on the Open Access Imaging Studies Series (OASIS) dataset that contains patients’ data on the different stages of AD. The DTC-HPT is designed with the primary objective to classify the nature of brain abnormality using the most relevantand potentially significant data attributes/parameters. The efficiency of DTC-HPT on AD classification is measured as Accuracy, Precision, Recall, and F1-Score. The correctness of AD classification by DTC-HPTwith an average accuracy of 99.10% endorse that this classification technique can be used for AD detection on the AD clinical datasets.}
}
@article{THEISE200417,
title = {Understanding cell lineages as complex adaptive systems},
journal = {Blood Cells, Molecules, and Diseases},
volume = {32},
number = {1},
pages = {17-20},
year = {2004},
note = {Stem Cell Plasticity},
issn = {1079-9796},
doi = {https://doi.org/10.1016/j.bcmd.2003.09.010},
url = {https://www.sciencedirect.com/science/article/pii/S1079979603002523},
author = {Neil D Theise and Mark d'Inverno},
keywords = {Stem cells, Lineage system, Reactive systems},
abstract = {Stem cells may be considered complex reactive systems because of their vast number in a living system, their reactive nature, and the influence of local environmental factors (such as the state of neighboring cells, tissue matrix, stem cell physiological processes) on their behavior. In such systems, emergent global behavior arises through the multitude of local interactions among the cell agents. Approaching hematopoietic and other stem cell lineages from this perspective have critical ramifications on current thinking relating to the plasticity of these lineage systems, the modeling of stem cell systems, and the interpretation of clinical data regarding many diseases within such models.}
}
@article{LILWALL1989268,
title = {Seismological Algorithms, Computational Methods and Computer Programs: Durk J. Doornbos (Editor), Academic Press/Harcourt Brace Jovanovich, 1988, 469 pp., £39.50, ISBN 0-12-220770-X},
journal = {Physics of the Earth and Planetary Interiors},
volume = {58},
number = {2},
pages = {268-269},
year = {1989},
issn = {0031-9201},
doi = {https://doi.org/10.1016/0031-9201(89)90062-9},
url = {https://www.sciencedirect.com/science/article/pii/0031920189900629},
author = {R.C. Lilwall}
}
@article{DESCIOLI2011204,
title = {The omission effect in moral cognition: toward a functional explanation},
journal = {Evolution and Human Behavior},
volume = {32},
number = {3},
pages = {204-215},
year = {2011},
issn = {1090-5138},
doi = {https://doi.org/10.1016/j.evolhumbehav.2011.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S1090513811000055},
author = {Peter DeScioli and Rebecca Bruening and Robert Kurzban},
keywords = {Omission, Transparency, Moral judgment, Moral psychology},
abstract = {Moral judgment involves much more than computations of the expected consequences of behavior. A prime example of the complexity of moral thinking is the frequently replicated finding that violations by omission are judged less morally wrong than violations by commission, holding intentions constant. Here we test a novel hypothesis: Omissions are judged less harshly because they produce little material evidence of wrongdoing. Evidence is crucial because moral accusations are potentially very costly unless supported by others. In our experiments, the omission effect was eliminated when physical evidence showed that an omission was chosen. Perpetrators who “opted out” by pressing a button that would clearly have no causal effects on the victim, rather than rescuing them, were judged as harshly as perpetrators who directly caused death. These results show that, to reduce condemnation, omissions must not only be noncausal, they must also leave little or no material evidence that a choice was made.}
}
@article{NEMETH2024101385,
title = {The interplay between subcortical and prefrontal brain structures in shaping ideological belief formation and updating},
journal = {Current Opinion in Behavioral Sciences},
volume = {57},
pages = {101385},
year = {2024},
issn = {2352-1546},
doi = {https://doi.org/10.1016/j.cobeha.2024.101385},
url = {https://www.sciencedirect.com/science/article/pii/S2352154624000366},
author = {Dezső Németh and Teodóra Vékony and Gábor Orosz and Zoltán Sarnyai and Leor Zmigrod},
abstract = {History illustrates that economic crises and other sociopolitical threats often lead to a rise of polarization and radicalism, whereby people become more susceptible to intolerant political messages, including propaganda and ideological rhetoric. Political science, sociology, economics, and psychology have explored many dimensions of this phenomenon, yet a critical piece of the puzzle is still missing: what cognitive and neural mechanisms in the brain mediate between these threats and responsiveness to political messages? To answer this question, here, we present a theory that combines cognitive neuroscience theories, namely stress-induced memory shift and competitive cognitive processes, with political science. Our Threat-based Neural Switch Theory posits that the processing of political information, similarly to other information processing, is shaped by the competitive interaction between goal-directed and habitual processes. Threats, including resource overload or scarcity, can shift neural networks toward receptiveness to oversimplified political messages. This theory sets out a research program aimed at discovering the cognitive and neural underpinning of how situational factors alter brain functions and modify political information processing.}
}
@article{YU2022102230,
title = {Spatial processing rather than logical reasoning was found to be critical for mathematical problem-solving},
journal = {Learning and Individual Differences},
volume = {100},
pages = {102230},
year = {2022},
issn = {1041-6080},
doi = {https://doi.org/10.1016/j.lindif.2022.102230},
url = {https://www.sciencedirect.com/science/article/pii/S1041608022001170},
author = {Mingxin Yu and Jiaxin Cui and Li Wang and Xing Gao and Zhanling Cui and Xinlin Zhou},
keywords = {Logical reasoning, Spatial processing, Mathematical problem-solving},
abstract = {Students' ability to solve mathematical problems is a standard mathematical skill; however, its cognitive correlates are unclear. Thus, this study aimed to examine whether spatial processing (mental rotation, paper folding, and the Corsi blocks test) and logical reasoning (abstract and concrete syllogisms) were correlated with mathematical problem-solving (word problems and geometric proofing) for college students. The regression results showed that after controlling for gender, age, general IQ, language processing, cognitive processing (visual perception, attention, and memory skills), and number sense and arithmetic computation skills, spatial processing skills still predicted mathematical problem-solving and geometry skills in Chinese college students. Contrastingly, logical reasoning measures related to syllogisms did not predict after controlling for these variables. Further, notably, it did not correlate significantly with geometry performance when no control variables were included. Our results suggest that spatial processing is a significant component of math skills involving word and geometry problems (even after controlling for multiple key cognitive factors).}
}
@article{SOLIMAN2025100131,
title = {A comparative analysis of encoder only and decoder only models for challenging LLM-generated STEM MCQs using a self-evaluation approach},
journal = {Natural Language Processing Journal},
volume = {10},
pages = {100131},
year = {2025},
issn = {2949-7191},
doi = {https://doi.org/10.1016/j.nlp.2025.100131},
url = {https://www.sciencedirect.com/science/article/pii/S294971912500007X},
author = {Ghada Soliman and Hozaifa Zaki and Mohamed Kilany},
keywords = {NLP, LLM, SLM, Self-evaluation, MCQ},
abstract = {Large Language Models (LLMs) have demonstrated impressive capabilities in various tasks, including Multiple-Choice Question Answering (MCQA) evaluated on benchmark datasets with few-shot prompting. Given the absence of benchmark Science, Technology, Engineering, and Mathematics (STEM) datasets on Multiple-Choice Questions (MCQs) created by LLMs, we employed various LLMs (e.g., Vicuna-13B, Bard, and GPT-3.5) to generate MCQs on STEM topics curated from Wikipedia. We evaluated open-source LLM models such as Llama 2-7B and Mistral-7B Instruct, along with an encoder model such as DeBERTa v3 Large, on inference by adding context in addition to fine-tuning with and without context. The results showed that DeBERTa v3 Large and Mistral-7B Instruct outperform Llama 2-7B, highlighting the potential of LLMs with fewer parameters in answering hard MCQs when given the appropriate context through fine-tuning. We also benchmarked the results of these models against closed-source models such as Gemini and GPT-4 on inference with context, showcasing the potential of narrowing the gap between open-source and closed-source models when context is provided. Our work demonstrates the capabilities of LLMs in creating more challenging tasks that can be used as self-evaluation for other models. It also contributes to understanding LLMs’ capabilities in STEM MCQs tasks and emphasizes the importance of context for LLMs with fewer parameters in enhancing their performance.}
}
@incollection{HUANG2006586,
title = {Neo-Gricean Pragmatics},
editor = {Keith Brown},
booktitle = {Encyclopedia of Language & Linguistics (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {586-590},
year = {2006},
isbn = {978-0-08-044854-1},
doi = {https://doi.org/10.1016/B0-08-044854-2/04529-6},
url = {https://www.sciencedirect.com/science/article/pii/B0080448542045296},
author = {Y. Huang},
keywords = {classical Gricean pragmatics, conversational implicature, division of pragmatic labor, explicature, Grice's circle, Horn-scales, I-impliciture, implicature, interaction between Q-, I, and M-implicatures, maxims, M-implicature, neo-Gricean pragmatics, pragmatic intrusion, pragmatics, presumptive meaning, Q-implicature, R-implicature},
abstract = {Since its inception, Gricean pragmatics has revolutionized pragmatic theorizing and has to date remained one of the cornerstones of contemporary thinking in linguistic pragmatics and the philosophy of language. This article undertakes to present and assess a neo-Gricean pragmatic theory of conversational implicature, focusing on the bipartite model developed by Laurence Horn and the tripartite model advanced by Stephen Levinson.}
}
@article{LEALVILLASECA2025105833,
title = {Interpreting Deepkriging for spatial interpolation in geostatistics},
journal = {Computers & Geosciences},
volume = {196},
pages = {105833},
year = {2025},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2024.105833},
url = {https://www.sciencedirect.com/science/article/pii/S0098300424003169},
author = {Fabian Leal-Villaseca and Edward Cripps and Mark Jessell and Mark Lindsay},
keywords = {Spatial statistics, Deep learning interpretability, Shapley values, Deepkriging, Batched Shapley},
abstract = {In the current era marked by an unprecedented abundance of data, the usage of conventional methods such as kriging persists in some applications of geostatistics, despite their limitations in adequately capturing the intricate relationships found in contemporary, multivariate datasets. Although deep neural networks (DNNs) have demonstrated remarkable efficacy in capturing complex nonlinear feature relationships across various domains, their success in geostatistical applications has been limited. This can be partly attributed to two significant challenges. Firstly, the opaque nature of these black box models raises concerns about the dependability of their outputs for critical decision-making, as the inner workings of the model remain less interpretable. Secondly, DNNs do not explicitly capture spatial dependencies within data. To address these shortcomings, we employ a methodology to interpret the recently proposed spatial DNNs known as Deepkriging, and we apply it to dry bulk rock density estimation, an often-overlooked aspect in mineral resource estimation. Through our adaptation of Shapley values—Batched Shapley—we overcome significant computational challenges to quantify feature importance for Deepkriging. This approach takes into account feature interactions, which is crucial for DNNs, as they rely on high-order interactions, especially in a complex application like mineral resource estimation. Additionally, we demonstrate in the 3D case that Deepkriging outperforms ordinary kriging and regression kriging in terms of mean squared errors, in both the purely spatial case and in the presence of auxiliary variables. Our study produces the first methodology to interpret Deepkriging, which is suitable for any model with a large number of features; it reaffirms the efficacy of Deepkriging through several comparisons in a 3D application, and most importantly; it underscores the adaptability and broader potential of DNNs to cater to various challenges in geostatistics.}
}
@article{WU2025200236,
title = {HC-means clustering algorithm for precision marketing on e-commerce platforms},
journal = {Systems and Soft Computing},
volume = {7},
pages = {200236},
year = {2025},
issn = {2772-9419},
doi = {https://doi.org/10.1016/j.sasc.2025.200236},
url = {https://www.sciencedirect.com/science/article/pii/S2772941925000547},
author = {Dan Wu and Xin Liu},
keywords = {Big data, Cluster analysis, K-means algorithm, Precision marketing, Customer segmentation},
abstract = {Abstracts
With the rapid development of e-commerce industry, precision marketing has become a key means for enterprises to enhance competitiveness and profitability. However, traditional marketing methods often cannot accurately identify the characteristics of customers, leading to the waste of e-commerce resources. In this context, e-commerce enterprises urgently need a more accurate and efficient marketing method to meet the growing business needs. To this end, this study attempts to optimize the traditional K-means algorithm, and fundamentally improve the clustering effect in precision marketing by optimizing the selection of initial clustering centers and similarity measurement methods. Based on this, the research constructs an e-commerce marketing system based on HC-means algorithm to more accurately divide customer groups, identify high-value customers, potential customers and lost customers, and formulate differentiated marketing strategies for different groups. Experiments show that the average accuracy of HC-means algorithm in Glass database is 93.71, which is 15.48–15.79 higher than the highest accuracy of other two kinds of algorithms in the same kind of database. When the cluster number is 8, the Mahalanobis distance of HC-Means is reduced by 2.1 and 1.2 respectively compared with K-means and DBSCAN, which indicates that the clustering results are more reasonable in data distribution. When the cluster number is 3, more than half of the customers' consumption interval days are mainly concentrated between 8–12 days, and about 10 % of the customers make purchases every 2 days. These accurate customer behavior insights provide a strong basis for marketing strategy development. To sum up, the HC-Means system constructed by the research has achieved remarkable results in e-commerce precision marketing, greatly improving user satisfaction, and providing a valuable reference scheme for e-commerce enterprises to optimize marketing mode and achieve sustainable development.}
}
@article{MAJUMDAR2023100126,
title = {A novel approach for communicating with patients suffering from completely locked-in-syndrome (CLIS) via thoughts: Brain computer interface system using EEG signals and artificial intelligence},
journal = {Neuroscience Informatics},
volume = {3},
number = {2},
pages = {100126},
year = {2023},
issn = {2772-5286},
doi = {https://doi.org/10.1016/j.neuri.2023.100126},
url = {https://www.sciencedirect.com/science/article/pii/S2772528623000110},
author = {Sharmila Majumdar and Amin Al-Habaibeh and Ahmet Omurtag and Bubaker Shakmak and Maryam Asrar},
keywords = {Brain, CLIS, EEG, Feature extraction, ASPS, ANN, ALS, MND, AI},
abstract = {This paper investigates the development of an intelligent system method to address completely locked-in-syndrome (CLIS) that is caused by some illnesses such as Amyotrophic Lateral Sclerosis (ALS) as the most predominant type of Motor Neuron Disease (MND). In the last stages of ALS and despite the limitations in body movements, patients however will have a fully functional brain and cognitive capabilities and able to feel pain but fail to communicate. This paper aims to address the CLIS problem by utilizing EEG signals that human brain generates when thinking about a specific feeling or imagination as a way to communicate. The aim is to develop a low-cost and affordable system for patients to use to communicate with carers and family members. In this paper, the novel implementation of the ASPS (Automated Sensor and Signal Processing Selection) approach for feature extraction of EEG is presented to select the most suitable Sensory Characteristic Features (SCFs) to detect human thoughts and imaginations. Artificial Neural Networks (ANN) are used to verify the results. The findings show that EEG signals are able to capture imagination information that can be used as a means of communication; and the ASPS approach allows the selection of the most important features for reliable communication. This paper explains the implementation and validation of ASPS approach in brain signal classification for bespoke arrangement. Hence, future work will present the results of relatively high number of volunteers, sensors and signal processing methods.}
}
@article{FRADKIN20231013,
title = {Theory-Driven Analysis of Natural Language Processing Measures of Thought Disorder Using Generative Language Modeling},
journal = {Biological Psychiatry: Cognitive Neuroscience and Neuroimaging},
volume = {8},
number = {10},
pages = {1013-1023},
year = {2023},
note = {Natural Language Processing in Psychiatry and Clinical Neuroscience Research},
issn = {2451-9022},
doi = {https://doi.org/10.1016/j.bpsc.2023.05.005},
url = {https://www.sciencedirect.com/science/article/pii/S2451902223001258},
author = {Isaac Fradkin and Matthew M. Nour and Raymond J. Dolan},
keywords = {Computational psychiatry, GPT-2, Natural language processing, Psychosis, Schizophrenia, Thought disorder},
abstract = {Background
Natural language processing (NLP) holds promise to transform psychiatric research and practice. A pertinent example is the success of NLP in the automatic detection of speech disorganization in formal thought disorder (FTD). However, we lack an understanding of precisely what common NLP metrics measure and how they relate to theoretical accounts of FTD. We propose tackling these questions by using deep generative language models to simulate FTD-like narratives by perturbing computational parameters instantiating theory-based mechanisms of FTD.
Methods
We simulated FTD-like narratives using Generative-Pretrained-Transformer-2 by either increasing word selection stochasticity or limiting the model’s memory span. We then examined the sensitivity of common NLP measures of derailment (semantic distance between consecutive words or sentences) and tangentiality (how quickly meaning drifts away from the topic) in detecting and dissociating the 2 underlying impairments.
Results
Both parameters led to narratives characterized by greater semantic distance between consecutive sentences. Conversely, semantic distance between words was increased by increasing stochasticity, but decreased by limiting memory span. An NLP measure of tangentiality was uniquely predicted by limited memory span. The effects of limited memory span were nonmonotonic in that forgetting the global context resulted in sentences that were semantically closer to their local, intermediate context. Finally, different methods for encoding the meaning of sentences varied dramatically in performance.
Conclusions
This work validates a simulation-based approach as a valuable tool for hypothesis generation and mechanistic analysis of NLP markers in psychiatry. To facilitate dissemination of this approach, we accompany the paper with a hands-on Python tutorial.}
}
@article{TSIGKINOPOULOU2017518,
title = {Respectful Modeling: Addressing Uncertainty in Dynamic System Models for Molecular Biology},
journal = {Trends in Biotechnology},
volume = {35},
number = {6},
pages = {518-529},
year = {2017},
note = {Special Issue: Computation and Modeling},
issn = {0167-7799},
doi = {https://doi.org/10.1016/j.tibtech.2016.12.008},
url = {https://www.sciencedirect.com/science/article/pii/S0167779916302311},
author = {Areti Tsigkinopoulou and Syed Murtuza Baker and Rainer Breitling},
abstract = {Although there is still some skepticism in the biological community regarding the value and significance of quantitative computational modeling, important steps are continually being taken to enhance its accessibility and predictive power. We view these developments as essential components of an emerging ‘respectful modeling’ framework which has two key aims: (i) respecting the models themselves and facilitating the reproduction and update of modeling results by other scientists, and (ii) respecting the predictions of the models and rigorously quantifying the confidence associated with the modeling results. This respectful attitude will guide the design of higher-quality models and facilitate the use of models in modern applications such as engineering and manipulating microbial metabolism by synthetic biology.}
}
@article{VALLEETOURANGEAU2016195,
title = {Insight with hands and things},
journal = {Acta Psychologica},
volume = {170},
pages = {195-205},
year = {2016},
issn = {0001-6918},
doi = {https://doi.org/10.1016/j.actpsy.2016.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S0001691816301755},
author = {Frédéric Vallée-Tourangeau and Sune Vork Steffensen and Gaëlle Vallée-Tourangeau and Miroslav Sirota},
keywords = {Problem solving, Insight, Task ecology, Enactivism, Methodological individualism},
abstract = {Two experiments examined whether different task ecologies influenced insight problem solving. The 17 animals problem was employed, a pure insight problem. Its initial formulation encourages the application of a direct arithmetic solution, but its solution requires the spatial arrangement of sets involving some degree of overlap. Participants were randomly allocated to either a tablet condition where they could use a stylus and an electronic tablet to sketch a solution or a model building condition where participants were given material with which to build enclosures and figurines. In both experiments, participants were much more likely to develop a working solution in the model building condition. The difference in performance elicited by different task ecologies was unrelated to individual differences in working memory, actively open-minded thinking, or need for cognition (Experiment 1), although individual differences in creativity were correlated with problem solving success in Experiment 2. The discussion focuses on the implications of these findings for the prevailing metatheoretical commitment to methodological individualism that places the individual as the ontological locus of cognition.}
}
@incollection{POULINDUBOIS2017653,
title = {Chapter 26 - The Development of Object Categories: What, When, and How?},
editor = {Henri Cohen and Claire Lefebvre},
booktitle = {Handbook of Categorization in Cognitive Science (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {San Diego},
pages = {653-671},
year = {2017},
isbn = {978-0-08-101107-2},
doi = {https://doi.org/10.1016/B978-0-08-101107-2.00027-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780081011072000270},
author = {Diane Poulin-Dubois and Sabina Pauen},
keywords = {Categorization, development, infant, child, categories, perceptual, conceptual},
abstract = {From birth, infants are exposed to a wealth of information from their surroundings. This makes early categorization abilities especially important for infants and children to process information and come to understand the world around them. As a result of several sophisticated experimental paradigms, it is well-established that early categorization abilities become refined over the developmental trajectory. Researchers have identified a global-to-basic shift in early categorical thinking, such that preverbal infants discriminate between global-level categories (i.e., dogs, cats, chairs, tables, etc.) before basic-level categories (i.e., different breeds of cats and dogs). However, differences in the literature regarding the timing of this shift emerge depending on the paradigm used to measure categorization. There is evidence to suggest that infants also use dynamic, causal, and functional information to guide their object categorization and discrimination. This chapter provides a comprehensive review of the research and theory on early categorization and concept development.}
}
@incollection{PATEL2023191,
title = {Chapter 6 - Human-intensive techniques},
editor = {Robert A. Greenes and Guilherme {Del Fiol}},
booktitle = {Clinical Decision Support and Beyond (Third Edition)},
publisher = {Academic Press},
edition = {Third Edition},
address = {Oxford},
pages = {191-215},
year = {2023},
isbn = {978-0-323-91200-6},
doi = {https://doi.org/10.1016/B978-0-323-91200-6.00030-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780323912006000309},
author = {Vimla L. Patel and Jane Shellum and Timothy Miksch and Edward H. Shortliffe},
keywords = {Knowledge acquisition, Expert decision making, Learning and expertise, Human problem solving, Institutional standard setting, Interactive transfer of expertise},
abstract = {This chapter focuses on human expertise as a source of knowledge, emphasizing human-intensive techniques for acquiring knowledge from a variety of knowledge sources. Much of such work deals with the acquisition of knowledge for the purpose of encoding it to be used in decision support systems. There are also approaches that deal with the role of experts in creating evidence-based guidelines and institutional protocols. Those who want to capture expert knowledge naturally seek interaction with human beings who are excellent at the same task for which the knowledge product is intended. As a result, we need to understand both the factual knowledge that is required to solve the relevant problems and the judgmental knowledge that characterizes an excellent decision maker. The chapter accordingly focuses on computational methods and cognitive perspectives that can assist with the elicitation of knowledge by interacting with expert human beings.}
}
@article{OMIDI20231,
title = {Molecular dynamic study of perovskite with improved thermal and mechanical stability for solar cells application: Calculation the final strength of the modeled atomic structures and the Young's modulus},
journal = {Engineering Analysis with Boundary Elements},
volume = {156},
pages = {1-7},
year = {2023},
issn = {0955-7997},
doi = {https://doi.org/10.1016/j.enganabound.2023.07.037},
url = {https://www.sciencedirect.com/science/article/pii/S0955799723003922},
author = {Mohammad Omidi and Zahra Karimi and Shirin Rahmani and Ali {Naderi Bakhtiyari} and Mahmood {Karimi Abdolmaleki}},
keywords = {Molecular dynamic, LAMMPS, Mechanical properties, Stress-strain, Solar cell},
abstract = {The Large-scale Atomic/Molecular Massively Parallel Simulator (LAMMPS) software is used to do molecular dynamics simulations, which entail modeling atom behavior over time using interatomic potentials. This approach is used to calculate perovskite structures' mechanical characteristics. For testing purposes, stress-strain curves are completed in the X, Y, and Z directions to represent the material's reaction to applied stress in terms of strain. The simulated structures are deformed inside the computational experiments using the loads and deform approaches command to get the stress-strain curves. The mechanical data of the structures may be retrieved by producing a deformation. These stress-strain curves are then compared in three axes of X, Y, and Z for XSnO3 (X= Cs, Rb, and K) at varied temperature and pressure settings. Finally, we applied this material to solar cell devices to find the performance of perovskite materials and calculated the efficiency.}
}
@incollection{GRANJOU20161,
title = {1 - The Time Beast},
editor = {Céline Granjou},
booktitle = {Environmental Changes},
publisher = {Elsevier},
pages = {1-43},
year = {2016},
isbn = {978-1-78548-026-3},
doi = {https://doi.org/10.1016/B978-1-78548-026-3.50001-5},
url = {https://www.sciencedirect.com/science/article/pii/B9781785480263500015},
author = {Céline Granjou},
keywords = {Analogism, Anthropophagy, Doctrine of the Apocalypse, Environmental change, Environmental humanities, Evolution, Multi-species ethnography, Nature/society partition, Plate tectonics, Sociology},
abstract = {Abstract:
This chapter will give insights into the historical shaping of the very peculiar notion of a nature without any future. We will retrace some of its roots in the secularization of Christian apocalypse, Newtonian physics and Linnean classification – while at the same time, Cartesian, Kantian and Hegelian philosophies perceived humans as subjects of reason, emancipation and civilization. We will revisit the way that, in the 19th Century, the Darwinian theory of evolution and, more recently, the development of geophysics, both contributed in the thinking of nature itself as able to instigate and create new futures.}
}
@incollection{GOMILA20121,
title = {1 - Introduction: Language as the Key Factor to Human Singularity},
editor = {Antoni Gomila},
booktitle = {Verbal Minds},
publisher = {Elsevier},
address = {London},
pages = {1-4},
year = {2012},
isbn = {978-0-12-385200-7},
doi = {https://doi.org/10.1016/B978-0-12-385200-7.00001-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780123852007000011},
author = {Antoni Gomila},
keywords = {Cognitive architecture, verbal minds, cognitive control, flexibility, language},
abstract = {Publisher Summary
This chapter explains language as a distinctive human trait characterized by flexibility and creativity of human brains. Language is the sole symbol of communication and representation of individuality and sociality. The role of language on human thinking is to define the cultural and behavioral novelties of human thoughts. Lately, language is going through pendulum dynamics from past 30 years because the communicative approaches are becoming hegemonic. There has been a lot of new evidence to support the constitutive approach and its becoming mainstream, however, the modern critics of the cognitive view of language react in a paradoxical manner and do not support a cognitive role of language. This chapter aims at providing a defined role of language in human cognition to analyze the different ways, in which the relation between language and cognition has been conceived, to review the evidence amassed in recent years on this relationship, and to conclude multiple ways to conceive of the relationship at its best accounts for the facts.}
}
@article{GOTLIEB2025104126,
title = {Pathology Education for Undergraduate and Graduate Students: It is Not Just for Clinical Trainees},
journal = {Laboratory Investigation},
volume = {105},
number = {5},
pages = {104126},
year = {2025},
issn = {0023-6837},
doi = {https://doi.org/10.1016/j.labinv.2025.104126},
url = {https://www.sciencedirect.com/science/article/pii/S0023683725000364},
author = {Avrum I. Gotlieb and Richard N. Mitchell}
}
@incollection{DEBEUKELAER2018455,
title = {Chapter 21 - Relating movements in aesthetic spaces: Immersing, distancing, and remembering},
editor = {Julia F. Christensen and Antoni Gomila},
series = {Progress in Brain Research},
publisher = {Elsevier},
volume = {237},
pages = {455-469},
year = {2018},
booktitle = {The Arts and The Brain},
issn = {0079-6123},
doi = {https://doi.org/10.1016/bs.pbr.2018.03.014},
url = {https://www.sciencedirect.com/science/article/pii/S0079612318300141},
author = {Sophie {De Beukelaer} and Ruben Azevedo and Manos Tsakiris},
keywords = {Aesthetic experience, Embodied simulation, Associative processing, Constructive memory, Aesthetic spaces},
abstract = {According to Aby Warburg, the aesthetic experience is informed by a pendulum-like movement of the observer's mind that allows him to immerse as well as to take distance from the artwork's composing elements. To account for Warburg's definition, we are proposing embodied simulation and associative processing as constitutive mechanisms of this pendulum-like movement within the aesthetic experience that enable the observer to relate to the displayed artistic material within aesthetic spaces. Furthermore, we suggest that associative processing elicits constructive memory processes that permit the development of a knowledge within which the objects of art become part of memory networks, potentially informing future ways of thinking, feeling, and behaving in real-world situations, as an individual or collectively.}
}
@article{HOBBS2019100055,
title = {Estimating peak water demand: Literature review of current standing and research challenges},
journal = {Results in Engineering},
volume = {4},
pages = {100055},
year = {2019},
issn = {2590-1230},
doi = {https://doi.org/10.1016/j.rineng.2019.100055},
url = {https://www.sciencedirect.com/science/article/pii/S2590123019300556},
author = {Ian Hobbs and Martin Anda and Parisa A. Bahri},
keywords = {Fixture unit, Peak water demand, Modified wistort method, Exhaustive enumeration method, Water demand calculator, Loading unit normalisation method},
abstract = {Since the 1940s, the models used to estimate peak water demand has been based largely upon variations and refinements of the probabilistic ‘fixture unit’ model. An approach originally advanced by Hunter (1940) in the United States of America (USA). Seeking an improved approach to the 'fixture unit' model, now widely recognised as outdated, is the key driving force behind the current work. Boosted by the development of computing power, the plumbing industry, researchers, and academics have, over the last decade, developed computational models as a means of estimating peak water demand. This paper builds on computational models embracing the estimation of peak water demand. A brief outline of the fixture unit and its limitations is provided with key developments in computational modeling comprising current developments from the USA and UK. A brief outline of computational models is presented: Modified Wistort Method (MWM); the Exhaustive Enumeration Method (EEM), and the Water Demand Calculator (WDC). Also presented, from the UK, is the Loading Unit Normalisation Assessment method (LUNA) aimed at an improved model to size domestic hot and cold-water systems. The analysis of the computational models suggests the WDC model is conceivably the most compatible with that of the plumbing industry's design requirements. Suggesting this model could easily be adapted to meet the requirements across international borders. Challenges for the international acceptance of the WDC are the field study requirements to determine p (probability of use) and q (fixture flow rate) values for all types of buildings.}
}
@article{JIANG2023104680,
title = {Using sequence mining to study students’ calculator use, problem solving, and mathematics achievement in the National Assessment of Educational Progress (NAEP)},
journal = {Computers & Education},
volume = {193},
pages = {104680},
year = {2023},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2022.104680},
url = {https://www.sciencedirect.com/science/article/pii/S0360131522002512},
author = {Yang Jiang and Gabrielle A. Cayton-Hodges and Leslie {Nabors Oláh} and Ilona Minchuk},
keywords = {Calculator, Assessment, Problem solving, Sequence mining, Process data, Mathematics},
abstract = {Using appropriate tools strategically to aid in problem solving is a crucial skill identified in K-12 mathematics curriculum standards. As more assessments transition from paper-and-pencil to digital formats, a variety of interactive tools have been made available to test takers in digital testing platforms. Using onscreen calculators as an example, this study illustrates how process data obtained from student interactions with a digitally-based large-scale assessment can be leveraged to explore how and how well test takers use interactive tools and unveil their mathematical problem-solving processes and strategies. Specifically, sequence mining techniques using the longest common subsequence were applied on process data collected from a nationally representative sample who took the National Assessment of Educational Progress (NAEP) mathematics assessment to examine patterns of eighth-grade students’ calculator-use behaviors and the content of calculator input across a series of items. Sequences of keystrokes executed on the onscreen calculator by test takers were compared to reference sequences identified by content experts as proficient and efficient use to infer how well and how consistently the calculator was used. Results indicated that calculator-use behaviors and content differed by item characteristics. Students were more likely to use calculators on calculation-demanding items that involve intensive and complex computations than on items that involve simple or no computation. Using the calculator on more calculation-demanding items and using it in a manner that is more efficient and more similar to reference sequences on these items were related to higher mathematical proficiency. Findings have implications for assessment design and can be used in educational practices to provide educators with actionable process-related information on tool use and problem solving.}
}
@incollection{KUMARI2025219,
title = {Chapter Nine - Harnessing artificial intelligence in identifying and isolation of marine peptides},
editor = {Akanksha Srivastava and Vaibhav Mishra},
series = {Methods in Microbiology},
publisher = {Academic Press},
volume = {56},
pages = {219-242},
year = {2025},
booktitle = {Artificial Intelligence in Microbiology: Scope and Challenges Volume 2},
issn = {0580-9517},
doi = {https://doi.org/10.1016/bs.mim.2024.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S0580951724000461},
author = {Priyanshi Kumari and Bhavya Gaur and Vaibhav Mishra},
keywords = {Marine peptides, Artificial intelligence, Therapeutic values, Machine learning, Deep learning model, AMP's discovery},
abstract = {Marine ecosystem is a vast and relatively unexplored environment, where innumerable resources reside, including marine microbes, animals, algae, and other organisms' those have potential to produce different bioactive microbial peptides. Moreover, marine peptides are structurally unique and known for their exceptional bioactivity, with minimal to no harmful side effects. These bioactive peptides, isolated from marine sources, exhibit various properties, including antimicrobial, antiviral, anti-obesity, antioxidant, anti-inflammatory, and more hence, are deemed as future drugs. Furthermore, discovery of potential active peptides is exorbitant and laborious with traditional methods. Whereas, advanced computational techniques like Artificial Intelligence (AI) and their prime models make easier in the prediction and detection of important marine peptides. In this chapter we are highlighting modern AI based Machine Learning (ML) and Deep Learning (DL) models including k-Nearest Neighbour (kNN), Random Forest (RF), Artificial Neural Networks (ANNs) as ML, Fuzzy Logic or Adaptive Neuro-Fuzzy Inference System (ANFIS), Support Vector Machine (SVMs), and many more other DL models. Moreover, employing these advanced AI models to ease the isolation and identification of the bioactive microbial peptides from marine environments.}
}
@article{AMALINA2023e19539,
title = {Cognitive and socioeconomic factors that influence the mathematical problem-solving skills of students},
journal = {Heliyon},
volume = {9},
number = {9},
pages = {e19539},
year = {2023},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2023.e19539},
url = {https://www.sciencedirect.com/science/article/pii/S2405844023067476},
author = {Ijtihadi Kamilia Amalina and Tibor Vidákovich},
keywords = {External factor, Internal factor, Mathematics thinking skills, Middle-school students, Structural equation modeling},
abstract = {Mathematical problem-solving is necessary to encounter professional, 21st-century, and everyday challenges. The relevant context of mathematical problem-solving is related to science, which is presented using natural language. Mathematical problem-solving requires both mathematical skills and nonmathematical skills, e.g., science knowledge and text comprehension skills. Thus, several internal and external factors affect success in mathematical problem-solving. In this study, we investigated the cognitive (i.e., mathematics domain-specific prior knowledge (DSPK), science background knowledge, and text comprehension skills) and socioeconomic status (SES) (i.e., parents' educational level and family income) factors that affect students' mathematical problem-solving skills. The data considered in this study included tests, documents, and a questionnaire from grade seven to nine students (n = 1067). In addition, a theoretical model was constructed using structural equation modeling. We found that this model was close to satisfying the critical values of fit indices. The model was then modified by deleting the nonsignificant paths, and the modified model exhibited a better fit. We found that most of the exploratory variables directly affected mathematical problem-solving skills, with the exception of the parents' educational levels. The strongest factor was mathematics DSPK. Both the father’s and mother’s educational levels indirectly influenced mathematical problem-solving skills through family income. In addition, text comprehension skills indirectly impacted mathematical problem-solving skills with science background knowledge acting as a mediator.}
}
@article{WU2025104172,
title = {TrustCNAV: Certificateless aggregate authentication of civil navigation messages in GNSS},
journal = {Computers & Security},
volume = {148},
pages = {104172},
year = {2025},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2024.104172},
url = {https://www.sciencedirect.com/science/article/pii/S0167404824004772},
author = {Zhijun Wu and Yun Bai and Yuan Zhang and Liang Liu and Meng Yue},
keywords = {Satellite navigation, Spoofing attacks, Elliptic curve, Aggregate authentication, Authentication protocol},
abstract = {The Global Navigation Satellite System (GNSS) is capable of accurate positioning because it can provide high-precision data. These data are transmitted to the receiver in the form of navigation messages, called civil navigation messages (CNAV). As it is transmitted in an open, transparent environment without data integrity protection mechanisms and secure data transmission measures, the CNAV is suspected to spoofing attacks. In 2023, the OPSGROUP has received approximately 50 reports of GPS spoofing activity. A spoofed plane's navigation system will show it as being in a different place - a security risk if a jet is guided to fly into a hostile country's airspace. To prevent the forging of GNSS positioning data by spoofing attacks targeting CNAV, we propose a certificateless aggregation authentication for CNAV by using the elliptic curve discrete logarithm problem and the combination of the GNAV structural characteristics, called TrustCNAV. Security proof and performance analysis indicate that this authentication scheme can resist spoofing attacks and ensure data security of CNAV, also it avoids pairing operations with high computational complexity, thus meeting security requirements without causing too much time and communication consumption.}
}
@article{GARBOCZI2001455,
title = {Elastic moduli of a material containing composite inclusions: effective medium theory and finite element computations},
journal = {Mechanics of Materials},
volume = {33},
number = {8},
pages = {455-470},
year = {2001},
issn = {0167-6636},
doi = {https://doi.org/10.1016/S0167-6636(01)00067-9},
url = {https://www.sciencedirect.com/science/article/pii/S0167663601000679},
author = {E.J. Garboczi and J.G. Berryman},
keywords = {Fnite element, Effective medium theory, Concrete, Microstructure, Random elastic},
abstract = {Concrete is a good example of a composite material in which the inclusions (rocks and sand) are surrounded by a thin shell of altered matrix material and embedded in the normal matrix material. Concrete, therefore, may be viewed as consisting of a matrix material containing composite inclusions. Assigning each of these phases different linear elastic moduli results in a complicated effective elastic moduli problem. A new kind of differential effective medium theory (D-EMT) is presented in this paper that is intended to address this problem. The key new idea is that each inclusion particle, surrounded by a shell of another phase, is mapped onto an effective particle of uniform elastic moduli. The resulting simpler composite, with a normal matrix, is then treated in usual D-EMT. Before use, however, the accuracy of this method must be determined, as effective medium theory of any kind is an uncertain approximation. One good way to assess the accuracy of effective medium theory is to compare to exact results for known microstructures and phase moduli. Exact results, however, only exist for certain microstructures (e.g., dilute limit of inclusions) or special choices of the moduli (e.g., equal shear moduli). Recently, a special finite element method has been developed that can compute the linear elastic moduli of an arbitrary digital image in 2D or 3D. If a random microstructure can be represented with enough resolution by a digital image, then its elastic moduli can be readily computed. This method is used, after proper error analysis, to provide stringent tests of the new D-EMT equations, which are found to compare favorably to numerically exact finite element simulations, in both 2D and 3D, with varying composite inclusion particle size distributions.}
}
@article{CASTILLO2018165,
title = {In search of missing time: A review of the study of time in leadership research},
journal = {The Leadership Quarterly},
volume = {29},
number = {1},
pages = {165-178},
year = {2018},
issn = {1048-9843},
doi = {https://doi.org/10.1016/j.leaqua.2017.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S1048984317300632},
author = {Elizabeth A. Castillo and Mai P. Trinh},
keywords = {Leadership, Time, Process, Computational science, Agent-based model},
abstract = {Many studies describe leadership as a dynamic process. However, few examine the passage of time as a critical dimension of that dynamism. This article illuminates this knowledge gap by conducting a systematic review of empirical studies on temporal effects of leadership to identify if and how time has been considered as a factor. After synthesizing key findings from the review, the article discusses methodological implications. We propose that a computational science approach, particularly agent-based modeling, is a fruitful path for future leadership research. This article contributes to leadership scholarship by shedding light on a missing variable (time) and offering a novel way to investigate the temporal, dynamic, emergent, and recursive aspects of leadership. We demonstrate the usefulness of agent-based modeling with an example of leader-member exchange relationship development.}
}
@incollection{HALFORD2008298,
title = {Cognitive Developmental Theories},
editor = {Marshall M. Haith and Janette B. Benson},
booktitle = {Encyclopedia of Infant and Early Childhood Development},
publisher = {Academic Press},
address = {San Diego},
pages = {298-308},
year = {2008},
isbn = {978-0-12-370877-9},
doi = {https://doi.org/10.1016/B978-012370877-9.00039-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780123708779000396},
author = {G.S. Halford},
abstract = {Theories of cognitive development are reviewed, beginning with pioneering theories by Piaget and Vygotsky. Neo-Piagetian theories which integrated Piagetian theory with other conceptions of cognition were developed by McLaughlin, Pascual-Leone, Case, Fischer, and Chapman. Complexity theories propose that children become capable of dealing with more complex relations as they develop. Information processing theories, neural net theories, dynamic systems theories, and theories of reasoning processes all provide models of the reasoning processes employed by children at different ages. Microgenetic analysis methods are used to study the processes of transition from one level of thinking to the next.}
}
@article{BOWLER2016117,
title = {Mindful makers: Question prompts to help guide young peoples' critical technical practices in maker spaces in libraries, museums, and community-based youth organizations},
journal = {Library & Information Science Research},
volume = {38},
number = {2},
pages = {117-124},
year = {2016},
issn = {0740-8188},
doi = {https://doi.org/10.1016/j.lisr.2016.04.006},
url = {https://www.sciencedirect.com/science/article/pii/S0740818815300840},
author = {Leanne Bowler and Ryan Champagne},
abstract = {This study examines question prompts as a means to scaffold reflection and reflexivity in the design, development, and use of technological artifacts in maker spaces for youth at public libraries, museums, and community-based organizations. Qualitative analysis is applied to data gathered in four focus groups with teens, three semi-structured interviews with adults who facilitate maker spaces, and six observation sessions. Outcomes include a rich description of critical thinking in the context of technology practice, and secondly, a set of eight activation questions that serve as a tool kit to encourage reflection and scaffold mindful and critical practices in community-based maker spaces for youth. Results from this study support the development of nstruments and practices to support mindful making and critical technical practice in maker spaces for youth.}
}
@article{FERGUSON20141885,
title = {Training in Minimally Invasive Lobectomy: Thoracoscopic Versus Robotic Approaches},
journal = {The Annals of Thoracic Surgery},
volume = {97},
number = {6},
pages = {1885-1892},
year = {2014},
issn = {0003-4975},
doi = {https://doi.org/10.1016/j.athoracsur.2014.01.055},
url = {https://www.sciencedirect.com/science/article/pii/S0003497514003403},
author = {Mark K. Ferguson and Konstantin Umanskiy and Cindy Warnes and Amy D. Celauro and Wickii T. Vigneswaran and Vivek N. Prachand},
abstract = {Background
Skills required for thoracoscopic and robotic operations likely differ. The needs and abilities of trainees learning these approaches require assessment.
Methods
Trainees performed initial components of minimally invasive lobectomies using thoracoscopic or robotic approaches. Component difficulty was scored by trainees using the NASA task load index (NASATLX). Performance of each component was graded by trainees and attending surgeons on a 5-point ordinal scale (naïve, beginning learner, advanced learner, competent, master).
Results
Eleven surgical trainees performed 87 replications among three lobectomy components (divide pulmonary ligament; dissect level 7/8/9 nodes; dissect level 4/5 nodes). Before performance NASATLX scores did not differ among components or between surgical approaches. Trainees' after performance NASATLX scores appropriately calibrated task load for the components. After performance NASATLX scores were significantly lower for thoracoscopy than before performance estimates; robotic scores were similar before surgery and after performance. Task load was higher for robotic than for thoracoscopic approaches. Trainees rated their performance higher than did attending surgeons in domains of knowledge and thinking, but ratings for other domains were similarly low. Ratings for performance improved significantly as component performance repetitions increased.
Conclusions
Trainees did not differentiate task load among components or surgical approaches before attempting them. Task load scores differentiated difficulty among initial components of lobectomy, and were greater for robotic than for thoracoscopic approaches. Trainees overestimated their level of cognitive performance compared with attending physician evaluation of trainee performance. The study provides insights into how to customize training for thoracoscopic and robotic lobectomy and identifies tools to assess training effectiveness.}
}
@article{KOSIKOV2021492,
title = {Data Enrichment in the Information Graphs Environment Based on a Specialized Architecture of Information Channels},
journal = {Procedia Computer Science},
volume = {190},
pages = {492-499},
year = {2021},
note = {2020 Annual International Conference on Brain-Inspired Cognitive Architectures for Artificial Intelligence: Eleventh Annual Meeting of the BICA Society},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921013922},
author = {Sergey Kosikov and Larisa Ismailova and Viacheslav Wolfengagen},
keywords = {data enrichment, information channels, conceptual constructions, informational graph, applicative computations, semantics},
abstract = {The paper considers the possibility of constructing a specialized computing system oriented at the transmission of data through information channels, that are determined taking into account the semantics of the selected data. In the process of computations the data is connected with semantic characteristics that describe the channel of computations, which can be considered as a method of semantic data enrichment. The system of information channels as a whole can be considered as an information graph describing the structuring of the processed data. The information graph supports the data model in the form of a network, the framework of which are objects and the relationships between them. The paper proposes language tools for determining the information graph and interpretation tools that provide practical computations. The set of information channels that make up the information graph can be considered as a low-level tool for data enrichment. The paper studies the possibility of determining tools of higher level. An applicative type language is proposed for defining information graphs, the syntax and semantics of the language are specified. The proposed language can be considered as an intermediate level tool for defining semantics. A procedure is proposed for compiling the language into a low-level construct, preserving the semantics of the language. The supporting system for the proposed computing system includes a low-level language interpreter, as well as an intermediate-level language compiler into a low-level language. The supporting system is implemented in an applicative programming environment. Some elements of the supporting system were tested when developing applied information systems in the field of jurisprudence.}
}
@article{GOLDMAN2025104323,
title = {The value of real-time automated explanations in stochastic planning},
journal = {Artificial Intelligence},
volume = {343},
pages = {104323},
year = {2025},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2025.104323},
url = {https://www.sciencedirect.com/science/article/pii/S0004370225000426},
author = {Claudia V. Goldman and Ronit Bustin and Wenyuan Qi and Zhengyu Xing and Rachel McPhearson-White and Sally Rogers},
keywords = {Explainable AI, Decision-Making, Human-Computer interaction},
abstract = {Recently, we are witnessing an increase in computation power and memory, leading to strong AI algorithms becoming applicable in areas affecting our daily lives. We focus on AI planning solutions for complex, real-life decision-making problems under uncertainty, such as autonomous driving. Human trust in such AI-based systems is essential for their acceptance and market penetration. Moreover, users need to establish appropriate levels of trust to benefit the most from these systems. Previous studies have motivated this work, showing that users can benefit from receiving (handcrafted) information about the reasoning of a stochastic AI planner, for example, controlling automated driving maneuvers. Our solution to automating these hand-crafted notifications with explainable AI algorithms, XAI, includes studying: (1) what explanations can be generated from an AI planning system, applied to a real-world problem, in real-time? What is that content that can be processed from a planner's reasoning that can help users understand and trust the system controlling a behavior they are experiencing? (2) when can this information be displayed? and (3) how shall we display this information to an end user? The value of these computed XAI notifications has been assessed through an online user study with 800 participants, experiencing simulated automated driving scenarios. Our results show that real time XAI notifications decrease significantly subjective misunderstanding of participants compared to those that received only a dynamic HMI display. Also, our XAI solution significantly increases the level of understanding of participants with prior ADAS experience and of participants that lack such experience but have non-negative prior trust to ADAS features. The level of trust significantly increases when XAI was provided to a more restricted set of the participants, including those over 60 years old, with prior ADAS experience and non-negative prior trust attitude to automated features.}
}
@article{DODERO20221227,
title = {Ship design assessment through virtual prototypes},
journal = {Procedia Computer Science},
volume = {200},
pages = {1227-1236},
year = {2022},
note = {3rd International Conference on Industry 4.0 and Smart Manufacturing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.01.323},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922003325},
author = {Matteo Dodero and Serena Bertagna and Luca Braidotti and Alberto Marinò and Vittorio Bucci},
keywords = {Ship design, early stage design, Virtual Prototype, Product Model Program},
abstract = {The traditional design process has been developed, through time, by trial and error, following an evolutive approach. By following this procedure, the design team focused its attention on only one conceptual design alternative at a time, which is perfected step by step until the expected outcome is obtained. Nevertheless nowadays, due to the high complexity of ships and increasingly stringent operational requirements, this approach appears to be obsolete in a market where cost and time reduction is a fundamental parameter. Indeed, to be competitive in the shipbuilding market, very accurate information should be available since the beginning of the process, to allow the design team a 360-degree exploration of a high number of alternatives and then identify the best design solution in no time. In this paper a new, rational, design process, necessary to raise efficiency and effectiveness of ship design, is presented. By using a multi-purpose design software, the authors were able to create a Virtual Prototype of a case study ship with ease and little training, obtaining, since early-stage design phases, some outputs of interest (such as longitudinally weight distribution of ship structures, preliminary midship section, GZ curves and powering curves) without great computational efforts. The most important benefit of using only one multipurpose software instead of multiple specific ones lies in the elimination of remarking activities for switching from one software to another, reducing loss of data’s risks during the process.}
}
@article{CELLERIER1990159,
title = {Psychology and computation: A response to Bunge},
journal = {New Ideas in Psychology},
volume = {8},
number = {2},
pages = {159-175},
year = {1990},
issn = {0732-118X},
doi = {https://doi.org/10.1016/0732-118X(90)90006-N},
url = {https://www.sciencedirect.com/science/article/pii/0732118X9090006N},
author = {G. Cellerier and J.-J. Ducret}
}
@article{YEE1991249,
title = {Dynamical approach study of spurious steady-state numerical solutions of nonlinear differential equations. I. The dynamics of time discretization and its implications for algorithm development in computational fluid dynamics},
journal = {Journal of Computational Physics},
volume = {97},
number = {2},
pages = {249-310},
year = {1991},
issn = {0021-9991},
doi = {https://doi.org/10.1016/0021-9991(91)90001-2},
url = {https://www.sciencedirect.com/science/article/pii/0021999191900012},
author = {H.C Yee and P.K Sweby and D.F Griffiths},
abstract = {The goal of this paper is to utilize the theory of nonlinear dynamics approach to investigate the possible sources of errors and slow convergence and nonconvergence of steady-state numerical solutions when using the time-dependent approach for nonlinear hyperbolic and parabolic partial differential equations terms. This interdisciplinary research belongs to a subset of a new field of study in numerical analysis sometimes referred to as “ the dynamics of numerics and the numerics of dynamics.” At the present time, this new interdisciplinary topic is still the property of an isolated discipline with all too little effort spent in pointing out an underlying generality that could make it adaptable to diverse fields of applications. This is the first of a series of research papers under the same topic. Our hope is to reach researchers in the fields of computational fluid dynamics (CFD) and, in particular, hypersonic and combustion related CFD. By simple examples (in which the exact solutions of the governing equations are known), the application of the apparently straightforward numerical technique to genuinely nonlinear problems can be shown to lead to incorrect or misleading results. One striking phenomenon is that with the same initial data, the continuum and its discretized counterpart can asymptotically approach different stable solutions. This behavior is especially important for employing a time-dependent approach to the steady state since the initial data are usually not known and a freestream condition or an intelligent guess for the initial conditions is often used. With the unique property of the different dependence of the solution on initial data for the partial differential equation and the discretized counterpart, it is not easy to delineate the true physics from numerical artifacts when numerical methods are the sole source of solution procedure for the continuum. Part I concentrates on the dynamical behavior of time discretization for scalar nonlinear ordinary differential equations in order to motivate this new yet unconventional approach to algorithm development in CFD and to serve as an introduction for parts 11 and III of the same series of research papers.}
}
@article{WU2025111756,
title = {A rapid indoor 3D wind field prediction model based on conditional generative adversarial networks},
journal = {Journal of Building Engineering},
volume = {100},
pages = {111756},
year = {2025},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2024.111756},
url = {https://www.sciencedirect.com/science/article/pii/S2352710224033242},
author = {Yaqi Wu and Xiaoqian Li and Xing Zheng and Chenxi Lei and Ye Yuan and Zhen Han and Gang Liu},
keywords = {3D wind field, Fast prediction, Pix2pix, Image encoding},
abstract = {The prediction of building performance during the early design phase is essential for architects and engineers. Given the complex nature of parameter inputs and the need for time efficiency, surrogate models have become a preferred method for predicting building performance. However, most surrogate models for indoor airflow could not predict the wind flow field information in three-dimensional (3D) space (named 3D wind field). The few advanced 3D data prediction models are often computationally expensive. This paper proposes a surrogate model based on Conditional Generative Adversarial Networks for the prediction of indoor 3D wind fields under natural ventilation. The core innovation lies in compressing indoor 3D wind field information into 2D planes via image encoding and subsequently obtaining wind field maps of arbitrary planes through data post-processing. By taking prefabricated houses as the case study, a database is constructed and the model is trained to predict the wind field at any cross-section within the space. The resulting surrogate model can generate predictions within a 3–5 s timeframe. To evaluate the accuracy of the model prediction, 21 testing planes were selected. Mean Absolute Error (MAE) and Mean Absolute Percentage Error (MAPE) were used to assess the numerical accuracy, and Structure Similarity Index Measure (SSIM) was used to comprehensively evaluate the visualization results of the wind field images. The results indicate that the model exhibits outstanding prediction performance for the planes, with an MAE of 0.1233, a MAPE of 12.20 %, and an SSIM of 0.9492 on the test set. Compared to simulation methods, this approach can improve prediction speed by 350 times-450 times, significantly enhancing the efficiency of obtaining 3D wind fields during the early design stages.}
}
@article{MELNIKOFF2018280,
title = {The Mythical Number Two},
journal = {Trends in Cognitive Sciences},
volume = {22},
number = {4},
pages = {280-293},
year = {2018},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2018.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S136466131830024X},
author = {David E. Melnikoff and John A. Bargh},
keywords = {dual process, dual system, type 1, type 2, automaticity},
abstract = {It is often said that there are two types of psychological processes: one that is intentional, controllable, conscious, and inefficient, and another that is unintentional, uncontrollable, unconscious, and efficient. Yet, there have been persistent and increasing objections to this widely influential dual-process typology. Critics point out that the ‘two types’ framework lacks empirical support, contradicts well-established findings, and is internally incoherent. Moreover, the untested and untenable assumption that psychological phenomena can be partitioned into two types, we argue, has the consequence of systematically thwarting scientific progress. It is time that we as a field come to terms with these issues. In short, the dual-process typology is a convenient and seductive myth, and we think cognitive science can do better.}
}
@article{GUEST1989560,
title = {An overview of vector and parallel processors in scientific computation},
journal = {Computer Physics Communications},
volume = {57},
number = {1},
pages = {560},
year = {1989},
issn = {0010-4655},
doi = {https://doi.org/10.1016/0010-4655(89)90285-3},
url = {https://www.sciencedirect.com/science/article/pii/0010465589902853},
author = {M. Guest}
}
@article{POLETTI20141803,
title = {Adverse childhood experiences worsen cognitive distortion during adult bipolar depression},
journal = {Comprehensive Psychiatry},
volume = {55},
number = {8},
pages = {1803-1808},
year = {2014},
issn = {0010-440X},
doi = {https://doi.org/10.1016/j.comppsych.2014.07.013},
url = {https://www.sciencedirect.com/science/article/pii/S0010440X14001825},
author = {Sara Poletti and Cristina Colombo and Francesco Benedetti},
abstract = {Background
Cognitive distortion is a central feature of depression, encompassing negative thinking, dysfunctional personality styles and dysfunctional attitudes. It has been hypothesized that ACEs could increase the vulnerability to depression by contributing to the development of a stable negative cognitive style. Nevertheless, little research has been carried out on possible associations between adverse childhood experiences (ACEs) and cognitive distortion, and whether any gender differences exist.
Aim
The aim of this study was to examine the association between ACEs and cognitive distortions and possible differences between genders in a sample of patients affected by bipolar disorder.
Method
130 patients with bipolar disorder (BD) (46 men and 84 females), completed the Risky Family Questionnaire to assess ACEs and the Cognition Questionnaire (CQ) to assess cognitive distortions.
Results
A positive association was found between ACE and the CQ total score. Investigating the 5 dimensions assessed through the CQ, only the dimension “generalization across situations” was significantly associated to ACE. An interaction between ACE and gender was found for “generalization across situations”, while no differential effect among females and males was found for CQ total score.
Conclusion
This is the first study to report a relationship between negative past experiences and depressive cognitive distortions in subjects affected by BD. Growing in a family environment affected by harsh parenting seems to a cognitive vulnerability to depression; this effect is especially strong in females.}
}
@article{HAPPE2025112240,
title = {Authentic interdisciplinary online courses for alternative pathways into computer science},
journal = {Journal of Systems and Software},
volume = {219},
pages = {112240},
year = {2025},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2024.112240},
url = {https://www.sciencedirect.com/science/article/pii/S016412122400284X},
author = {Lucia Happe and Kai Marquardt},
keywords = {Interdisciplinary teaching, e-learning, Interest, Engagement, Diversity, Gender, Computer science education},
abstract = {The field of computer science (CS) is facing a crucial challenge in broadening participation and embracing diversity, especially among underrepresented gender groups. The presented interdisciplinary educational program is an efficient response to this challenge, designed to catalyze diversity in CS through engagement with complex, interest-driven problems. This paper outlines the program’s structure, elucidates the pedagogical underpinnings, and reflects on the emergent challenges and opportunities. We delve into how the fusion of CS with other academic disciplines can allure a more varied demographic, emphasizing the engagement of female high school students—a demographic pivotally positioned yet significantly untapped in CS. Through a systematic survey analysis, we measure the program’s efficacy in increasing interest in CS and in cultivating an appreciation for its application in addressing real-world, cross-disciplinary challenges. Our findings affirm the program’s success in bridging the engagement gap by leveraging students’ intrinsic interests, thus charting alternative pathways into the CS field. These insights underscore the critical role of interdisciplinary approaches, establishing a new standard for transformative CS educational methods.}
}
@article{YUKSEL2025100890,
title = {Transformation of labor: Educational robotics coding in elementary schools for 21st century skills},
journal = {Entertainment Computing},
volume = {52},
pages = {100890},
year = {2025},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2024.100890},
url = {https://www.sciencedirect.com/science/article/pii/S1875952124002581},
author = {Akça Okan Yüksel and Bilal Atasoy and Selçuk Özdemir},
keywords = {21st century skills, Educational robotics coding, Elementary school students, Sociocultural, Cognitive, Affective},
abstract = {This study aims to examine the effects of educational robotics activities on students’ 21st century skills. In the study, explanatory mixed method design was used. As a quantitative data collection tool, the 21st century skills scale was utilized [47]. In addition to quantitative data, qualitative data were collected from the teachers and students through semi-structured interviews. Within the scope of the research, students participated in robotic design courses with Arduino over the learning management system for ten weeks and participated in a competition with their products at the end of the activity. The activity was held with the participation of 62 students and 10 teachers. The findings of the study showed that educational robotic activities caused a significant increase in the affective domain of the students. While it did not cause any significant increase in the cognitive and sociocultural domains, the average scores of students increased on post-tests compared to the pretests for these two domains. In addition, students’ 21st century skills did not differ according to gender. Although there is no statistically significant difference in pretest–posttest results due to grade level, an increase was observed in posttest averages at each grade level. The observed increase in the post-tests, although not statistical, reveals the positive effect of educational robotic coding in terms of students’ 21st century skills. To support the results of the quantitative data, the analysis of the qualitative data revealed a consensus of both teachers and students on the contribution of such applications to the advancement of contemporary skills. In conclusion, the results of this study show that educational robotic coding can be used to develop 21st century skills of elementary school students.}
}
@article{MANORAT2025100403,
title = {Artificial intelligence in computer programming education: A systematic literature review},
journal = {Computers and Education: Artificial Intelligence},
volume = {8},
pages = {100403},
year = {2025},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2025.100403},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X25000438},
author = {Pisut Manorat and Suppawong Tuarob and Siripen Pongpaichet},
keywords = {Artificial intelligence, Machine learning, Computer programming education, Systematic literature review},
abstract = {The demand for skilled programmers and the increasing complexity of coding skills have led to a rise in the adoption of artificial intelligence (AI) and machine learning (ML) technologies in computer programming education. Previous research has explored the potential of AI in aspects such as grading assignments, generating feedback, detecting plagiarism, and identifying at-risk students, but there is a lack of systematic reviews focused on AI-powered teaching processes in computer programming classes. To provide a more comprehensive understanding of AI and ML's role in computer programming education, this systematic review examines a wider range of applications across the entire pedagogical process. Analyzing 119 relevant research papers published between 2012 and 2024, this review offers an overview of AI and ML tools and techniques used in various educational contexts. Aligned with instructional design models, the reviewed literature is categorized into four key areas: course design, classroom implementation, assessment and feedback, and performance monitoring. This systematic review not only highlights the practical tools available to instructors but also identifies research trends and potential areas for future exploration in the field of computer programming education.}
}
@article{KOICHU2015233,
title = {Proving as problem solving: The role of cognitive decoupling},
journal = {The Journal of Mathematical Behavior},
volume = {40},
pages = {233-244},
year = {2015},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2015.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S0732312315300067},
author = {Boris Koichu and Uri Leron},
keywords = {Proving, Problem solving, Cognitive decoupling, Cycles in problem solving, Drawings and diagrams, Dual process theory},
abstract = {This paper discusses the process of proving from a novel theoretical perspective, imported from cognitive psychology research. This perspective highlights the role of hypothetical thinking, mental representations and working memory capacity in proving, in particular the effortful mechanism of cognitive decoupling: problem solvers need to form in their working memory two closely related models of the problem situation – the so-called primary and secondary representations – and to keep the two models decoupled, that is, keep the first fixed while performing various transformations on the second, while constantly struggling to protect the primary representation from being “contaminated” by the secondary one. We first illustrate the framework by analyzing a common scenario of introducing complex numbers to college-level students. The main part of the paper consists of re-analyzing, from the perspective of cognitive decoupling, previously published data of students searching for a non-trivial proof of a theorem in geometry. We suggest alternative (or additional) explanations for some well-documented phenomena, such as the appearance of cycles in repeated proving attempts, and the use of multiple drawings.}
}
@article{KARVONEN2023101166,
title = {Fundamental concepts of cognitive mimetics},
journal = {Cognitive Systems Research},
volume = {82},
pages = {101166},
year = {2023},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2023.101166},
url = {https://www.sciencedirect.com/science/article/pii/S1389041723001006},
author = {Antero Karvonen and Tuomo Kujala and Tommi Kärkkäinen and Pertti Saariluoma},
keywords = {AI design, Cognitive mimetics, Design, Artificial intelligence},
abstract = {The rapid development and widespread adoption of Artificial Intelligence (AI) technologies have made the development of AI-specific design methods an important topic to advance. In recent decades, the centre of gravity in AI has shifted away from cognitive science and related fields like psychology. However, there is a clear need and potential for added value in returning to stronger interaction. One potential challenge for this interaction may be the lack of common conceptual grounds and design languages. In this article, we aim to contribute to the development of conceptual interfaces for human-based AI-specific design methods through the idea of cognitive mimetics. We begin by introducing basic concepts from mimetic design and interpret them in the context of this thematic area. These provide some of the basic building blocks for a design language and bring to the surface key questions. These in turn provide a ground for explicating cognitive mimetics. In the second part of this paper, we focus on specifying a key aspect in cognitive mimetics: the contents of information processes. Others engaged in this field can derive value from using or developing the basic conceptual machinery to specify their own approaches in this interdisciplinary field that is still shaping itself. Furthermore, those who resonate with the idea of cognitive mimetics, as specified here, can join in taking this particular approach further.}
}
@article{SMITH2020108208,
title = {Imprecise action selection in substance use disorder: Evidence for active learning impairments when solving the explore-exploit dilemma},
journal = {Drug and Alcohol Dependence},
volume = {215},
pages = {108208},
year = {2020},
issn = {0376-8716},
doi = {https://doi.org/10.1016/j.drugalcdep.2020.108208},
url = {https://www.sciencedirect.com/science/article/pii/S0376871620303732},
author = {Ryan Smith and Philipp Schwartenbeck and Jennifer L. Stewart and Rayus Kuplicki and Hamed Ekhtiari and Martin P. Paulus},
keywords = {Substance use disorders, Computational modeling, Active inference, Learning rate, Explore-exploit dilemma, Directed exploration},
abstract = {Background
Substance use disorders (SUDs) are a major public health risk. However, mechanisms accounting for continued patterns of poor choices in the face of negative life consequences remain poorly understood.
Methods
We use a computational (active inference) modeling approach, combined with multiple regression and hierarchical Bayesian group analyses, to examine how treatment-seeking individuals with one or more SUDs (alcohol, cannabis, sedatives, stimulants, hallucinogens, and/or opioids; N = 147) and healthy controls (HCs; N = 54) make choices to resolve uncertainty within a gambling task. A subset of SUDs (N = 49) and HCs (N = 51) propensity-matched on age, sex, and verbal IQ were also compared to replicate larger group findings.
Results
Results indicate that: (a) SUDs show poorer task performance than HCs (p = 0.03, Cohen’s d = 0.33), with model estimates revealing less precise action selection mechanisms (p = 0.004, d = 0.43), a lower learning rate from losses (p = 0.02, d = 0.36), and a greater learning rate from gains (p = 0.04, d = 0.31); and (b) groups do not differ significantly in goal-directed information seeking.
Conclusions
Findings suggest a pattern of inconsistent behavior in response to positive outcomes in SUDs combined with a tendency to attribute negative outcomes to chance. Specifically, individuals with SUDs fail to settle on a behavior strategy despite sufficient evidence of its success. These learning impairments could help account for difficulties in adjusting behavior and maintaining optimal decision-making during and after treatment.}
}
@article{KANWISHER2025102969,
title = {Animal models of the human brain: Successes, limitations, and alternatives},
journal = {Current Opinion in Neurobiology},
volume = {90},
pages = {102969},
year = {2025},
issn = {0959-4388},
doi = {https://doi.org/10.1016/j.conb.2024.102969},
url = {https://www.sciencedirect.com/science/article/pii/S0959438824001314},
author = {Nancy Kanwisher},
abstract = {The last three decades of research in human cognitive neuroscience have given us an initial “parts list” for the human mind in the form of a set of cortical regions with distinct and often very specific functions. But current neuroscientific methods in humans have limited ability to reveal exactly what these regions represent and compute, the causal role of each in behavior, and the interactions among regions that produce real-world cognition. Animal models can help to answer these questions when homologues exist in other species, like the face system in macaques. When homologues do not exist in animals, for example for speech and music perception, and understanding of language or other people's thoughts, intracranial recordings in humans play a central role, along with a new alternative to animal models: artificial neural networks.}
}
@article{RUDD2025108517,
title = {Fixational eye movements and edge integration in lightness perception},
journal = {Vision Research},
volume = {227},
pages = {108517},
year = {2025},
issn = {0042-6989},
doi = {https://doi.org/10.1016/j.visres.2024.108517},
url = {https://www.sciencedirect.com/science/article/pii/S0042698924001615},
author = {Michael E. Rudd and Idris Shareef},
keywords = {Lightness perception, ON and OFF cells, Fixational eye movements, Staircase Gelb illusion, Chevreul’s illusion, Fading of stabilized images},
abstract = {A neural theory of human lightness computation is described and computer-simulated. The theory proposes that lightness is derived from transient ON and OFF cell responses in the early visual pathways that have different characteristic neural gains and that are generated by fixational eye movements (FEMs) as the eyes transit luminance edges in the image. The ON and OFF responses are combined with corollary discharge signals that encode the eye movement direction to create directionally selective ON and OFF responses. Cortical neurons with large-scale receptive fields independently integrate the outputs of all of the directional ON or OFF responses whose associated eye movement directions point towards their receptive field centers, with a spatial weighting determined by the receptive field profile. Lightness is computed by subtracting the spatially integrated OFF activity from spatially integrated ON activity and normalizing the difference signal so that the maximum response in the spatial lightness map at any given time equals a fixed activation level corresponding to the percept of white. Two different mechanisms for ON and OFF cells responses are considered and simulated, and both are shown to produce an overall lightness model that explains a host of quantitative and qualitative lightness phenomena, including the Staircase Gelb and related illusions, failures of lightness constancy in the simultaneous contrast illusion, Chevreul’s illusion, lightness filling-in, and perceptual fading of stabilized images. The neural plausibility of the two variants of the theory, as well as its implication for lightness constancy and failures of lightness constancy are discussed.}
}
@article{SEOW2021436,
title = {How Local and Global Metacognition Shape Mental Health},
journal = {Biological Psychiatry},
volume = {90},
number = {7},
pages = {436-446},
year = {2021},
note = {BPS 90/7Pharmacologic Prevention and Treatment of Posttraumatic Stress Disorder},
issn = {0006-3223},
doi = {https://doi.org/10.1016/j.biopsych.2021.05.013},
url = {https://www.sciencedirect.com/science/article/pii/S0006322321013299},
author = {Tricia X.F. Seow and Marion Rouault and Claire M. Gillan and Stephen M. Fleming},
keywords = {Confidence, Mental health, Metacognition, Self-beliefs, Self-efficacy, Transdiagnostic psychiatry},
abstract = {Metacognition is the ability to reflect on our own cognition and mental states. It is a critical aspect of human subjective experience and operates across many hierarchical levels of abstraction—encompassing local confidence in isolated decisions and global self-beliefs about our abilities and skills. Alterations in metacognition are considered foundational to neurologic and psychiatric disorders, but research has mostly focused on local metacognitive computations, missing out on the role of global aspects of metacognition. Here, we first review current behavioral and neural metrics of local metacognition that lay the foundation for this research. We then address the neurocognitive underpinnings of global metacognition uncovered by recent studies. Finally, we outline a theoretical framework in which higher hierarchical levels of metacognition may help identify the role of maladaptive metacognitive evaluation in mental health conditions, particularly when combined with transdiagnostic methods.}
}
@article{DUAN2023103365,
title = {Mining multigranularity decision rules of concept cognition for knowledge graphs based on three-way decision},
journal = {Information Processing & Management},
volume = {60},
number = {4},
pages = {103365},
year = {2023},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2023.103365},
url = {https://www.sciencedirect.com/science/article/pii/S0306457323001024},
author = {Jiangli Duan and Guoyin Wang and Xin Hu and Deyou Xia and Di Wu},
keywords = {Granular computing, Cognitive intelligence, Concept cognition, Knowledge graph, Three-way decision},
abstract = {Machine understanding and thinking require prior knowledge consisting of explicit and implicit knowledge. The current knowledge base contains various explicit knowledge but not implicit knowledge. As part of implicit knowledge, the typical characteristics of the things referred to by the concept are available by concept cognition for knowledge graphs. Therefore, this paper attempts to realize concept cognition for knowledge graphs from the perspective of mining multigranularity decision rules. Specifically, (1) we propose a novel multigranularity three-way decision model that merges the ideas of multigranularity (i.e., from coarse granularity to fine granularity) and three-way decision (i.e., acceptance, rejection, and deferred decision). (2) Based on the multigranularity three-way decision model, an algorithm for mining multigranularity decision rules is proposed. (3) The monotonicity of positive or negative granule space ensured that the positive (or negative) granule space from coarser granularity does not need to participate in the three-classification process at a finer granularity, which accelerates the process of mining multigranularity decision rules. Moreover, the experimental results show that the multigranularity decision rule is better than the two-way decision rule, frequent decision rule and single granularity decision rule, and the monotonicity of positive or negative granule space can accelerate the process of mining multigranularity decision rules.}
}
@incollection{MONTEIRO202253,
title = {4 - An artificial intelligent cognitive approach for classification and recognition of white blood cells employing deep learning for medical applications},
editor = {Deepak Gupta and Utku Kose and Ashish Khanna and Valentina Emilia Balas},
booktitle = {Deep Learning for Medical Applications with Unique Data},
publisher = {Academic Press},
pages = {53-69},
year = {2022},
isbn = {978-0-12-824145-5},
doi = {https://doi.org/10.1016/B978-0-12-824145-5.00012-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128241455000125},
author = {Ana Carolina Borges Monteiro and Reinaldo Padilha França and Rangel Arthur and Yuzo Iano},
keywords = {Artificial intelligence, Biomedical signals, CNN, Cognitive computing, Cognitive health care, Cognitive models, Deep learning, Digital image, Erythrocytes, Health care, Health care data, Health care informatics, Image processing, Leukocytes, Python},
abstract = {Cognitive computing aims to implement a unified computational theory similar to human thought, consisting of systems whose objective is to mimic human mental tasks based on the concepts of artificial intelligence and machine learning generating and understanding knowledge. Deep learning is an abstraction of the biological neural network, and can understood as a complex structure interconnected by simple processing elements (neurons), can perform operations as calculations in parallel, for data processing and representation of knowledge. Convolutional neural networks for image recognition through deep learning has been modeled to determine good performance with respect to digital image recognition, especially in medical areas, which is a classic problem of computational classification. In this context, an artificial intelligent cognitive approach was developed achieving an accuracy of 84.19%, which used Python employing Jupyter Notebook, with a dataset of 12,500 medical digital images of human blood smear fields of nonpathologic leukocytes.}
}
@incollection{TVERSKY197817,
title = {2 - Judgment under Uncertainty: Heuristics and Biases: Biases in judgments reveal some heuristics of thinking under uncertainty},
editor = {PETER DIAMOND and MICHAEL ROTHSCHILD},
booktitle = {Uncertainty in Economics},
publisher = {Academic Press},
pages = {17-34},
year = {1978},
isbn = {978-0-12-214850-7},
doi = {https://doi.org/10.1016/B978-0-12-214850-7.50008-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780122148507500085},
author = {Amos Tversky and Daniel Kahneman},
abstract = {Publisher Summary
Many decisions are based on beliefs concerning the likelihood of uncertain events such as the outcome of an election, the guilt of a defendant, or the future value of the dollar. Occasionally, beliefs concerning uncertain events are expressed in numerical form as odds or subjective probabilities. In general, the heuristics are quite useful, but sometimes they lead to severe and systematic errors. The subjective assessment of probability resembles the subjective assessment of physical quantities such as distance or size. These judgments are all based on data of limited validity, which are processed according to heuristic rules. However, the reliance on this rule leads to systematic errors in the estimation of distance. This chapter describes three heuristics that are employed in making judgments under uncertainty. The first is representativeness, which is usually employed when people are asked to judge the probability that an object or event belongs to a class or event. The second is the availability of instances or scenarios, which is often employed when people are asked to assess the frequency of a class or the plausibility of a particular development, and the third is adjustment from an anchor, which is usually employed in numerical prediction when a relevant value is available.}
}
@incollection{MILLER2020205,
title = {Chapter 10 - AI, autonomous machines and human awareness: Towards shared machine-human contexts in medicine},
editor = {William F. Lawless and Ranjeev Mittu and Donald A. Sofge},
booktitle = {Human-Machine Shared Contexts},
publisher = {Academic Press},
pages = {205-220},
year = {2020},
isbn = {978-0-12-820543-3},
doi = {https://doi.org/10.1016/B978-0-12-820543-3.00010-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128205433000109},
author = {D. Douglas Miller and Elena A. Wood},
keywords = {Medicine, Health care, Artificial intelligence, Medical education, Applications, Challenges},
abstract = {Medical curricula trend to integrate clinical skills training and to create efficiencies in preclinical medical sciences, but the rapid emergence big data-intensive health care has led to initiating collaborations among data scientists, computer engineers, and medical educators that might generate novel educational high-technology platforms and innovative AI practice applications. The preprocessing of big data improves neural network feature recognition, improving the speed and accuracy of AI diagnostics and permitting chronic disease predictions. Applications of generative adversarial networks to create virtual patient phenotypes and image sets exposes medical learners to endless illness presentations, improving system-1 critical thinking for differential diagnosis development. AI offers great potential for education data managers working in support of medical educators and learners. These opportunities to build a shared context, in keeping with these themes of this book, include emerging data-driven AI applications for medical education and provider training include individual aptitude-based career advising, early identification of learners with academic difficulties, highly focused e-tutoring interventions, and natural language processing of standardized exam questions.}
}
@article{YEAP1988297,
title = {Towards a computational theory of cognitive maps},
journal = {Artificial Intelligence},
volume = {34},
number = {3},
pages = {297-360},
year = {1988},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(88)90064-1},
url = {https://www.sciencedirect.com/science/article/pii/0004370288900641},
author = {Wai K. Yeap},
abstract = {A computational theory of cognitive maps is developed which can explain some of the current findings about cognitive maps in the psychological literature and which provides a coherent framework for future development. The theory is tested with several computer implementations which demonstrate how the shape of the environment is computed and how one's conceptual representation of the environment is derived. We begin with the idea that the cognitive mapping process should be studied as two loosely coupled modules: The first module, known as the raw cognitive map, is computed from information made explicit in Marr's 212-D sketch and not from high-level descriptions of what we perceive. The second module, known as the full cognitive map, takes the raw cognitive map as input and produces different “abstract representations” for solving high-level spatial tasks faced by the individual.}
}
@article{GRUJIC20243381,
title = {Neurobehavioral meaning of pupil size},
journal = {Neuron},
volume = {112},
number = {20},
pages = {3381-3395},
year = {2024},
issn = {0896-6273},
doi = {https://doi.org/10.1016/j.neuron.2024.05.029},
url = {https://www.sciencedirect.com/science/article/pii/S0896627324004069},
author = {Nikola Grujic and Rafael Polania and Denis Burdakov},
keywords = {pupil, arousal, cognition, noradrenaline, orexin, hypocretin},
abstract = {Summary
Pupil size is a widely used metric of brain state. It is one of the few signals originating from the brain that can be readily monitored with low-cost devices in basic science, clinical, and home settings. It is, therefore, important to investigate and generate well-defined theories related to specific interpretations of this metric. What exactly does it tell us about the brain? Pupils constrict in response to light and dilate during darkness, but the brain also controls pupil size irrespective of luminosity. Pupil size fluctuations resulting from ongoing “brain states” are used as a metric of arousal, but what is pupil-linked arousal and how should it be interpreted in neural, cognitive, and computational terms? Here, we discuss some recent findings related to these issues. We identify open questions and propose how to answer them through a combination of well-defined tasks, neurocomputational models, and neurophysiological probing of the interconnected loops of causes and consequences of pupil size.}
}
@article{KRAJNAK2021132976,
title = {Reactive islands for three degrees-of-freedom Hamiltonian systems},
journal = {Physica D: Nonlinear Phenomena},
volume = {425},
pages = {132976},
year = {2021},
issn = {0167-2789},
doi = {https://doi.org/10.1016/j.physd.2021.132976},
url = {https://www.sciencedirect.com/science/article/pii/S0167278921001330},
author = {Vladimír Krajňák and Víctor J. García-Garrido and Stephen Wiggins},
keywords = {Phase space of Hamiltonian systems, Stable and unstable manifolds, Normally hyperbolic invariant manifolds, Reactive islands, Spherinders, Lagrangian descriptors},
abstract = {We develop the geometrical, analytical, and computational framework for reactive island theory for three degrees-of-freedom time-independent Hamiltonian systems. In this setting, the dynamics occurs in a 5-dimensional energy surface in phase space and is governed by four-dimensional stable and unstable manifolds of a three-dimensional normally hyperbolic invariant sphere. The stable and unstable manifolds have the geometrical structure of spherinders and we provide the means to investigate the ways in which these spherinders and their intersections determine the dynamical evolution of trajectories. This geometrical picture is realized through the computational technique of Lagrangian descriptors. In a set of trajectories, Lagrangian descriptors allow us to identify the ones closest to a stable or unstable manifold. Using an approximation of the manifold on a surface of section we are able to calculate the flux between two regions of the energy surface.}
}
@article{RIVIERE2024637,
title = {Proceedings from the inaugural Artificial Intelligence in Primary Immune Deficiencies (AIPID) conference},
journal = {Journal of Allergy and Clinical Immunology},
volume = {153},
number = {3},
pages = {637-642},
year = {2024},
issn = {0091-6749},
doi = {https://doi.org/10.1016/j.jaci.2024.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S0091674924000332},
author = {Jacques G. Rivière and Pere {Soler Palacín} and Manish J. Butte},
keywords = {Artificial intelligence, machine learning, large language models, natural language processing, electronic health records, inborn errors of immunity, diagnosis, ethics},
abstract = {Here, we summarize the proceedings of the inaugural Artificial Intelligence in Primary Immune Deficiencies conference, during which experts and advocates gathered to advance research into the applications of artificial intelligence (AI), machine learning, and other computational tools in the diagnosis and management of inborn errors of immunity (IEIs). The conference focused on the key themes of expediting IEI diagnoses, challenges in data collection, roles of natural language processing and large language models in interpreting electronic health records, and ethical considerations in implementation. Innovative AI-based tools trained on electronic health records and claims databases have discovered new patterns of warning signs for IEIs, facilitating faster diagnoses and enhancing patient outcomes. Challenges in training AIs persist on account of data limitations, especially in cases of rare diseases, overlapping phenotypes, and biases inherent in current data sets. Furthermore, experts highlighted the significance of ethical considerations, data protection, and the necessity for open science principles. The conference delved into regulatory frameworks, equity in access, and the imperative for collaborative efforts to overcome these obstacles and harness the transformative potential of AI. Concerted efforts to successfully integrate AI into daily clinical immunology practice are still needed.}
}
@article{THOMPSON2013256,
title = {The role of answer fluency and perceptual fluency in the monitoring and control of reasoning: Reply to Alter, Oppenheimer, and Epley (2013)},
journal = {Cognition},
volume = {128},
number = {2},
pages = {256-258},
year = {2013},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2013.03.003},
url = {https://www.sciencedirect.com/science/article/pii/S0010027713000553},
author = {Valerie A. Thompson and Rakefet Ackerman and Yael Sidi and Linden J. Ball and Gordon Pennycook and Jamie A. {Prowse Turner}},
keywords = {Perceptual fluency, Answer fluency, Dual process theories, Metacognition, Intuition, Analytic thinking},
abstract = {In this reply, we provide an analysis of Alter et al. (2013) response to our earlier paper (Thompson et al., 2013). In that paper, we reported difficulty in replicating Alter, Oppenheimer, Epley, and Eyre’s (2007) main finding, namely that a sense of disfluency produced by making stimuli difficult to perceive, increased accuracy on a variety of reasoning tasks. Alter, Oppenheimer, and Epley (2013) argue that we misunderstood the meaning of accuracy on these tasks, a claim that we reject. We argue and provide evidence that the tasks were not too difficult for our populations (such that no amount of “metacognitive unease” would promote correct responding) and point out that in many cases performance on our tasks was well above chance or on a par with Alter et al.’s (2007) participants. Finally, we reiterate our claim that the distinction between answer fluency (the ease with which an answer comes to mind) and perceptual fluency (the ease with which a problem can be read) is genuine, and argue that Thompson et al. (2013) provided evidence that these are distinct factors that have different downstream effects on cognitive processes.}
}
@article{MACLENNAN2015410,
title = {Living science: Science as an activity of living beings},
journal = {Progress in Biophysics and Molecular Biology},
volume = {119},
number = {3},
pages = {410-419},
year = {2015},
note = {Integral Biomathics: Life Sciences, Mathematics, and Phenomenological Philosophy},
issn = {0079-6107},
doi = {https://doi.org/10.1016/j.pbiomolbio.2015.08.009},
url = {https://www.sciencedirect.com/science/article/pii/S0079610715001224},
author = {Bruce J. MacLennan},
keywords = {Philosophy of science, Phenomenology, Embodied cognition, Causality, Analytical psychology, Goethe},
abstract = {The philosophy of science should accommodate itself to the facts of human existence, using all aspects of human experience to adapt more effectively, as individuals, species, and global ecosystem. This has several implications: (1) Our nature as sentient beings interacting with other sentient beings requires the use of phenomenological methods to investigate consciousness. (2) Our embodied, situated, purposeful physical interactions with the world are the foundation of scientific understanding. (3) Aristotle's four causes are essential for understanding living systems and, in particular, the final cause aids understanding the role of humankind, and especially science, in the global ecosystem. (4) In order to fulfill this role well, scientists need to employ the full panoply of human faculties. These include the consciousness faculties (thinking, sensation, feeling, intuition), and therefore, as advocated by many famous scientists, we should cultivate our aesthetic sense, emotions, imagination, and intuition. Our unconscious faculties include archetypal structures common to all humans, which can guide scientific discovery. By striving to engage the whole of human nature, science will fulfill better its function for humans and the global ecosystem.}
}
@article{JONES201795,
title = {An exploratory study on student understandings of derivatives in real-world, non-kinematics contexts},
journal = {The Journal of Mathematical Behavior},
volume = {45},
pages = {95-110},
year = {2017},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2016.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S0732312316301791},
author = {Steven R. Jones},
keywords = {Calculus, Derivative, Applications, Real-world, Student understanding},
abstract = {Much research on calculus students’ understanding of applied derivatives has been done in kinematics-based contexts (i.e. position, velocity, acceleration). However, given the wide range of applications in science and engineering that are not based on kinematics, nor even explicitly on time, it is important to know how students understand applied derivatives in non-kinematics contexts. In this study, interviews with six students and surveys with 38 students were used to explore students’ “ways of understanding” and “ways of thinking” regarding applied, non-kinematics derivatives. In particular, six categories of ways of understanding emerged from the data as having been shared by a substantial portion of the students in this study: (1) covariation, (2) invoking time, (3) other symbols as constants, (4) other symbols as implicit functions, (5) implicit differentiation, and (6) output values as amounts instead of rates of change.}
}