@article{KARIMI2024,
title = {Substances led to Psychosis: A Systematic Review},
journal = {The Open Psychology Journal},
volume = {17},
year = {2024},
issn = {1874-3501},
doi = {https://doi.org/10.2174/0118743501297735240510161825},
url = {https://www.sciencedirect.com/science/article/pii/S1874350124000245},
author = {Isaac Karimi and Nioosha Pooyanmehr},
keywords = {Psychosis, Illicit drugs, Youth, Antibiotics, Neurologic symptoms, Alcohol},
abstract = {Background
Psychosis is one of the mind-related disorders that has been common in the new generation, and it has an increasing trend. Psychosis is a variable feature of mood that could be a result of substance use, which includes a few psychiatric and neurologic symptoms. Common symptoms of psychoses are delusions, hallucinations, disorganized thinking, grossly disorganized, or abnormal motor behavior. An array of illicit substances and drugs that can lead to psychosis include cannabinoids, cocaine, amphetamines, methamphetamines, alcohol, etc.
Objectives
The main aim of this review was to discover, analyze, and combine the information concerning substances that could potentially cause psychoses.
Methods
We conducted a literature search on the following network databases: PubMed, Scholar, Science Direct, PubChem, Scopus, and Web-Of-Science. We selected 14 studies potentially relevant articles published from 1990 to 2023 for detailed evaluation. The systematic review was done adhering to PRISMA guidelines. We gathered the important primary studies of eligible systematic reviews and collected data on the interventions employed in these studies to comprehend the strategies that were pursued.
Results
Our result indicated that there are a few substances, which include Cannabinoids, Alcohol, Amphetamine, Cocaine, Nicotine, Kratom, Cathinone, etc., that may lead to psychoses with average to high possibility.
Conclusion
Evidence regarding frequently encountered substances that might contribute to psychosis presents an opportunity to develop customized interventions in the form of user-friendly menus aimed at meeting individuals' requirements and urging them to refrain from consumption.}
}
@article{BEJAR2005117,
title = {Sensor networks and distributed CSP: communication, computation and complexity},
journal = {Artificial Intelligence},
volume = {161},
number = {1},
pages = {117-147},
year = {2005},
note = {Distributed Constraint Satisfaction},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2004.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S000437020400150X},
author = {Ramón Béjar and Carmel Domshlak and Cèsar Fernández and Carla Gomes and Bhaskar Krishnamachari and Bart Selman and Magda Valls},
keywords = {Distributed CSP benchmark, Phase transitions, Randomized combinatorial search, Communication network delays, NP-completeness},
abstract = {We introduce SensorDCSP, a naturally distributed benchmark based on a real-world application that arises in the context of networked distributed systems. In order to study the performance of Distributed CSP (DisCSP) algorithms in a truly distributed setting, we use a discrete-event network simulator, which allows us to model the impact of different network traffic conditions on the performance of the algorithms. We consider two complete DisCSP algorithms: asynchronous backtracking (ABT) and asynchronous weak commitment search (AWC), and perform performance comparison for these algorithms on both satisfiable and unsatisfiable instances of SensorDCSP. We found that random delays (due to network traffic or in some cases actively introduced by the agents) combined with a dynamic decentralized restart strategy can improve the performance of DisCSP algorithms. In addition, we introduce GSensorDCSP, a plain-embedded version of SensorDCSP that is closely related to various real-life dynamic tracking systems. We perform both analytical and empirical study of this benchmark domain. In particular, this benchmark allows us to study the attractiveness of solution repairing for solving a sequence of DisCSPs that represent the dynamic tracking of a set of moving objects.}
}
@article{IANDOLI2014298,
title = {Socially augmented argumentation tools: Rationale, design and evaluation of a debate dashboard},
journal = {International Journal of Human-Computer Studies},
volume = {72},
number = {3},
pages = {298-319},
year = {2014},
issn = {1071-5819},
doi = {https://doi.org/10.1016/j.ijhcs.2013.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S1071581913001043},
author = {Luca Iandoli and Ivana Quinto and Anna {De Liddo} and Simon {Buckingham Shum}},
keywords = {Computer-supported argument visualization, Grounding process, Common ground, Debate dashboard, Collective deliberation, Visual feedback},
abstract = {Collaborative Computer-Supported Argument Visualization (CCSAV) is a technical methodology that offers support for online collective deliberation over complex dilemmas. As compared with more traditional conversational technologies, like wikis and forums, CCSAV is designed to promote more critical thinking and evidence-based reasoning, by using representations that highlight conceptual relationships between contributions, and through computational analytics that assess the structural integrity of the network. However, to date, CCSAV tools have achieved adoption primarily in small-scale educational contexts, and only to a limited degree in real world applications. We hypothesise that by reifying conversations as logical maps to address the shortcomings of chronological streams, CCSAV tools underestimate the importance of participation and interaction in enhancing collaborative knowledge-building. We argue, therefore, that CCSAV platforms should be socially augmented in order to improve their mediation capability. Drawing on Clark and Brennan influential Common Ground theory, we designed a Debate Dashboard, which augmented a CCSAV tool with a set of widgets that deliver meta-information about participants and the interaction process. An empirical study simulating a moderately sized collective deliberation scenario provides evidence that this experimental version outperformed the control version on a range of indicators, including usability, mutual understanding, quality of perceived collaboration, and accuracy of individual decisions. No evidence was found that the addition of the Debate Dashboard impeded the quality of the argumentation or the richness of content.}
}
@article{TURKSON2020110464,
title = {Sustainability assessment of energy production: A critical review of methods, measures and issues},
journal = {Journal of Environmental Management},
volume = {264},
pages = {110464},
year = {2020},
issn = {0301-4797},
doi = {https://doi.org/10.1016/j.jenvman.2020.110464},
url = {https://www.sciencedirect.com/science/article/pii/S0301479720303984},
author = {Charles Turkson and Adolf Acquaye and Wenbin Liu and Thanos Papadopoulos},
keywords = {Sustainability, Energy production, Systematic review, Systems thinking, Energy policy, Sustainability assessment},
abstract = {Sustainable operations of energy production systems have become an increasingly important policy agenda globally because of the massive pressure placed on energy resources needed to support economic development and population growth. Due to the increasing research interest in examining the operational impacts of energy production systems on the society and the environment, this paper critically reviews the academic literature on the clean, affordable and secure supply of energy focussing on methods of assessments, measures of sustainability and emerging issues in the literature. While there have been some surveys on the sustainability of energy production systems they have either tended to focus on one assessment approach or one type of energy generation technology. This study builds on previous studies by providing a broader and comprehensive examination of the literature across generation technologies and assessment methods. A systematic review of 128 scholarly articles covering a 20-year period, ending 2018, and gathered from ProQuest, Scopus, and manual search is conducted. Synthesis and critical evaluation of the reviewed papers highlight a number of research gaps that exist within the sustainable energy production systems research domain. In addition, using mapping and cluster analyses, the paper visually highlights the network of dominant research issues, which emerged from the review.}
}
@article{WISTEN199777,
title = {Distributed computation of dynamic traffic equilibria},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {5},
number = {2},
pages = {77-93},
year = {1997},
note = {Parallel Computing in Transport Research},
issn = {0968-090X},
doi = {https://doi.org/10.1016/S0968-090X(97)00003-X},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X9700003X},
author = {M.B. Wisten and M.J. Smith},
abstract = {The dynamic traffic assignment problem is formulated in the space of splitting rates rather than link and route flows. A distributed algorithm for computation of dynamic user-equilibria is specified. The algorithm has been implemented on a Meiko Computing Surface with 32 T800 processors and some numerical results are given. We do not yet have a general proof of convergence for the algorithm but we have been able to demonstrate convergence with all test networks used.}
}
@article{ZAKI2024100188,
title = {A data-driven framework to inform sustainable management of animal manure in rural agricultural regions using emerging resource recovery technologies},
journal = {Cleaner Environmental Systems},
volume = {13},
pages = {100188},
year = {2024},
issn = {2666-7894},
doi = {https://doi.org/10.1016/j.cesys.2024.100188},
url = {https://www.sciencedirect.com/science/article/pii/S2666789424000266},
author = {Mohammed T. Zaki and Lewis S. Rowles and Jeff Hallowell and Kevin D. Orner},
keywords = {Machine learning, Life cycle assessment, Techno-economic analysis, Pyrolysis, Hydrothermal carbonization, Carbon dioxide removal},
abstract = {Thermochemical conversion technologies are emerging as preferred resource recovery practices for managing animal manure in agricultural regions. Although the implementation of such technologies has been previously studied, difficulties exist in maintaining balance between high rate of resource recovery and low environmental, economic, and social impacts, particularly in rural regions with limited resources. We developed a data-driven framework by integrating machine learning with life cycle thinking that can be used as an open-source tool to help overcome these barriers. The framework was applied to compare two emerging technologies: pyrolysis versus hydrothermal carbonization for managing the excess poultry litter in a rural agricultural region. Among different machine learning models, random forest regression was the most successful to predict resource recovery of both technologies. Next, sustainability analysis indicated that the environmental (global warming), economic (annual worth), and social (system intrusiveness) impacts of pyrolysis was lower than hydrothermal carbonization. Finally, the framework revealed that implementation of pyrolysis at 600 °C for 1 h with the heating rate of 20 °C/min would result in the highest rate of resource recovery that corresponded to the lowest impacts. These results can be helpful in providing operational conditions for implementing emerging resource recovery technologies in rural agricultural regions.}
}
@article{MARTINS20183890,
title = {2MBio, a novel tool to encourage creative participatory conceptual design of bioenergy systems – The case of wood fuel energy systems in south Mozambique},
journal = {Journal of Cleaner Production},
volume = {172},
pages = {3890-3906},
year = {2018},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2017.05.062},
url = {https://www.sciencedirect.com/science/article/pii/S0959652617309873},
author = {Ricardo Martins and Judith A. Cherni and Nuno Videira},
keywords = {Design thinking, Systems thinking, Mozambique, Participatory design tools, Wood fuel energy systems, Bioenergy},
abstract = {This paper proposes a new conceptual design tool for bioenergy systems, the 2MBio, and its implementation on the case of wood fuel energy systems (WES) in South Mozambique. Dependence on wood fuel characterises most Sub-Saharan countries and WES are complex socio-ecological systems dynamically linked to crucial development issues, e.g., deforestation and poverty. In Mozambique WES supply over 70% of the national energy needs through an informal business network worth around one million euros each year. In contrast with the 2MBio, currently available tools often aim at supporting decision-making on WES with off-the-shelf expert solutions and optimisation of WES efficiency, supply chains and resource management. While relevant and useful, such approaches are frequently unsuitable to engage the knowledge and creativity of a wide range of crucial actors. The 2MBio addresses this gap providing a simple, visual platform on paper that supports from illiterate to professional users, to stimulate creative ideas and apply current knowledge while designing their own WES. The results of implementation in real settings in South Mozambique produced relevant design breakthroughs. Compared with the absence of any other support tool, and faced with same design challenges, the 2MBio participatory design workshops in south Mozambique resulted in comprehensive analysis of wood fuel energy systems, and innovative integrated WES solutions design. The proposed approach raised participants’ awareness about opportunities and constrains linked to their WES while also facilitating information sharing new learning dynamics and enhance creativity.}
}
@article{BIRO2015876,
title = {Measuring the Level of Algorithmic Skills at the End of Secondary Education in Hungary},
journal = {Procedia - Social and Behavioral Sciences},
volume = {176},
pages = {876-883},
year = {2015},
note = {International Educational Technology Conference, IETC 2014, 3-5 September 2014, Chicago, IL, USA},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2015.01.553},
url = {https://www.sciencedirect.com/science/article/pii/S187704281500590X},
author = {Piroska Biró and Mária Csernoch and János Máth and Kálmán Abari},
keywords = {level of digital thinking, algorithmic skills, school leaving exams in Informatics and Mathematics},
abstract = {Students starting their tertiary studies in Informatics are found to have a low level of algorithmic skills and understanding of programming, which leads to the high number of drop out students and failed semesters during their studies. The students’ low level of programming skills contrasts with their excellent results in the school leaving exams. To find out the reasons for this we have launched the TAaAS project (Testing Algorithmic and Application Skills), which focuses on the students’ algorithmic skills and programming ability in traditional and non-traditional programming environments. Our analyses proved that school leaving exams are not able to measure these abilities of the students, and beyond that, are not able to distinguish between the different levels of the students. Students are accepted into the universities and start their studies based on the misleading results of the school leaving exams.}
}
@article{KOK2016342,
title = {Crowd behavior analysis: A review where physics meets biology},
journal = {Neurocomputing},
volume = {177},
pages = {342-362},
year = {2016},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2015.11.021},
url = {https://www.sciencedirect.com/science/article/pii/S0925231215017403},
author = {Ven Jyn Kok and Mei Kuan Lim and Chee Seng Chan},
keywords = {Crowd behavior analysis, Biologically inspired, Physics-inspired, Computer vision, Survey},
abstract = {Although the traits emerged in a mass gathering are often non-deliberative, the act of mass impulse may lead to irrevocable crowd disasters. The two-fold increase of carnage in crowd since the past two decades has spurred significant advances in the field of computer vision, towards effective and proactive crowd surveillance. Computer vision studies related to crowd are observed to resonate with the understanding of the emergent behavior in physics (complex systems) and biology (animal swarm). These studies, which are inspired by biology and physics, share surprisingly common insights, and interesting contradictions. However, this aspect of discussion has not been fully explored. Therefore, this survey provides the readers with a review of the state-of-the-art methods in crowd behavior analysis from the physics and biologically inspired perspectives. We provide insights and comprehensive discussions for a broader understanding of the underlying prospect of blending physics and biology studies in computer vision.}
}
@article{GRIFFITHS2020873,
title = {Understanding Human Intelligence through Human Limitations},
journal = {Trends in Cognitive Sciences},
volume = {24},
number = {11},
pages = {873-883},
year = {2020},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2020.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S1364661320302151},
author = {Thomas L. Griffiths},
keywords = {artificial intelligence, inductive bias, meta-learning, rational meta-reasoning, cultural evolution},
abstract = {Recent progress in artificial intelligence provides the opportunity to ask the question of what is unique about human intelligence, but with a new comparison class. I argue that we can understand human intelligence, and the ways in which it may differ from artificial intelligence, by considering the characteristics of the kind of computational problems that human minds have to solve. I claim that these problems acquire their structure from three fundamental limitations that apply to human beings: limited time, limited computation, and limited communication. From these limitations we can derive many of the properties we associate with human intelligence, such as rapid learning, the ability to break down problems into parts, and the capacity for cumulative cultural evolution.}
}
@article{PITOWSKY1996161,
title = {Laplace's demon consults an oracle: The computational complexity of prediction},
journal = {Studies in History and Philosophy of Science Part B: Studies in History and Philosophy of Modern Physics},
volume = {27},
number = {2},
pages = {161-180},
year = {1996},
issn = {1355-2198},
doi = {https://doi.org/10.1016/1355-2198(96)85115-X},
url = {https://www.sciencedirect.com/science/article/pii/135521989685115X},
author = {Itamar Pitowsky}
}
@article{SILVA2017137,
title = {Evaluating the usefulness of the structural accessibility layer for planning practice – Planning practitioners’ perception},
journal = {Transportation Research Part A: Policy and Practice},
volume = {104},
pages = {137-149},
year = {2017},
issn = {0965-8564},
doi = {https://doi.org/10.1016/j.tra.2017.05.014},
url = {https://www.sciencedirect.com/science/article/pii/S0965856417304755},
author = {Cecília Silva and Tiago Patatas and Ana Amante},
keywords = {Accessibility instrument, Implementation gap, Planning practice, Usefulness in practice},
abstract = {There has been a growing attention on accessibility concepts from both planning practice and research recognising their relevance in understanding the evolution of urban areas. However, despite the large number of accessibility measures available in the literature, they are not widely used to support urban planning practices. Much has been said about the implementation gap of Planning Support Systems with a significant attention paid to usability and more recently to the usefulness of Accessibility Instruments. The paper aims to assess the usefulness of a specific accessibility instrument – the Structural Accessibility Layer (SAL) – and by doing so exploring the strengths of accessibility instruments holding similar characteristics. To this end, we follow a multidimensional assessment framework under development in the Planning Support System literature. This paper explores the main findings of a workshop bringing together local planning practitioners and the developers of the SAL in an experiment using the SAL. The assessment of usefulness of SAL identified the instrument’s strengths with regard to insight into participants’ assumptions, communication, commitment and development of shared language. Regardless, the low fit between planning concerns of participants (in this case study context) and of the SAL seemed to limit its potential use in practice and as such undermines the strengths identified in the usefulness assessment. The assessment developed here only partially confirmed objectives and purposes defined for the SAL. Results confirm the usefulness of the SAL as diagnosis tool, however, the ability of the SAL to contribute to a joint thinking of land use and transport constraints on mobility was not confirmed. Finally, this research raises questions on the role of PSS in changing strategic thinking in planning and how this might conflict with the current PSS research concern in improving usefulness of tools.}
}
@article{MASHALEH20242245,
title = {IoT Smart Devices Risk Assessment Model Using Fuzzy Logic and PSO},
journal = {Computers, Materials and Continua},
volume = {78},
number = {2},
pages = {2245-2267},
year = {2024},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2023.047323},
url = {https://www.sciencedirect.com/science/article/pii/S1546221824001267},
author = {Ashraf S. Mashaleh and Noor Farizah Binti Ibrahim and Mohammad Alauthman and Mohammad Almseidin and Amjad Gawanmeh},
keywords = {IoT botnet detection, risk assessment, fuzzy logic, particle swarm optimization (PSO), cybersecurity, interconnected devices},
abstract = {Increasing Internet of Things (IoT) device connectivity makes botnet attacks more dangerous, carrying catastrophic hazards. As IoT botnets evolve, their dynamic and multifaceted nature hampers conventional detection methods. This paper proposes a risk assessment framework based on fuzzy logic and Particle Swarm Optimization (PSO) to address the risks associated with IoT botnets. Fuzzy logic addresses IoT threat uncertainties and ambiguities methodically. Fuzzy component settings are optimized using PSO to improve accuracy. The methodology allows for more complex thinking by transitioning from binary to continuous assessment. Instead of expert inputs, PSO data-driven tunes rules and membership functions. This study presents a complete IoT botnet risk assessment system. The methodology helps security teams allocate resources by categorizing threats as high, medium, or low severity. This study shows how CICIoT2023 can assess cyber risks. Our research has implications beyond detection, as it provides a proactive approach to risk management and promotes the development of more secure IoT environments.}
}
@article{ZHANG201499,
title = {Profiles of psychiatric symptoms among amphetamine type stimulant and ketamine using inpatients in Wuhan, China},
journal = {Journal of Psychiatric Research},
volume = {53},
pages = {99-102},
year = {2014},
issn = {0022-3956},
doi = {https://doi.org/10.1016/j.jpsychires.2014.02.010},
url = {https://www.sciencedirect.com/science/article/pii/S0022395614000508},
author = {Yao Zhang and Zaifeng Xu and Sheng Zhang and Alethea Desrosiers and Richard S. Schottenfeld and Marek C. Chawarski},
keywords = {Amphetamine type stimulants (ATS), Ketamine, Psychiatric symptoms},
abstract = {Amphetamine type stimulants (ATS) and ketamine have emerged as major drug problems in China, and chronic extensive exposure to these substances frequently co-occurs with psychiatric symptoms. This study compares the psychiatric symptoms of patients reporting ATS use only, ATS and ketamine use, or ketamine use only who were admitted to an inpatient psychiatry ward in Wuhan, China between 2010 and 2011. Data on 375 study participants collected during their ward admission and extracted from their clinical records included their socio-demographics, scores on the Brief Psychiatric Rating Scale (BPRS), and urine toxicology screens.
Results
The ketamine-only group had significantly lower total BPRS scores and significantly lower scores on Thinking Disorder, Activity, and Hostility-Suspicion BPRS subscales than the ATS-only and ATS + ketamine groups (p < 0.001 for all comparisons). The ketamine-only group also had significantly higher scores on the subscales of Anxiety-Depression and Anergia. The ATS-only group had significantly higher scores on subscales of Thinking Disorder, Activity, and Hostility-Suspicion and significantly lower scores on Anxiety-Depression and Anergia subscales than the ketamine-only and ATS + ketamine groups (p < 0.001 for all comparisons). A K-means cluster method identified three distinct clusters of patients based on the similarities of their BPRS subscale profiles, and the identified clusters differed markedly on the proportions of participants reporting different primary drugs of abuse. The study findings suggest that ketamine and ATS users present with different profiles of psychiatric symptoms at admission to inpatient treatment.}
}
@article{GRAGERT199711,
title = {Differential geometric computations and computer algebra},
journal = {Mathematical and Computer Modelling},
volume = {25},
number = {8},
pages = {11-24},
year = {1997},
issn = {0895-7177},
doi = {https://doi.org/10.1016/S0895-7177(97)00055-1},
url = {https://www.sciencedirect.com/science/article/pii/S0895717797000551},
author = {P.K.H Gragert and P.H.M Kersten},
keywords = {Computer algebra, Differential geometry, Literate programming, Supersymmetry},
abstract = {The use of computer algebra in the field of differential geometry and its applications to geometric structures of partial differential equations is discussed. The differential geometric setting is shortly described; a number of programs are slightly touched, some examples given, and an application to the construction of supersymmetric extensions of the Korteweg-de Vries equation is demonstrated.}
}
@article{MUEHLENSIEPEN2022,
title = {Factors Associated With Telemedicine Use Among German General Practitioners and Rheumatologists: Secondary Analysis of Data From a Nationwide Survey},
journal = {Journal of Medical Internet Research},
volume = {24},
number = {11},
year = {2022},
issn = {1438-8871},
doi = {https://doi.org/10.2196/40304},
url = {https://www.sciencedirect.com/science/article/pii/S1438887122007373},
author = {Felix Muehlensiepen and Pascal Petit and Johannes Knitza and Martin Welcker and Nicolas Vuillerme},
keywords = {telemedicine, rheumatology, primary care, secondary analysis, health services research},
abstract = {Background
Previous studies have demonstrated telemedicine (TM) to be an effective tool to complement rheumatology care and address workforce shortage. With the outbreak of the SARS-CoV-2 pandemic, TM experienced a massive upswing. However, in rheumatology care, the use of TM stagnated again shortly thereafter. Consequently, the factors associated with physicians’ willingness to use TM (TM willingness) and actual use of TM (TM use) need to be thoroughly investigated.
Objective
This study aimed to identify the factors that determine TM use and TM willingness among German general practitioners and rheumatologists.
Methods
We conducted a secondary analysis of data from a German nationwide cross-sectional survey with general practitioners and rheumatologists. Bayesian univariate and multivariate logistic regression analyses were applied to the data to determine which factors were associated with TM use and TM willingness. The predictor variables (covariates) that were studied individually included sociodemographic factors (eg, age and sex), work characteristics (eg, practice location and medical specialty), and self-assessed knowledge of TM. All the variables positively and negatively associated with TM use and TM willingness in the univariate analysis were then considered for Bayesian model averaging analysis after a selection based on the variance inflation factor (≤2.5). All analyses were stratified by sex.
Results
Univariate analysis revealed that out of 83 variables, 36 (43%) and 34 (41%) variables were positively or negatively associated (region of practical equivalence≤5%) with TM use and TM willingness, respectively. The Bayesian model averaging analysis allowed us to identify 13 and 17 factors of TM use and TM willingness, respectively. Among these factors, being female, having very poor knowledge of TM, treating <500 patients per quarter, and not being willing to use TM were negatively associated with TM use, whereas having good knowledge of TM and treating >1000 patients per quarter were positively associated with TM use. In addition, being aged 51 to 60 years, thinking that TM is not important for current and future work, and not currently using TM were negatively associated with TM willingness, whereas owning a smart device and working in an urban area were positively associated with TM willingness.
Conclusions
The results point to the close connection between health care professionals’ knowledge of TM and actual TM use. These results lend support to the integration of digital competencies into medical education as well as hands-on training for health care professionals. Incentive programs for physicians aged >50 years and practicing in rural areas could further encourage TM willingness.}
}
@article{STORAASLI1993349,
title = {Computational mechanics analysis tools for parallel-vector supercomputers},
journal = {Computing Systems in Engineering},
volume = {4},
number = {4},
pages = {349-354},
year = {1993},
note = {Parallel Computational Methods for Large-Scale Structural Analysis and Design},
issn = {0956-0521},
doi = {https://doi.org/10.1016/0956-0521(93)90002-E},
url = {https://www.sciencedirect.com/science/article/pii/095605219390002E},
author = {O.O. Storaasli and D.T. Nguyen and M.A. Baddourah and J. Qin},
abstract = {Computational algorithms for structural analysis on parallel-vector supercomputers are reviewed. These parallel algorithms, developed by the authors, are for the assembly of structural equations, “out-of-core” strategies for linear equation solution, massively distributed-memory equation solution, unsymmetric equation solution, general eigen-solution, geometrically nonlinear finite element analysis, design sensitivity analysis for structural dynamics, optimization algorithm and domain decomposition. The source code for many of these algorithms is available from NASA Langley.}
}
@article{SAJID2023103174,
title = {Thermal case classification of solar-powered cars for binary tetra hybridity nanofluid using Cash and Carp method with Hamilton-Crosser model},
journal = {Case Studies in Thermal Engineering},
volume = {49},
pages = {103174},
year = {2023},
issn = {2214-157X},
doi = {https://doi.org/10.1016/j.csite.2023.103174},
url = {https://www.sciencedirect.com/science/article/pii/S2214157X2300480X},
author = {Tanveer Sajid and Wasim Jamshed and Nek Muhammad Katbar and Mohamed R. Eid and Assmaa Abd-Elmonem and Nesreen Sirelkhtam Elmki Abdalla and Sayed M. {El Din} and Gilder Cieza Altamirano},
keywords = {Solar sports car, Solar sheet, Reiner-Philippoff tetrhybrid nanofluid, Thermal radiation, Heat generation},
abstract = {Solar energy is the most important source of thermal energy that comes from the sun. This kind of energy has enormous potential applications in fields of technology such as photovoltaic panels, renewable power, solar light poles, and solar pumps used for water extraction. The era in which we are living is all about the applications of solar energy in industrial sectors most importantly in solar sports car manufacturing. This article presents a new way of thinking about the heat transport analyses of photovoltaic hybrid vehicles, by factoring Casson-Sutterby liquid with the inclusion of various effects like variable thermal conduction, thermal radiation, heat generation, and tetrahybrid nanoparticles. To solve the modelled equations in regards to both momentum and energy, another well-computational approach known as the Cash and Carp method was used. The effects of a wide variety of factors on temperature, shear stress, and velocity fields, as well as the surface drag coefficient and Nusselt number, are briefly described and illustrated in the form of tables and figures. It then found that the thermal radiation, heat production, and thermal conductivity parameters and insertion of agglomerative tetrhybrid nanoparticles in the base fluid amplify heat transfer rate, it has been shown that the performance of the solar car increases in terms of heat transition. In comparison to standard nanofluid, tetrahybrid nanofluid is the most effective medium for the transmission of heat. From the regression analysis, it is observed that the error in terms of Nusselt number is smaller 0.0151 for the case ε=1.5, and increases to 0.0151 in the case of ε=2.5. Relative percentage error is smaller 4.62% in the case of heat generation Q=0.7 but a maximum of 15.8% in the case of thermal radiation Rd=2.}
}
@article{ZHANG2024100479,
title = {Open source implementations of numerical algorithms for computing the complete elliptic integral of the first kind},
journal = {Results in Applied Mathematics},
volume = {23},
pages = {100479},
year = {2024},
issn = {2590-0374},
doi = {https://doi.org/10.1016/j.rinam.2024.100479},
url = {https://www.sciencedirect.com/science/article/pii/S2590037424000499},
author = {Hong-Yan Zhang and Wen-Juan Jiang},
keywords = {Complete elliptic integral of the first kind (CEI-1), Algorithm design, Orthogonal polynomials, Verification-validation-testing (VVT), STEM education, Computer programming},
abstract = {The complete elliptic integral of the first kind (CEI-1) plays a significant role in mathematics, physics and engineering. There is no simple formula for its computation, thus numerical algorithms are essential for coping with the practical problems involved. The commercial implementations for the numerical solutions, such as the functions ellipticK and EllipticK provided by MATLAB and Mathematica respectively, are based on Kcs(m) instead of the usual form K(k) such that Kcs(k2)=K(k) and m=k2. It is necessary to develop open source implementations for the computation of the CEI-1 in order to avoid potential risks of using commercial software and possible limitations due to the unknown factors. In this paper, the infinite series method, arithmetic-geometric mean (AGM) method, Gauss–Chebyshev method and Gauss–Legendre methods are discussed in details with a top-down strategy. The four key algorithms for computing the CEI-1 are designed, verified, validated and tested, which can be utilized in R& D and be reused properly. Numerical results show that our open source implementations based on K(k) are equivalent to the commercial implementation based on Kcs(m). The general algorithms for computing orthogonal polynomials developed are valuable for the STEM education and scientific computation.}
}
@article{GAO20241233,
title = {Hetero-Bäcklund transformation, bilinear forms and multi-solitons for a (2＋1)-dimensional generalized modified dispersive water-wave system for the shallow water},
journal = {Chinese Journal of Physics},
volume = {92},
pages = {1233-1239},
year = {2024},
issn = {0577-9073},
doi = {https://doi.org/10.1016/j.cjph.2024.10.004},
url = {https://www.sciencedirect.com/science/article/pii/S0577907324003940},
author = {Xin-Yi Gao},
keywords = {Shallow water, Nonlinear and dispersive long gravity waves, (2＋1)-dimensional generalized modified dispersive water-wave system, Hetero-Bäcklund transformation, Bilinear form, Soliton, Symbolic computation},
abstract = {This shallow-water-directed paper plans to consider a (2＋1)-dimensional generalized modified dispersive water-wave (2DGMDWW) system, which describes the nonlinear and dispersive long gravity waves travelling along two horizontal directions in the shallow water of uniform depth. With symbolic computation, (1) a hetero-Bäcklund transformation is constructed, coupling the solutions as for the 2DGMDWW system with the solutions as for a known (2＋1)-dimensional Boiti-Leon-Pempinelli system describing the water waves in an infinitely narrow channel of constant depth, with that hetero-Bäcklund transformation dependent on the shallow-water coefficients in the 2DGMDWW system, with the former solutions indicating certain shallow-water-wave patterns for the height of the water surface and the horizontal velocity of the water wave, while with the latter solutions related to the horizontal velocity and elevation of the water wave; (2) two sets of the bilinear forms are obtained, each set of which is shown to depend on the shallow-water coefficients in the 2DGMDWW system and to be linked to certain shallow-water-wave patterns for the height of the water surface and the horizontal velocity of the water wave; and (3) two sets of the N-soliton solutions are also worked out, each set of which is seen to rely on the shallow-water coefficients in the 2DGMDWW system and to represent the existence of N-solitonic shallow-water-wave patterns with respect to the height of the water surface and the horizontal velocity of the water wave, with N as a positive integer.}
}
@article{PIERONI2016412,
title = {Transforming a Traditional Product Offer into PSS: A Practical Application},
journal = {Procedia CIRP},
volume = {47},
pages = {412-417},
year = {2016},
note = {Product-Service Systems across Life Cycle},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2016.03.036},
url = {https://www.sciencedirect.com/science/article/pii/S2212827116300051},
author = {Marina Pieroni and Caio Marques and Carina Campese and Daniel Guzzo and Glauco Mendes and Janaína Costa and Maiara Rosa and Maicon Gouveia de Oliveira and Victor Macul and Henrique Rozenfeld},
keywords = {product-service system, servitization, business model, design thinking, practical application, action research},
abstract = {In the last decades, companies have shifted from traditional business models based on selling products to product-service systems (PSS). Despite this tendency, there is a paucity of complete methodologies and tools to guide companies on how the transition should occur. To address this issue, the goal of this research is to present a complete framework to support manufacturing companies in the servitization journey. This novel proposal involves the application of design thinking to define the value proposition integrated with a PSS oriented business model creation, that goes beyond generic methods normally applied; and the specification of business process architecture to support PSS implementation. This research followed a prescriptive approach by means of action research technique. Key findings of the framework application are presented.}
}
@article{ZAHEDI2024103730,
title = {How hypnotic suggestions work – A systematic review of prominent theories of hypnosis},
journal = {Consciousness and Cognition},
volume = {123},
pages = {103730},
year = {2024},
issn = {1053-8100},
doi = {https://doi.org/10.1016/j.concog.2024.103730},
url = {https://www.sciencedirect.com/science/article/pii/S1053810024000977},
author = {Anoushiravan Zahedi and Steven {Jay Lynn} and Werner Sommer},
keywords = {Hypnosis, Theory, Suggestibility, Hypnotizability, Hypnotic Suggestions (HS), Posthypnotic Suggestions (PHS), Direct Verbal Suggestions},
abstract = {In recent decades, hypnosis has increasingly moved into the mainstream of scientific inquiry. Hypnotic suggestions are frequently implemented in behavioral, neurocognitive, and clinical investigations and interventions. Despite abundant reports about the effectiveness of suggestions in altering behavior, perception, cognition, and agency, no consensus exists regarding the mechanisms driving these changes. This article reviews competing theoretical accounts that address the genesis of subjective, behavioral, and neurophysiological responses to hypnotic suggestions. We systematically analyze the broad landscape of hypnosis theories that best represent our estimation of the current status and future avenues of scientific thinking. We start with procedural descriptions of hypnosis, suggestions, and hypnotizability, followed by a comparative analysis of systematically selected theories. Considering that prominent theoretical perspectives emphasize different aspects of hypnosis, our review reveals that each perspective possesses salient strengths, limitations, and heuristic values. We highlight the necessity of revisiting extant theories and formulating novel evidence-based accounts of hypnosis.}
}
@article{WASKAN2003259,
title = {Intrinsic cognitive models},
journal = {Cognitive Science},
volume = {27},
number = {2},
pages = {259-283},
year = {2003},
issn = {0364-0213},
doi = {https://doi.org/10.1016/S0364-0213(02)00119-2},
url = {https://www.sciencedirect.com/science/article/pii/S0364021302001192},
author = {Jonathan A Waskan},
keywords = {Philosophy, Artificial intelligence, Psychology, Representation, Philosophy of mind, Philosophy of computation, Causal reasoning, Knowledge representation, Computer simulation},
abstract = {Theories concerning the structure, or format, of mental representation should (1) be formulated in mechanistic, rather than metaphorical terms; (2) do justice to several philosophical intuitions about mental representation; and (3) explain the human capacity to predict the consequences of worldly alterations (i.e., to think before we act). The hypothesis that thinking involves the application of syntax-sensitive inference rules to syntactically structured mental representations has been said to satisfy all three conditions. An alternative hypothesis is that thinking requires the construction and manipulation of the cognitive equivalent of scale models. A reading of this hypothesis is provided that satisfies condition (1) and which, even though it may not fully satisfy condition (2), turns out (in light of the frame problem) to be the only known way to satisfy condition (3).}
}
@article{CROSSLEY2024100865,
title = {A large-scale corpus for assessing written argumentation: PERSUADE 2.0},
journal = {Assessing Writing},
volume = {61},
pages = {100865},
year = {2024},
issn = {1075-2935},
doi = {https://doi.org/10.1016/j.asw.2024.100865},
url = {https://www.sciencedirect.com/science/article/pii/S1075293524000588},
author = {S.A. Crossley and Y. Tian and P. Baffour and A. Franklin and M. Benner and U. Boser},
keywords = {Corpus linguistics, Writing assessment, Argumentation, Individual differences},
abstract = {This research methods article introduces the open source PERSUADE 2.0 corpus. The PERSUADE 2.0 corpus comprises over 25,000 argumentative essays produced by 6th-12th grade students in the United States for 15 prompts on two writing tasks: independent and source-based writing. The PERSUADE 2.0 corpus also provides detailed individual and demographic information for each writer. The goal of the PERSUADE 2.0 corpus is to advance research into relationships between discourse elements, their effectiveness, writing quality, writing tasks and prompts, and demographic and individual differences.}
}
@article{ZHAI2023101373,
title = {Can reflective interventions improve students’ academic achievement? A meta-analysis},
journal = {Thinking Skills and Creativity},
volume = {49},
pages = {101373},
year = {2023},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2023.101373},
url = {https://www.sciencedirect.com/science/article/pii/S1871187123001414},
author = {Na Zhai and Yong Huang and Xiaomei Ma and Jingchun Chen},
keywords = {Reflection, Reflective intervention, Academic achievement, Meta-analysis},
abstract = {Reflection is widely acknowledged as a crucial skill for successful learning and decision-making. Recent evidence has shown that reflection can enhance motivation for in-depth learning, improve cognitive and metacognitive strategies, and promote self-regulated learning. While some studies have reported the positive effects of reflective interventions on student academic outcomes, conflicting findings exist. To provide a comprehensive understanding of the effectiveness of reflective interventions on academic achievement, this meta-analysis synthesized data from 25 quantitative studies (comprising 29 effect sizes) conducted between 2012 and 2022, with a total of 2,111 participants. The results revealed a significant overall effect of reflective interventions on academic achievement (g = 0.793, p < 0.001). Further moderator analyses indicated that the effectiveness of reflective interventions was influenced by factors such as learning mode, intervention duration, the role of reflective writing, and culture. However, education level, discipline, teacher or expert feedback, peer interaction, and technological scaffolding did not significantly affect the impact of reflective interventions across studies. These findings highlight the importance of fostering reflective thinking and refining the detailed design of reflective interventions to enhance students’ academic achievement.}
}
@article{LOMBARDI2024e00322,
title = {Semantic modelling and HBIM: A new multidisciplinary workflow for archaeological heritage},
journal = {Digital Applications in Archaeology and Cultural Heritage},
volume = {32},
pages = {e00322},
year = {2024},
issn = {2212-0548},
doi = {https://doi.org/10.1016/j.daach.2024.e00322},
url = {https://www.sciencedirect.com/science/article/pii/S2212054824000079},
author = {Matteo Lombardi and Dario Rizzi},
keywords = {Digital archaeology, Semantic modelling, HBIM, Blender, BlenderBIM, Extended matrix},
abstract = {The aim of the study is to describe a methodological approach to represent, interpret, model and manage pluristratified archaeological contexts. The proposed methodology envisages a digital workflow, a BIM-thinking strategy, which integrates geometric and texture data obtained from laser and photogrammetric scans with information about construction techniques and materials, archaeological reports and documentation. The integration is based on a balanced combination of open-source and proprietary solutions, allowing professionals to work with their “comfort software” and assuring interoperability through the adoption of Open Standards. Experimentations are being conducted exploring the potential of connecting semantic 3D modelling and virtual reconstructions based on archaeological data made with Blender and the Extended Matrix Tool, with BIM software and capabilities thanks to the BlenderBIM addon. The proposed workflow, in combination with the described data-sharing-oriented process, adopts a new approach towards 3D models in order to promote a more sustainable mindset towards 3D dataset life-cycle by optimizing their usage and reducing waste on different levels, such as re-documenting the same structure twice. The expected overall result is the ability to generate semantic models that can enhance our understanding of the context as much as foster multidisciplinary BIM (Building Information Modelling) collaboration thus improving archaeological research, documentation and conservation practices.}
}
@article{DELEON2003507,
title = {On the computation of the Lichnerowicz–Jacobi cohomology},
journal = {Journal of Geometry and Physics},
volume = {44},
number = {4},
pages = {507-522},
year = {2003},
issn = {0393-0440},
doi = {https://doi.org/10.1016/S0393-0440(02)00056-6},
url = {https://www.sciencedirect.com/science/article/pii/S0393044002000566},
author = {Manuel {de León} and Belén López and Juan C. Marrero and Edith Padrón},
keywords = {Jacobi manifolds, Poisson manifolds, Lie algebroids, Lichnerowicz–Jacobi cohomology, Contact manifolds, Locally conformal symplectic manifolds},
abstract = {Lichnerowicz–Jacobi cohomology of Jacobi manifolds is reviewed. The use of the associated Lie algebroid allows to prove that the Lichnerowicz–Jacobi cohomology is invariant under conformal changes of the Jacobi structure. We also compute the Lichnerowicz–Jacobi cohomology for a large variety of examples.}
}
@article{VANSANTEN19902001,
title = {Computational advances in catalyst modelling.},
journal = {Chemical Engineering Science},
volume = {45},
number = {8},
pages = {2001-2011},
year = {1990},
issn = {0009-2509},
doi = {https://doi.org/10.1016/0009-2509(90)80073-N},
url = {https://www.sciencedirect.com/science/article/pii/000925099080073N},
author = {R.A. {van Santen}},
keywords = {Molecular Catalysis, Theoretical Chemistry, Catalyst Modelling, Zeolite Stability, Theoretical Kinitics.},
abstract = {Fruitful theoretical approaches to predict catalyst stability, to simulate transition states or assist catalyst characterization become available due to the computational possibilities generated by supercomputers. Advances in theoretical chemistry and catalysis provide the conceptual framework that enables application in catalyst modelling. Especially in zeolite catalysis computational techniques are increasingly applied. Because of their well-defined structures they are very suitable for the application of graphics approaches. Techniques have been developed to determine interaction-potentials on the basis of quantumchemical cluster-calculations and to verify them by comparison with experimental and spectroscopic data. Stimulated by quantum chemical studies in chemisorption as well as organometallic chemistry, computational studies of reaction intermediates in homogeneous as well as heterogeneous catalytic reactions have been undertaken. The development of potential energy surface parametrization schemes is of importance to enable the application of molecular dynamics studies to catalyst stability and reactivity}
}
@article{AKBAR2025111281,
title = {Unlocking the potential of EEG in Alzheimer's disease research: Current status and pathways to precision detection},
journal = {Brain Research Bulletin},
volume = {223},
pages = {111281},
year = {2025},
issn = {0361-9230},
doi = {https://doi.org/10.1016/j.brainresbull.2025.111281},
url = {https://www.sciencedirect.com/science/article/pii/S0361923025000930},
author = {Frnaz Akbar and Imran Taj and Syed Muhammad Usman and Ali Shariq Imran and Shehzad Khalid and Imran Ihsan and Ammara Ali and Amanullah Yasin},
keywords = {Electroencephalogram, Alzheimer’s disease, EEG, Mild cognitive impairment, Frontal temporal dementia, Neuro-degenerative},
abstract = {Alzheimer’s disease (AD) affects millions of individuals worldwide and is considered a serious global health issue due to its gradual neuro-degenerative effects on cognitive abilities such as memory, thinking, and behavior. There is no cure for this disease but early detection along with a supportive care plan may aid in improving the quality of life for patients. Automated detection of AD is challenging because its symptoms vary in patients due to genetic, environmental, or other co-existing health conditions. In recent years, multiple researchers have proposed automated detection methods for AD using MRI and fMRI. These approaches are expensive, have poor temporal resolution, do not offer real-time insights, and have not proven to be very accurate. In contrast, only a limited number of studies have explored the potential of Electroencephalogram (EEG) signals for AD detection. In contrast, Electroencephalogram (EEG) signals present a cost-effective, non-invasive, and high-temporal-resolution alternative for AD detection. Despite their potential, the application of EEG signals in AD research remains under-explored. This study reviews publicly available EEG datasets, the variety of machine learning models developed for automated AD detection, and the performance metrics achieved by these methods. It provides a critical analysis of existing approaches, highlights challenges, and identifies key areas requiring further investigation. Key findings include a detailed evaluation of current methodologies, prevailing trends, and potential gaps in the field. What sets this work apart is its in-depth analysis of EEG signals for Alzheimer’s Disease detection, providing a stronger and more reliable foundation for understanding the potential role of EEG in this area.}
}
@article{THIBODEAU2017852,
title = {How Linguistic Metaphor Scaffolds Reasoning},
journal = {Trends in Cognitive Sciences},
volume = {21},
number = {11},
pages = {852-863},
year = {2017},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2017.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S1364661317301535},
author = {Paul H. Thibodeau and Rose K. Hendricks and Lera Boroditsky},
keywords = {analogy, decision making, framing, language and thought, metaphor, reasoning},
abstract = {Language helps people communicate and think. Precise and accurate language would seem best suited to achieve these goals. But a close look at the way people actually talk reveals an abundance of apparent imprecision in the form of metaphor: ideas are ‘light bulbs’, crime is a ‘virus’, and cancer is an ‘enemy’ in a ‘war’. In this article, we review recent evidence that metaphoric language can facilitate communication and shape thinking even though it is literally false. We first discuss recent experiments showing that linguistic metaphor can guide thought and behavior. Then we explore the conditions under which metaphors are most influential. Throughout, we highlight theoretical and practical implications, as well as key challenges and opportunities for future research.}
}
@article{RASMUSSEN2007195,
title = {Reinventing solutions to systems of linear differential equations: A case of emergent models involving analytic expressions},
journal = {The Journal of Mathematical Behavior},
volume = {26},
number = {3},
pages = {195-210},
year = {2007},
note = {An Inquiry Oriented Approach to Differential Equations},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2007.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S0732312307000338},
author = {Chris Rasmussen and Howard Blumenfeld},
keywords = {Modeling, Undergraduate mathematics, Realistic mathematics education, Student thinking, Proportional reasoning},
abstract = {An enduring challenge in mathematics education is to create learning environments in which students generate, refine, and extend their intuitive and informal ways of reasoning to more sophisticated and formal ways of reasoning. Pressing concerns for research, therefore, are to detail students’ progressively sophisticated ways of reasoning and instructional design heuristics that can facilitate this process. In this article we analyze the case of student reasoning with analytic expressions as they reinvent solutions to systems of two differential equations. The significance of this work is twofold: it includes an elaboration of the Realistic Mathematics Education instructional design heuristic of emergent models to the undergraduate setting in which symbolic expressions play a prominent role, and it offers teachers insight into student thinking by highlighting qualitatively different ways that students reason proportionally in relation to this instructional design heuristic.}
}
@article{LIU2023109530,
title = {Quantum computing for power systems: Tutorial, review, challenges, and prospects},
journal = {Electric Power Systems Research},
volume = {223},
pages = {109530},
year = {2023},
issn = {0378-7796},
doi = {https://doi.org/10.1016/j.epsr.2023.109530},
url = {https://www.sciencedirect.com/science/article/pii/S0378779623004194},
author = {Hualong Liu and Wenyuan Tang},
keywords = {Quantum computing, Optimization, Power systems, Renewable energy, Climate neutrality},
abstract = {As a large number of renewable energy resources are connected to power systems, the operation, planning, and optimization of power systems have been becoming more and more complex. Power flow calculation, unit commitment, economic dispatch, energy pricing, and power system planning are essentially computation problems. A lot of computing resources are required for these problems, which are non-trivial, especially for large-scale power systems with the high penetration of renewable energy. Traditionally, the calculation and optimization of power systems are completed by classical computers based on the classical computing theory and the von Neumann architecture. However, with Moore’s law getting closer and closer to the limit, the importance of quantum computing has become increasingly prominent. Quantum computing has been applied to some fields to a certain extent, yet the applications of quantum computing in power systems are rare. As the power industry is the foundation of the national economy, introducing quantum computing into the power system has far-reaching and crucial significance, such as improving the penetration of renewable energy, enhancing the computing efficiency, and helping in achieving the goal of net zero and climate neutrality by 2050. This paper first introduces the core concepts, essential ideas and theories of quantum computing, and then reviews the existing literature on the applications of quantum computing in power systems, and puts forward our critical thinking about the applications of quantum computing in power systems. In brief, this paper is dedicated to a tutorial on quantum computing targeting power system professionals and a review of its applications in power systems. The main contributions of this paper are: (1) introduce quantum computing into the field of power engineering in a thoroughly detailed way and delineate the analysis methodologies of quantum circuits systematically without losing mathematical rigor; (2) based on Dirac’s notation, the related formulae are derived meticulously with sophisticated schematic diagrams; (3) elaborate and derive some critical quantum algorithms in depth, which play an important role in the applications of quantum computing in power systems; (4) critically summarize and comment on the existing literature on the applications of quantum computing in power systems; (5) the future applications and challenges of quantum computing in power systems are prospected and remarked.}
}
@article{ZHOU1997497,
title = {Three-dimensional computations of solution hydrodynamics during the growth of potassium dihydrogen phosphate I. Spin up and steady rotation},
journal = {Journal of Crystal Growth},
volume = {180},
number = {3},
pages = {497-509},
year = {1997},
note = {Modelling in Crystal Growth},
issn = {0022-0248},
doi = {https://doi.org/10.1016/S0022-0248(97)00251-0},
url = {https://www.sciencedirect.com/science/article/pii/S0022024897002510},
author = {Yuming Zhou and Jeffrey J. Derby},
keywords = {Solution growth, Three-dimensional modeling, Fluid flow},
abstract = {A novel, massively parallel implementation of the Galerkin finite element method is used to study three-dimensional, time-dependent flows which occur during the rapid growth of potassium dihydrogen phosphate crystals from solution in a system employed by researchers at Lawrence Livermore National Laboratory. Computations for the hydrodynamics of system spin up and steady rotation indicate the importance of time-dependent flow phenomena and emphasize the significant role played by the support and crystal geometry in forming the complicated flows in this system. Predicted flow structures correlate well with experimental observations of inclusion formation.}
}
@article{LI2023101752,
title = {The role of inhibition in overcoming arithmetic natural number bias in the Chinese context: Evidence from behavioral and ERP experiments},
journal = {Learning and Instruction},
volume = {86},
pages = {101752},
year = {2023},
issn = {0959-4752},
doi = {https://doi.org/10.1016/j.learninstruc.2023.101752},
url = {https://www.sciencedirect.com/science/article/pii/S095947522300021X},
author = {Xiaodong Li and Ping Xu and Ronghuan Jiang and Shuang Chen},
keywords = {Inhibitory control, Negative priming, Natural number bias, Arithmetic operation, Event-related potential},
abstract = {The natural number bias (NNB) in arithmetic operations refers to the application of natural number properties to reasoning about rational numbers. Previous studies found the NNB interferes with students’ problem-solving. However, few studies have examined it in the Chinese context or the underlying mechanism by which it can be overcome. Addressing these gaps, in Experiments 1a (n = 31) and 1b (n = 30), we found that Chinese students demonstrate the NNB despite linguistic differences between Chinese and western languages. Experiment 2 (n = 38) adopted a negative priming paradigm and found that inhibitory control was necessary to overcome the NNB. Experiment 3 (n = 34) employed the event-related potential technique; we observed increased P2 amplitude when students solved congruent problems, and increased N2 and decreased P3 amplitude when they solved incongruent problems. These results indicated that the NNB is rooted in intuitive thinking, and overcoming this bias relies on inhibition.}
}
@article{POWELL2016147,
title = {Deconstructing intellectual curiosity},
journal = {Personality and Individual Differences},
volume = {95},
pages = {147-151},
year = {2016},
issn = {0191-8869},
doi = {https://doi.org/10.1016/j.paid.2016.02.037},
url = {https://www.sciencedirect.com/science/article/pii/S0191886916300927},
author = {Christopher Powell and Ted Nettelbeck and Nicholas R. Burns},
keywords = {Curiosity, Intellectual curiosity, Epistemic Curiosity, Need for Cognition, Typical Intellectual Engagement, Intellect},
abstract = {Scales of Need for Cognition (NFC), Typical Intellectual Engagement (TIE), and Epistemic Curiosity (EC) measure intellectual curiosity (IC). These scales correlate strongly and have been factor-analyzed individually but not together. Here N=396 (143 males) undergraduates completed measures of NFC, TIE, and EC. Six factors, labeled Intellectual Avoidance, Deprivation, Problem Solving, Abstract Thinking, Reading, and Wide Interest, were identified. TIE is the broadest scale, measuring all factors except Deprivation; NFC measures Intellectual Avoidance and Problem Solving, plus Abstract Thinking and Deprivation to a lesser degree; and EC largely measures Deprivation. Moreover, Reading may not fit in the IC domain; higher-order factor analysis indicated that, whereas items measuring Reading loaded more strongly on their first-order factor, items measuring the other factors strongly loaded on a general factor of IC. These results are significant for understanding the contents of these scales, and for future scale development.}
}
@article{HAYES201739,
title = {Regression-based statistical mediation and moderation analysis in clinical research: Observations, recommendations, and implementation},
journal = {Behaviour Research and Therapy},
volume = {98},
pages = {39-57},
year = {2017},
note = {Best Practice Guidelines for Modern Statistical Methods in Applied Clinical Research},
issn = {0005-7967},
doi = {https://doi.org/10.1016/j.brat.2016.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S0005796716301887},
author = {Andrew F. Hayes and Nicholas J. Rockwood},
keywords = {Mediation analysis, Moderation, Interaction, Regression analysis, Mechanisms},
abstract = {There have been numerous treatments in the clinical research literature about various design, analysis, and interpretation considerations when testing hypotheses about mechanisms and contingencies of effects, popularly known as mediation and moderation analysis. In this paper we address the practice of mediation and moderation analysis using linear regression in the pages of Behaviour Research and Therapy and offer some observations and recommendations, debunk some popular myths, describe some new advances, and provide an example of mediation, moderation, and their integration as conditional process analysis using the PROCESS macro for SPSS and SAS. Our goal is to nudge clinical researchers away from historically significant but increasingly old school approaches toward modifications, revisions, and extensions that characterize more modern thinking about the analysis of the mechanisms and contingencies of effects.}
}
@article{BARILE2022467,
title = {Platform-based innovation ecosystems: Entering new markets through holographic strategies},
journal = {Industrial Marketing Management},
volume = {105},
pages = {467-477},
year = {2022},
issn = {0019-8501},
doi = {https://doi.org/10.1016/j.indmarman.2022.07.003},
url = {https://www.sciencedirect.com/science/article/pii/S0019850122001614},
author = {Sergio Barile and Cristina Simone and Francesca Iandolo and Antonio Laudando},
keywords = {Platforms, Innovation ecosystems, Platform-based innovation ecosystems, Holographic strategies, Digital algorithms, Platform envelopment},
abstract = {The platformization seems to be a demiurgic force, increasingly (re)shaping this millennium and its socio-economic, technological and physical structures, institutions, and human lives. Innovation ecosystems are experiencing this platformization, leading to the rise of platform-based innovation ecosystems. However, the industrial and managerial literature still lacks a shared definition, a consistent theoretical and strategic framework to explain how platform-based innovation ecosystems emerge and replicate from market to market. This conceptual work attempts to fill those gaps by integrating the extant literature on innovation ecosystems in two ways. First, moving from the literature on innovation ecosystems and industry platforms, using systems thinking framing, it explains the platformization of innovation ecosystems through the double lens structure-system. Second, it identifies the holographic strategy as one of the typical patterns featuring platform-based innovation ecosystem envelopment beyond extant market boundaries. These conceptualizations have insightful theoretical, managerial, and policy implications. In particular, the work discusses the ecosystem as a valid unit of analysis for understanding such an unprecedented shaped-by-platform landscape. Then, it describes the growth strategies of the platform-based innovation ecosystem supporting the platform sponsor in mastering multipoint competition. Eventually, the study pinpoints crucial issues for policymakers in regulating the impact that platformization is having on society.}
}
@article{KELTNER2021216,
title = {A taxonomy of positive emotions},
journal = {Current Opinion in Behavioral Sciences},
volume = {39},
pages = {216-221},
year = {2021},
issn = {2352-1546},
doi = {https://doi.org/10.1016/j.cobeha.2021.04.013},
url = {https://www.sciencedirect.com/science/article/pii/S2352154621000991},
author = {Dacher Keltner and Alan Cowen},
abstract = {Within social functionalist theory (SFT), emotions structure attachment relations, cooperative alliances, hierarchies, and collectives. Within this line of thinking, a rich array of positive emotions enable the formation and negotiation of these relationships. Guided by these arguments, we synthesize how top-down confirmatory studies and data-driven, computational studies converge on evidence for 11 positive emotions with distinct experience, expression, and physiology. This taxonomy includes amusement, awe, compassion, contentment, desire, love, joy, interest, pride, relief, and triumph. We conclude by considering how recent taxonomic efforts will advance emotion science in mapping the distinct forms and functions of the positive emotions.}
}
@article{MAHONY2020104668,
title = {New ideas for non-animal approaches to predict repeated-dose systemic toxicity: Report from an EPAA Blue Sky Workshop},
journal = {Regulatory Toxicology and Pharmacology},
volume = {114},
pages = {104668},
year = {2020},
issn = {0273-2300},
doi = {https://doi.org/10.1016/j.yrtph.2020.104668},
url = {https://www.sciencedirect.com/science/article/pii/S0273230020300945},
author = {Catherine Mahony and Randolph S. Ashton and Barbara Birk and Alan R. Boobis and Tom Cull and George P. Daston and Lorna Ewart and Thomas B. Knudsen and Irene Manou and Sebastian Maurer-Stroh and Luigi Margiotta-Casaluci and Boris P. Müller and Pär Nordlund and Ruth A. Roberts and Thomas Steger-Hartmann and Evita Vandenbossche and Mark R. Viant and Mathieu Vinken and Maurice Whelan and Zvonar Zvonimir and Mark T.D. Cronin},
keywords = {Repeated dose toxicity testing, Alternatives, Safety assessment, Chemical legislation, , , Read-across, },
abstract = {The European Partnership for Alternative Approaches to Animal Testing (EPAA) convened a ‘Blue Sky Workshop’ on new ideas for non-animal approaches to predict repeated-dose systemic toxicity. The aim of the Workshop was to formulate strategic ideas to improve and increase the applicability, implementation and acceptance of modern non-animal methods to determine systemic toxicity. The Workshop concluded that good progress is being made to assess repeated dose toxicity without animals taking advantage of existing knowledge in toxicology, thresholds of toxicological concern, adverse outcome pathways and read-across workflows. These approaches can be supported by New Approach Methodologies (NAMs) utilising modern molecular technologies and computational methods. Recommendations from the Workshop were based around the needs for better chemical safety assessment: how to strengthen the evidence base for decision making; to develop, standardise and harmonise NAMs for human toxicity; and the improvement in the applicability and acceptance of novel techniques. “Disruptive thinking” is required to reconsider chemical legislation, validation of NAMs and the opportunities to move away from reliance on animal tests. Case study practices and data sharing, ensuring reproducibility of NAMs, were viewed as crucial to the improvement of non-animal test approaches for systemic toxicity.}
}
@article{ZORAN2025101135,
title = {Digital gastronomy 2.0: A 15-year transformative journey in culinary-tech evolution and interaction},
journal = {International Journal of Gastronomy and Food Science},
volume = {39},
pages = {101135},
year = {2025},
issn = {1878-450X},
doi = {https://doi.org/10.1016/j.ijgfs.2025.101135},
url = {https://www.sciencedirect.com/science/article/pii/S1878450X25000368},
author = {Amit Raphael Zoran},
abstract = {This paper reviews 15 years of exploration and development in Digital Gastronomy (DG), tracing its progression from foundational frameworks to AI-integrated culinary systems. The journey begins with integrating computational tools like laser cooking, 3D printing, CNC milling, and modular molds, which expand the possibilities of creativity and precision in the kitchen. Building on these technologies, the Meta-Recipe (MR) framework introduces a structured approach to recipe design, allowing chefs to adapt dishes dynamically while maintaining culinary coherence. The concept of “Digital Alchemy” extends this foundation, blending AI-driven methods with traditional healing and sustainable practices to emphasize well-being and environmental consciousness. These advancements culminate in the vision of an AI-augmented kitchen, conceptualized as a collaborative and adaptive space that bridges culinary artistry with algorithmic precision. This research highlights DG's potential as an evolving interdisciplinary field, offering new gastronomy, creativity, and sustainability directions.}
}
@article{GARGALO2024108504,
title = {A process systems engineering view of environmental impact assessment in renewable and sustainable energy production: Status and perspectives},
journal = {Computers & Chemical Engineering},
volume = {180},
pages = {108504},
year = {2024},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2023.108504},
url = {https://www.sciencedirect.com/science/article/pii/S0098135423003745},
author = {Carina L. Gargalo and Haoshui Yu and Nikolaus Vollmer and Ahmad Arabkoohsar and Krist V. Gernaey and Gürkan Sin},
keywords = {Renewable and sustainable energy systems, Environmental impact assessment, Process systems engineering, Life cycle assessment, Sustainability},
abstract = {With the increasing concern for climate change, renewable and sustainable energy production has attracted considerable attention from the scientific community, industrial practitioners, and policy and decision-makers. There are many technological alternatives for each sub-category of complex sustainable energy systems. Life cycle assessment (LCA) can be an effective tool to compare the environmental impacts of each pathway and identify the most promising alternatives from an environmental impact perspective. This contribution first reviews the environmental assessment methods and tools developed over the years. Secondly, a comprehensive review of the contribution of the PSE community to the environmental impact analysis of renewable energy systems is performed. It is observed that while LCA is the preferred method, these studies differed widely concerning the choice of impact assessment method used, the level of details shared concerning the underlying LCA calculations, and whether or not sensitivity and uncertainty analyses were carried out, among many others. This makes the comparison of results from different studies difficult and often impossible. It is clear that the PSE community, with its emphasis on systems thinking and holistic approaches, plays a critical role in the design, integration, and operation of complex sustainable energy systems. However, the thorough calculations necessary to ensure a robust and transparent LCA analysis require a shared methodology and a detailed description of the rules. Such explicit, systematic, and transparent methods will set the bar for a minimum requirement for thorough LCA calculations, ensuring fair comparison and discussions of different technical solutions developed in the wider PSE community for sustainable renewables.}
}
@article{ABEYSEKERA2024100213,
title = {ChatGPT and academia on accounting assessments},
journal = {Journal of Open Innovation: Technology, Market, and Complexity},
volume = {10},
number = {1},
pages = {100213},
year = {2024},
issn = {2199-8531},
doi = {https://doi.org/10.1016/j.joitmc.2024.100213},
url = {https://www.sciencedirect.com/science/article/pii/S2199853124000076},
author = {Indra Abeysekera},
keywords = {Academia, Accounting, Assessments, ChatGPT, Multiple Choice Questions, Sustainable Development Goals of the United Nations},
abstract = {ChatGPT is considered a risk and an opportunity for academia. An area of threat in contemporary settings is whether it can become a student agent for assessments in academia. This study determines how ChatGPT can become a human agent for students on two financial accounting course units, multiple choice question assessments. The study provided five numerical-based and five narrative-based multiple choice questions. There were ten questions for the Introductory Financial Accounting and 10 for the Advanced Financial Accounting course units. ChatGPT received one question at a time requesting a solution. In the Introductory Financial Accounting section, ChatGPT produced incorrect answers because it incorrectly assumed the underlying assumptions contained in those questions. In Advanced Financial Accounting, ChatGPT presented incorrect answers because of the complexity of the task contained in those questions. ChatGPT demonstrated similar competencies in providing solutions to numerical-based and narrative-based questions. ChatGPT obtained the correct answers to sit in the 80th percentile in the Introductory Financial Accounting course unit assessment and the 50th percentile in the Advanced Financial course unit assessment. ChatGPT4 showed improved performance, with the 90th percentile for Introductory Financial Accounting and the 70th percentile for Advanced Financial Accounting. The findings indicate that the knowledge construct requires reflective thinking with ChatGPT in the ecosystem, and what is assumed and assessable knowledge must be revisited.}
}
@article{LI2023113687,
title = {Twins transformer: Cross-attention based two-branch transformer network for rotating bearing fault diagnosis},
journal = {Measurement},
volume = {223},
pages = {113687},
year = {2023},
issn = {0263-2241},
doi = {https://doi.org/10.1016/j.measurement.2023.113687},
url = {https://www.sciencedirect.com/science/article/pii/S0263224123012514},
author = {Jie Li and Yu Bao and WenXin Liu and PengXiang Ji and LeKang Wang and Zhongbing Wang},
keywords = {Attention mechanisms, Cross-attention, Fault diagnosis, Transformer},
abstract = {Due to the inherent shortcomings of traditional depth models, the Transformer model based on the self-attention mechanism has become popular in the field of fault diagnosis. The current Transformer's self-attentive mechanism provides an alternative way of thinking, which can make direct association between each signal. However, it can only focus on the association information within a sequence, and it is difficult to understand the information gap between samples. Therefore, this paper proposes the two-branch Twins attention, which for the first time uses cross-attention to focus on information associations between samples. Twins attention uses cross-attention to learn information associations between samples in addition to retaining the information associations within sequences learned by self-attention. The performance of the proposed model was validated on four popular bearing datasets. Compared to the original transformer structure, the average accuracy of each dataset improved by 1.73% to 99.42%, leading the noise experiments.}
}
@article{CUI2024124662,
title = {Cooperative interference to achieve interval many-objective evolutionary algorithm for association privacy secure computing migration},
journal = {Expert Systems with Applications},
volume = {255},
pages = {124662},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.124662},
url = {https://www.sciencedirect.com/science/article/pii/S095741742401529X},
author = {Zhihua Cui and Zhenyu Shi and Qi Li and Tianhao Zhao and Wensheng Zhang and Jinjun Chen},
keywords = {Mobile edge computing, Computing migration, Physical layer security(PLS), Interval many-objective optimization, Evolutionary algorithm},
abstract = {In this paper, we study secure computing migration scenarios in uncertain environments with the presence of multiple malicious eavesdroppers (MEs). Specifically, when edge servers (ESs) execute tasks delivered by smart devices (SDs), SDs may move beyond the coverage of ESs, and computing migration (CM) of unfinished tasks is required to ensure service continuity. There is a risk of privacy leakage during task migration, and MEs use colluding eavesdropping to eavesdrop on the migrated tasks, and we consider eavesdropping on the associated tasks through data sharing among MEs to improve the eavesdropping efficiency. For eavesdropping in MEs, we achieve eavesdropping strikes using cooperative interference by jammers, which benefit by providing jamming services. In addition, uncertain computational scenarios directly affect the efficiency of task execution, and we consider the uncertainty factor in the malicious eavesdropping environment. To this end, this paper proposes the secure computational migration of associative privacy in uncertain environments (SCMAPUE) model, which transforms uncertainties into interval parameters, and optimizes the five objectives of migration delay, maximum completion time, energy consumption, load balancing and migration reliability to achieve efficient task execution and reliable migration. Aiming at the model characteristics, this paper designs an interval many-objective evolutionary algorithm for reliable migration (IMaOEA-RM), which employs a condition-based interval confidence strategy and a multi-access secure migration selection strategy to improve the convergence of the algorithm, and utilizes a dual-migration crossover strategy in order to adjust the jammer partners and improve the population diversity. Simulation results show that our proposed IMaOEA-RM algorithm can provide a more reliable and efficient migration scheme than existing algorithms.}
}
@article{SELVAKKUMARAN2020111053,
title = {Review of the use of system dynamics (SD) in scrutinizing local energy transitions},
journal = {Journal of Environmental Management},
volume = {272},
pages = {111053},
year = {2020},
issn = {0301-4797},
doi = {https://doi.org/10.1016/j.jenvman.2020.111053},
url = {https://www.sciencedirect.com/science/article/pii/S0301479720309816},
author = {Sujeetha Selvakkumaran and Erik O. Ahlgren},
keywords = {System dynamics, Modelling, Local, Energy transitions, Multi-level perspective},
abstract = {Local energy transition processes are complex socio-technical transitions requiring careful study. The use of System Dynamics (SD) in modelling and analyzing local energy transitions is especially suitable given the characteristics of SD. Our aim is to systematically categorize the different ways SD is used and useful to scrutinize local energy transitions, and to see if we can discern any common themes that can be useful to researchers looking to scrutinize local energy transitions, using SD. The study is exploratory in nature, with peer-reviewed journal and conference articles analyzed using content analysis. The six categories on which the articles are analyzed are: the sector the article studies; the transition that is studied in the article; the modelling depth in the article; the objective of the article; the justification for using SD provided in the article and the levels of interaction with ‘local’. Our findings show most of the local energy transitions have been studied using simulatable Stock and Flow Diagrams in SD methodology. The important sectors in the energy field are represented in terms of SD modelling of local energy transitions, including electricity, transport, district heating etc. Most of the local energy transitions scrutinized by SD in the articles have descriptive objectives, with some prescriptive, and just one evaluative objective. In terms of justification for using SD provided by the articles analyzed in this study, we found four major themes along which the justifications that were provided. They are dynamics, feedbacks, delays and complexity, systematic thinking, bridging disciplines and actor interactions and behaviour. The ‘dynamics, feedbacks, delays and complexity’ theme is the most cited justification for the use of SD in scrutinizing local energy transitions, followed by systematic thinking.}
}
@incollection{SALIMI201883,
title = {Chapter 2 - Fundamentals of Systemic Approach},
editor = {Fabienne Salimi and Frederic Salimi},
booktitle = {A Systems Approach to Managing the Complexities of Process Industries},
publisher = {Elsevier},
pages = {83-180},
year = {2018},
isbn = {978-0-12-804213-7},
doi = {https://doi.org/10.1016/B978-0-12-804213-7.00002-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128042137000025},
author = {Fabienne Salimi and Frederic Salimi},
keywords = {Systems engineering, systems thinking, critical thinking, Safety Critical Element (SCE), Project Management, Complexity, Emergence, SE Competency, Type of Systems, IIoT, Big Data},
abstract = {System thinking, system engineering, and complexity management are the back bone of any operational excellence and process safety management system. This chapter aims to give a solid but concise background for the fundamentals of system engineering, system thinking, and complexity management for process industry. Different type of processes, requirement engineering and management, safety critical systems, critical thinking, and SE competency framework are discussed. It also addresses issues that pertain to human judgment and how people employ rules of thumb and heuristics to problem-solving situations. Various modes of engineering are discussed along with the complexities and concerns within each: cognitive systems engineering, control engineering, software engineering, industrial engineering, performance engineering, and several others. A distinction is also made between technical performance measures and key performance parameters. A list of leading indicators, insights, and requirements are then delineated among the various aspects of system engineering. Finally, an overall analysis of systems thinking, which concerns the process of understanding how various systems are implemented, is provided.}
}
@article{GANAPATHY20158064,
title = {Optimum steepest descent higher level learning radial basis function network},
journal = {Expert Systems with Applications},
volume = {42},
number = {21},
pages = {8064-8077},
year = {2015},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2015.06.036},
url = {https://www.sciencedirect.com/science/article/pii/S0957417415004388},
author = {Kirupa Ganapathy and V. Vaidehi and Jesintha B. Chandrasekar},
keywords = {Neural network, Radial basis function, Dynamic learning, Optimum steepest descent, Higher level components, Healthcare},
abstract = {Dynamically changing real world applications, demands for rapid and accurate machine learning algorithm. In neural network based machine learning algorithms, radial basis function (RBF) network is a simple supervised learning feed forward network. With its simplicity, this network is highly suitable to model and control the nonlinear systems. Existing RBF networks in literature are applied to static applications and also faces challenges such as increased model size, neuron removal, improper center selection etc leading to erroneous output. To overcome the challenges and handle complex real world problems, this paper proposes a new optimum steepest descent based higher level learning radial basis function network (OSDHL-RBFN). The proposed OSDHL-RBFN implements major components inspired from the human brain for efficient learning, adaptive structure and accurate classification. Higher level learning and thinking components of the proposed network are sample deletion, neuron addition, neuron migration, sample navigation and neuroplasticity. These components helps the classifier to think before learning the samples and regulates the learning strategy. The knowledge gained from the trained samples are used by the network to identify the incomplete sample, optimal center and bond strength of hidden & output neurons. Adaptive network structure is employed to minimize classification error. The proposed work also uses optimum steepest descent method for weight parameter update to minimize the sum square error. OSDHL-RBFN is tested and evaluated in both static and dynamic environments on nine benchmark classification (binary and multiclass) problems for balanced, unbalanced, small, large, low dimensional and high dimensional datasets. The overall and class wise efficiency of OSDHL-RBFN is improved when compared to other RBFN’s in the literature. The performance results clearly show that the proposed OSDHL-RBFN reduces the architecture complexity and computation time compared to other RBFN’s. Overall, the proposed OSDHL-RBFN is efficient and suitable for dynamic real world applications in terms of detection time and accuracy. As a case study, OSDHL-RBFN is implemented in real time remote health monitoring application for classifying the various abnormality levels in vital parameters.}
}
@incollection{TILLAS2017101,
title = {Chapter 7 - On the Redundancies of “Social Agency”},
editor = {Jon Leefmann and Elisabeth Hildt},
booktitle = {The Human Sciences after the Decade of the Brain},
publisher = {Academic Press},
address = {San Diego},
pages = {101-120},
year = {2017},
isbn = {978-0-12-804205-2},
doi = {https://doi.org/10.1016/B978-0-12-804205-2.00007-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128042052000070},
author = {A. Tillas},
keywords = {Structure, agency, concepts, intuitions, decision-making, actions},
abstract = {This chapter presents a philosophical argument about the “structure vs agency” debate—one of the central debates in social sciences. I do not argue for the primacy of either of the two but suggest an empirically vindicated view about the nature of thinking, in light of which the traditional debate as well as the notion of “social agency,” is redundant. I argue that thinking is contingent on the weightings of the synaptic connections between neuronal groups grounding it. In turn, socialization is a process of adjusting or conditioning the appropriate synaptic connection weightings. Both conscious (reasoning) and unconscious (intuitions) determinants of sociologically nontrivial actions derive from perceptual encounters with our sociophysical environment. In turn, agents—as social scientists use the term—simply do not exist. Finally, I appeal to neuroscientific evidence and show that we still qualify as agents, if only with regards to sociologically trivial actions.}
}
@article{JONES2001325,
title = {NMR quantum computation},
journal = {Progress in Nuclear Magnetic Resonance Spectroscopy},
volume = {38},
number = {4},
pages = {325-360},
year = {2001},
issn = {0079-6565},
doi = {https://doi.org/10.1016/S0079-6565(00)00033-9},
url = {https://www.sciencedirect.com/science/article/pii/S0079656500000339},
author = {J.A. Jones}
}
@article{SAWLEY1994363,
title = {A comparative study of the use of the data-parallel approach for compressible flow calculations},
journal = {Parallel Computing},
volume = {20},
number = {3},
pages = {363-373},
year = {1994},
issn = {0167-8191},
doi = {https://doi.org/10.1016/S0167-8191(06)80019-0},
url = {https://www.sciencedirect.com/science/article/pii/S0167819106800190},
author = {M.L. Sawley and C.M. Bergman},
keywords = {Computational fluid dynamics, Euler equations, Data-parallel programming, Portability, Performance results},
abstract = {The results are presented of an investigation into the use of the data-parallel programming approach on four different massively-parallel computers: the MasPar MP-1 and MP-2 and the Thinking Machines CM-200 and CM-5. A code to calculate inviscid compressible flow, originally written in FORTRAN 77 for a traditional vector computer, has been re-written entirely in Fortran 90 to take advantage of the compilers available on the massively-parallel computers. It is shown that the discretization of the governing equations on a regular mesh is well adapted to data parallelism. For a typical test problem of supersonic flow through a ramped duct, computational speeds have been achieved using these massively-parallel computers that are superior to those obtained using a single processor of a Cray Y-MP. In addition, this study has enabled the question of code portability between the different computers to be assessed.}
}
@article{SIMON1993431,
title = {Experience in using SIMD and MIMD parallelism for computational fluid dynamics},
journal = {Applied Numerical Mathematics},
volume = {12},
number = {5},
pages = {431-442},
year = {1993},
issn = {0168-9274},
doi = {https://doi.org/10.1016/0168-9274(93)90103-X},
url = {https://www.sciencedirect.com/science/article/pii/016892749390103X},
author = {Horst D. Simon and Leonardo Dagum},
keywords = {Parallel architectures, MIMD, SIMD, computational fluid dynamics.},
abstract = {One of the key objectives of the Applied Research Branch in the Numerical Aerodynamic Simulation (NAS) Systems Division at NASA Ames Research Center is the accelerated introduction of highly parallel machines into a fully operational environment. In this report we summarize some of the experiences with the parallel testbed machines at the NAS Applied Research Branch. We discuss the performance results obtained from the implementation of two computational fluid dynamics (CFD) applications, an unstructured grid solver and a particle simulation, on the Connection Machine CM-2 and the Intel iPSC/860.}
}
@article{RANGEL2012970,
title = {Value normalization in decision making: theory and evidence},
journal = {Current Opinion in Neurobiology},
volume = {22},
number = {6},
pages = {970-981},
year = {2012},
note = {Decision making},
issn = {0959-4388},
doi = {https://doi.org/10.1016/j.conb.2012.07.011},
url = {https://www.sciencedirect.com/science/article/pii/S0959438812001201},
author = {Antonio Rangel and John A Clithero},
abstract = {A sizable body of evidence has shown that the brain computes several types of value-related signals to guide decision making, such as stimulus values, outcome values, and prediction errors. A critical question for understanding decision-making mechanisms is whether these value signals are computed using an absolute or a normalized code. Under an absolute code, the neural response used to represent the value of a given stimulus does not depend on what other values might have been encountered. By contrast, under a normalized code, the neural response associated with a given value depends on its relative position in the distribution of values. This review provides a simple framework for thinking about value normalization, and uses it to evaluate the existing experimental evidence.}
}
@article{TAKANO201922,
title = {Difficulty in updating positive beliefs about negative cognition is associated with increased depressed mood},
journal = {Journal of Behavior Therapy and Experimental Psychiatry},
volume = {64},
pages = {22-30},
year = {2019},
issn = {0005-7916},
doi = {https://doi.org/10.1016/j.jbtep.2019.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S0005791617302926},
author = {Keisuke Takano and Julie {Van Grieken} and Filip Raes},
keywords = {depression, Rumination, Memory, Reinforcement learning, Q-learning},
abstract = {Background and objectives
Depressed people hold positive beliefs about negative cognition (e.g., rumination is useful to find a solution), which may motivate those individuals to engage in sustained negative thinking. However, in reality, rumination often leads to unfavorable outcomes. Thus, such beliefs create a large discrepancy between one's expectations and the actual outcome. Therefore, we hypothesized that this prediction error would be associated with increased depressed mood.
Methods
We observed how people update their positive beliefs about negative cognition within a volatile environment, in which negative cognition does not always result in a beneficial outcome. Forty-six participants were offered two response options (retrieving a negative or positive personal memory) and subsequently provided either an economic reward or punishment. Retrieving a negative (rather than positive) memory was initially reinforced, although this action-outcome contingency was reversed during the task. In the control condition, positive memory retrieval was initially reinforced, although a contingency reversal was employed to encourage negative memory retrieval.
Results
Model-based computational modeling revealed that participants who showed a delay in switching from negative to positive (but not from positive to negative) responses experienced increased levels of depressed mood. This delay in switching was also found to be associated with depressive symptoms and trait rumination.
Limitations
The non-clinical nature of the sample may limit the clinical implications of the results.
Conclusions
Difficulty in updating positive beliefs (or outcome predictions) for negative cognition may play an important role in depressive symptomatology.}
}
@article{COOPER2022100755,
title = {Balboa security v. M&M systems: Forensic accounting for determining commercial damages},
journal = {Journal of Accounting Education},
volume = {58},
pages = {100755},
year = {2022},
issn = {0748-5751},
doi = {https://doi.org/10.1016/j.jaccedu.2021.100755},
url = {https://www.sciencedirect.com/science/article/pii/S0748575121000427},
author = {John R. Cooper and Brett S. Kawada},
keywords = {Forensic accounting, Commercial damages, Litigation, Supplier-customer relationship},
abstract = {The ability of accounting students to apply skills beyond traditional accounting in a thoughtful and analytical way is becoming increasingly important, especially in fraud detection and forensic accounting. This case provides an opportunity for students to use critical thinking and problem-solving skills in applying accounting knowledge to a supplier-customer commercial damages litigation matter. Students are provided with a fact pattern of a supplier-customer relationship where they analyze issues related to commercial damages stemming from sources common in real world forensic accounting cases. Students evaluate the facts, which include not only financial data but also interviews with key personnel of parties to the legal action, and demonstrate an understanding of the issues involved in the case through responses of questions regarding overriding forensic accounting and professional practice issues. Students will also prepare a written commercial damages report demonstrating the ability to effectively communicate their analyses.}
}
@article{ZHURAVLEV2023104934,
title = {Three levels of information processing in the brain},
journal = {Biosystems},
volume = {229},
pages = {104934},
year = {2023},
issn = {0303-2647},
doi = {https://doi.org/10.1016/j.biosystems.2023.104934},
url = {https://www.sciencedirect.com/science/article/pii/S0303264723001090},
author = {Aleksandr V. Zhuravlev},
keywords = {Brain, Information, Consciousness, The hard problem of consciousness, Qualia, Entropy},
abstract = {Information, the measure of order in a complex system, is the opposite of entropy, the measure of chaos and disorder. We can distinguish several levels at which information is processed in the brain. The first one is the level of serial molecular genetic processes, similar in some aspects to digital computations (DC). At the same time, higher cognitive activity is probably based on parallel neural network computations (NNC). The advantage of neural networks is their intrinsic ability to learn, adapting their parameters to specific tasks and to external data. However, there seems to be a third level of information processing as well, which involves subjective consciousness and its units, so called qualia. They are difficult to study experimentally, and the very fact of their existence is hard to explain within the framework of modern physical theory. Here I propose a way to consider consciousness as the extension of basic physical laws – namely, total entropy dissipation leading to a system simplification. At the level of subjective consciousness, the brain seems to convert information embodied by neural activity to a more simple and compact form, internally observed as qualia. Whereas physical implementations of both DC and NNC are essentially approximate and probabilistic, qualia-associated computations (QAC) make the brain capable of recognizing general laws and relationships. While elaborating a behavioral program, the conscious brain does not act blindly or gropingly but according to the very meaning of such general laws, which gives it an advantage compared to any artificial intelligence system.}
}
@article{LANDINO2025,
title = {Neighbor cells restrain furrowing during Xenopus epithelial cytokinesis},
journal = {Developmental Cell},
year = {2025},
issn = {1534-5807},
doi = {https://doi.org/10.1016/j.devcel.2025.03.010},
url = {https://www.sciencedirect.com/science/article/pii/S1534580725001571},
author = {Jennifer Landino and Eileen Misterovich and Lotte {van den Goor} and Babli Adhikary and Shahana Chumki and Lance A. Davidson and Ann L. Miller},
keywords = {cytokinesis, epithelium, Rho GTPase, actin, myosin II, ɑ-actinin, vinculin, , optogenetics, computational modeling},
abstract = {Summary
Cytokinesis challenges epithelial tissue homeostasis by generating forces that pull on neighboring cells. Junction reinforcement at the furrow in Xenopus epithelia regulates the speed of furrowing, suggesting that cytokinesis is subject to resistive forces from epithelial neighbors. We show that contractility factors accumulate near the furrow in neighboring cells, and increasing neighbor cell stiffness slows furrowing. Optogenetically increasing contractility in one or both neighbor cells slows furrowing or induces cytokinetic failure. Uncoupling mechanotransduction between dividing cells and their neighbors increases the furrow ingression rate, alters topological cell packing following cytokinesis, and impairs barrier function at the furrow. Computational modeling validates our findings and provides additional insights about epithelial mechanics during cytokinesis. We conclude that forces from the cytokinetic array must be carefully balanced with restraining forces generated by neighbor cells to regulate the speed and success of cytokinesis and maintain epithelial homeostasis.}
}
@article{XIANG2025102682,
title = {JCANet: Multi-domain federated lightweight self-attention CSI feedback network},
journal = {Physical Communication},
volume = {71},
pages = {102682},
year = {2025},
issn = {1874-4907},
doi = {https://doi.org/10.1016/j.phycom.2025.102682},
url = {https://www.sciencedirect.com/science/article/pii/S1874490725000850},
author = {Jianhong Xiang and Zilu Li and Wei Liu},
keywords = {CSI feedback, Deep learning, FDD, Massive MIMO, Self-attention mechanism},
abstract = {In frequency division duplex (FDD) massive MIMO systems, as the number of antennas increases, the amount of downlink channel state information (CSI) data fed back from the user’s end increases significantly, many deep learning (DL)-based CSI compression feedback methods show their potential. Existing networks mostly extract channel features in the angle-delay domain through complex convolutional structures, neglecting the frequency correlation among subcarriers, which makes it difficult to fully capture global features with long-distance dependencies. Moreover, these approaches suffer from high complexity. To address these issues, we propose a multi-domain joint lightweight self-attention feedback network (JCANet). First, a multi-domain joint strategy is proposed at the encoder side. On the basis of designing angular-delay domain convolution to extract local features of channel information, a frequency domain convolution (FCv) branch is used to span multiple subcarriers to capture the global features of the channel, achieving multi-domain extraction of channel information features. Then, a lightweight multi-scale cross-layer self-attention (LMSCA) module is proposed on the decoder side, which utilizes the multi-scale information of the CSI matrix to establish correlations and long-range dependencies between input sequences under low complexity. Simulation results show that JCANet achieves higher performance with lower computational complexity compared to other lightweight networks.}
}
@article{BARROUILLET2011151,
title = {Dual-process theories of reasoning: The test of development},
journal = {Developmental Review},
volume = {31},
number = {2},
pages = {151-179},
year = {2011},
note = {Special Issue: Dual-Process Theories of Cognitive Development},
issn = {0273-2297},
doi = {https://doi.org/10.1016/j.dr.2011.07.006},
url = {https://www.sciencedirect.com/science/article/pii/S0273229711000177},
author = {Pierre Barrouillet},
keywords = {Dual-process theories, Cognitive development, Conditional reasoning},
abstract = {Dual-process theories have become increasingly influential in the psychology of reasoning. Though the distinction they introduced between intuitive and reflective thinking should have strong developmental implications, the developmental approach has rarely been used to refine or test these theories. In this article, I review several contemporary dual-process accounts of conditional reasoning that theorize the distinction between the two systems of reasoning as a contrast between heuristic and analytic processes, probabilistic and mental model reasoning, or emphasize the role of metacognitive processes in reflective reasoning. These theories are evaluated in the light of the main developmental findings. It is argued that a proper account of developmental phenomena requires the integration of the main strengths of these three approaches. I propose such an integrative theory of conditional understanding and argue that the modern dual-process framework could benefit from earlier contributions that made the same distinction between intuition and reflective thinking, such as Piaget’s theory.}
}
@article{INDLEKOFER20021035,
title = {Number theory—probabilistic, heuristic, and computational approaches},
journal = {Computers & Mathematics with Applications},
volume = {43},
number = {8},
pages = {1035-1061},
year = {2002},
issn = {0898-1221},
doi = {https://doi.org/10.1016/S0898-1221(02)80012-8},
url = {https://www.sciencedirect.com/science/article/pii/S0898122102800128},
author = {K.-H Indlekofer},
keywords = {Probabilistic number theory, Asymptotic results on arithmetic function, Computational number theory, Stone-Cech compactification, Measure and integration on },
abstract = {After the description of the models of Kubilius, Novoselov and Schwarz, and Spilker, respectively, a probability theory for finitely additive probability measures is developed by use of the Stone-Cech compactification of N. The new model is applied to the result of Erdős and Wintner about the limit distribution of additive functions and to the famous result of Szemerédi in combinatorial number theory. Further, it is explained how conjectures on prime values of irreducible polynomials are used in the search for large prime twins and Sophie Germain primes.}
}
@article{ESCOLAGASCON2023111893,
title = {Who falls for fake news? Psychological and clinical profiling evidence of fake news consumers},
journal = {Personality and Individual Differences},
volume = {200},
pages = {111893},
year = {2023},
issn = {0191-8869},
doi = {https://doi.org/10.1016/j.paid.2022.111893},
url = {https://www.sciencedirect.com/science/article/pii/S0191886922003981},
author = {Álex Escolà-Gascón and Neil Dagnall and Andrew Denovan and Kenneth Drinkwater and Miriam Diez-Bosch},
keywords = {Fake news, Pseudoscientific information, Cognitive biases, Individual differences, Clinical prevention},
abstract = {Awareness of the potential psychological significance of false news increased during the coronavirus pandemic, however, its impact on psychopathology and individual differences remains unclear. Acknowledging this, the authors investigated the psychological and psychopathological profiles that characterize fake news consumption. A total of 1452 volunteers from the general population with no previous psychiatric history participated. They responded to clinical psychopathology assessment tests. Respondents solved a fake news screening test, which allowed them to be allocated to a quasi-experimental condition: group 1 (non-fake news consumers) or group 2 (fake news consumers). Mean comparison, Bayesian inference, and multiple regression analyses were applied. Participants with a schizotypal, paranoid, and histrionic personality were ineffective at detecting fake news. They were also more vulnerable to suffer its negative effects. Specifically, they displayed higher levels of anxiety and committed more cognitive biases based on suggestibility and the Barnum Effect. No significant effects on psychotic symptomatology or affective mood states were observed. Corresponding to these outcomes, two clinical and therapeutic recommendations related to the reduction of the Barnum Effect and the reinterpretation of digital media sensationalism were made. The impact of fake news and possible ways of prevention are discussed.}
}
@article{METTLER2024101932,
title = {Same same but different: How policies frame societal-level digital transformation},
journal = {Government Information Quarterly},
volume = {41},
number = {2},
pages = {101932},
year = {2024},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2024.101932},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X24000248},
author = {Tobias Mettler and Gianluca Miscione and Claus D. Jacobs and Ali A. Guenduez},
keywords = {Digital transformation, IS policy research, Computational content analysis, Narratives},
abstract = {The digital transformation (DT) is not only forcing companies to rethink their business models but is also challenging governments to address the question of how information technology will change society today and in the future. By setting the legal boundaries and acting as an investor and promoter of the domestic digital economy, governments actively influence in which ways this transformational process takes place. The vision and objectives how DT should be realized on state level is portrayed in well-crafted DT policies. Yet, little is known how governments, as strategic actors, see their role in the DT and how they frame these documents. In this paper, we argue that policymaking about DT is isomorphic in the global context, rather than a differentiator for countries to gain a competitive edge. Using machine learning to analyze a vast text corpus of policy documents, we identify the common repertoire of narratives used by governments from all around the globe to picture their vision of the DT and show that DT policies appear to be almost context-free due to their high similarity.}
}
@article{MANCHO200655,
title = {A tutorial on dynamical systems concepts applied to Lagrangian transport in oceanic flows defined as finite time data sets: Theoretical and computational issues},
journal = {Physics Reports},
volume = {437},
number = {3},
pages = {55-124},
year = {2006},
issn = {0370-1573},
doi = {https://doi.org/10.1016/j.physrep.2006.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S0370157306003401},
author = {Ana M. Mancho and Des Small and Stephen Wiggins},
keywords = {Lagrangian transport, Geophysical fluid flows, Finite time hyperbolicity, Finite time Lyapunov exponents, Stable and unstable manifolds, Transport barriers},
abstract = {In the past 15 years the framework and ideas from dynamical systems theory have been applied to a variety of transport and mixing problems in oceanic flows. The motivation for this approach comes directly from advances in observational capabilities in oceanography (e.g., drifter deployments, remote sensing capabilities, satellite imagery, etc.) which reveal space–time structures that are highly suggestive of the structures one visualizes in the global, geometrical study of dynamical systems theory. In this tutorial, we motivate this approach by showing the relationship between fluid transport in two-dimensional time-periodic incompressible flows and the geometrical structures that exist for two-dimensional area-preserving maps, such as hyperbolic periodic orbits, their stable and unstable manifolds and KAM (Kolmogorov–Arnold–Moser) tori. This serves to set the stage for the attempt to “transfer” this approach to more realistic flows modelling the ocean. However, in order to accomplish this several difficulties must be overcome. The first difficulty that confronts us that any attempt to carry out a dynamical systems approach to transport requires us to obtain the appropriate “dynamical system”, which is the velocity field describing the fluid flow. In general, adequate model velocity fields are obtained by numerical solution of appropriate partial differential equations describing the dynamical evolution of the velocity field. Numerical solution of the partial differential equations can only be done for a finite time interval, and since the ocean is generally not time-periodic, this leads to a new type of dynamical system: a finite-time, aperiodically time-dependent velocity field defined as a data set on a space–time grid. The global, geometrical analysis of transport in such dynamical systems requires both new concepts and new analytical and computational tools, as well as the necessity to discard some of the standard ideas and results from dynamical systems theory. The purpose of this tutorial is to describe these new concepts and analytical tools first using simple dynamical systems where quantities can be computed exactly. We then discuss their computational implications and implementation in the context of a model geophysical flow: a turbulent wind-driven double-gyre in the quasigeostrophic approximation.}
}
@article{DURSO2015336,
title = {The Threat-Strategy Interview},
journal = {Applied Ergonomics},
volume = {47},
pages = {336-344},
year = {2015},
issn = {0003-6870},
doi = {https://doi.org/10.1016/j.apergo.2014.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S0003687014001409},
author = {Francis T. Durso and Sadaf Kazi and Ashley N. Ferguson},
keywords = {Strategies, Knowledge elicitation, Threat and error management},
abstract = {Operators in dynamic work environments use strategies to manage threats in order to achieve task goals. We introduce a structured interview method, the Threat-Strategy Interview (TSI), and an accompanying qualitative analysis to induce operator-level threats, strategies, and the cues that give rise to them. The TSI can be used to elicit knowledge from operators who are on the front line of managing threats to provide an understanding of strategic thinking, which in turn can be applied toward a variety of problems.}
}
@article{HROBARIK20066,
title = {Computational study of bonding trends in the metalloactinyl series EThM and MThM′ (E=N−, O, F+; M, M′=Ir−, Pt, Au+)},
journal = {Chemical Physics Letters},
volume = {431},
number = {1},
pages = {6-12},
year = {2006},
issn = {0009-2614},
doi = {https://doi.org/10.1016/j.cplett.2006.08.144},
url = {https://www.sciencedirect.com/science/article/pii/S0009261406013741},
author = {Peter Hrobárik and Michal Straka and Pekka Pyykkö},
abstract = {The title systems, including EThE′, are treated at DFT level using a B3LYP functional and small-core quasirelativistic pseudopotentials. Most of the studied systems are bent, like their isoelectronic ThO2 analogue, except for some anionic systems containing Ir. The bond lengths vary considerably and can lie above or below the sum of triple-bond covalent radii. Among the studied systems, the iridium-containing species show the strongest back-donation to Th. The bonding can be simply understood and could theoretically go up to a ‘24-electron principle’ limit at the actinide.}
}
@article{GUPTA19971,
title = {Future Challenges for Fuzzy-Neural Computing Systems},
journal = {IFAC Proceedings Volumes},
volume = {30},
number = {25},
pages = {1-6},
year = {1997},
note = {IFAC Symposium on Artificial Intelligence in Real Time Control (AIRTC'97), Kuala Lumpur, Malaysia, 22-25 September 1997},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)41292-4},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017412924},
author = {Madan M. Gupta},
keywords = {Neural Systems, Fuzzy Systems, Fuzzy Logic, Neural Fuzzy Computing},
abstract = {Recently, several significant advances have been made in two distinct theoretical areas. These theoretical advances have created an innovative field of theoretical and applied interest: fuzzy neural systems. Researchers have provided a theoretical basis in the field while industry has used this theoretical basis to create a new class of machines using the innovative technology of fuzzy neural networks. The theory of fuzzy logic provides a mathematical framework for capturing the uncertainties associated with human cognitive processes, such as thinking and reasoning. It also provides a mathematical morphology for emulating certain perceptual and linguistic attributes associated with human cognition. On the other hand, computational neural network paradigms have evolved in the process of understanding the incredible learning and adaptive features of neuronal mechanisms inherent in certain biological species. The integration of these two fields, fuzzy logic and neural networks, has the potential for combining the benefits of these two fascinating fields into a single capsule. The intent of this paper is to describe the basic notions of biological and computational neuronal morphologies, and to describe the principles and architectures of fuzzy neural networks.}
}
@article{BUSBY20161029,
title = {Agent-based computational modelling of social risk responses},
journal = {European Journal of Operational Research},
volume = {251},
number = {3},
pages = {1029-1042},
year = {2016},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2015.12.034},
url = {https://www.sciencedirect.com/science/article/pii/S037722171501173X},
author = {J.S. Busby and B.S.S. Onggo and Y. Liu},
keywords = {OR in societal problem analysis, Multiagent systems, Risk management},
abstract = {A characteristic aspect of risks in a complex, modern society is the nature and degree of the public response – sometimes significantly at variance with objective assessments of risk. A large part of the risk management task involves anticipating, explaining and reacting to this response. One of the main approaches we have for analysing the emergent public response, the social amplification of risk framework, has been the subject of little modelling. The purpose of this paper is to explore how social risk amplification can be represented and simulated. The importance of heterogeneity among risk perceivers, and the role of their social networks in shaping risk perceptions, makes it natural to take an agent-based approach. We look in particular at how to model some central aspects of many risk events: the way actors come to observe other actors more than external events in forming their risk perceptions; the way in which behaviour both follows risk perception and shapes it; and the way risk communications are fashioned in the light of responses to previous communications. We show how such aspects can be represented by availability cascades, but also how this creates further problems of how to represent the contrasting effects of informational and reputational elements, and the differentiation of private and public risk beliefs. Simulation of the resulting model shows how certain qualitative aspects of risk response time series found empirically – such as endogenously-produced peaks in risk concern – can be explained by this model.}
}
@article{SAVIN202110,
title = {Main topics in EIST during its first decade: A computational-linguistic analysis},
journal = {Environmental Innovation and Societal Transitions},
volume = {41},
pages = {10-17},
year = {2021},
note = {Celebrating a decade of EIST: What’s next for transition studies?},
issn = {2210-4224},
doi = {https://doi.org/10.1016/j.eist.2021.06.006},
url = {https://www.sciencedirect.com/science/article/pii/S221042242100037X},
author = {Ivan Savin and Jeroen {van den Bergh}},
keywords = {Machine learning, Topic modelling, Literature review},
abstract = {We analyse 465 articles published in EIST from June 2011 until June 2021 to identify topics addressed in the journal. We find eight main topics and assess how their shares changed over time as well as how many citations they received. The topics with the largest shares in all publications are “Theory of socio-technical transitions” and “Urban regimes and niches”. The two most cited topics, “Theory of socio-technical transitions” and “Geography and diffusion of eco-innovations”, showed a rising share over time, while the share of topic “Finance, investment and growth” declined. We further assess the geographical coverage of topics, through affiliations of the corresponding authors. The resulting map indicates dominant topics for the 34 countries that contributed to publications in EIST.}
}
@article{SUEHR2025109636,
title = {Multi-sphere Rigid-Body Particles in a Parallelized LEBC with LIGGGHTS},
journal = {Computer Physics Communications},
pages = {109636},
year = {2025},
issn = {0010-4655},
doi = {https://doi.org/10.1016/j.cpc.2025.109636},
url = {https://www.sciencedirect.com/science/article/pii/S0010465525001389},
author = {Elizabeth Suehr and Manuel Gale and Ramon Lopez and Raymond L. Fontenot and Peter Liever and Jennifer S Curtis},
keywords = {DEM (Discrete Element Method), LEBC (Lees-Edwards Boundary Condition), LIGGGHTS (LAMMPS improved for general granular and granular heat transfer simulations), Parallelization, Multi-sphere},
abstract = {A method for the Message Passing Interface (MPI) parallelization of the Lees-Edwards boundary condition (LEBC) within the LIGGGHTS framework for multi-sphere rigid particles was created, allowing for the simulation of very detailed complex shapes. Double-send and double-receive communication was added to LIGGGHTS to allow for shared information across disjointed processor domains along the shearing boundary of the LEBC. The verification of this method is performed via 3D shearing simulations of single spheres and sphere clumps and rods with aspect ratios 2, 4, and 6. The predicted shear stress employing the new parallelized LEBC method matches stress values from granular kinetic theory and previously published simulation results. No LEBC simulations for DEM or multi-sphere rigid particles are known to be parallelized, allowing for computationally difficult LEBC multi-sphere simulations to be performed for the first time.}
}
@article{JING2020644,
title = {A Learner Model Integrating Cognitive and Metacognitive And Its Application on Scratch Programming Projects},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {5},
pages = {644-649},
year = {2020},
note = {3rd IFAC Workshop on Cyber-Physical & Human Systems CPHS 2020},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2021.04.154},
url = {https://www.sciencedirect.com/science/article/pii/S2405896321002913},
author = {Sifeng Jing and Ying Tang and Xiwei Liu and Xiaoyan Gong},
keywords = {learner model, cognitive state, metacognitive ability, individualized teaching},
abstract = {learner’s cognitive and metacognitive are key personal profile for individualized teaching. To evaluate learner’s comprehensive characteristics, existing learner model were reviewed. Two challenges of constructing an accurate and comprehensive learner model integrating cognitive and metacognitive were summarized. A plan of constructing a comprehensive learner model was made based on analysis of existing massive online learning environment, sensor information technology and educational data-mining. As a case study, a method of how to map learning data onto learners’ cognitive and metacognitive was proposed based on an analysis of a number of pupils’ Scratch projects. Three mapping table were established. Pupil’s cognitive skill could be evaluated from technology shown from Scratch project, namely, data structure, algorithm, computational practices and overall evaluation. Content shown from Scratch project were used to infer pupil’s cognitive style. Meta-cognitive ability can be measured from computational practices and behavior in programming process.}
}
@article{DAVID2019646,
title = {Development of Escape Room Game using VR Technology},
journal = {Procedia Computer Science},
volume = {157},
pages = {646-652},
year = {2019},
note = {The 4th International Conference on Computer Science and Computational Intelligence (ICCSCI 2019) : Enabling Collaboration to Escalate Impact of Research Results for Society},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.08.223},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919311421},
author = {David David and  Edwin and Edward Arman and  Hikari and Natalia Chandra and Nadia Nadia},
keywords = {Virtual Reality, Presence, Prototype, Unity, Samsung Gear VR},
abstract = {Escape room is one of the media games that can improve the logic of thinking. Puzzles in the escape room traditionally have disadvantages because the type of puzzle that is made requires a lot of material. The purpose of this research is to produce a game with Escape Room as the basic theme with Virtual Reality technology. Virtual Reality technology is used to develop presence in users, attendance is about the intimacy of users with the gaming world. By using Virtual Reality, the puzzle elements that are created can be replaced regularly without the need to change the building’s skeleton. The development method used is a prototype model using Unity game machines. The research method was carried out using a questionnaire for user analysis. The application generated from this research is the Escape Room VR game that can be played on an Android smartphone that is compatible with Samsung Gear VR. The application can be used as an additional means for traditional Escape Room games.}
}
@article{STANCIU2015312,
title = {Embodied Creativity: A Critical Analysis of an Underdeveloped Subject},
journal = {Procedia - Social and Behavioral Sciences},
volume = {187},
pages = {312-317},
year = {2015},
note = {INTERNATIONAL CONFERENCE PSIWORLD 2014 - 5th edition},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2015.03.058},
url = {https://www.sciencedirect.com/science/article/pii/S1877042815018510},
author = {Marius M. Stanciu},
keywords = {Embodied, Creativity, Cognition, Research, Review},
abstract = {While the idea that cognition is embodied appeared in the literature more than four decades ago, studies concerned with how and to what degree might the body and the environment influence creative thinking represent a relatively recent scientific endeavor. In this paper we wish to provide a critical examination of the core ideas of this new field, suggesting new experimental paradigms for testing the more radical and often ignored assertions of the embodied cognition program. We conclude that given the extremely small number of papers that are produced on this subject, as well as its obscurity within the scientific community, future research will have to expand its theoretical considerations greatly if the field is to survive and flourish.}
}
@article{MARINI201828,
title = {Life cycle perspective in RC building integrated renovation},
journal = {Procedia Structural Integrity},
volume = {11},
pages = {28-35},
year = {2018},
note = {XIV INTERNATIONAL CONFERENCE ON BUILDING PATHOLOGY AND CONSTRUCTIONS REPAIR, FLORENCE, ITALY, JUNE 20-22, 2018},
issn = {2452-3216},
doi = {https://doi.org/10.1016/j.prostr.2018.11.005},
url = {https://www.sciencedirect.com/science/article/pii/S2452321618301069},
author = {A. Marini and C. Passoni and A. Belleri},
keywords = {Life Cycle thinking, Deep renovation, Integrated retrofit, Resilience, Sustainability},
abstract = {Enormous resources are invested in Europe for the transition into a sustainable, low carbon, and resilient society. In the construction sector, these concepts are slowly being applied to the renovation of the existing building stock by enforcing their deep and holistic renovation targeting sustainability, safety and resilience. Effectiveness of such an approach to the renovation with respect to traditional retrofit actions emerges when broadening the time frame of the analyses, shifting from the construction time to a life cycle perspective. In this case, the potential of the holistic approach becomes clear in reducing costs, impacts on the inhabitants and impacts on the environment over the building life cycle. Within such a new perspective, new technology options are needed to innovatively combine structural retrofit, architectural restyling and energy efficiency measures. Furthermore, a new design approach conjugating the principles of sustainability, safety and resilience over the building life cycle is required. In such a transition, synergistic and cooperative work of researchers, design professionals, and all the stakeholders in the construction sector is required. In this paper, the basic features of an expanded Life Cycle Thinking (eLCT) approach will be presented, which not only entails the use of recyclable/reusable materials, but also encourages interventions carried out from the outside the buildings to reduce building downtime and avoid inhabitant relocation. In addition, such an expanded LCT fosters the adoption of reparable, easy maintainable, adaptable and fully demountable solutions, such as those featuring dry, demountable and pre-fabricated components. Finally, it addresses the need to account for the End of Life scenario from the initial design stages to guarantee selective dismantling and reuse or recycle to reduce construction waste. Finally, a discussion on the main barriers and challenges in the transition towards this new approach to the renovation of existing building stock is briefly presented.}
}
@article{ANURADHA2022100429,
title = {A RNN based offloading scheme to reduce latency and preserve energy using RNNBOS},
journal = {Measurement: Sensors},
volume = {24},
pages = {100429},
year = {2022},
issn = {2665-9174},
doi = {https://doi.org/10.1016/j.measen.2022.100429},
url = {https://www.sciencedirect.com/science/article/pii/S2665917422000630},
author = {C. Anuradha and M. Ponnavaikko},
keywords = {Computational offloading, Mobile edge computing, Deep neural network, Energy consumption and mobile cloud computing},
abstract = {Mobile cloud computing is currently evolving quickly in today's trend and it provides infinite number of applications to the people those who are using regularly.MCC means the mobile gadgets are strongly tied up with cloud technology to execute various application for attaining many tasks. Mobile devices contain different application according to its own capacity to hold each application. In which many applications are in need of connecting with cloud storage. A new proposed technique named RNNBOS (Recurrent Neural Network Based Offloading scheme) is used to compute calculations in terms of energy source of mobile device along with active conditions of network, Load computations, delay possibility of request from device and quantitative amount of data being transferred for this purpose. We have simulated the above technique using python tool and observed RNN based offloading scheme is good in execution of application using MCC.}
}
@article{KULIK20242338,
title = {Reaction: The challenge of open-shell transition metal catalysis in “systems chemistry”},
journal = {Chem},
volume = {10},
number = {8},
pages = {2338-2339},
year = {2024},
issn = {2451-9294},
doi = {https://doi.org/10.1016/j.chempr.2024.06.026},
url = {https://www.sciencedirect.com/science/article/pii/S245192942400305X},
author = {Heather J. Kulik},
abstract = {Professor Heather J. Kulik is a professor in chemical engineering and chemistry at MIT. She received her BE in chemical engineering from the Cooper Union in 2004 and her PhD from the Department of Materials Science and Engineering at MIT in 2009. She completed postdocs at Lawrence Livermore and Stanford prior to joining MIT as a faculty member in 2013. Her research in computational inorganic chemistry has been recognized by an ONR YIP, a DARPA Director’s fellowship, an NSF CAREER Award, a Sloan Fellowship, an AIChE CoMSEF Impact Award, and a Hans Fischer Senior Fellowship from TU Munich, among others.}
}
@article{LIU202257,
title = {Hierarchical neighborhood entropy based multi-granularity attribute reduction with application to gene prioritization},
journal = {International Journal of Approximate Reasoning},
volume = {148},
pages = {57-67},
year = {2022},
issn = {0888-613X},
doi = {https://doi.org/10.1016/j.ijar.2022.05.011},
url = {https://www.sciencedirect.com/science/article/pii/S0888613X22000809},
author = {Keyu Liu and Tianrui Li and Xibei Yang and Hengrong Ju and Xin Yang and Dun Liu},
keywords = {Gene selection, Granular computing, Multi-granularity attribute reduction, Neighborhood rough set, Trilevel thinking},
abstract = {As a prominent model of granular computing, neighborhood rough set provides clear granularity organization and expression in terms of inherent parameter (neighborhood radius). Such characteristic is widely captured in a plenitude of attribute reduction procedures, while igniting a tricky issue of tuning parameters. In this study, we therefore propose a parameter-free multi-granularity attribute reduction scheme. Fundamentally, our scheme applies three-way decision as thinking in threes. First, data-aware multi-granularity structure is automatically induced from self-contained distance space instead of manually edited or appointed granularities. Second, a novel multi-granularity feature evaluation criterion named hierarchical neighborhood entropy is defined to measure the feature significance. Finally, a sequential forward searching algorithm is designed to find the optimal reduct. With application to gene prioritization, our method performed on microarray data is experimentally demonstrated to be more effective and efficient in differentially expressed genes discovery as compared with other well-established attribute reduction algorithms.}
}
@article{SHUKLA2024117388,
title = {Association of road traffic noise exposure and school childrens’ cognition: A structural equation model approach},
journal = {Environmental Research},
volume = {240},
pages = {117388},
year = {2024},
issn = {0013-9351},
doi = {https://doi.org/10.1016/j.envres.2023.117388},
url = {https://www.sciencedirect.com/science/article/pii/S0013935123021928},
author = {Avnish Shukla and Bhaven N. Tandel},
keywords = {School children, Cognition, Traffic noise index (TNI), Exploratory factor analysis (EFA), Structural equation modeling (SEM)},
abstract = {This study explores the complex relationship between traffic noise and school children's cognition, acknowledging existing empirical inconsistencies and aiming to contribute to a richer understanding of this pivotal issue. Schools adjacent to noisy roads were selected, and outdoor noise levels were measured employing a Kimo dB300 sound level meter, focusing on noise level indices LAeq, L10, and L90. Subsequent calculations were performed to determine the noise pollution level (Lnp), noise climate (NC), and traffic noise index (TNI), revealing a severe noise exposure when compared to standard guidelines. A perception questionnaire for various noise and acoustic factors influencing cognition was developed, and 1524 student responses were collected. Data analysis incorporated Principal Component Analysis (PCA) and Exploratory Factor Analysis (EFA) for dimension reduction, revealing three latent factors labelled 'annoyance,' 'behaviour,' and 'cognition'. Further, Structural Equation Modeling (SEM) was utilized to explore multivariate relationships between variables and latent factors. Resultant path coefficients were obtained as 0.12, 0.98, and 0.10 for the impact of 'behaviour' and 'annoyance' on 'cognition' and the correlation between 'annoyance' and 'behaviour', respectively. Findings underscore a potent positive impact of annoyance, stemming from acute ambient noise exposure, on the deterioration of children's cognition. While suggesting that ambient noise may be correlated with adverse health impacts due to its influence on cognition, this study emphasizes the pressing necessity for noise mitigation in roadside schools and stringent enforcement of noise pollution guidelines in academic zones.}
}
@article{NDUNGO2020,
title = {mSphere of Influence: Learning from Nature—Antibody Profiles Important for Protection of Young Infants},
journal = {mSphere},
volume = {5},
number = {5},
year = {2020},
issn = {2379-5042},
doi = {https://doi.org/10.1128/msphere.01021-20},
url = {https://www.sciencedirect.com/science/article/pii/S2379504220001356},
author = {Esther Ndungo},
keywords = {antibody profiles, enteric pathogens, maternal-infant immunity, systems serology},
abstract = {Esther Ndungo works in the field of maternal-infant immunity against enteric pathogens. In this mSphere of Influence article, she reflects on how the paper “Fc glycan-mediated regulation of placental antibody transfer” by Jennewein et al. (M. F. Jennewein, I. Goldfarb, S. Dolatshahi, C. Cosgrove, et al., Cell 178:202–215.e14, 2019, https://doi.org/10.1016/j.cell.2019.05.044) impressed upon her the value of thinking “outside the box” and looking to nature to guide her research.
ABSTRACT
Esther Ndungo works in the field of maternal-infant immunity against enteric pathogens. In this mSphere of Influence article, she reflects on how the paper “Fc glycan-mediated regulation of placental antibody transfer” by Jennewein et al. (M. F. Jennewein, I. Goldfarb, S. Dolatshahi, C. Cosgrove, et al., Cell 178:202–215.e14, 2019, https://doi.org/10.1016/j.cell.2019.05.044) impressed upon her the value of thinking “outside the box” and looking to nature to guide her research.}
}
@article{KRYSSANOV2001329,
title = {Understanding design fundamentals: how synthesis and analysis drive creativity, resulting in emergence},
journal = {Artificial Intelligence in Engineering},
volume = {15},
number = {4},
pages = {329-342},
year = {2001},
note = {Methodology of Emergent Sythesis},
issn = {0954-1810},
doi = {https://doi.org/10.1016/S0954-1810(01)00023-1},
url = {https://www.sciencedirect.com/science/article/pii/S0954181001000231},
author = {V.V Kryssanov and H Tamaki and S Kitamura},
keywords = {Engineering design, Creativity, Semiotics, Emergence},
abstract = {This paper presents results of an ongoing interdisciplinary study to develop a computational theory of creativity for engineering design. Human design activities are surveyed, and popular computer-aided design methodologies are examined. It is argued that semiotics has the potential to merge and unite various design approaches into one fundamental theory that is naturally interpretable and so comprehensible in terms of computer use. Reviewing related work in philosophy, psychology, and cognitive science provides a general and encompassing vision of the creativity phenomenon. Basic notions of algebraic semiotics are given and explained in terms of design. This is to define a model of the design creative process, which is seen as a process of semiosis, where concepts and their attributes represented as signs organized into systems are evolved, blended, and analyzed, resulting in the development of new concepts. The model allows us to formally describe and investigate essential properties of the design process, namely its dynamics and non-determinism inherent in creative thinking. A stable pattern of creative thought — analogical and metaphorical reasoning — is specified to demonstrate the expressive power of the modeling approach; illustrative examples are given. The developed theory is applied to clarify the nature of emergence in design: it is shown that while emergent properties of a product may influence its creative value, emergence can simply be seen as a by-product of the creative process. Concluding remarks summarize the research, point to some unresolved issues, and outline directions for future work.}
}
@article{CIPRIANI2024102277,
title = {Personality traits and climate change denial, concern, and proactivity: A systematic review and meta-analysis},
journal = {Journal of Environmental Psychology},
volume = {95},
pages = {102277},
year = {2024},
issn = {0272-4944},
doi = {https://doi.org/10.1016/j.jenvp.2024.102277},
url = {https://www.sciencedirect.com/science/article/pii/S0272494424000501},
author = {Enrico Cipriani and Sergio Frumento and Angelo Gemignani and Danilo Menicucci},
keywords = {Climate change, Personality, Communication, Big five, Climate change denial, Climate change concern},
abstract = {Climate Change is a global issue which touches the lives of all human beings, each of whom have their own unique outlooks and motivations. Hence, the high degree of complexity which emerges from the involvement of such a large number of people might be better understood through the lenses of their individual differences. We performed a systematic review and meta-analysis following PRISMA guidelines. We searched keywords on Web of Science™ and Scopus®, and included peer-reviewed articles which quantitatively examined correlations between personality and climate attitudes. After screening, 74 papers were included in our review. From these articles, k = 100 samples were extracted and included in meta-analysis models. Our results show that Climate Change Denial is positively correlated with Social Dominance Orientation (r = 0.39) and Right-Wing Authoritarianism (r = 0.42), and negatively with Openness (r = −0.14), Conscientiousness (r = −0.05), Agreeableness (r = −0.11), Consideration of Future Consequences (r = −0.38), and Actively Open-Minded Thinking (r = −0.38). Concern for Climate Change correlates with Openness (r = 0.10), Neuroticism (r = 0.12), Consideration of Future Consequences (r = 0.34), and negatively with Social Dominance Orientation (r = -0.36) and Right-Wing Authoritarianism (r = −0.22). Finally, Proactivity towards Climate Change correlates positively with Openness (r = 0.17), Extraversion (r = 0.09), Agreeableness (r = 0.05), Neuroticism (r = 0.10), Consideration of Future Consequences (r = 0.39), and negatively with Social Dominance Orientation (r = -0.25) and Right-Wing Authoritarianism (r = -0.31). Moderation analysis shows geographical variations in the Social Dominance Orientation and Climate Denial relationship. We conclude that some personality traits – such as Openness – transversally affect climate change attitudes. Moreover, meta-analytic data suggest that the personality involvement in Climate Change may be dependent on the socio-political context of different countries. Future research, policies, and communication campaigns should take these peculiarities into account.}
}
@article{MATSUDA2009970,
title = {Multiple cognitive deficits in patients during the mild cognitive impairment stage of Alzheimer's disease: how are cognitive domains other than episodic memory impaired?},
journal = {International Psychogeriatrics},
volume = {21},
number = {5},
pages = {970-976},
year = {2009},
issn = {1041-6102},
doi = {https://doi.org/10.1017/S1041610209990330},
url = {https://www.sciencedirect.com/science/article/pii/S1041610224025894},
author = {Osamu Matsuda and Masahiko Saito},
keywords = {Alzheimer's disease, COGNISTAT, mild cognitive impairment},
abstract = {ABSTRACT
Background: Little is known about how cognitive domains other than episodic memory are affected during the mild cognitive impairment (MCI) stage of Alzheimer's disease (AD). We attempted to clarify this issue in this study. Methods: Fifty-seven Japanese subjects were divided into two groups: one comprising people in the MCI stage of AD (MCI group, n = 28) and the other of normal controls (NC group, n = 29). Cognitive functions were assessed using the Japanese version of the neurobehavioral cognitive status examination (J-COGNISTAT). Results: The MCI group performed significantly worse than the NC group on subtests that assessed orientation, confrontational naming, constructive ability, episodic memory, and abstract thinking. Three-quarters of the MCI group had deficits in memory and other non-mnemonic domains, particularly constructive ability and abstract thinking. However, within-subject comparisons showed that the MCI group performed significantly worse on the memory subtest compared to any other subtest. Conclusions: Besides episodic memory, multiple non-mnemonic cognitive domains, such as constructive ability and abstract thinking, are also impaired during the MCI stage of AD; however, these non-mnemonic deficits are smaller than episodic memory impairment.}
}
@article{TSUTAKAWA2020102972,
title = {Envisioning how the prototypic molecular machine TFIIH functions in transcription initiation and DNA repair},
journal = {DNA Repair},
volume = {96},
pages = {102972},
year = {2020},
issn = {1568-7864},
doi = {https://doi.org/10.1016/j.dnarep.2020.102972},
url = {https://www.sciencedirect.com/science/article/pii/S1568786420302214},
author = {Susan E. Tsutakawa and Chi-Lin Tsai and Chunli Yan and Amer Bralić and Walter J. Chazin and Samir M. Hamdan and Orlando D. Schärer and Ivaylo Ivanov and John A. Tainer},
keywords = {TFIIH, Helicase, Transcription initiation, Transcription-coupled repair, Nucleotide excision repair, XPB, XPD, Translocase, DNA damage, DNA repair},
abstract = {Critical for transcription initiation and bulky lesion DNA repair, TFIIH provides an exemplary system to connect molecular mechanisms to biological outcomes due to its strong genetic links to different specific human diseases. Recent advances in structural and computational biology provide a unique opportunity to re-examine biologically relevant molecular structures and develop possible mechanistic insights for the large dynamic TFIIH complex. TFIIH presents many puzzles involving how its two SF2 helicase family enzymes, XPB and XPD, function in transcription initiation and repair: how do they initiate transcription, detect and verify DNA damage, select the damaged strand for incision, coordinate repair with transcription and cell cycle through Cdk-activating-kinase (CAK) signaling, and result in very different specific human diseases associated with cancer, aging, and development from single missense mutations? By joining analyses of breakthrough cryo-electron microscopy (cryo-EM) structures and advanced computation with data from biochemistry and human genetics, we develop unified concepts and molecular level understanding for TFIIH functions with a focus on structural mechanisms. We provocatively consider that TFIIH may have first evolved from evolutionary pressure for TCR to resolve arrested transcription blocks to DNA replication and later added its key roles in transcription initiation and global DNA repair. We anticipate that this level of mechanistic information will have significant impact on thinking about TFIIH, laying a robust foundation suitable to develop new paradigms for DNA transcription initiation and repair along with insights into disease prevention, susceptibility, diagnosis and interventions.}
}
@incollection{OLIVEIRA200793,
title = {3 - Fundamentals of Quantum Computation and Quantum Information},
editor = {Ivan S. Oliveira and Tito J. Bonagamba and Roberto S. Sarthour and Jair C.C. Freitas and Eduardo R. deAzevedo},
booktitle = {NMR Quantum Information Processing},
publisher = {Elsevier Science B.V.},
address = {Amsterdam},
pages = {93-136},
year = {2007},
isbn = {978-0-444-52782-0},
doi = {https://doi.org/10.1016/B978-044452782-0/50005-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780444527820500051},
author = {Ivan S. Oliveira and Tito J. Bonagamba and Roberto S. Sarthour and Jair C.C. Freitas and Eduardo R. deAzevedo}
}
@article{LIU2018164,
title = {Neural and genetic determinants of creativity},
journal = {NeuroImage},
volume = {174},
pages = {164-176},
year = {2018},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2018.02.067},
url = {https://www.sciencedirect.com/science/article/pii/S1053811918301745},
author = {Zhaowen Liu and Jie Zhang and Xiaohua Xie and Edmund T. Rolls and Jiangzhou Sun and Kai Zhang and Zeyu Jiao and Qunlin Chen and Junying Zhang and Jiang Qiu and Jianfeng Feng},
abstract = {Creative thinking plays a vital role in almost all aspects of human life. However, little is known about the neural and genetic mechanisms underlying creative thinking. Based on a cross-validation based predictive framework, we searched from the whole-brain connectome (34,716 functional connectivities) and whole genome data (309,996 SNPs) in two datasets (all collected by Southwest University, Chongqing) consisting of altogether 236 subjects, for a better understanding of the brain and genetic underpinning of creativity. Using the Torrance Tests of Creative Thinking score, we found that high figural creativity is mainly related to high functional connectivity between the executive control, attention, and memory retrieval networks (strong top-down effects); and to low functional connectivity between the default mode network, the ventral attention network, and the subcortical and primary sensory networks (weak bottom-up processing) in the first dataset (consisting of 138 subjects). High creativity also correlates significantly with mutations of genes coding for both excitatory and inhibitory neurotransmitters. Combining the brain connectome and the genomic data we can predict individuals' creativity scores with an accuracy of 78.4%, which is significantly better than prediction using single modality data (gene or functional connectivity), indicating the importance of combining multi-modality data. Our neuroimaging prediction model built upon the first dataset was cross-validated by a completely new dataset of 98 subjects (r = 0.267, p = 0.0078) with an accuracy of 64.6%. In addition, the creativity–related functional connectivity network we identified in the first dataset was still significantly correlated with the creativity score in the new dataset (p<10−3). In summary, our research demonstrates that strong top-down control versus weak bottom-up processes underlie creativity, which is modulated by competition between the glutamate and GABA neurotransmitter systems. Our work provides the first insights into both the neural and the genetic bases of creativity.}
}
@article{LUO202571,
title = {HybProm: An attention-assisted hybrid CNN-BiLSTM model for the interpretable prediction of DNA promoter},
journal = {Methods},
volume = {235},
pages = {71-80},
year = {2025},
issn = {1046-2023},
doi = {https://doi.org/10.1016/j.ymeth.2025.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S1046202325000349},
author = {Rentao Luo and Jiawei Liu and Lixin Guan and Mengshan Li},
keywords = {Promoter, Deep learning, Attention, Gene sequences, Bioinformatics},
abstract = {Promoter prediction is essential for analyzing gene structures, understanding regulatory networks, transcription mechanisms, and precisely controlling gene expression. Recently, computational and deep learning methods for promoter prediction have gained attention. However, there is still room to improve their accuracy. To address this, we propose the HybProm model, which uses DNA2Vec to transform DNA sequences into low-dimensional vectors, followed by a CNN-BiLSTM-Attention architecture to extract features and predict promoters across species, including E. coli, humans, mice, and plants. Experiments show that HybProm consistently achieves high accuracy (90%-99%) and offers good interpretability by identifying key sequence patterns and positions that drive predictions.}
}
@incollection{LEE2016135,
title = {Chapter 7 - Identifying and Tracking Emotional and Cognitive Mathematical Processes of Middle School Students in an Online Discussion Group},
editor = {Sharon Y. Tettegah and Michael P. McCreery},
booktitle = {Emotions, Technology, and Learning},
publisher = {Academic Press},
address = {San Diego},
pages = {135-153},
year = {2016},
series = {Emotions and Technology},
isbn = {978-0-12-800649-8},
doi = {https://doi.org/10.1016/B978-0-12-800649-8.00002-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012800649800002X},
author = {Amos Lee and Sharon Tettegah},
keywords = {Online discourse, Math discussions, Math learning, Systemic functional linguistics, Identification analysis},
abstract = {Math discussions are important when learning math. Explaining one’s thinking, listening to other’s thoughts, and reflecting are but a few of the benefits derived from discussions held in class. However, with the growth of online courses, how do math discussions change when in an online setting? While much research exists about math discussions in classrooms, there is not much research on math discussions held online. Due to the important role of discussions in learning math, along with the growing trend of online classes, this study begins to take a look at how students make sense of and keep track of each other’s comments in an online discussion. In these online discussions, turn taking is not as intuitive as face-to-face interactions. Making sense of the discussion sequence and theme can also be challenging. In this study, I found that students used terms that represented mathematical operations to better explain their thought processes and also kept track of how their peers used these terms as well. These findings suggest that, for these students, when in an online discussion, the terms used were of importance when trying to make their thinking clear to their classmates. Also, in these groups, the mathematical terms were commonly used and re-used by more than one individual in trying to gain a consensus in their group thinking. These findings are important when thinking about how to best foster math discussion and learning in an online environment and for designing online classes that institutions use to supplement or support students.}
}
@article{DIETRICH200722,
title = {Who’s afraid of a cognitive neuroscience of creativity?},
journal = {Methods},
volume = {42},
number = {1},
pages = {22-27},
year = {2007},
note = {Neurocognitive Mechanisms of Creativity: A Toolkit},
issn = {1046-2023},
doi = {https://doi.org/10.1016/j.ymeth.2006.12.009},
url = {https://www.sciencedirect.com/science/article/pii/S1046202306003100},
author = {Arne Dietrich},
keywords = {Consciousness, Insight, Prefrontal cortex, Right brain, Divergent thinking, Neuroimaging, Attention},
abstract = {This article has two goals. First, the ideas outlined here can be seen as a sustained and disciplined demolition project aimed at sanitizing our bad habits of thinking about creativity. Apart from the enormous amount of fluff out there, the study of creativity is, quite unfortunately, still dominated by a number of rather dated ideas that are either so simplistic that nothing good can possibly come out of them or, given what we know about the brain, factually mistaken. As cognitive neuroscience is making more serious contact with the knowledge base of creativity, we must, from the outset, clear the ground of these pernicious fossil traces from a bygone era. The best neuroimaging techniques help little if we don’t know what to look for. Second, as an antidote to these theoretical duds, the article offers fresh ideas on possible mechanisms of creativity. Given that they are grounded in current understanding of cognitive and neural processes, it is hoped that these ideas represent steps broadly pointing in the right direction. In the end, the fundamental question we must ask ourselves is what, exactly, are the mental processes—or their critical elements—that yield creative thoughts.}
}
@article{NOVOTOTSKYVLASOV1995S114,
title = {PS-12-13 Event-related brain activity analysis by mean wave halfperiod duration computation method},
journal = {Electroencephalography and Clinical Neurophysiology/Electromyography and Motor Control},
volume = {97},
number = {4},
pages = {S114},
year = {1995},
issn = {0924-980X},
doi = {https://doi.org/10.1016/0924-980X(95)92838-D},
url = {https://www.sciencedirect.com/science/article/pii/0924980X9592838D},
author = {V.Y. Novototsky-Vlasov}
}
@article{WEISSMAN2011516,
title = {A computational framework for authoring and searching product design specifications},
journal = {Advanced Engineering Informatics},
volume = {25},
number = {3},
pages = {516-534},
year = {2011},
note = {Special Section: Engineering informatics in port operations and logistics},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2011.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S1474034611000061},
author = {Alexander Weissman and Martin Petrov and Satyandra K. Gupta},
keywords = {Product design specifications, Engineering design, Requirements engineering},
abstract = {The development of product design specifications (PDS) is an important part of the product development process. Incompleteness, ambiguity, or inconsistency in the PDS can lead to problems during the design process and may require unnecessary design iterations. This generally results in increased design time and cost. Currently, in many organizations, PDS are written using word processors. Since documents written by different authors can be inconsistent in style and word choice, it is difficult to automatically search for specific requirements. Moreover, this approach does not allow the possibility of automated design verification and validation against the design requirements and specifications. In this paper, we present a computational framework and a software tool based on this framework for writing, annotating, and searching computer-interpretable PDS. Our approach allows authors to write requirement statements in natural language to be consistent with the existing authoring practice. However, using mathematical expressions, keywords from predefined taxonomies, and other metadata the author of PDS can then annotate different parts of the requirement statements. This approach provides unambiguous meaning to the information contained in PDS, and helps to eliminate mistakes later in the process when designers must interpret requirements. Our approach also enables users to construct a new PDS document from the results of the search for requirements of similar devices and in similar contexts. This capability speeds up the process of creating PDS and helps authors write more detailed documents by utilizing previous, well written PDS documents. Our approach also enables checking for internal inconsistencies in the requirement statements.}
}
@article{PRONK202443,
title = {Qualitative systems mapping in promoting physical activity and cardiorespiratory fitness: Perspectives and recommendations},
journal = {Progress in Cardiovascular Diseases},
volume = {83},
pages = {43-48},
year = {2024},
note = {Cardiorespiratory Fitness and Physical Activity: An Update of Evidence, Global Status and Recommendations},
issn = {0033-0620},
doi = {https://doi.org/10.1016/j.pcad.2024.02.013},
url = {https://www.sciencedirect.com/science/article/pii/S0033062024000355},
author = {Nicolaas P. Pronk and Bruce Y. Lee},
keywords = {Systems mapping, Causal loop diagram, Physical activity, Cardiorespiratory fitness, Complexity},
abstract = {The purpose of this report is to provide a perspective on the use of qualitative systems mapping, provide examples of physical activity (PA) systems maps, discuss the role of PA systems mapping in the context of iterative learning to derive breakthrough interventions, and provide actionable recommendations for future work. Systems mapping methods and applications for PA are emerging in the scientific literature in the study of complex health issues and can be used as a prelude to mathematical/computational modeling where important factors and relationships can be elucidated, data needs can be prioritized and guided, interventions can be tested and (co)designed, and metrics and evaluations can be developed. Examples are discussed that describe systems mapping based on Group Model Building or literature reviews. Systems maps are highly informative, illustrate multiple components to address PA and physical inactivity issues, and make compelling arguments against single intervention action. No studies were identified in the literature scan that considered cardiorespiratory fitness the focal point of a systems maps. Recommendations for future research and education are presented and it is concluded that systems mapping represents a valuable yet underutilized tool for visualizing the complexity of PA promotion.}
}
@article{ANDERSON1998214,
title = {Stereovision: beyond disparity computations},
journal = {Trends in Cognitive Sciences},
volume = {2},
number = {6},
pages = {214-222},
year = {1998},
issn = {1364-6613},
doi = {https://doi.org/10.1016/S1364-6613(98)01180-2},
url = {https://www.sciencedirect.com/science/article/pii/S1364661398011802},
author = {Barton L Anderson},
keywords = {sterovision, disparity, 3-D sterograms, perceptual grouping, occlusion},
abstract = {One of the most powerful sources of information about three-dimensional (3-D) structure is provided by stereovision (or stereopsis). For over a century, theoretical and empirical investigations into this ability have focused on the role of binocular disparity in generating percepts of 3-D structure. Recent work in image segmentation demonstrates that stereovision can cause large changes in perceptual organization that cannot be understood on the basis of binocular disparity alone. It is argued that these phenomena reveal the need for theoretical tools beyond those that have dominated the study of visual perception over the past three decades.}
}
@article{MOLINA2025115592,
title = {Exploring the neurophysiological basis of misinformation: A behavioral and neural complexity analysis},
journal = {Behavioural Brain Research},
volume = {487},
pages = {115592},
year = {2025},
issn = {0166-4328},
doi = {https://doi.org/10.1016/j.bbr.2025.115592},
url = {https://www.sciencedirect.com/science/article/pii/S0166432825001780},
author = {R. Molina and Y. Crespo and J.R. Árbol and A.V. Arias-Orduña and A.J. Ibáñez-Molina and S. Iglesias-Parro},
keywords = {Misinformation, Disinformation, Social media, Counter-disinformation, Electroencephalography, Sample entropy, Neural complexity},
abstract = {The proliferation of misinformation on social media platforms poses significant challenges to public health, political discourse, and social cohesion. This study investigates the efficacy of a World Health Organization (WHO) infodemic intervention in mitigating the spread of misinformation and explores the underlying neural mechanisms involved in information processing. A sample of 77 university students was randomly assigned to an experimental group, which was exposed to the WHO's infodemic intervention, or a control group, which received a campaign on healthy lifestyle habits. Participants viewed a series of manipulated and non-manipulated tweets before and after the intervention, rating their likelihood to share, verify, and perceive the truthfulness of the information. Electroencephalogram (EEG) data were collected throughout the experiment to assess neural complexity using Sample Entropy (SampEn) measures. Results revealed that the experimental group significantly reduced their intention to share information and perceived truthfulness of both manipulated and non-manipulated items post-intervention. The control group showed no significant changes. EEG analysis demonstrated higher SampEn scores in the frontal and temporal regions for the experimental group post-intervention, indicating increased neural complexity and more homogeneous activation patterns. These findings suggest that the WHO intervention effectively enhanced participants' critical evaluation of information, reflected in both behavioral and neurophysiological changes. This study contributes to the growing body of research on misinformation interventions by providing evidence for the effectiveness of passive, less demanding campaigns in fostering critical thinking and information discernment. Moreover, it offers novel insights into the neural correlates of information processing following such interventions, highlighting the potential of combining behavioral and neurophysiological measures in misinformation research. These findings have important implications for developing targeted strategies to combat misinformation, enhance digital literacy, and inform future public health and policy initiatives in the digital era.}
}
@article{FAN2025126317,
title = {Deep dive into clarity: Leveraging signal-to-noise ratio awareness and knowledge distillation for underwater image enhancement},
journal = {Expert Systems with Applications},
volume = {269},
pages = {126317},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.126317},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424031841},
author = {Guodong Fan and Jingchun Zhou and Chengpei Xu and Zheng Cheng},
keywords = {Underwater image enhancement, SNR-Aware Transformer, Knowledge distillation},
abstract = {This paper presents an innovative dual-branch solution designed for underwater image enhancement (UIE), leveraging the synergistic combination of Signal-to-Noise Ratio (SNR) aware transformers and convolutional models. SNR-Net dynamically enhances pixel quality through spatial-varying operations. While transformers excel in capturing long-range dependencies, they face challenges in weak local relation learning. To address this, we introduce a SNR prior to guide transformer learning, incorporating a novel self-attention mechanism that avoids tokens from regions with very low SNR. Conversely, CNNs, optimized for exploiting local patterns, suffer from limited receptive fields and weak diversity representation. To overcome this limitation, we enhance the receptive field and multi-scale perception of CNNs by introducing a MR-ResNet module. Additionally, we incorporate a Selective Kernel Merging Module (SKMM), an attention-based feature merging module. These enhancements empower our approach to learn an enriched set of features that selectively combine contextual information from both branches while preserving high-quality spatial details. Finally, through knowledge distillation and contrastive learning, SNR-KD significantly reduces the number of parameters and computations of SNR-Net with minimal impact on performance. Extensive experiments validate the effectiveness of our methods, namely SNR-Net and SNR-KD, demonstrating their state-of-the-art performance compared to other recent UIE methods. The code of our model is publicly available at: https://github.com/Alexande-rChan/SNR-UIE.}
}
@article{SEEMAN202211461,
title = {Understanding chemistry: from “heuristic (soft) explanations and reasoning by analogy” to “quantum chemistry”††Dedicated to Dudley Herschbach in celebration of his 90th year who, when asked whether he was a theoretician or an experimentalist, responded, “The molecules don't know and don't care.”},
journal = {Chemical Science},
volume = {13},
number = {39},
pages = {11461-11486},
year = {2022},
issn = {2041-6520},
doi = {https://doi.org/10.1039/d2sc02535c},
url = {https://www.sciencedirect.com/science/article/pii/S2041652023014347},
author = {Jeffrey I. Seeman and Dean J. Tantillo},
abstract = {ABSTRACT
“Soft theories,” i.e., “heuristic models based on reasoning by analogy” largely drove chemistry understanding for 150 years or more. But soft theories have their limitations and with the expansion of chemistry in the mid-20th century, more and more inexplicable (by soft theory) experimental results were being obtained. In the past 50 years, quantum chemistry, most often in the guise of applied theoretical chemistry including computational chemistry, has provided (a) the underlying “hard evidence” for many soft theories and (b) the explanations for chemical phenomena that were unavailable by soft theories. In this publication, we define “hard theories” as “theories derived from quantum chemistry.” Both soft and hard theories can be qualitative and quantitative, and the “Houk quadrant” is proposed as a helpful categorization tool. Furthermore, the language of soft theories is often used appropriately to describe quantum chemical results. A valid and useful way of doing science is the appropriate use and application of both soft and hard theories along with the best nomenclature available for successful communication of results and ideas.}
}
@article{CHANG20114075,
title = {Dynamic multi-criteria evaluation of co-evolution strategies for solving stock trading problems},
journal = {Applied Mathematics and Computation},
volume = {218},
number = {8},
pages = {4075-4089},
year = {2011},
issn = {0096-3003},
doi = {https://doi.org/10.1016/j.amc.2011.09.032},
url = {https://www.sciencedirect.com/science/article/pii/S0096300311012033},
author = {Ying-Hua Chang and Tz-Ting Wu},
keywords = {Co-evolutionary model, Evolution strategies, Artificial neural network, Dynamic stock trading decision making, Optimization},
abstract = {Risk and return are interdependent in a stock portfolio. To achieve the anticipated return, comparative risk should be considered simultaneously. However, complex investment environments and dynamic change in decision making criteria complicate forecasts of risk and return for various investment objects. Additionally, investors often fail to maximize their profits because of improper capital allocation. Although stock investment involves multi-criteria decision making (MCDM), traditional MCDM theory has two shortfalls: first, it is inappropriate for decisions that evolve with a changing environment; second, weight assignments for various criteria are often oversimplified and inconsistent with actual human thinking processes. In 1965, Rechenberg proposed evolution strategies for solving optimization problems involving real number parameters and addressed several flaws in traditional algorithms, such as their use of point search only and their high probability of falling into optimal solution area. In 1992, Hillis introduced the co-evolutionary concept that the evolution of living creatures is interactive with their environments (multi-criteria) and constantly improves the survivability of their genes, which then expedites evolutionary computation. Therefore, this research aimed to solve multi-criteria decision making problems of stock trading investment by integrating evolutionary strategies into the co-evolutionary criteria evaluation model. Since co-evolution strategies are self-calibrating, criteria evaluation can be based on changes in time and environment. Such changes not only correspond with human decision making patterns (i.e., evaluation of dynamic changes in criteria), but also address the weaknesses of multi-criteria decision making (i.e., simplified assignment of weights for various criteria). Co-evolutionary evolution strategies can identify the optimal capital portfolio and can help investors maximize their returns by optimizing the preoperational allocation of limited capital. This experimental study compared general evolution strategies with artificial neural forecast model, and found that co-evolutionary evolution strategies outperform general evolution strategies and substantially outperform artificial neural forecast models. The co-evolutionary criteria evaluation model avoids the problem of oversimplified adaptive functions adopted by general algorithms and the problem of favoring weights but failing to adaptively adjust to environmental change, which is a major limitation of traditional multi-criteria decision making. Doing so allows adaptation of various criteria in response to changes in various capital allocation chromosomes. Capital allocation chromosomes in the proposed model also adapt to various criteria and evolve in ways that resemble thinking patterns.}
}
@article{CHEVRETTE20212024,
title = {The confluence of big data and evolutionary genome mining for the discovery of natural products},
journal = {Natural Product Reports},
volume = {38},
number = {11},
pages = {2024-2040},
year = {2021},
issn = {0265-0568},
doi = {https://doi.org/10.1039/d1np00013f},
url = {https://www.sciencedirect.com/science/article/pii/S0265056822008789},
author = {Marc G. Chevrette and Athina Gavrilidou and Shrikant Mantri and Nelly Selem-Mojica and Nadine Ziemert and Francisco Barona-Gómez},
abstract = {ABSTRACT
This review covers literature between 2003–2021 The development and application of genome mining tools has given rise to ever-growing genetic and chemical databases and propelled natural products research into the modern age of Big Data. Likewise, an explosion of evolutionary studies has unveiled genetic patterns of natural products biosynthesis and function that support Darwin's theory of natural selection and other theories of adaptation and diversification. In this review, we aim to highlight how Big Data and evolutionary thinking converge in the study of natural products, and how this has led to an emerging sub-discipline of evolutionary genome mining of natural products. First, we outline general principles to best utilize Big Data in natural products research, addressing key considerations needed to provide evolutionary context. We then highlight successful examples where Big Data and evolutionary analyses have been combined to provide bioinformatic resources and tools for the discovery of novel natural products and their biosynthetic enzymes. Rather than an exhaustive list of evolution-driven discoveries, we highlight examples where Big Data and evolutionary thinking have been embraced for the evolutionary genome mining of natural products. After reviewing the nascent history of this sub-discipline, we discuss the challenges and opportunities of genomic and metabolomic tools with evolutionary foundations and/or implications and provide a future outlook for this emerging and exciting field of natural product research.}
}
@article{STONE2022419,
title = {On second thoughts: changes of mind in decision-making},
journal = {Trends in Cognitive Sciences},
volume = {26},
number = {5},
pages = {419-431},
year = {2022},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2022.02.004},
url = {https://www.sciencedirect.com/science/article/pii/S1364661322000407},
author = {Caleb Stone and Jason B. Mattingley and Dragan Rangelov},
keywords = {decision-making, change of mind, sequential sampling, metacognition},
abstract = {The ability to change initial decisions in the face of new or potentially conflicting information is fundamental to adaptive behavior. From perceptual tasks to multiple-choice tests, research has shown that changes of mind often improve task performance by correcting initial errors. Decision makers must, however, strike a balance between improvements that might arise from changes of mind and potential energetic, temporal, and psychological costs. In this review, we provide an overview of the change-of-mind literature, focusing on key behavioral findings, computational mechanisms, and neural correlates. We propose a conceptual framework that comprises two core decision dimensions – time and evidence source – which link changes of mind across decision contexts, as a first step toward an integrated psychological account of changes of mind.}
}
@article{XIA2025108857,
title = {LSDNet: Lightweight strip-steel surface defect detection networks for edge device environment},
journal = {Optics and Lasers in Engineering},
volume = {186},
pages = {108857},
year = {2025},
issn = {0143-8166},
doi = {https://doi.org/10.1016/j.optlaseng.2025.108857},
url = {https://www.sciencedirect.com/science/article/pii/S0143816625000442},
author = {Xuhui Xia and Jiale Guo and Zelin Zhang and Lei Wang and Yuyao Guo},
keywords = {Cold-rolled strip steel, Defect classification, Lightweight network, Feature extraction},
abstract = {Online recognizing defects of the strip-steel surface on resource-constrained embedded devices is a difficult problem. The traditional deep learning model with deep network layers and large parameter counts cannot balance the efficiency and the accuracy. This paper proposes a specialized lightweight deep learning detection model (LSDNet) for strip-steel surface defects. Moreover, LSDNet effectively classifies and recognizes these defects with fewer model parameters. LSDNet adopts Mobilenetv2 as the basic framework and constructs a new feature extraction module. The SPD-Conv module enhances the feature learning capacity for small targets and reduces model redundancy, while the ECANet module improves feature extraction capabilities. Additionally, the parameter-free attention mechanism (SimAM) is incorporated after the initial and final convolutional layers to boost recognition accuracy. Computational efficiency is achieved by substituting fully connected layers with a spatially invariant global average pooling layer, thereby preserving essential depth information. Dropout layers are deployed to enhance generalization, and dynamic learning rate adjustments optimize the training process. Experimental results demonstrate that the proposed LSDNet achieves a classification accuracy of 98.60 %, an F1−score of 98.57 %, with only 0.76 million parameters and 0.095 billion FLOPs for strip-steel surface defects. Compared to Mobilenetv2, LSDNet reduces the parameter count by 2.749 million and improves the classification accuracy by 1.69 %. This method performs better than other classification models in balancing recognition efficiency and accuracy.}
}
@article{PACINI200969,
title = {Synergy: A Framework for Leadership Development and Transformation},
journal = {Perioperative Nursing Clinics},
volume = {4},
number = {1},
pages = {69-74},
year = {2009},
note = {Leadership},
issn = {1556-7931},
doi = {https://doi.org/10.1016/j.cpen.2008.10.010},
url = {https://www.sciencedirect.com/science/article/pii/S1556793108001022},
author = {Christine M. Pacini},
keywords = {Synergy, Leadership development, Orientation, Professional development, Staff development, Clinical education},
abstract = {Given the current demands of the health care environment, the need for nurses minimally competent in clinical judgment, caring practice, advocacy and moral agency, collaboration, responsiveness to diversity, systems thinking, inquiry, and facilitation of learning is critical in light of ever-increasing contextual complexity and variability of patient needs. The Synergy Model provides an exemplary and relevant framework for clinical practice with the ultimate aim of improving patient outcomes. Tenets of accountability and professionalism are central to the model and, in its entirety, it provides a practical and useful approach for thinking about and redesigning educational products and processes in clinical settings.}
}
@article{ZHAN2024121679,
title = {Conceptualizing future groundwater models through a ternary framework of multisource data, human expertise, and machine intelligence},
journal = {Water Research},
volume = {257},
pages = {121679},
year = {2024},
issn = {0043-1354},
doi = {https://doi.org/10.1016/j.watres.2024.121679},
url = {https://www.sciencedirect.com/science/article/pii/S0043135424005803},
author = {Chuanjun Zhan and Zhenxue Dai and Shangxian Yin and Kenneth C. Carroll and Mohamad Reza Soltanian},
keywords = {Groundwater model, Deep learning, Machine intelligence, Multisource data, Human expertise},
abstract = {Groundwater models are essential for understanding aquifer systems behavior and effective water resources spatio-temporal distributions, yet they are often hindered by challenges related to model assumptions, parametrization, uncertainty, and computational efficiency. Machine intelligence, especially deep learning, promises a paradigm shift in overcoming these challenges. A critical examination of existing machine-driven methods reveals the inherent limitations, particularly in terms of the interpretability and the ability to generalize findings. To overcome these challenges, we develop a ternary framework that synergizes the valuable insights from multisource data, human expertise, and machine intelligence. This framework capitalizes on the distinct strengths of each element: the value and relevance of multisource data, the innovative capacity of human expertise, and the analytical efficiency of machine intelligence. Our goal is to conceptualize sustainable water management practices and enhance our understanding and predictive capabilities of groundwater systems. Unlike approaches that rely solely on abundant data, our framework emphasizes the quality and strategic use of available data, combined with human intellect and advanced computing, to overcome current limitations and pave the way for more realistic groundwater simulations.}
}
@incollection{MOHAN2025541,
title = {Chapter 51 - Exploring the exciting potential and challenges of brain computer interfaces},
editor = {M.A. Ansari and R.S. Anand and Pragati Tripathi and Rajat Mehrotra and Md Belal Bin Heyat},
booktitle = {Artificial Intelligence in Biomedical and Modern Healthcare Informatics},
publisher = {Academic Press},
pages = {541-550},
year = {2025},
isbn = {978-0-443-21870-5},
doi = {https://doi.org/10.1016/B978-0-443-21870-5.00051-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780443218705000510},
author = {Anand Mohan and R.S. Anand},
keywords = {Brain–computer interface (BCI), EEG, Machine learning, Motor imagery, PSD},
abstract = {Electroencephalogram (EEG) signals contain various information about the cognitive thinking, emotion, and thoughts of a person. Verbal communication is the normal form of interaction method used, but various kinds of physically disabled people who are not in the condition to express themselves can be assisted using the EEG signal rehabilitation technique. EEG signals can be used effectively in rehabilitation by using brain–computer interfaces (BCIs). BCI is a technology that allows interaction between the brain and a computer. This kind of technique can be used to treat patients with paralyzed muscles and locked in syndromes by helping them interact with others using their EEG signals. The application of BCI can be in medical field, education, and security. In this chapter, all aspects of BCIs are discussed in great detail and also have worked on motor imaginary-based dataset and have used linear discriminant analysis (LDA) algorithm as the classifier, which showed 91% accuracy.}
}